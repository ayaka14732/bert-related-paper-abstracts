<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="zh-CN" xml:lang="zh-CN">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>BERT 相关论文摘要</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <style>
    body { max-width: inherit; }
    pre { font-family: inherit; font-size: 80%; white-space: pre-wrap; }
    #TOC li { list-style: initial; }

    #TOC + * { counter-reset: article; }
    ul li { counter-increment: article; }
    ul li::before { content: counter(article) ". "; }
    #TOC ul li::before { content: none; }
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">BERT 相关论文摘要</h1>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#survey-paper">Survey paper</a></li>
<li><a href="#downstream-task">Downstream task</a>
<ul>
<li><a href="#qa-mc-dialogue">QA, MC, Dialogue</a></li>
<li><a href="#slot-filling-and-intent-detection">Slot filling and Intent Detection</a></li>
<li><a href="#analysis">Analysis</a></li>
<li><a href="#word-segmentation-parsing-ner">Word segmentation, parsing, NER</a></li>
<li><a href="#pronouncoreference-resolution">Pronoun/coreference resolution</a></li>
<li><a href="#word-sense-disambiguation">Word sense disambiguation</a></li>
<li><a href="#sentiment-analysis">Sentiment analysis</a></li>
<li><a href="#relation-extraction">Relation extraction</a></li>
<li><a href="#knowledge-base">Knowledge base</a></li>
<li><a href="#text-classification">Text classification</a></li>
<li><a href="#wsc-wnli-nli">WSC, WNLI, NLI</a></li>
<li><a href="#commonsense">Commonsense</a></li>
<li><a href="#extractive-summarization">Extractive summarization</a></li>
<li><a href="#grammatical-error-correction">Grammatical error correction</a></li>
<li><a href="#ir">IR</a></li>
</ul></li>
<li><a href="#generation">Generation</a></li>
<li><a href="#quality-evaluator">Quality evaluator</a></li>
<li><a href="#modification-multi-task-masking-strategy-etc.">Modification (multi-task, masking strategy, etc.)</a>
<ul>
<li><a href="#tokenization">Tokenization</a></li>
<li><a href="#prompt">Prompt</a></li>
</ul></li>
<li><a href="#sentence-embedding">Sentence embedding</a></li>
<li><a href="#transformer-variants">Transformer variants</a></li>
<li><a href="#probe">Probe</a></li>
<li><a href="#inside-bert">Inside BERT</a></li>
<li><a href="#multi-lingual">Multi-lingual</a></li>
<li><a href="#other-than-english-models">Other than English models</a></li>
<li><a href="#domain-specific">Domain specific</a></li>
<li><a href="#multi-modal">Multi-modal</a></li>
<li><a href="#model-compression">Model compression</a></li>
<li><a href="#misc.">Misc.</a></li>
</ul>
</nav>
<p>Star this project on GitHub: <a href="https://github.com/ayaka14732/bert-related-paper-abstracts">ayaka14732/bert-related-paper-abstracts</a></p>
<p>Upstream: <a href="https://github.com/tomohideshibata/BERT-related-papers">tomohideshibata/BERT-related-papers</a></p>
<p>This is a list of BERT-related papers. Any feedback is welcome.</p>
<h1 id="survey-paper">Survey paper</h1>
<ul>
<li><a href="https://arxiv.org/abs/1910.07370">Evolution of transfer learning in natural language processing</a>
<pre>在本文中，我们提出了一项研究，最近的进展有助于通过使用半监督训练将迁移学习引入NLP。我们讨论了最前沿的方法和体系结构，如BERT、GPT、ELMo、ULMFit等。传统上，自然语言处理中的任务是通过基于规则和统计的方法来完成的。然而，由于自然语言的广泛性，这些方法不能很好地推广，也无法了解语言的细微差别。因此，机器学习算法（如朴素贝叶斯和决策树）与传统模型（如单词袋和N-gram）相结合，被用来解决这个问题。最终，随着高级递归神经网络架构（如LSTM）的出现，我们能够在一些自然语言处理任务（如文本分类和机器翻译）中实现最先进的性能。我们将讨论迁移学习如何为NLP带来著名的ImageNet时刻。一些高级体系结构（如Transformer及其变体）允许从业者利用从无关任务中获得的知识，大大加快收敛速度，并在目标任务中提供更好的性能。本次调查旨在通过深入学习为自然语言处理的最新进展提供一个简洁而全面的理解，特别关注迁移学习的细节及其潜在优势。</pre></li>
<li><a href="https://arxiv.org/abs/2003.08271">Pre-trained Models for Natural Language Processing: A Survey</a>
<pre>最近，预训练模型（PTM）的出现将自然语言处理（NLP）带入了一个新时代。在本次调查中，我们对NLP的PTM进行了全面回顾。本文首先简要介绍了语言表征学习及其研究进展。然后，我们根据四个角度的分类法对现有的PTM进行系统分类。接下来，我们将描述如何使PTM的知识适应下游任务。最后，我们概述了PTMs未来研究的一些潜在方向。本调查旨在为理解、使用和开发各种NLP任务的PTM提供实践指南。</pre></li>
<li><a href="https://arxiv.org/abs/2003.07278">A Survey on Contextual Embeddings</a>
<pre>上下文嵌入，如ELMo和BERT，超越了Word2Vec等全局单词表示，并在广泛的自然语言处理任务中实现了突破性的性能。上下文嵌入根据每个单词的上下文为其指定一个表示，从而捕获不同上下文中单词的用法，并编码跨语言传输的知识。在这项调查中，我们回顾了现有的上下文嵌入模型、跨语言多语言预训练、上下文嵌入在下游任务中的应用、模型压缩和模型分析。</pre></li>
<li><a href="https://arxiv.org/abs/2007.04239">A Survey on Transfer Learning in Natural Language Processing</a>
<pre>深度学习模型通常需要大量的数据。然而，这些大型数据集并非总是可以获得的。这在许多具有挑战性的NLP任务中很常见。例如，考虑神经机器翻译，在这样的大型数据集中，特别是对于低资源语言来说，不太可能。深度学习模型的另一个限制是对巨大计算资源的需求。这些障碍促使研究质疑使用大型培训模型进行知识转移的可能性。随着许多大型模型的出现，对迁移学习的需求也在增加。在这项调查中，我们介绍了自然语言处理领域中最新的迁移学习进展。我们还提供了一个分类法，对文献中不同的迁移学习方法进行分类。</pre></li>
<li><a href="https://arxiv.org/abs/2010.00854">Which *BERT? A Survey Organizing Contextualized Encoders</a> (EMNLP2020)
<pre>预先训练的上下文化文本编码器现在是NLP社区的主要内容。我们提出了一项关于语言表征学习的调查，目的是巩固最近各种努力中所获得的一系列共同经验教训。虽然重大进展仍在快速推进，但我们发现，现在已经在不同方向发现了足够多的进展，我们可以开始根据共同主题组织进展。通过这个组织，我们在解释最近的贡献和选择使用哪种模型时强调了重要的考虑因素。</pre></li>
<li><a href="https://arxiv.org/abs/2104.10640">The NLP Cookbook: Modern Recipes for Transformer based Deep Learning Architectures</a>
<pre>近年来，自然语言处理（NLP）模型在文本分类、机器翻译、认知对话系统、自然语言理解信息检索（NLU）和自然语言生成（NLG）等语言和语义任务中取得了巨大成功。这一壮举主要归功于开创性的变压器架构，导致了诸如BERT、GPT（I、II、III）等设计。尽管这些大型模型取得了前所未有的性能，但它们的计算成本很高。因此，最近的一些NLP体系结构利用了迁移学习、剪枝、量化和知识提取的概念，以实现适度的模型大小，同时保持与前辈几乎相似的性能。此外，为了从知识提取的角度缓解语言模型带来的数据量挑战，已经构建了知识检索器，以便以更高的效率和准确性从大量数据库中提取显式数据文档。最近的研究也集中于通过有效地关注较长的输入序列来进行更好的推理。在本文中，我们总结和检查了当前最先进的（SOTA）NLP模型，这些模型已用于许多NLP任务，以获得最佳性能和效率。我们提供了对不同体系结构的详细理解和功能、NLP设计的分类、比较评估以及NLP的未来方向。</pre></li>
<li><a href="https://arxiv.org/abs/2106.07139">Pre-Trained Models: Past, Present and Future</a>
<pre>大规模预训练模型（PTM）如BERT和GPT最近取得了巨大成功，成为人工智能（AI）领域的一个里程碑。由于复杂的预训练目标和巨大的模型参数，大规模PTM可以有效地从大量标记和未标记的数据中获取知识。通过将知识存储到大参数中并对特定任务进行微调，隐含在大参数中的丰富知识可以使各种下游任务受益，这已通过实验验证和实证分析得到广泛证明。现在AI社区的共识是采用PTM作为下游任务的主干，而不是从头开始学习模型。在本文中，我们深入研究了预训练的历史，特别是它与迁移学习和自我监督学习的特殊关系，以揭示PTM在AI发展谱中的关键地位。此外，我们全面回顾了PTMs的最新突破。这些突破是由计算能力的激增和数据可用性的增加推动的，朝着四个重要方向发展：设计有效的体系结构、利用丰富的上下文、提高计算效率以及进行解释和理论分析。最后，我们讨论了PTMs存在的一系列问题和研究方向，希望我们的观点能够对PTMs的未来研究起到启发和推动作用。</pre></li>
<li><a href="https://arxiv.org/abs/2106.04554">A Survey of Transformers</a>
<pre>Transformers在许多人工智能领域取得了巨大成功，如自然语言处理、计算机视觉和音频处理。因此，自然会吸引学术界和行业研究人员的大量兴趣。到目前为止，已经提出了各种各样的变压器变型（又称X-formers），但是，仍然缺少对这些变压器变型的系统和全面的文献综述。在这项调查中，我们提供了一个全面的审查各种X-形成者。我们首先简要介绍vanilla Transformer，然后提出一种新的X-former分类法。接下来，我们将从三个角度介绍各种X-Former：体系结构修改、预培训和应用。最后，我们概述了未来研究的一些潜在方向。</pre></li>
<li><a href="https://arxiv.org/abs/2108.05542">AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing</a>
<pre>基于转换器的预训练语言模型（T-PTLM）在几乎所有NLP任务中都取得了巨大的成功。这些模型的发展始于GPT和BERT。这些模型建立在变压器、自监督学习和转移学习的基础上。基于转换的PTLM使用自监督学习从大量文本数据中学习通用语言表示，并将此知识转移到下游任务中。这些模型为下游任务提供了良好的背景知识，从而避免了从头开始训练下游模型。在这篇全面的调查报告中，我们首先简要概述了自我监督学习。接下来，我们解释各种核心概念，如预训练、预训练方法、预训练任务、嵌入和下游适应方法。接下来，我们将介绍一种新的T-PTLM分类法，然后简要概述各种基准，包括内在和外在基准。我们总结了使用T-PTLM的各种有用库。最后，我们强调了将进一步改进这些模型的一些未来研究方向。我们坚信，本综合调查报告将作为学习核心概念以及了解T-PTLMs最新情况的良好参考。</pre></li>
<li><a href="https://arxiv.org/abs/2109.12575">Paradigm Shift in Natural Language Processing</a>
<pre>在深度学习时代，大多数NLP任务的建模已经收敛到几种主流范式。例如，我们通常采用序列标记范式来解决词性标记、NER、组块等任务，而采用分类范式来解决情感分析等任务。随着预训练语言模型的快速发展，近年来出现了一种范式转换的趋势，即通过将一个NLP任务转换为另一个NLP任务来解决它。范式转换在许多任务上取得了巨大成功，成为提高模型性能的一种有希望的方法。此外，其中一些范例在统一大量NLP任务方面显示出巨大的潜力，使得构建单一模型来处理不同的任务成为可能。在本文中，我们回顾了近年来这种范式转换的现象，重点介绍了几种有可能解决不同NLP任务的范式。</pre></li>
<li><a href="https://arxiv.org/abs/2111.01243">Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey</a>
<pre>大型的、预先训练好的基于转换器的语言模型，如BERT，极大地改变了自然语言处理（NLP）领域。我们对最近使用这些大型语言模型通过预训练、微调、提示或文本生成方法来解决NLP任务的工作进行了综述。我们还介绍了使用预先训练的语言模型来生成用于训练增强或其他目的的数据的方法。最后，我们讨论了局限性，并提出了未来研究的方向。</pre></li>
</ul>
<h1 id="downstream-task">Downstream task</h1>
<h2 id="qa-mc-dialogue">QA, MC, Dialogue</h2>
<ul>
<li><a href="https://arxiv.org/abs/2005.06249">Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond</a>
<pre>机器阅读理解（MRC）旨在教会机器阅读和理解人类语言，这是自然语言处理（NLP）的一个长期目标。随着深层神经网络的兴起和语境化语言模型（CLM）的发展，MRC的研究经历了两个重大突破。MRC和CLM作为一种现象，对NLP社区产生了巨大的影响。在这项调查中，我们对MRC进行了全面和比较性的回顾，涵盖了以下总体研究主题：1）MRC和CLM的起源和发展，特别关注CLM的作用；2） MRC和CLM对NLP社区的影响；3） MRC的定义、数据集和评估；4） 从人类认知过程的角度，从两级编译码器解决体系结构的角度来看，通用MRC体系结构和技术方法；5） 先前的亮点、新出现的主题和我们的实证分析，其中我们特别关注在MRC研究的不同时期起作用的内容。我们建议对这些主题进行全视图分类和新的分类。我们得出的主要观点是：1）MRC促进了从语言处理到理解的进程；2） MRC系统的快速改进得益于CLMs的发展；3） MRC的主题正逐渐从浅层文本匹配转向认知推理。</pre></li>
<li><a href="https://arxiv.org/abs/2006.11880">A Survey on Machine Reading Comprehension: Tasks, Evaluation Metrics, and Benchmark Datasets</a>
<pre>机器阅读理解（MRC）是一个具有挑战性的自然语言处理（NLP）研究领域，具有广泛的现实应用。近年来这一领域的巨大进步主要归功于大规模数据集和深度学习的出现。目前，许多MRC模型在各种基准数据集上已经超过了人类的表现，尽管现有的MRC模型与真正的人类阅读理解之间存在着明显的巨大差距。这表明需要改进现有的数据集、评估指标和模型，以将当前的MRC模型推向“真正的”理解。为了解决目前缺乏对现有MRC任务、评估指标和数据集进行全面调查的问题，本文（1）分析了57个MRC任务和数据集，并提出了一种具有4种不同属性的更精确的MRC任务分类方法；（2） 总结了MRC任务的9个评价指标、MRC数据集的7个属性和10个特征；（3） 我们还讨论了MRC研究中的关键开放问题，并强调了未来的研究方向。此外，我们还收集、整理并在同伴网站上发布了我们的数据(https://mrc-datasets.github.io/)MRC研究人员可以直接访问每个MRC数据集、论文、基线项目和排行榜。</pre></li>
<li><a href="https://arxiv.org/abs/1901.08634">A BERT Baseline for the Natural Questions</a>
<pre>本技术说明描述了自然问题的新基线。我们的模型基于BERT，并将原始数据集论文中报告的模型F1分数与人类上限之间的差距分别减少了30%和50%，这两个上限分别与长答案和短答案任务相关。该基线已提交至官方NQ排行榜，网址为ai.google.com/research/NaturalQuestions。代码、预处理数据和预训练模型可在https://github.com/google-research/language/tree/master/language/question_answering/bert_joint.</pre></li>
<li><a href="https://arxiv.org/abs/1905.13453">MultiQA: An Empirical Investigation of Generalization and Transfer in Reading Comprehension</a> (ACL2019)
<pre>最近已经创建了大量的阅读理解（RC）数据集，但很少有人分析它们是否可以相互推广，以及现有数据集在多大程度上可以用于改进新数据集的性能。在本文中，我们对十个RC数据集进行了这样的调查，对一个或多个源RC数据集进行了培训，评估了泛化，并将其转移到目标RC数据集。我们分析了有助于泛化的因素，并表明在源RC数据集上进行训练并转移到目标数据集可以显著提高性能，即使存在来自BERT的强大上下文表示（Devlin et al.，2019）。我们还发现，在多源RC数据集上进行训练可以实现健壮的泛化和传输，并且可以降低新RC数据集的示例收集成本。根据我们的分析，我们提出了MultiQA，一种基于BERT的模型，在多个RC数据集上进行训练，从而在五个RC数据集上获得最先进的性能。为了研究界的利益，我们共享我们的基础设施。</pre></li>
<li><a href="https://arxiv.org/abs/1905.10044">BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions</a> (NAACL2019) [<a href="https://github.com/google-research-datasets/boolean-questions">github</a>]
<pre>在本文中，我们研究了自然出现的是/否问题——也就是说，它们是在无提示和无约束的环境中产生的。我们建立了一个阅读理解的数据集，BoolQ，这些问题，并表明他们出乎意料地具有挑战性。他们经常查询复杂的、非因素信息，并且需要复杂的推论来解决。我们还探讨了一系列迁移学习基线的有效性。我们发现，从蕴涵数据传输比从释义或提取QA数据传输更有效，而且令人惊讶的是，即使从大量预先训练的语言模型（如BERT）开始，它仍然非常有益。我们最好的方法是在MultiNLI上训练BERT，然后在我们的训练集上重新训练它。它实现了80.4%的准确率，而人工注释器的准确率为90%（和62%的多数基线），为未来的工作留下了巨大的差距。</pre></li>
<li><a href="https://arxiv.org/abs/2004.04849">Natural Perturbation for Robust Question Answering</a>
<pre>虽然最近的模型在许多NLP数据集上取得了人类水平的分数，但我们观察到它们对输入的微小变化相当敏感。作为通过构造全新示例的训练集来解决此问题的标准方法的替代方法，我们建议通过示例的最小扰动来实现。具体来说，我们的方法包括首先收集一组种子示例，然后应用人类驱动的自然扰动（与基于规则的机器扰动相反），这通常也会改变黄金标签。局部扰动的优点是，与写出全新的例子相比，创建局部扰动相对更容易（因此也更便宜）。为了评估这种现象的影响，我们考虑最近的问题回答数据集（BOOLQ），并研究我们的方法的好处作为扰动成本比的函数，扰动现有问题的相对成本与从头创建新的成本。我们发现，当自然扰动的创建成本较低时，使用它们训练模型更为有效：此类模型表现出更高的鲁棒性和更好的泛化性，同时保留了原始BoolQ数据集的性能。</pre></li>
<li><a href="https://arxiv.org/abs/1911.06137">Unsupervised Domain Adaptation on Reading Comprehension</a>
<pre>阅读理解（RC）已经在各种数据集中进行了研究，深度神经网络带来了更高的性能。然而，这些模型在不同领域的泛化能力尚不清楚。为了缓解这个问题，我们将研究RC上的无监督域自适应，其中在标记的源域上训练模型，并仅使用未标记的样本应用于目标域。我们首先表明，即使使用强大的BERT上下文表示，当在一个数据集上训练的模型直接应用于另一个目标数据集时，性能仍然不能令人满意。为了解决这个问题，我们提出了一种新的条件对抗式自我训练方法（CASe）。具体而言，我们的方法利用在源数据集上微调的BERT模型以及置信度过滤，在目标域中生成可靠的伪标记样本以进行自我训练。另一方面，它通过跨领域的条件对抗学习进一步减少了领域分布差异。大量实验表明，我们的方法在多个大规模基准数据集上达到了与监督模型相当的精度。</pre></li>
<li><a href="https://arxiv.org/abs/1912.10435">BERTQA – Attention on Steroids</a>
<pre>在这项工作中，我们扩展了Transformers（BERT）的双向编码器表示，重点是定向协同注意，以在SQUAD2.0数据集上获得改进的F1性能。BERT所基于的Transformer体系结构将层次全局注意力放在上下文和查询的连接上。我们在BERT体系结构中添加的内容通过一组改进的Transformer编码器单元，通过更加集中的上下文到查询（C2Q）和查询到上下文（Q2C）的关注，增强了这种关注。此外，我们还探索在共同注意体系结构中添加基于卷积的特征提取，以将局部信息添加到自我注意中。我们发现，共同注意力显著提高了无答案F1，在基本架构中提高了4分，在大型架构中提高了1分。添加跳过连接后，无应答F1进一步改进，而不会导致has应答F1中的额外丢失。局部特征提取的添加增加了关注度，在基础架构中产生了77.03的总体dev F1。我们将我们的发现应用于包含两倍多层的大型BERT模型，并进一步使用了我们自己的通过反向翻译创建的SQUAD 2.0数据集的增强版本，我们将其命名为SQUAD 2.Q。最后，我们进行了超参数调整，并为最终的F1/EM 82.317/79.442（关注类固醇，PCE测试排行榜）整合了我们的最佳模型。</pre></li>
<li><a href="https://arxiv.org/abs/2002.10670">Exploring BERT Parameter Efficiency on the Stanford Question Answering Dataset v2.0</a>
<pre>在本文中，我们探讨了BERT arXiv:1810.04805在斯坦福问答数据集（SQuAD2.0）2.0版上的参数效率。在冻结不同数量的最终变压器层以及包括arXiv:1902.00751中提出的适配器层的同时，我们评估了BERT的参数效率。此外，我们还尝试使用上下文感知卷积（CACNN）过滤器（如arXiv:1709.08294v3中所述）作为SQuAD2.0任务的最终增强层。这一探索的部分动机是arXiv:1907.10597，它为扩大人工智能模型的评估标准以包括各种资源效率度量提供了一个令人信服的理由。虽然我们没有根据arXiv:1907.10597中提出的浮点运算效率来评估这些模型，但我们检查了与训练时间、推理时间和模型参数总数相关的效率。我们的结果在很大程度上证实了适配模块的arXiv:1902.00751的结果，同时也表明，由于训练和推理时间的增加，通过添加上下文感知卷积滤波器而获得的F1分数是不实际的。</pre></li>
<li><a href="https://arxiv.org/abs/2004.06076">Adversarial Augmentation Policy Search for Domain and Cross-Lingual Generalization in Reading Comprehension</a>
<pre>阅读理解模型往往过于适应训练数据集的细微差别，在对抗性评估中失败。使用对抗性增强数据集进行训练可以提高对抗性攻击的鲁棒性，但会影响模型的泛化。在这项工作中，我们提出了几种有效的敌手和自动数据扩充策略搜索方法，目的是使阅读理解模型对敌手评估更具鲁棒性，同时提高对源领域以及新领域和语言的泛化能力。我们首先提出了三种新的产生QA对手的方法，它们在上下文中引入了多个混淆点，显示了对干扰词插入位置的依赖，并揭示了将对抗策略与句法和语义解释方法相结合的复合效应。接下来，我们发现，使用统一采样的对手来扩充训练数据集可以提高对抗性攻击的鲁棒性，但会导致原始未经整理数据集的性能下降。我们通过RL和更有效的贝叶斯策略搜索方法来解决这个问题，以便在大的搜索空间中自动学习每个对手转换概率的最佳增强策略组合。利用这些学习到的策略，我们证明了对抗性训练可以显著提高领域内、领域外和跨语言（德语、俄语、土耳其语）的泛化能力。</pre></li>
<li><a href="https://arxiv.org/abs/2004.10157">Logic-Guided Data Augmentation and Regularization for Consistent Question Answering</a> (ACL2020)
<pre>许多自然语言问题需要在两个实体或事件之间进行定性、定量或逻辑比较。本文通过集成逻辑规则和神经模型来解决提高比较问题回答的准确性和一致性的问题。我们的方法利用逻辑和语言知识来增加标记的训练数据，然后使用基于一致性的正则化器来训练模型。为了提高预测的全局一致性，我们的方法在各种问答（QA）任务（包括多项选择定性推理、因果推理和抽取式机器阅读理解）中比以前的方法有了很大的改进。特别是，我们的方法显著提高了基于RoBERTa的模型在数据集上的性能1-5%。我们将WIQA和QuaRel的最新水平提高了约5-8%，并将HotpotQA的一致性违规降低了58%。我们进一步证明，我们的方法可以从有限的数据中有效地学习。</pre></li>
<li><a href="https://arxiv.org/abs/2005.00700">UnifiedQA: Crossing Format Boundaries With a Single QA System</a>
<pre>问答（QA）任务的提出使用了多种格式，如提取跨度选择、多项选择等。这导致了格式专门化模型，甚至导致QA社区中的隐式划分。我们认为，这种界限是人为的，也许是不必要的，因为我们试图教授的推理能力不受形式的制约。作为证据，我们使用语言建模的最新进展构建了一个预先训练好的QA模型UnifiedQA，它在跨越4种不同格式的17个QA数据集上表现得出奇地好。UNIFIEDQA与9个不同的模型相匹配，这些模型是在单个数据集上进行训练的。即使面对12个观察到的格式的看不见的数据集，UnifiedQA的表现也出人意料地好，从其非格式的训练数据中显示出了强大的泛化能力。最后，只需将这个预先训练好的QA模型微调为专门的模型，就可以在6个数据集上实现最新水平，从而将UnifiedQA建立为构建QA系统的强大起点。</pre></li>
<li><a href="https://arxiv.org/abs/2012.00955">How Can We Know When Language Models Know?</a>
<pre>最近的研究表明，语言模型（LM）捕获了关于事实或常识的不同类型的知识。然而，由于没有一个模型是完美的，它们在许多情况下仍然无法提供适当的答案。在本文中，我们提出了这样一个问题：“我们如何知道语言模型何时有信心知道特定查询的答案？”我们从校准的角度来研究这个问题，概率模型的预测概率的性质实际上与正确性的概率有很好的相关性。我们检查了三个强大的生成模型——T5、BART和GPT-2——并研究它们在QA任务中的概率是否得到了很好的校准，发现答案是一个相对强调的否。然后我们检查了校准这些模型的方法，以使它们的置信度得分通过微调与正确性的可能性更好地关联，事后概率修改，或预测输出或输入的调整。在不同数据集上的实验证明了我们方法的有效性。我们还进行了分析，以研究这些方法的优点和局限性，为校准LMs方法的进一步改进提供了依据。我们已经在https://github.com/jzbjyb/lm-calibration.</pre></li>
<li><a href="https://arxiv.org/abs/1908.05514">A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning</a> (EMNLP2019)
<pre>在阅读理解和问答领域取得了迅速的进展，有几个系统在一些简化的环境中实现了人的平等。然而，当这些模型应用于更现实的场景时，例如答案涉及各种类型，多个文本字符串是正确答案，或者需要离散推理能力，这些模型的性能会显著降低。在本文中，我们介绍了多类型多跨度网络（MTMSN），这是一种神经阅读理解模型，它将多类型答案预测器与多跨度提取方法相结合，用于动态生成一个或多个文本跨度，多类型答案预测器设计用于支持各种答案类型（例如跨度、计数、否定和算术表达式）。此外，还提出了一种算术表达式重新排序机制，对候选表达式进行排序，以进一步确认预测结果。实验表明，我们的模型在DROP hidden测试集上达到了79.9 F1，创造了新的最先进的结果。源代码\footnote{\url{https://github.com/huminghao16/MTMSN}}已发布，以便于将来的工作。</pre></li>
<li><a href="https://arxiv.org/abs/1909.13375">A Simple and Effective Model for Answering Multi-span Questions</a> [<a href="https://github.com/eladsegal/tag-based-multi-span-extraction">github</a>]
<pre>阅读理解模型（RC）通常将其输出空间限制为输入的所有单个连续跨距的集合，以缓解学习问题并避免需要显式生成文本的模型。然而，将答案强制为单个跨度可能具有限制性，最近的一些数据集还包括多跨度问题，即答案为文本中一组非连续跨度的问题。自然，返回单跨的模型无法回答这些问题。在这项工作中，我们提出了一个简单的体系结构，通过将任务转换为序列标记问题来回答多跨度问题，即预测每个输入标记是否应该是输出的一部分。我们的模型将DROP和Quoref中的跨度提取问题的性能分别提高了9.9和5.5 EM点。</pre></li>
<li><a href="https://arxiv.org/abs/2004.04487">Injecting Numerical Reasoning Skills into Language Models</a> (ACL2020)
<pre>众所周知，大型预训练语言模型（LMs）编码了大量的语言信息。然而，高级推理技能，如数值推理，很难仅从语言建模目标学习。因此，现有的数值推理模型使用了灵活性有限的专用体系结构。在这项工作中，我们证明了数值推理适合于自动数据生成，因此可以通过生成大量数据并在多任务设置中进行训练，将此技能注入预先训练的LMs。我们表明，在此数据上对我们的模型GenBERT进行预训练，可以显著提高下降时的性能（49.3$\rightarrow$72.3 F1），达到与同等大小的最先进模型相匹配的性能，同时使用简单和通用的编码器-解码器架构。此外，GenBERT很好地概括了数学单词问题数据集，同时在标准RC任务上保持了高性能。我们的方法提供了一个将技能注入大型预训练LMs的通用方法，只要该技能适合自动数据扩充。</pre></li>
<li><a href="https://arxiv.org/abs/2005.08516">Towards Question Format Independent Numerical Reasoning: A Set of Prerequisite Tasks</a>
<pre>数字推理对于准确地理解世界通常很重要。最近，一些特定格式的数据集被提出，例如自然语言推理（NLI）、阅读理解（RC）和问答（QA）环境中的数字推理。针对这些数据集，还提出了几种特定于格式的模型和体系结构。然而，在执行与问题格式无关的数值推理时，非常需要一个能够评估模型能力的基准，因为（i）我们想要教授的数值推理能力不受问题格式的控制，（ii）数值推理技术具有最佳可能的应用，它必须能够以一种非单一格式、任务、数据集或域独有的方式处理语言和推理。为了实现这一目标，我们引入了NUMBERGAME，这是一个多方面的基准，用于评估八种不同格式的数值推理任务的模型性能。我们在汇编中添加了四种现有的问题类型。我们添加的两种新类型是关于需要外部数字知识、常识知识和领域知识的问题。为了构建一个更实用的数值推理系统，NUMBERGAME要求除数值推理之外的四种能力：（i）直接从数据中检测问题格式（ii）找到每种格式都可以转换为的中间通用格式（iii）结合常识知识（iv）处理跨格式的数据不平衡。我们建立了几个基线，包括一个基于知识搜索的新模型。然而，与人类基线相比，所有基线的表现都很差，这表明我们的基准很硬。我们的工作推进了通用系统开发的最新进展，展示了这些未充分探索的任务的范围。</pre></li>
<li><a href="https://arxiv.org/abs/1812.03593">SDNet: Contextualized Attention-based Deep Network for Conversational Question Answering</a>
<pre>会话问答（CQA）是一项新颖的问答任务，需要理解对话语境。与传统的单轮机器阅读理解（MRC）任务不同，CQA包括段落理解、共指消解和语境理解。在本文中，我们提出了一种创新的基于情境化注意的深度神经网络SDNet，将情境融合到传统的MRC模型中。我们的模型利用相互注意和自我注意来理解对话语境，并从文章中提取相关信息。此外，我们还展示了一种新的方法来集成最新的BERT上下文模型。实证结果表明了我们的模型的有效性，该模型在CoQA排行榜上创造了最新的结果，比以前的最佳模型高出1.6%。我们的集合模型进一步将结果提高了2.7%。</pre></li>
<li><a href="https://arxiv.org/abs/1910.02610">Multi-hop Question Answering via Reasoning Chains</a>
<pre>多跳问答要求模型从文本的不同部分收集信息来回答问题。目前大多数方法都是通过神经网络以端到端的方式学习处理这项任务，而不需要保持推理过程的显式表示。我们提出了一种在文本上提取离散推理链的方法，该推理链由一系列导致答案的句子组成。然后，我们将提取的链反馈给基于BERT的QA模型，以进行最终答案预测。关键是，我们不依赖黄金注释链或“支持事实”：在训练时，我们使用基于命名实体识别和共指解析的启发式方法推导伪黄金推理链。我们在测试时也不依赖这些注释，因为我们的模型学习仅从原始文本中提取链。我们在最近提出的两个大型多跳问答数据集上测试了我们的方法：WikiHop和HotpotQA，并在WikiHop上实现了最先进的性能，在HotpotQA上实现了强大的性能。我们的分析显示了对高性能至关重要的链的特性：特别是，顺序建模提取很重要，以上下文感知的方式处理每个候选句子也很重要。此外，人类评估表明，我们提取的链允许人类高度自信地给出答案，这表明这些是该任务的一个强大的中间抽象。</pre></li>
<li><a href="https://arxiv.org/abs/1911.00484">Select, Answer and Explain: Interpretable Multi-hop Reading Comprehension over Multiple Documents</a>
<pre>多文档的可解释多跳阅读理解（RC）是一个具有挑战性的问题，因为它需要对多个信息源进行推理，并通过提供支持证据来解释答案预测。在本文中，我们提出了一个有效且可解释的选择、回答和解释（SAE）系统来解决多文档RC问题。我们的系统首先过滤掉与答案无关的文档，从而减少分散注意力的信息量。这是通过一个文档分类器来实现的，该分类器使用一种新的成对学习对损失进行排序。然后将选定的答案相关文档输入到模型中，共同预测答案和支持句。该模型采用多任务学习目标进行优化，在标记级上进行答案预测，在句子级上支持句子预测，并在这两个任务之间进行基于注意的交互。在HotpotQA（一个具有挑战性的多跳RC数据集）上进行评估，与排行榜上的其他现有系统相比，拟议的SAE系统在干扰器设置方面实现了最高的竞争力。</pre></li>
<li><a href="https://arxiv.org/abs/1909.07598">Multi-step Entity-centric Information Retrieval for Multi-Hop Question Answering</a> (EMNLP2019 WS)
<pre>多跳问答（QA）需要一个信息检索（IR）系统，该系统可以找到回答问题所需的支持证据，这使得检索过程非常具有挑战性。本文介绍了一种IR技术，该技术使用最初检索到的证据中存在的实体信息来学习对其他相关证据进行“emph{hop}”。在一个有超过500万个维基百科段落的环境中，我们的方法大大提高了检索性能。检索到的证据还通过\textbf{10.59}F1提高了\hotpot基准上现有QA模型（无任何培训）的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2004.13821">Fine-tuning Multi-hop Question Answering with Hierarchical Graph Network</a>
<pre>在本文中，我们提出了一个多跳问答的两阶段模型。第一阶段是一个层次图网络，用于对多跳问题进行推理，并能够利用文档的自然结构（即段落、问题、句子和实体）捕获不同级别的粒度。推理过程转化为节点分类任务（即段落节点和句子节点）。第二阶段是语言模型微调任务。总之，第一阶段使用图形神经网络选择并连接支持句作为一个段落，第二阶段使用语言模型微调范式找到答案范围。</pre></li>
<li><a href="https://www.aclweb.org/anthology/2020.acl-main.414/">Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering</a> (ACL2020)</li>
<li><a href="https://arxiv.org/abs/2004.07347">HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data</a>
<pre>现有的问答数据集侧重于处理同质信息，仅基于文本或KB/表信息。然而，由于人类知识分布在异构形式上，单独使用异构信息可能会导致严重的覆盖问题。为了填补这一空白，我们推出了HybridQAhttps://github.com/wenhuchen/HybridQA，一个新的大规模问答数据集，需要对异构信息进行推理。每个问题都与一个Wikipedia表和多个与表中实体链接的自由形式语料库对齐。这些问题旨在汇总表格信息和文本信息，即缺少任何一种形式都会导致问题无法回答。我们使用三种不同的模型进行测试：1）仅表格模型。2） 纯文本模型。3） 一种混合模型，它结合不同的信息来寻找答案。实验结果表明，两条基线的EM得分均在20%以下，而混合模型的EM得分可达40%以上。这一差距表明有必要在HybridQA中聚合异构信息。然而，混合模型的得分仍然远远落后于人的表现。因此，HybridQA可以作为研究异构信息问答的一个具有挑战性的基准。</pre></li>
<li><a href="https://arxiv.org/abs/2010.12623">Unsupervised Multi-hop Question Answering by Question Generation</a> (NAACL2021)
<pre>获取多跳问答（QA）的训练数据既耗时又占用大量资源。我们探讨了在不引用任何人类标记的多跳问答对（即无监督的多跳问答）的情况下，训练一个性能良好的多跳问答模型的可能性。我们提出了MQA-QG，这是一个无监督的框架，可以从同质和异构数据源生成类似于人的多跳训练数据。MQA-QG首先从每个数据源选择/生成相关信息，然后集成多个信息以形成多跳问题，从而生成问题。仅使用生成的训练数据，我们就可以训练出一个合格的多跳QA，对于HybridQA和HotpotQA数据集，其监督学习性能分别达到61%和83%。我们还表明，使用生成的数据对QA系统进行预培训将大大减少对人工标注的培训数据的需求。我们的代码可在https://github.com/teacherpeterpan/Unsupervised-Multi-hop-QA.</pre></li>
<li><a href="https://arxiv.org/abs/1902.01718">End-to-End Open-Domain Question Answering with BERTserini</a> (NAALC2019)
<pre>我们演示了一个端到端的问答系统，该系统集成了BERT和开源的Anserini信息检索工具包。与当今大多数问答和阅读理解模型（在少量输入文本上运行）不同，我们的系统将IR的最佳实践与基于BERT的阅读器相结合，以端到端的方式从大量维基百科文章中识别答案。我们报告了在标准基准测试集合上比先前结果的重大改进，表明使用SHADK微调预训练的BERT足以实现识别答案跨度的高精度。</pre></li>
<li><a href="https://arxiv.org/abs/1906.00300">Latent Retrieval for Weakly Supervised Open Domain Question Answering</a> (ACL2019)
<pre>最近关于开放领域问答（QA）的工作假设对支持证据进行强有力的监督和/或假设使用黑盒信息检索（IR）系统检索候选证据。我们认为这两种方法都是次优的，因为黄金证据并不总是可用的，而且QA与IR有根本的不同。我们首次展示了在没有任何IR系统的情况下，通过问答字符串对联合学习检索器和读取器是可能的。在此设置中，从所有维基百科检索的证据被视为潜在变量。因为从零开始学习是不切实际的，所以我们用反向完形填空任务对检索器进行预训练。我们在五个QA数据集的开放版本上进行评估。在提问者已经知道答案的数据集上，传统的IR系统（如BM25）就足够了。在用户真正在寻找答案的数据集上，我们证明了学习检索是至关重要的，在精确匹配方面比BM25高出19个百分点。</pre></li>
<li><a href="https://arxiv.org/abs/2004.04906">Dense Passage Retrieval for Open-Domain Question Answering</a> (EMNLP2020)
<pre>开放域问答依赖于高效的段落检索来选择候选上下文，而传统的稀疏向量空间模型（如TF-IDF或BM25）是事实上的方法。在这项工作中，我们表明检索实际上可以单独使用密集表示来实现，其中嵌入是通过简单的双编码器框架从少量的问题和文章中学习的。在广泛的开放域QA数据集上进行评估时，我们的dense retriever在排名前20位的通道检索准确率方面远远超过强大的Lucene-BM25系统9%-19%，并帮助我们的端到端QA系统在多个开放域QA基准上建立新的最先进水平。</pre></li>
<li><a href="https://arxiv.org/abs/2106.00882">Efficient Passage Retrieval with Hashing for Open-domain Question Answering</a> (ACL2021)
<pre>大多数最先进的开放领域问答系统使用神经检索模型将段落编码为连续向量，并从知识源中提取出来。然而，这样的检索模型通常需要大量内存才能运行，因为它们的通道索引非常大。在本文中，我们介绍了二进制通道检索器（BPR），这是一种高效的记忆神经检索模型，它将哈希学习技术集成到最先进的密集通道检索器（DPR）中，以使用紧凑的二进制代码而不是连续向量来表示通道索引。BPR是在两个任务上以多任务为目标进行训练的：基于二进制代码的有效候选生成和基于连续向量的精确重排序。与DPR相比，BPR在两个标准的开放域问答基准测试（自然问题和TriviaQA）上大幅降低了内存成本，从65GB降低到2GB，且不会降低准确性。我们的代码和经过培训的模型可在https://github.com/studio-ousia/bpr.</pre></li>
<li><a href="https://arxiv.org/abs/2101.00408">End-to-End Training of Neural Retrievers for Open-Domain Question Answering</a>
<pre>最近关于训练开放领域问答（OpenQA）神经检索器的工作采用了有监督和无监督两种方法。然而，目前尚不清楚无监督和有监督的方法如何最有效地用于神经检索器。在这项工作中，我们系统地研究了猎犬的预训练。我们首先提出了一种无监督的预训练方法，该方法使用反完形填空任务和掩蔽显著跨度，然后使用问题上下文对进行监督微调。在自然问题和TriviaQA数据集的前20名检索准确率中，这种方法比以前的最佳结果绝对提高了2+分。我们还探讨了在OpenQA模型中对读卡器和检索器组件进行端到端监督培训的两种方法。在第一种方法中，读者分别考虑每个检索到的文档，而在第二种方法中，读者一起考虑所有检索到的文档。我们的实验证明了这些方法的有效性，因为我们获得了最新的结果。在自然问题数据集上，我们获得了84的前20名检索准确率，比最近的DPR模型提高了5个点。此外，我们在答案提取方面取得了良好的效果，比最近的REALM和RAG等模型高出3+个百分点。我们进一步将端到端培训扩展到大型机型，并显示出与小型机型相比性能的持续提升。</pre></li>
<li><a href="https://arxiv.org/abs/2107.13602">Domain-matched Pre-training Tasks for Dense Retrieval</a>
<pre>在模型尺寸不断增大的大型数据集上进行预培训，现在已被证明是提高几乎所有NLP任务性能的有效方法。一个值得注意的例外是信息检索，到目前为止，额外的预培训未能产生令人信服的结果。我们表明，通过正确的训练前设置，可以克服这一障碍。我们通过对大型bi编码器模型进行预培训来证明这一点：1）最近发布的一组6500万个综合生成的问题，2）从pushshift.io提供的现有Reddit对话数据集中获得的2亿个评论后对。我们在一组信息检索和对话检索基准上进行了评估，结果表明，与受监督的基线相比有了实质性的改进。</pre></li>
<li><a href="https://arxiv.org/abs/2104.07800">Towards Robust Neural Retrieval Models with Synthetic Pre-Training</a>
<pre>最近的研究表明，常用的机器阅读理解（MRC）数据集可以用来训练高性能的神经信息检索（IR）系统。然而，到目前为止，神经IR的评估仅限于标准的监督学习设置，在这些设置中，神经IR的表现优于传统的术语匹配基线。我们对神经IR进行域内和域外评估，并试图提高其在不同场景下的鲁棒性，包括零炮设置。我们表明，使用序列到序列生成器生成的合成训练示例可以有效地实现这一目标：在我们的实验中，使用合成示例进行预训练可以提高五个不同测试集的域内和域外评估的检索性能。</pre></li>
<li><a href="https://arxiv.org/abs/2109.08535">Simple Entity-Centric Questions Challenge Dense Retrievers</a> (EMNLP2021) [<a href="https://github.com/princeton-nlp/EntityQuestions">github</a>]
<pre>由于密集检索模型的成功，开放域问答在最近迅速流行起来，而密集检索模型仅使用少数有监督的训练示例就超过了稀疏模型。然而，在本文中，我们证明了当前的稠密模型还不是检索的圣杯。我们首先构造EntityQuestions，这是一组基于Wikidata事实的简单、实体丰富的问题（例如，“Arve Furset出生在哪里？”），并观察到密集检索器的性能大大低于稀疏方法。我们研究了这个问题，发现密集检索器只能推广到普通实体，除非在训练期间明确观察到问题模式。我们将讨论解决这一关键问题的两个简单解决方案。首先，我们证明了数据扩充无法解决泛化问题。其次，我们认为一个更健壮的段落编码器有助于使用专门的问题编码器更好地适应问题。我们希望我们的工作能够阐明创建一个健壮、通用的密集检索器所面临的挑战，该检索器能够在不同的输入分布中很好地工作。</pre></li>
<li><a href="https://arxiv.org/abs/2109.08133">Phrase Retrieval Learns Passage Retrieval, Too</a> (EMNLP2021) [<a href="https://github.com/princeton-nlp/DensePhrases">github</a>]
<pre>与稀疏检索方法相比，密集检索方法在一系列自然语言处理问题中表现出了巨大的潜力。其中，密集短语检索是最细粒度的检索单元，因为短语可以直接用作答疑和填槽任务的输出。在这项工作中，我们遵循检索短语自然需要检索较大的文本块的直觉，并研究短语检索是否可以作为包括段落和文档在内的粗略级别检索的基础。我们首先观察到，与通道检索器相比，密集短语检索系统在没有任何再培训的情况下，已经实现了更好的通道检索精度（+3-5%，在前5个精度中），这也有助于在更少通道的情况下实现更优的端到端QA性能。然后，我们解释了为什么短语级监督比段落级监督有助于更好地学习细粒度蕴涵，并且还表明短语检索可以得到改进，从而在实体链接和基于知识的对话等文档检索任务中获得有竞争力的性能。最后，我们演示了短语过滤和矢量量化如何将索引的大小减少4-10倍，从而使密集短语检索成为多粒度检索中一个实用且通用的解决方案。</pre></li>
<li><a href="https://arxiv.org/abs/2007.01282">Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering</a>
<pre>开放领域问答的生成模型已经证明是有竞争力的，不需要借助外部知识。虽然这种方法很有前途，但它需要使用具有数十亿个参数的模型，而这些参数的训练和查询成本很高。在本文中，我们将研究这些模型从检索可能包含证据的文本段落中获益的程度。我们在自然问题和TriviaQA开放基准上获得了最新的结果。有趣的是，我们观察到，随着检索通道数量的增加，该方法的性能显著提高。这证明了生成性模型善于聚合和组合来自多个段落的证据。</pre></li>
<li><a href="https://arxiv.org/abs/2005.00038">Progressively Pretrained Dense Corpus Index for Open-Domain Question Answering</a> (EACL2021)
<pre>为了从大型语料库中提取答案，开放领域问答系统通常依靠信息检索技术来缩小搜索空间。由于TF-IDF等标准倒排索引方法的有效性，因此通常使用这种方法。然而，它们的检索性能是有限的，因为它们仅仅使用了浅层和稀疏的词汇特征。为了打破IR瓶颈，最近的研究表明，通过预训练一个有效的段落编码器，将段落索引为密集向量，可以实现更强的检索性能。经过训练后，语料库可以预编码为低维向量并存储在索引结构中，在索引结构中检索可以作为最大内积搜索有效地实现。尽管有很好的结果，预训练如此密集的索引是昂贵的，并且通常需要非常大的批量。在这项工作中，我们提出了一种简单且资源高效的方法来预训练段落编码器。首先，我们不使用启发式创建的伪问题段落对进行预训练，而是利用现有的预训练序列到序列模型来构建一个强大的问题生成器，以创建高质量的预训练数据。其次，我们提出了一种渐进式预训练算法，以确保每个批次中都存在有效的阴性样本。在三个数据集中，我们的方法优于现有的密集检索方法，该方法在预训练时使用的计算资源是现有密集检索方法的7倍。</pre></li>
<li><a href="https://arxiv.org/abs/2009.12756">Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval</a>
<pre>我们提出了一种简单有效的多跳密集检索方法，用于回答复杂的开放领域问题，在HotpotQA和multi-evidence FEVER两个多跳数据集上实现了最先进的性能。与以前的工作相反，我们的方法不需要访问任何特定于语料库的信息，例如文档间超链接或人类注释实体标记，并且可以应用于任何非结构化文本语料库。我们的系统还产生了一个更好的效率-精度权衡，匹配HotpotQA上发布的最佳精度，同时推理速度快10倍。</pre></li>
<li><a href="https://arxiv.org/abs/2104.05883">Multi-Step Reasoning Over Unstructured Text with Beam Dense Retrieval</a> (NAACL2021) [<a href="https://github.com/henryzhao5852/BeamDR">github</a>]
<pre>复杂的问题回答通常需要找到由多个证据组成的推理链。当前的方法结合了结构化知识和非结构化文本的优点，假设文本语料库是半结构化的。在稠密检索方法的基础上，我们提出了一种新的多步骤检索方法（BeamDR），该方法通过稠密表示中的beam搜索迭代形成证据链。当对多跳问答进行评估时，BeamDR在不使用任何半结构化信息的情况下与最先进的系统竞争。通过密集空间中的查询组合，BeamDR捕获推理链中证据之间的隐式关系。该守则可于https://github.com/ henryzhao5852/BeamDR。</pre></li>
<li><a href="https://arxiv.org/abs/2010.12527">Retrieve, Read, Rerank, then Iterate: Answering Open-Domain Questions of Varying Reasoning Steps from Text</a>
<pre>我们开发了一个统一的系统来直接回答文本开放领域的问题，这些问题可能需要不同数量的检索步骤。我们使用一个单一的多任务转换器模型，以迭代的方式执行所有必要的子任务——检索支持事实，重新排列它们，并从所有检索到的文档中预测答案。我们避免做出关键的假设，因为以前的工作无法很好地转移到现实世界中，包括利用回答每个问题所需的固定数量检索步骤的知识，或使用结构化元数据，如可用性有限的知识库或web链接。相反，我们设计了一个系统，可以回答任何文本集合上的开放域问题，而无需事先了解推理的复杂性。为了模拟这种设置，我们将现有的一步和两步数据集与新的203个问题集合相结合，构建了一个新的基准，这些问题需要三个维基百科页面来回答，在此过程中统一了维基百科语料库版本。我们证明了我们的模型在现有基准和新基准上都具有竞争力。</pre></li>
<li><a href="https://arxiv.org/abs/2010.08191">RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering</a>
<pre>在开放域问答中，密集段落检索已经成为一种新的检索相关段落以寻找答案的范式。通常，采用双编码器架构来学习问题和段落的密集表示，以进行语义匹配。然而，由于训练和推理之间的差异、未标记的正片的存在以及有限的训练数据等挑战，很难有效地训练双编码器。为了应对这些挑战，我们提出了一种称为RocketQA的优化训练方法，以改进密集通道检索。我们在RocketQA中做出了三项主要的技术贡献，即跨批次负片、去噪硬负片和数据增强。实验结果表明，RocketQA在MSMARCO和自然问题上都显著优于以前的最新模型。我们还进行了广泛的实验来检验这三种策略在RocketQA中的有效性。此外，我们还证明了基于RocketQA检索器的端到端QA性能可以得到提高。</pre></li>
<li><a href="https://arxiv.org/abs/2002.03932">Pre-training Tasks for Embedding-based Large-scale Retrieval</a> (ICLR2020)
<pre>我们考虑大规模查询文档检索问题：给定一个查询（例如，一个问题），从一个大的文档语料库返回相关文档集（例如，包含答案的段落）。这个问题通常分两步解决。检索阶段首先减少解决方案空间，返回候选文档的子集。评分阶段然后对文档重新排序。关键的是，检索算法不仅要求高召回率，而且要求高效，在时间上返回与文档数量次线性的候选文档。与评分阶段不同，由于伯特式交叉注意模型的预训练任务，评分阶段最近取得了重大进展，而检索阶段的研究仍然较少。大多数以前的工作依赖于经典的信息检索（IR）方法，如BM-25（令牌匹配+TF-IDF权重）。这些模型只接受稀疏的手工特征，不能针对感兴趣的不同下游任务进行优化。本文对基于嵌入的检索模型进行了全面的研究。我们表明，学习基于强嵌入的变压器模型的关键因素是一组预训练任务。通过充分设计段落级预训练任务，变压器模型可以比广泛使用的BM-25以及无变压器的嵌入模型显著改进。我们研究的段落级预训练任务是反完形填空任务（ICT）、身体优先选择（BFS）、维基链接预测（WLP）以及三者的组合。</pre></li>
<li><a href="https://arxiv.org/abs/1908.08167">Multi-passage BERT: A Globally Normalized BERT Model for Open-domain Question Answering</a> (EMNLP2019)
<pre>BERT模型已成功地应用于开放域QA任务。然而，以前的工作通过查看与独立训练实例相同的问题对应的段落来训练BERT，这可能会导致不同段落的答案得分不可比。为了解决这个问题，我们提出了一个多段落的BERT模型来对同一问题的所有段落的答案分数进行全局标准化，这一变化使我们的QA模型能够利用更多的段落找到更好的答案。此外，我们发现通过滑动窗口将文章拆分为100个单词的段落可以将性能提高4%。通过利用通道ranker选择高质量通道，多通道BERT额外增加2%。在四个标准基准上的实验表明，我们的多通道BERT在所有基准上都优于所有最先进的模型。特别是，在OpenSQuAD数据集上，我们的模型比所有非BERT模型分别提高了21.4%EM和21.5%F_1$，比基于BERT的模型提高了5.8%EM和6.5%F_1$。</pre></li>
<li><a href="https://arxiv.org/abs/2009.06354">QED: A Framework and Dataset for Explanations in Question Answering</a> [<a href="https://github.com/google-research-datasets/QED">github</a>]
<pre>一个问答系统，除了提供答案外，还提供对导致该答案的推理的解释，在可调试性、可扩展性和信任度方面具有潜在优势。为此，我们提出了QED，这是一个语言学知识丰富、可扩展的问答解释框架。QED解释根据形式语义概念（如指称平等、句子和蕴涵）指定问题和答案之间的关系。我们描述并公开发布了一个基于Google Natural Questions数据集子集的专家注释QED解释数据集，并报告了两项任务的基线模型——给出答案的事后解释生成，以及联合问题回答和解释生成。在联合环境中，一个有希望的结果表明，在相对少量的QED数据上进行训练可以改善问题回答。除了描述QED方法的形式化、语言理论动机外，我们还描述了一项大型用户研究，该研究表明，QED解释的存在显著提高了未经培训的评分员发现由强大的神经QA基线造成的错误的能力。</pre></li>
<li><a href="https://arxiv.org/abs/1911.10470">Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering</a> (ICLR2020)
<pre>回答需要在web规模上进行多跳推理的问题需要检索多个证据文档，其中一个文档通常与问题几乎没有词汇或语义关系。本文介绍了一种新的基于图的递归检索方法，该方法学习在Wikipedia图上检索推理路径，以回答多跳开放域问题。我们的检索器模型训练了一个递归神经网络，该网络通过对先前检索到的文档进行条件处理，学习在推理路径中顺序检索证据段落。我们的读者模型对推理路径进行排序，并提取最佳推理路径中包含的答案范围。实验结果在三个开放域QA数据集中显示了最新的结果，显示了我们方法的有效性和鲁棒性。值得注意的是，我们的方法在HotpotQA方面取得了显著的改进，比以前的最佳模型高出14个百分点。</pre></li>
<li><a href="https://arxiv.org/abs/2007.00814">Relevance-guided Supervision for OpenQA with ColBERT</a>
<pre>开放领域问答系统（OpenQA）通常依赖于检索器在大型语料库中查找候选段落，以及阅读器从这些段落中提取答案。在最近的许多工作中，检索器是一个学习过的组件，它使用问题和段落的粗粒度向量表示。我们认为，这种建模选择对于处理自然语言问题的复杂性来说表达能力不足。为了解决这个问题，我们定义了ColBERT QA，它将可伸缩的神经检索模型ColBERT应用于OpenQA。科尔伯特在问题和段落之间创建细粒度的交互。我们提出了一种有效的弱监督策略，该策略迭代地使用ColBERT创建自己的训练数据。这极大地改进了OpenQA对自然问题、团队和琐事的检索，结果系统在所有三个数据集上都达到了最先进的提取OpenQA性能。</pre></li>
<li><a href="https://arxiv.org/abs/2010.10757">RECONSIDER: Re-Ranking using Span-Focused Cross-Attention for Open Domain Question Answering</a>
<pre>开放领域问答（QA）的最新机器阅读理解（MRC）模型通常使用远程监督的正面示例和启发式检索的负面示例来训练跨度选择。该培训计划可能解释了经验观察结果，即这些模型在其前几项预测中实现了较高的召回率，但总体准确率较低，因此需要对答案重新排序。我们为跨度提取任务开发了一种简单有效的重新排序方法（重新考虑），该方法改进了大型预训练MRC模型的性能。Reinvision根据从MRC模型的高置信度预测中提取的正面和负面示例进行训练，并使用段落中的跨度注释在较小的候选集上执行以跨度为中心的重新排序。因此，Reinvision学会了消除封闭的假阳性段落，并在四项QA任务上达到了新的水平，包括自然问题与真实用户问题的精确匹配准确率为45.5%，TriviaQA的准确匹配准确率为61.7%。</pre></li>
<li><a href="https://arxiv.org/abs/2104.08445">Joint Passage Ranking for Diverse Multi-Answer Retrieval</a>
<pre>我们研究多答案检索，这是一个探索不足的问题，需要检索文章以涵盖给定问题的多个不同答案。这项任务需要对检索到的段落进行联合建模，因为模型不应重复检索包含相同答案的段落，而以丢失不同的有效答案为代价。在本文中，我们介绍了JPR，第一个用于多答案检索的联合通道检索模型。JPR利用自回归重排序器选择一系列的段落，每个段落都以先前选择的段落为条件。JPR被训练为在每个时间步选择包含新答案的段落，并使用树解码算法实现多样性程度的灵活性。与以前的方法相比，JPR在三个多答案数据集上实现了显著更好的答案覆盖率。当与下游问题回答相结合时，改进的检索能够生成更大的答案生成模型，因为它们需要考虑更少的段落，建立一个新的最新的状态。</pre></li>
<li><a href="https://arxiv.org/abs/2009.13013">SPARTA: Efficient Open-Domain Question Answering via Sparse Transformer Matching Retrieval</a>
<pre>我们介绍了SPARTA，一种新的神经检索方法，它在性能、泛化性和开放领域问答的可解释性方面表现出巨大的潜力。与许多使用密集向量最近邻搜索的神经排序方法不同，SPARTA学习的稀疏表示可以有效地实现为反向索引。由此产生的表示支持可伸缩的神经检索，不需要昂贵的近似向量搜索，并且比密集的对应项具有更好的性能。我们在4个开放领域问答（OpenQA）任务和11个检索问答（ReQA）任务上验证了我们的方法。SPARTA在中英文数据集中的各种开放领域问答任务中取得了最新的成果，包括开放团队、自然问题、，CMRC和etc分析还确认，所提出的方法创建了人类可解释的表示，并允许灵活控制性能和效率之间的权衡。</pre></li>
<li><a href="https://arxiv.org/abs/2011.05435">Don’t Read Too Much into It: Adaptive Computation for Open-Domain Question Answering</a> (EMNLP2020 WS)
<pre>大多数开放域问题回答方法包括一个轻量级的检索器，用于选择一组候选段落，以及一个计算昂贵的阅读器，用于检查段落以确定正确答案。以前的工作表明，随着检索到的段落数量的增加，读者的表现也会随之增加。但是，他们假设所有检索到的通道都具有同等的重要性，并将相同的计算量分配给它们，从而导致计算成本大幅增加。为了降低这一成本，我们建议使用自适应计算来控制为要阅读的文章分配的计算预算。我们首先介绍了一种在单独通道上运行的技术，该技术依赖于任何时候的预测和早期退出概率的每层估计。然后，我们介绍SkylineBuilder，这是一种基于强化学习训练的资源分配策略，动态决定在每一步分配计算的通道的方法。我们在SQuAD Open上的结果表明，与几种强静态和自适应方法相比，具有全局优先级的自适应计算得到了改进，从而使计算量减少了4.3倍，同时保持了完整模型95%的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2102.10697">Pruning the Index Contents for Memory Efficient Open-Domain QA</a> [<a href="https://github.com/KNOT-FIT-BUT/R2-D2">github</a>]
<pre>这项工作提出了一个新的管道，展示了通过结合最先进的方法可以实现什么。具体地说，它提出了由检索器、段落重排器、抽取式读卡器、生成式读卡器组成的新型R2-D2（秩两次，读两次）管道，并提出了一种简单的组合方式。此外，以前的工作通常附带大量外部文档索引，其规模为数十GiB。这项工作提供了一种简单的方法来修剪大量索引的内容，这样，开放域QA系统连同索引、操作系统和库组件一起可以放入6GiB docker映像，同时只保留原始索引内容的8%，仅损失3%的EM精度。</pre></li>
<li><a href="https://arxiv.org/abs/2010.10999">Is Retriever Merely an Approximator of Reader?</a>
<pre>开放域问答（QA）的最新技术依赖于一个高效的检索器，该检索器大大减少了昂贵阅读器的搜索空间。社区中一个相当被忽视的问题是检索器和读者之间的关系，特别是，如果检索器的整个目的只是为了快速接近读者。我们的经验证据表明答案是否定的，读者和检索者即使仅在准确性方面也是互补的。我们仔细推测，检索器的体系结构约束（最初用于实现近似搜索）似乎也会使模型在大规模搜索中更加健壮。然后，我们建议将读取器提取到检索器中，以便检索器在保持自身利益的同时吸收读取器的力量。实验结果表明，该方法可以提高开放领域问答任务中现成检索器的文档召回率和端到端问答准确率。</pre></li>
<li><a href="https://arxiv.org/abs/2009.13815">Neural Retrieval for Question Answering with Cross-Attention Supervised Data Augmentation</a>
<pre>将问题和答案独立投射到共享嵌入空间的神经模型允许从大型语料库中进行有效的连续空间检索。独立计算问题和答案的嵌入会导致与匹配问题和答案相关的信息的后期融合。虽然后期融合对有效检索至关重要，但其性能不如使用早期融合的模型（例如，基于伯特的分类器，问答对之间存在交叉注意）。我们提出了一种使用精确早期融合模型的有监督数据挖掘方法，以改进高效后期融合检索模型的训练。我们首先训练一个精确的分类模型，并在问题和答案之间交叉关注。然后使用精确的交叉注意模型注释额外的段落，以便为神经检索模型生成加权训练示例。使用额外数据生成的检索模型在精度上显著优于使用gold注释直接训练的检索模型，精度为$N$(P@N）和平均倒数排名（MRR）。</pre></li>
<li><a href="https://arxiv.org/abs/2004.14560">RikiNet: Reading Wikipedia Pages for Natural Question Answering</a> (ACL2020)
<pre>阅读长文档来回答开放领域的问题在自然语言理解中仍然具有挑战性。在本文中，我们介绍了一种新的模型，称为RikiNet，它通过阅读维基百科页面自然地回答问题。RikiNet包含一个动态段落双注意阅读器和一个多级级联答案预测器。读者利用一套互补的注意机制动态地表示文档和问题。然后将这些表示输入预测器，以级联方式获得短答案的范围、长答案的段落和答案类型。在自然问题（NQ）数据集上，单个RikiNet在长答案和短答案任务上分别达到74.3 F1和57.9 F1。据我们所知，这是第一个超越单一人类绩效的单一模型。此外，乐团RikiNet在长答案和短答案任务中获得76.1 F1和61.3 F1成绩，在官方NQ排行榜上获得最佳表现</pre></li>
<li><a href="https://arxiv.org/abs/2005.00766">BERT-kNN: Adding a kNN Search Component to Pretrained Language Models for Better QA</a>
<pre>Khandelwal等人（2020年）使用k-最近邻（kNN）组件来提高语言模型性能。我们证明了这种思想对开放域问答（QA）是有益的。为了提高对培训过程中遇到的事实的回忆，我们将BERT（Devlin等人，2019）与传统的信息检索步骤（IR）和嵌入文本集合的大型数据库上的kNN搜索相结合。我们的贡献如下：i）BERT kNN在完形填空式QA方面的表现大大优于BERT，无需进一步培训。ii）我们表明，伯特经常识别正确的回答类别（例如，美国城市），但只有kNN恢复了事实上正确的答案（例如，“迈阿密”）。iii）与BERT相比，BERT kNN在稀有事实方面表现出色。iv）BERT kNN可以轻松处理BERT培训集未涵盖的事实，例如最近发生的事件。</pre></li>
<li><a href="https://arxiv.org/abs/2002.12591">DC-BERT: Decoupling Question and Document for Efficient Contextual Encoding</a> (SIGIR2020)
<pre>最近关于开放领域问答的研究已经通过使用预先训练的语言模型（如BERT）实现了显著的性能改进。最先进的方法通常遵循“检索和读取”管道，并在将检索到的文档馈送到阅读器模块之前，使用基于BERT的重新排序器过滤检索到的文档。BERT检索器将问题和每个检索到的文档的连接作为输入。尽管这些方法在QA准确性方面取得了成功，但由于连接，它们几乎无法处理高吞吐量的传入问题，每个问题都有大量检索到的文档。为了解决效率问题，我们提出了DC-BERT，这是一种解耦的上下文编码框架，具有双BERT模型：在线BERT只对问题进行一次编码，离线BERT对所有文档进行预编码并缓存其编码。在团队开放式和自然问题开放式数据集上，DC-BERT的文档检索速度提高了10倍，同时与最先进的开放式领域问题回答方法相比，保留了大部分（约98%）的QA性能。</pre></li>
<li><a href="https://arxiv.org/abs/1906.06045">Learning to Ask Unanswerable Questions for Machine Reading Comprehension</a> (ACL2019)
<pre>具有无法回答问题的机器阅读理解是一项具有挑战性的任务。在这项工作中，我们提出了一种数据扩充技术，根据可回答问题及其包含答案的相应段落自动生成相关的不可回答问题。我们引入了一个成对序列模型来生成无法回答的问题，该模型有效地捕获了问题和段落之间的交互作用。我们还提出了一种利用现有阅读理解数据集为我们的问题生成模型构建训练数据的方法。实验结果表明，与序列对序列基线相比，成对序列模型的性能始终更好。我们进一步使用自动生成的无法回答的问题作为对2.0班数据集进行数据扩充的一种手段，使用BERT基本模型得到1.9绝对F1改进，使用BERT大模型得到1.7绝对F1改进。</pre></li>
<li><a href="https://arxiv.org/abs/1906.04980">Unsupervised Question Answering by Cloze Translation</a> (ACL2019)
<pre>为问答（QA）获取培训数据既耗时又占用大量资源，现有的QA数据集仅适用于有限的领域和语言。在这项工作中，我们探讨了抽取式质量保证实际需要高质量培训数据的程度，并探讨了无监督抽取式质量保证的可能性。我们首先学习以无监督的方式生成上下文、问题和答案三元组，然后使用这些三元组自动合成提取的QA培训数据来解决这个问题。为了生成这样的三元组，我们首先从大量文档中随机抽取上下文段落，然后从这些段落中随机抽取名词短语或命名实体作为答案。接下来，我们将上下文中的答案转换为“填空”完形填空问题，最后将其转换为自然问题。我们提出并比较了各种无监督的完形填空到自然问题翻译的方法，包括使用自然问题和完形填空问题的非对齐语料库以及基于规则的方法训练无监督的NMT模型。我们发现，现代质量保证模型可以学习回答人类的问题，令人惊讶的是，只使用合成的训练数据。我们证明，在完全不使用团队训练数据的情况下，我们的方法在团队v1上实现了56.4 F1（当答案为命名实体时为64.5 F1），优于早期监督模型。</pre></li>
<li><a href="https://arxiv.org/abs/1908.04942">Reinforcement Learning Based Graph-to-Sequence Model for Natural Question Generation</a> (ICLR2020)
<pre>自然问题生成（QG）旨在从文章和答案中生成问题。以往关于QG的工作要么（i）忽略文本中隐藏的丰富结构信息，（ii）仅依赖交叉熵损失，导致暴露偏差和训练/测试测量之间的不一致，要么（iii）未能充分利用答案信息。为了解决这些局限性，本文提出了一种基于强化学习（RL）的QG图到序列（Graph2Seq）模型。我们的模型包括一个Graph2Seq生成器和一个基于双向门控图神经网络的编码器来嵌入通道，以及一个混合评估器，该混合评估器具有结合交叉熵和RL损失的混合目标，以确保生成语法和语义有效的文本。我们还引入了一个有效的深度对齐网络，用于在单词和上下文层面将答案信息整合到文章中。我们的模型是端到端可训练的，并取得了新的最先进的分数，比现有的方法在标准阵容基准上有显著的优势。</pre></li>
<li><a href="https://www.aclweb.org/anthology/D19-5821/">A Recurrent BERT-based Model for Question Generation</a> (EMNLP2019 WS)</li>
<li><a href="https://arxiv.org/abs/2002.09758">Unsupervised Question Decomposition for Question Answering</a> [<a href="https://github.com/facebookresearch/UnsupervisedDecomposition">github</a>]
<pre>我们的目标是通过将硬问题分解为现有问答系统能够回答的更简单的子问题来改进问答（QA）。由于用分解来标记问题很麻烦，我们采用无监督的方法来生成子问题，这也使我们能够利用互联网上的数百万个问题。具体而言，我们提出了一种一对N无监督序列转换（ONUS）算法，该算法学习将一个困难的多跳问题映射到许多简单的单跳子问题。我们用一个现成的QA模型回答子问题，并给出一个重组模型的结果答案，该模型将这些问题组合成最终答案。我们展示了HotpotQA在原始、域外和多跳开发集上的强大基线上的重大QA改进。ONUS自动学习分解不同类型的问题，同时匹配用于QA的监督和启发式分解方法的效用，并在流利程度上超过这些方法。定性地说，我们发现使用子问题很有希望阐明为什么QA系统会做出预测。</pre></li>
<li><a href="https://arxiv.org/abs/2004.01909">Conversational Question Reformulation via Sequence-to-Sequence Architectures and Pretrained Language Models</a>
<pre>本文提出了一个基于序列到序列结构和预训练语言模型（PLM）的会话问题重构（CQR）的实证研究。我们利用PLM解决CQR任务的共同目标最大似然估计中提出的强令牌对令牌独立性假设。在面向任务的对话系统的CQR基准测试中，我们将在最近引入的鸭式数据集上评估微调PLM作为域内任务，并使用TREC 2019 CAsT轨迹的数据作为域外任务验证模型。通过研究具有不同参数数量的各种体系结构，我们证明，与类似的转换器体系结构相比，最新的文本到文本传输转换器（T5）在鸭式和CAsT上以更少的参数实现了最佳结果。</pre></li>
<li><a href="https://arxiv.org/abs/2004.11892">Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering</a> (ACL2020)
<pre>随着在线信息量的增加以及对快速访问这些内容的需求的增长，问答（QA）的需求也在不断增加。QA的一种常见方法是在特定于任务的标记数据集上微调预先训练的语言模型。然而，这种范式依赖于稀缺且成本高昂的大规模人类标记数据。我们提出了一种无监督的方法来使用生成的伪训练数据训练QA模型。我们表明，通过在相关检索句子而不是原始上下文句子上应用简单模板来生成用于QA培训的问题，通过允许模型学习更复杂的上下文-问题关系来提高下游QA性能。在该数据上训练QA模型比以前的无监督模型在班数据集上的F1分数相对提高了14%左右，当答案是命名实体时，相对提高了20%，在班上实现了最先进的无监督QA性能。</pre></li>
<li><a href="https://arxiv.org/abs/2005.12522">What Are People Asking About COVID-19? A Question Classification Dataset</a>
<pre>我们展示了COVID-Q，一组来自13个来源的1690个关于COVID-19的问题，我们将其注释为15个问题类别和207个问题簇。我们的数据集中最常见的问题是关于新冠病毒的传播、预防和社会影响，我们发现许多出现在多个来源中的问题没有得到CDC和FDA等知名组织的FAQ网站的回答。我们将数据公开发布在https://github.com/JerryWei03/COVID-Q. 对于将问题分为15个类别，当对每个类别20个示例进行培训时，BERT基线的准确率为58.1%，而对于问题聚类任务，BERT+三重损失基线的准确率为49.5%。我们希望COVID-Q可以直接用于开发应用系统，或者作为模型评估的特定领域资源。</pre></li>
<li><a href="https://arxiv.org/abs/1911.02365">Learning to Answer by Learning to Ask: Getting the Best of GPT-2 and BERT Worlds</a>
<pre>自动生成问题的目的是从上下文中生成问题，相应的答案是给定文章的子跨度。然而，大多数方法大多依赖启发式规则生成问题，最近也提出了神经网络方法。在这项工作中，我们提出了一种变体的自我注意变压器网络架构模型，以产生有意义的和多样化的问题。为此，我们提出了一个易于使用的模型，该模型由转换器-解码器GPT-2模型和转换器-编码器BERT组成，用于下游任务的问答。该模型以端到端的方式进行训练，其中语言模型被训练为产生问题-答案感知的输入表示，这有助于生成以答案为中心的问题。我们在班1.1数据集上从文本生成神经问题的结果表明，我们的方法可以生成语义正确且多样的问题。此外，我们还评估了我们提出的方法在下游问题回答任务中的性能。分析表明，我们提出的生成与应答协作框架相对地改进了这两个任务，并且在半监督环境下尤其强大。研究结果进一步表明，在小数据环境下，一条稳健且相对精简的管道有助于问题的生成。</pre></li>
<li><a href="https://www.aclweb.org/anthology/papers/P/P19/P19-1226/">Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension</a> (ACL2019)</li>
<li><a href="https://arxiv.org/abs/2104.06378">QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering</a> (NAACL2021) [<a href="https://github.com/michiyasunaga/qagnn">github</a>] [<a href="http://ai.stanford.edu/blog/qagnn/">blog</a>]
<pre>使用来自预先训练的语言模型（LMs）和知识图（KG）的知识回答问题的问题提出了两个挑战：给定QA上下文（问题和答案选择），方法需要（i）从大型KG中识别相关知识，以及（ii）对QA上下文和KG执行联合推理。在这项工作中，我们提出了一个新的模型QA-GNN，该模型通过两个关键创新来解决上述挑战：（i）相关性评分，其中我们使用LMs来估计KG节点相对于给定QA上下文的重要性，以及（ii）联合推理，其中我们将QA上下文和KG连接起来形成一个联合图，并通过图神经网络相互更新其表示。我们在CommonsenseQA和OpenBookQA数据集上评估QA-GNN，并展示其相对于现有LM和LM+KG模型的改进，以及其执行可解释和结构化推理的能力，例如，正确处理问题中的否定。</pre></li>
<li><a href="https://arxiv.org/abs/1908.04530">Incorporating Relation Knowledge into Commonsense Reading Comprehension with Multi-task Learning</a> (CIKM2019)
<pre>本文主要研究如何利用外部关系知识，通过多任务学习提高机器阅读理解能力。MRC中的大多数传统方法都假设用于获得正确答案的知识通常存在于给定文档中。然而，在实际任务中，部分知识可能不会被提及，机器应该具备利用外部知识的能力。在本文中，我们将关系知识集成到MRC模型中进行常识推理。具体来说，基于预先训练的语言模型（LM）。我们设计了两个辅助关系感知任务来预测两个单词之间是否存在任何常识关系以及两个单词之间的关系类型，以便更好地建模文档和候选答案选项之间的交互。我们在两个多选基准数据集上进行了实验：SemEval-2018任务11和完形填空故事测试。实验结果证明了该方法的有效性，在两个数据集上都取得了优于可比基线的性能。</pre></li>
<li><a href="https://arxiv.org/abs/1908.05147">SG-Net: Syntax-Guided Machine Reading Comprehension</a>
<pre>对于机器阅读理解来说，有效地从错综复杂、冗长的文章中建模语言知识并克服噪音是提高机器阅读理解能力的关键。传统的注意模型只关注所有的词，没有明确的约束，这导致了对一些不必要的词的不准确的关注。在这项工作中，我们建议使用语法来指导文本建模，将显式语法约束纳入注意机制，以获得更好的语言动机词表示。具体地说，对于自关注网络（SAN）赞助的基于转换器的编码器，我们在SAN中引入了感兴趣的语法依赖（SDOI）设计，以形成具有语法引导的自关注的SDOI-SAN。然后，语法引导网络（SG Net）由这个额外的SDOI-SAN和原始Transformer编码器中的SAN组成，通过一个双上下文体系结构实现更好的语言表示。为了验证其有效性，将所提出的SG网络应用于基于变压器编码器的典型预训练语言模型BERT。在包括2.0队和RACE在内的流行基准上进行的大量实验表明，拟议的SG网络设计有助于在强大的基线上实现实质性的性能改进。</pre></li>
<li><a href="https://arxiv.org/abs/1910.00458">MMM: Multi-stage Multi-task Learning for Multi-choice Reading Comprehension</a>
<pre>问答式机器阅读理解（MRC）是测试智能系统理解人类语言能力的一种重要方法，旨在回答给定上下文段落的问题。多选问答（MCQA）是MRC中最困难的任务之一，因为它通常需要更高级的阅读理解技能，如逻辑推理、总结和算术运算，而与答案通常是给定段落内文本跨度的抽取式问答相比。此外，大多数现有的MCQA数据集都很小，这使得学习任务更加困难。我们介绍了MMM，一个多阶段多任务的多选择阅读理解学习框架。我们的方法包括两个连续的阶段：使用域外数据集的粗调整阶段和使用较大域内数据集的多任务学习阶段，以帮助模型在有限的数据下更好地泛化。此外，我们提出了一种新的多步注意网络（MAN）作为该任务的顶级分类器。我们证明，MMM在四个具有代表性的MCQA数据集上显著提高了最新水平。</pre></li>
<li><a href="https://arxiv.org/abs/1909.00277">Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning</a> (EMNLP2019)
<pre>理解叙述需要字里行间的阅读，而这反过来又需要解释事件的可能原因和影响，即使没有明确提及。在本文中，我们介绍了COSMOSQA，一个由35600个问题组成的大规模数据集，这些问题需要基于常识的阅读理解，被表述为多项选择题。与大多数现有的阅读理解数据集形成鲜明对比的是，这些数据集的问题集中在对上下文段落的事实和字面理解上，我们的数据集集中在阅读人们日常叙述的不同集合中的字里行间，提出诸如“什么可能是…？”或“如果。。。“这需要超出上下文中确切文本范围的推理。为了建立Cosmos QA的基线性能，我们对几种最先进的阅读理解神经结构进行了实验，并提出了一种新的结构，该结构比竞争基线有所改进。实验结果表明，机器理解能力（68.4%）和人类理解能力（94%）之间存在着显著的差距，为今后的常识机器理解研究指明了方向。数据集、代码和排行榜可在https://wilburone.github.io/cosmos.</pre></li>
<li><a href="https://arxiv.org/abs/2002.04326">ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning</a> (ICLR2020)
<pre>最近强大的预训练语言模型在大多数流行的阅读理解数据集上都取得了显著的成绩。现在是引入更具挑战性的数据集的时候了，以推动该领域向更全面的文本推理发展。在本文中，我们介绍了一个新的阅读理解数据集需要逻辑推理（RECOR）提取标准化的研究生入学考试。正如早期的研究所表明的，人类注释的数据集通常包含偏见，模型常常利用这些偏见来实现高精度，而没有真正理解文本。为了全面评估重合闸模型的逻辑推理能力，我们提出识别有偏数据点并将其划分为易集，而将其余数据点划分为硬集。实证结果表明，最先进的模型能够以较高的精度捕捉数据集中包含的偏差，并且易于设置。然而，它们在硬集上挣扎，性能差，接近随机猜测，这表明需要更多的研究来从本质上增强当前模型的逻辑推理能力。</pre></li>
<li><a href="https://arxiv.org/abs/1911.06948">Robust Reading Comprehension with Linguistic Constraints via Posterior Regularization</a>
<pre>尽管机器阅读理解（RC）有了很大的发展，但现有的RC模型仍然容易受到攻击，并且对不同类型的对抗性示例不具有鲁棒性。神经模型过度自信地预测语义不同对抗性示例的错误答案，而过度敏感地预测语义等价对抗性示例的错误答案。现有的提高此类神经模型鲁棒性的方法仅仅缓解了这两个问题中的一个，而忽略了另一个问题。在本文中，我们借助于外部语言知识，同时解决了现有RC模型中存在的过度自信问题和过度敏感问题。我们首先结合外部知识施加不同的语言约束（实体约束、词汇约束和谓词约束），然后通过后验正则化对RC模型进行正则化。语言约束对语义不同和语义等价的对抗性例子都有更合理的预测，后验正则化提供了一种有效的机制来整合这些约束。我们的方法可以应用于任何现有的神经RC模型，包括最先进的BERT模型。大量实验表明，该方法显著提高了基本RC模型的鲁棒性，并能更好地同时处理这两个问题。</pre></li>
<li><a href="https://arxiv.org/abs/1911.01528">BAS: An Answer Selection Method Using BERT Language Model</a>
<pre>近年来，问答系统越来越受到用户的欢迎和广泛应用。尽管这些系统越来越流行，但它们的性能甚至不足以用于文本数据，需要进一步研究。这些系统由几个部分组成，其中一个部分是答案选择组件。此组件从候选答案列表中检测最相关的答案。以往研究中提出的方法试图提供一个独立的模型来进行答案选择任务。一个独立的模型不能用一个小的训练数据集来理解问答的句法和语义特征。为了填补这一空白，可以使用语言模型来实现答案选择部分。此操作使模型能够更好地理解语言，以便比以前的工作更好地理解问题和答案。在这项研究中，我们将提出“BAS”（伯特答案选择），它使用伯特语言模型来理解语言。在TrecQA原始数据集、TrecQA Clean数据集和WikiQA数据集上应用该模型的实证结果表明，使用诸如BERT之类的健壮语言模型可以提高性能。使用更健壮的分类器还可以增强语言模型对答案选择组件的影响。结果表明，语言理解是自然语言处理任务（如答案选择）的基本要求。</pre></li>
<li><a href="https://arxiv.org/abs/2011.07208">Utilizing Bidirectional Encoder Representations from Transformers for Answer Selection</a> (AMMCS2019)
<pre>近年来，人们发现，为大型数据集中的语言建模任务预先训练一个基于转换器的模型，然后为下游任务对其进行微调非常有用。这种预先训练的语言模型的一个主要优点是，它们可以有效地吸收句子中每个单词的上下文。然而，对于诸如答案选择任务这样的任务，预先训练的语言模型还没有被广泛使用。为了研究它们在此类任务中的有效性，本文采用了来自Transformer（BERT）语言模型的预训练双向编码器表示，并在两个问答（QA）数据集和三个社区问答（CQA）数据集上对其进行微调，以完成答案选择任务。我们发现，对回答选择任务的BERT模型进行微调非常有效，与之前的最新技术相比，QA数据集和CQA数据集的最大改善率分别为13.1%和18.7%。</pre></li>
<li><a href="https://arxiv.org/abs/1911.04118">TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection</a> (AAAI2020)
<pre>我们提出了TANDA，这是一种为自然语言任务微调预训练转换器模型的有效技术。具体来说，我们首先通过使用大型高质量数据集对模型进行微调，将预先训练好的模型转换为一般任务的模型。然后，我们执行第二个微调步骤，以使传输的模型适应目标域。我们展示了我们的方法对答案句子选择的好处，这是问答中一个众所周知的推理任务。利用自然问题数据集，我们构建了一个大规模数据集来支持转移步骤。我们的方法建立了两个著名基准——WikiQA和TREC-QA的最新水平，分别获得92%和94.3%的MAP分数，大大超过了最近工作中获得的83.4%和87.5%的最高分数。我们的经验表明，TANDA生成了更稳定、更健壮的模型，从而减少了选择最优超参数所需的工作量。此外，我们还表明，TANDA的转移步骤使得自适应步骤对噪声更加鲁棒。这样可以更有效地使用噪声数据集进行微调。最后，我们还使用受不同类型噪声影响的特定领域数据集，确认了TANDA在工业环境中的积极影响。</pre></li>
<li><a href="https://arxiv.org/abs/2005.02534">The Cascade Transformer: an Application for Efficient Answer Sentence Selection</a> (ACL2020)
<pre>基于大型transformer的语言模型已被证明在许多分类任务中非常有效。然而，它们的计算复杂性妨碍了它们在需要对大量候选对象进行分类的应用中的使用。虽然以前的工作已经研究了减少模型大小的方法，但是在推理过程中提高批量吞吐量的技术却很少受到关注。在本文中，我们介绍了级联变压器，这是一种简单而有效的技术，可以将基于变压器的模型应用到级联变压器中。每个ranker用于在批处理中修剪候选子集，从而显著提高推理时的吞吐量。来自transformer模型的部分编码在重排者之间共享，从而提供进一步的加速。与最先进的变压器模型相比，我们的方法减少了37%的计算量，而对准确性几乎没有影响，这在两个英语问答数据集上进行了测量。</pre></li>
<li><a href="https://arxiv.org/abs/2005.08294">Support-BERT: Predicting Quality of Question-Answer Pairs in MSDN using Deep Bidirectional Transformer</a>
<pre>来自社区支持网站（如Microsoft Developers Network、Stackoverflow、Github等）的问答质量难以定义，而质量问答预测模型的实施更具挑战性。以前的工作分别使用元特征（如上升投票数、发布问题或答案的人的可信度、文章标题和上下文朴素自然语言处理特征）解决了问题质量模型和答案质量模型。然而，文献中缺乏社区问答网站的综合问答质量模型。在这篇简短的文章中，我们使用最近开发的使用双向转换器的深度学习模型来解决社区支持网站的质量问答建模问题。我们调查了转移学习在问答质量建模中的适用性，该模型使用了最初使用维基百科在单独任务中训练的变压器双向编码器表示（BERT）。研究发现，进一步对BERT模型进行预训练，并对从微软开发人员网络（MSDN）提取的Q&As进行微调，可以将自动质量预测的性能提高到80%以上。此外，这些实现是为了在Azure知识库系统中使用AzureML在实时场景中部署微调模型而执行的。</pre></li>
<li><a href="https://arxiv.org/abs/2002.00293">Beat the AI: Investigating Adversarial Human Annotations for Reading Comprehension</a>
<pre>注释方法的创新已经成为阅读理解（RC）数据集和模型的催化剂。挑战当前RC模型的一个最新趋势是在注释过程中加入一个模型：人类以敌对的方式创建问题，以至于模型无法正确回答问题。在这项工作中，我们研究了这种注释方法，并将其应用于三种不同的设置中，总共收集了36000个样本，注释循环中的模型越来越强。这使我们能够探索诸如对抗效应的再现性、从使用不同模型在回路中强度收集的数据转移，以及推广到不使用模型收集的数据等问题。我们发现，对敌对收集的样本进行训练会导致对非敌对收集的数据集的强泛化，但随着循环中的模型越来越强，性能会逐渐恶化。此外，我们发现更强的模型仍然可以从收集的数据集中学习，这些数据集中的模型在循环中非常弱。当使用循环中的BiDAF模型收集数据进行训练时，RoBERTa在无法回答的问题上达到39.9F1——仅略低于使用RoBERTa自身收集的数据进行训练时（41.0F1）。</pre></li>
<li><a href="https://arxiv.org/abs/2004.14004">Benchmarking Robustness of Machine Reading Comprehension Models</a>
<pre>机器阅读理解（MRC）是评价模型自然语言理解（NLU）能力的重要测试平台。这一领域进展迅速，新车型在各种基准上取得了令人印象深刻的性能。然而，现有的基准只评估域内测试集上的模型，而没有考虑它们在测试时间扰动或对抗性攻击下的鲁棒性。为了填补这一重要空白，我们构建了一个新的模型不可知基准AdvRACE（对抗性种族），用于评估MRC模型在四种不同类型的对抗性攻击下的鲁棒性，包括我们的新型干扰物提取和生成攻击。我们表明，最先进的（SOTA）模型容易受到所有这些攻击。我们的结论是，构建更稳健的MRC模型有很大的空间，我们的基准可以帮助激励和衡量这一领域的进展。我们在以下位置发布数据和代码：https://github.com/NoviScl/AdvRACE .</pre></li>
<li><a href="https://arxiv.org/abs/2004.02709">Evaluating NLP Models via Contrast Sets</a>
<pre>有监督学习的标准测试集评估分布泛化。不幸的是，当一个数据集存在系统性缺口（例如注释工件）时，这些评估是误导性的：模型可以学习简单的决策规则，这些规则在测试集上表现良好，但不能捕获数据集的预期功能。我们提出了一种新的NLP注释范式，有助于填补测试数据中的系统性缺口。特别是，在构建数据集之后，我们建议数据集作者以小而有意义的方式手动扰动测试实例，从而（通常）更改金标签，创建对比度集。对比集提供了模型决策边界的局部视图，可用于更准确地评估模型的真实语言能力。我们通过为10个不同的NLP数据集（例如，DROP阅读理解、UD解析、IMDb情绪分析）创建对比集来证明对比集的有效性。虽然我们的对比集没有明显的对抗性，但它们的模型性能明显低于原始测试集——在某些情况下高达25%。我们将对比集作为新的评估基准发布，并鼓励未来的数据集构建工作遵循类似的注释过程。</pre></li>
<li><a href="https://arxiv.org/abs/2003.04808">Undersensitivity in Neural Reading Comprehension</a>
<pre>当前的阅读理解模型很好地推广到分布测试集，但在逆向选择的输入上表现不佳。以前大多数关于对抗性输入的工作都研究过敏感：语义不变的文本扰动，导致模型的预测在不应该改变时发生变化。在这项工作中，我们将重点放在互补问题上：过度预测不敏感，输入文本有意义地改变，但模型的预测没有改变，即使它应该改变。我们提出了一种嘈杂的对抗性攻击，该攻击在问题的语义变化中搜索，模型错误地预测了相同的答案，并且概率更高。尽管包含无法回答的问题，但SQuAD2.0和NewsQA模型都容易受到这种攻击。这表明，虽然准确，模型往往依赖于虚假模式，并没有充分考虑在一个问题中指定的信息。我们将数据增强和对抗性训练作为防御手段进行了实验，发现两者都能显著降低对隐藏数据和隐藏攻击空间的攻击脆弱性。解决不敏感问题还可以改善AddSent和AddOneSent的结果，并且当面临训练/评估分布不匹配时，模型更具普遍性：它们不太容易过度依赖仅存在于训练集中的预测线索，并且比传统模型高出10.9%。</pre></li>
<li><a href="https://www.aclweb.org/anthology/2020.fever-1.4/">Developing a How-to Tip Machine Comprehension Dataset and its Evaluation in Machine Comprehension by BERT</a> (ACL2020 WS)</li>
<li><a href="https://arxiv.org/abs/1905.12848">A Simple but Effective Method to Incorporate Multi-turn Context with BERT for Conversational Machine Comprehension</a> (ACL2019 WS)
<pre>会话机器理解（CMC）要求理解多轮对话的上下文。使用训练前语言模型BERT已经成功地进行了单轮机器理解，而由于BERT对输入序列的数量和长度有限制，因此还没有建立用BERT对多轮问答的建模。在本文中，我们提出了一个简单而有效的方法，与伯特为CMC。我们的方法使用伯特编码一段独立的条件与每个问题和每个答案在一个多回合的背景。然后，该方法根据用BERT编码的段落表示预测答案。对具有代表性的CMC数据集QuAC和CoQA的实验表明，我们的方法优于最近发表的方法（+0.8 F1关于QuAC和+2.1 F1关于CoQA）。此外，我们对对话历史的数量和类型对CMC准确性的影响进行了详细分析，我们发现黄金回答历史（在实际对话中可能没有给出）对两个数据集的模型性能贡献最大。</pre></li>
<li><a href="https://arxiv.org/abs/1908.05117">FlowDelta: Modeling Flow Information Gain in Reasoning for Conversational Machine Comprehension</a> (ACL2019 WS)
<pre>会话机器理解需要深入理解对话流程，之前的工作建议FlowQA在推理中隐式建模上下文表示，以更好地理解。本文提出通过对话推理对信息获取进行显式建模，以使模型能够关注更多的信息线索。该模型在会话质量保证数据集QuAC和顺序指令理解数据集SCONE中实现了最先进的性能，这表明了该机制的有效性，并证明了其对不同质量保证模型和任务的泛化能力。</pre></li>
<li><a href="https://arxiv.org/abs/1905.05412">BERT with History Answer Embedding for Conversational Question Answering</a> (SIGIR2019)
<pre>会话搜索是信息检索领域的一个新兴话题。多回合会话搜索的主要挑战之一是对会话历史进行建模以回答当前问题。现有的方法要么预先预测历史转向当前问题，要么使用复杂的注意机制来模拟历史。我们提出了一种概念简单但非常有效的方法，称为历史答案嵌入。它能够将会话历史无缝集成到基于BERT（来自变压器的双向编码器表示）的会话问答（ConvQA）模型中。我们首先解释了我们的观点，即ConvQA是一种简化但具体的会话搜索设置，然后我们提供了一个解决ConvQA的通用框架。我们进一步证明了我们在该框架下的方法的有效性。最后，我们分析了在不同环境下不同次数的历史转折的影响，为ConvQA中的会话历史建模提供了新的见解。</pre></li>
<li><a href="https://arxiv.org/abs/1908.00059">GraphFlow: Exploiting Conversation Flow with Graph Neural Networks for Conversational Machine Comprehension</a> (ICML2019 WS)
<pre>会话机器理解（MC）被证明比传统的MC更具挑战性，因为它需要更好地利用会话历史。然而，大多数现有的方法不能有效地捕获会话历史，因此在处理涉及共指或省略的问题时存在困难。此外，在对段落文本进行推理时，大多数人只是将其视为一个单词序列，而没有探究单词之间丰富的语义关系。在本文中，我们首先提出了一种简单而有效的图结构学习技术，在每个会话回合动态地构造一个问题和会话历史感知的上下文图。然后，我们提出了一种新的递归图神经网络，并在此基础上，我们引入了一种流机制来建模上下文图序列中的时间依赖关系。所提出的GraphFlow模型能够有效地捕获对话中的对话流，并且在CoQA、QuAC和DoQA基准上与现有的最新方法相比具有竞争力。此外，可视化实验表明，我们提出的模型可以为推理过程提供良好的解释性。</pre></li>
<li><a href="https://arxiv.org/abs/2004.02349">TAPAS: Weakly Supervised Table Parsing via Pre-training</a> (ACL2020)
<pre>通过表格回答自然语言问题通常被视为一项语义分析任务。为了降低完整逻辑形式的收集成本，一种流行的方法关注由外延而不是逻辑形式组成的弱监督。然而，从弱监督中训练语义解析器会带来困难，此外，生成的逻辑形式仅用作检索表示之前的中间步骤。在本文中，我们提出了TAPAS，一种不生成逻辑形式的表上问答方法。TAPAS从弱监督中训练，并通过选择表单元格和选择性地对此类选择应用相应的聚合运算符来预测表示。TAPAS扩展了BERT的体系结构，将表格编码为输入，通过有效的文本段和从Wikipedia抓取的表格的联合预训练进行初始化，并进行端到端训练。我们用三个不同的语义解析数据集进行实验，发现TAPAS优于或竞争语义解析模型，通过改进从55.1到67.2的SDA的最新的准确性，并在WiKISQL和WiKiq上与最先进的技术相媲美，但是采用更简单的模型架构。此外，我们还发现，从WIKISQL到WIKITQ的迁移学习（在我们的设置中很简单）的准确率为48.7，比最先进的技术高4.2个百分点。</pre></li>
<li><a href="https://arxiv.org/abs/2005.08314">TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data</a> (ACL2020)
<pre>近年来，基于文本的自然语言（NL）理解任务的预训练语言模型（LMs）蓬勃发展。此类模型通常针对自由形式的NL文本进行训练，因此可能不适用于对结构化数据进行语义解析等任务，这些任务需要对自由形式的NL问题和结构化表格数据（如数据库表）进行推理。在本文中，我们提出了TaBERT，一种预训练的LM，它可以联合学习NL语句和（半）结构化表的表示。塔伯特在2600万张表格及其英语上下文的大型语料库上接受培训。在实验中，使用TaBERT作为特征表示层的神经语义解析器在具有挑战性的弱监督语义解析基准WikiTableQuestions上获得了新的最佳结果，同时在文本到SQL数据集Spider上具有竞争力。该模型的实施将在http://fburl.com/TaBERT .</pre></li>
<li><a href="https://arxiv.org/abs/2010.00571">Understanding tables with intermediate pre-training</a> (EMNLP2020 Findings)
<pre>表蕴涵是一项二元分类任务，用于发现一个句子是否受到表内容的支持或反驳，它需要解析语言和表结构以及数值和离散推理。虽然有大量关于文本蕴涵的研究，但表蕴涵的研究较少。我们采用TAPAS（Herzig et al.，2020），一种基于表格的BERT模型来识别蕴涵。基于数据扩充的好处，我们创建了一个由数百万个自动创建的训练示例组成的平衡数据集，这些示例在微调之前的中间步骤中学习。这一新数据不仅对表蕴涵有用，而且对序列表QA任务SQA（Iyyer等人，2017）也有用。为了能够使用较长的示例作为BERT模型的输入，我们将表修剪技术作为预处理步骤进行评估，以在精度适度下降的情况下大幅提高训练和预测效率。不同的方法在TabFact（Chen等人，2020）和SQA数据集上建立了新的最新技术。</pre></li>
<li><a href="https://arxiv.org/abs/2009.13845">GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing</a> (ICLR2021)
<pre>我们提出了GraPPa，这是一种有效的表语义分析预训练方法，可以在文本和表格数据的联合表示中学习成分归纳偏差。我们通过从现有文本到SQL数据集的同步上下文无关语法（SCFG），在高质量的表上构造合成问题SQL对。我们使用一个新的文本模式链接目标对我们的模型进行预训练，该目标预测每个问题SQL对SQL中表字段的语法角色。为了保持模型表示真实世界数据的能力，我们还包括对几个现有的表和语言数据集进行蒙蔽语言建模（MLM），以规范预训练过程。在四种流行的全监督和弱监督表语义解析基准测试中，GraPPa在特征表示层上的表现明显优于RoBERTa large，并在所有这些测试上建立了最新的结果。</pre></li>
<li><a href="https://arxiv.org/abs/2005.09207">Table Search Using a Deep Contextualized Language Model</a> (SIGIR2020)
<pre>预训练的语境化语言模型（如BERT）在各种自然语言处理基准上取得了令人印象深刻的结果。得益于多个预训练任务和大规模训练语料库，预训练模型可以捕捉复杂的句法词关系。在本文中，我们使用深层上下文化语言模型BERT来完成adhoc表的检索任务。我们研究了如何在考虑BERT的表结构和输入长度限制的情况下对表内容进行编码。我们还提出了一种方法，该方法结合了以前关于表检索的文献中的特征，并与BERT联合训练它们。在公共数据集上的实验中，我们表明，在不同的评估指标下，我们的最佳方法可以在很大程度上优于以前的最新方法和BERT基线。</pre></li>
<li><a href="https://arxiv.org/abs/2103.12011">Open Domain Question Answering over Tables via Dense Retrieval</a> (NAACL2021)
<pre>开放领域QA的最新进展导致了基于密集检索的强大模型，但只专注于检索文本段落。在这项工作中，我们首次解决了表上的开放域QA问题，并证明了设计用于处理表格上下文的检索器可以改进检索。我们为我们的检索器提供了一个有效的预训练程序，并利用挖掘的硬底片提高检索质量。由于缺少相关数据集，我们将自然问题的子集（Kwiatkowski等人，2019）提取到表QA数据集中。我们发现我们的检索器将检索结果从72.0提高到81.1recall@10在基于BERT的检索器上，端到端的QA结果从33.8到37.7精确匹配。</pre></li>
<li><a href="https://arxiv.org/abs/2104.08303">Capturing Row and Column Semantics in Transformer Based Question Answering over Tables</a> (NAACL2021)
<pre>基于转换器的体系结构最近被用于回答表上的问题。为了提高这项任务的准确性，开发了专门的预培训技术，并将其应用于数百万个开放域web表。在本文中，我们提出了两种新的方法，证明了一种方法可以在不使用任何这些专门的预训练技术的情况下，在表QA任务上获得优异的性能。第一个模型称为RCI交互，它利用基于转换器的体系结构，独立地对行和列进行分类以识别相关单元。虽然该模型在最近的基准测试中获得了极高的精度，但我们提出的第二个模型，称为RCI表示，通过具体化现有表的嵌入，为在线QA系统提供了显著的效率优势。在最近的基准测试上的实验证明，所提出的方法可以有效地在表上定位单元值（高达98%）Hit@1WikiSQL查询问题的准确性）。此外，交互模型的性能优于基于transformer的最新方法，这些方法是在非常大的表语料库（TAPAS和TaBERT）上预先训练的，在标准WikiSQL基准上实现了约3.4%和约18.86%的额外精度改进。</pre></li>
<li><a href="https://arxiv.org/abs/2109.04312">MATE: Multi-view Attention for Table Transformer Efficiency</a> (EMNLP2021)
<pre>这项工作提出了一种稀疏注意转换器体系结构，用于对包含大型表的文档进行建模。表格在网络上无处不在，而且信息丰富。然而，web上超过20%的关系表具有20行或更多行（Cafarella等人，2008），这些大型表对电流互感器模型提出了挑战，电流互感器模型通常限于512个令牌。在这里，我们提出MATE，一种新的转换器体系结构，用于对web表的结构进行建模。MATE使用稀疏注意力的方式，使头部能够有效地关注表中的行或列。该体系结构在速度和内存方面呈线性扩展，可以使用当前加速器处理包含8000多个令牌的文档。MATE还对表格数据具有更合适的归纳偏差，并为三个表格推理数据集设置了新的最新技术。对于HybridQA（Chen等人，2020b），一个涉及包含表格的大型文档的数据集，我们将最佳先验结果提高了19个点。</pre></li>
<li><a href="https://arxiv.org/abs/2005.00242">TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions</a> (EMNLP2020)
<pre>阅读的一个关键部分是能够理解文本中描述的事件之间的时间关系，即使这些关系没有明确说明。然而，目前的机器阅读理解基准实际上没有测试时间现象的问题，因此在这些基准上训练的系统没有能力回答诸如“在[某个事件]之前/之后发生了什么？”之类的问题，一个新的英语阅读理解基准建立在3.2k的新闻片段上，其中有21k个查询时间关系的人工生成问题。结果表明，RoBERTa large在扭矩测试集上获得了51%的精确匹配分数，比人的表现落后约30%。</pre></li>
<li><a href="https://arxiv.org/abs/1908.01519">Beyond English-only Reading Comprehension: Experiments in Zero-Shot Multilingual Transfer for Bulgarian</a> (RANLP2019)
<pre>最近，阅读理解模型在大规模数据集上取得了接近人类的表现，如SHAND、CoQA、MS Macro、RACE等。这在很大程度上是由于发布了预训练的情境化表示，如BERT和ELMo，可以针对目标任务进行微调。尽管取得了这些进步，并且创建了更具挑战性的数据集，但大部分工作仍然是针对英语的。在这里，我们研究了在大规模英语数据集上进行微调的多语种BERT对阅读理解（例如种族）的有效性，并将其应用于保加利亚多项选择阅读理解。我们提出了一个新的数据集，其中包含2221个来自十二年级各种科目（历史、生物、地理和哲学）预科考试的问题，以及412个来自历史在线测验的额外问题。虽然测验作者没有给出相关的上下文，但我们结合了维基百科的知识，检索与问题+每个答案选项组合相匹配的文档。此外，我们还尝试了不同的索引和预训练策略。评估结果显示准确率为42.23%，远高于基线24.89%。</pre></li>
<li><a href="https://www.aclweb.org/anthology/P19-1227/">XQA: A Cross-lingual Open-domain Question Answering Dataset</a> (ACL2019)</li>
<li><a href="https://arxiv.org/abs/2010.11856">XOR QA: Cross-lingual Open-Retrieval Question Answering</a> (NAACL2021) [<a href="https://nlp.cs.washington.edu/xorqa/">website</a>]
<pre>多语言问答任务通常假设答案与问题使用相同的语言。然而在实践中，许多语言都面临着信息稀缺和信息不对称两个问题：信息稀缺是指语言中很少有参考文献；信息不对称是指问题涉及到其他文化的概念。这项工作将开放检索问答扩展到跨语言环境，使一种语言的问题能够通过另一种语言的答案内容得到回答。我们构建了一个大规模的数据集，该数据集基于TyDi QA中缺乏相同语言答案的问题。我们的任务公式称为跨语言开放检索问答（XOR QA），包括来自7种不同非英语语言的40k个信息检索问题。基于这个数据集，我们介绍了三个新任务，涉及使用多语言和英语资源进行跨语言文档检索。我们使用最先进的机器翻译系统和跨语言预训练模型建立基线。实验结果表明，XOR问答是一项具有挑战性的任务，它将促进多语言问答新技术的发展。我们的数据和代码可在https://nlp.cs.washington.edu/xorqa.</pre></li>
<li><a href="https://arxiv.org/abs/1909.00361">Cross-Lingual Machine Reading Comprehension</a> (EMNLP2019)
<pre>尽管社区在机器阅读理解（MRC）任务方面取得了很大的进展，但以前的工作大多是解决基于英语的MRC问题，而在其他语言方面的工作很少，主要是因为缺乏大规模的训练数据。在本文中，我们提出了非英语语言的跨语言机器阅读理解（CLMRC）任务。首先，我们为CLMRC任务提供了几种简单易行的反译方法。然而，将答案准确地对齐到另一种语言是困难的，并且可能会引入额外的噪音。在此背景下，我们提出了一种新的模型Dual-BERT，它利用丰富的资源语言（如英语）提供的大规模训练数据，在双语环境中学习短文和问题之间的语义关系，然后利用所学知识提高低资源语言的阅读理解能力。我们在两个中文机器阅读理解数据集CMRC 2018和DRCD上进行了实验。结果表明，与各种最先进的系统相比，CLMRC任务有了很大程度的一致和显著的改进，这表明了CLMRC任务的潜力。现有资源：https://github.com/ymcui/Cross-Lingual-MRC</pre></li>
<li><a href="https://arxiv.org/abs/1909.09587">Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model</a>
<pre>由于收集每种语言的训练数据是不可行的，因此跨语言迁移学习越来越受到人们的关注。本文采用多语种语料库预训练的语言表征模型，系统地探讨了阅读理解任务中的零镜头跨语言迁移学习。实验结果表明，使用预先训练好的语言表示法，零镜头学习是可行的，并且不需要将源数据翻译成目标语言，甚至会降低性能。我们进一步探讨了模型在零镜头设置下学习到了什么。</pre></li>
<li><a href="https://arxiv.org/abs/1910.04659">Multilingual Question Answering from Formatted Text applied to Conversational Agents</a>
<pre>语言模型（如BERT、XLNet等）的最新进展使得在阅读理解等复杂NLP任务上的表现超过了人类。然而，用于培训的标记数据集大多以英语提供，这使得很难确认其他语言的进步。幸运的是，模型现在已经在数百种语言的未标记数据上进行了预训练，并展示了有趣的从一种语言到另一种语言的转换能力。在本文中，我们证明了多语言BERT自然能够将抽取式问答任务（eQA）从英语迁移到其他语言。更具体地说，它的表现优于此前最为人所知的转移到日本和法国的基线。此外，使用最近发布的大型eQA法语数据集，我们能够进一步证明：（1）零射击转移提供的结果非常接近于目标语言的直接训练；（2）转移和目标训练相结合是总体上的最佳选择。最后，我们给出了一个实际应用：一个名为Kate的多语言会话代理，它可以直接从内部网页面的内容中用多种语言回答与人力资源相关的问题。</pre></li>
<li><a href="https://arxiv.org/abs/1910.05040">BiPaR: A Bilingual Parallel Dataset for Multilingual and Cross-lingual Reading Comprehension on Novels</a> (EMNLP2019)
<pre>本文介绍了一个支持多语种和跨语种阅读理解的双语并行新型机器阅读理解（MRC）数据集BiPaR。BiPaR与现有阅读理解数据集的最大区别在于，BiPaR中的每个三元组（段落、问题、答案）都是用两种语言并行编写的。我们从中英文小说中收集了3667个双语平行段落，通过众包人员按照严格的质量控制程序构建了14668个平行问答对。我们深入分析了BiPaR，发现BiPaR在问题前缀、答案类型以及问题和段落之间的关系方面提供了良好的多样性。我们还观察到，回答小说问题需要阅读理解技巧，如共指消解、多句推理和理解内隐因果关系等。通过BiPaR，我们建立了单语、多语和跨语MRC基线模型。即使对于该数据集上相对简单的单语MRC，实验表明，就EM和F1分数而言，强大的BERT基线比人类落后30多个百分点，这表明BiPaR为小说的单语、多语和跨语言MRC提供了一个具有挑战性的测试平台。该数据集可在https://multinlp.github.io/BiPaR/.</pre></li>
<li><a href="https://arxiv.org/abs/1910.07475">MLQA: Evaluating Cross-lingual Extractive Question Answering</a>
<pre>问答（QA）模型显示了快速的进步，这得益于大型、高质量基准数据集的可用性。这种带注释的数据集收集起来既困难又昂贵，而且很少以英语以外的其他语言存在，这使得其他语言的培训QA系统具有挑战性。构建大型单语训练数据集的另一种方法是开发跨语言系统，该系统可以转换为目标语言，而不需要该语言的训练数据。为了发展这种系统，必须投资于高质量的多语文评价基准，以衡量进展情况。我们提出了MLQA，一个多方向一致的采掘QA评估基准，旨在促进这一领域的研究。MLQA包含7种语言的QA实例，即英语、阿拉伯语、德语、西班牙语、印地语、越南语和简体中文。它由超过12K个英语QA实例和5K个其他语言QA实例组成，每个QA实例平均在4种语言之间并行。MLQA是在维基百科文章中使用一种新的对齐上下文策略构建的，并作为现有抽取QA数据集的跨语言扩展。我们在MLQA上评估了当前最先进的跨语言表示，并提供了基于机器翻译的基线。在所有情况下，迁移结果都明显落后于培训语言表现。</pre></li>
<li><a href="https://arxiv.org/abs/2010.12008">Multilingual Synthetic Question and Answer Generation for Cross-Lingual Reading Comprehension</a>
<pre>我们提出了一种简单的方法，通过使用单一生成模型大规模生成多语言问答对。这些合成样本可用于改善目标语言上多语言QA模型的零炮性能。我们提出的生成模型的多任务训练只需要英语中的标记训练样本，因此不需要目标语言中的此类样本，这使得它适用于比有标记数据的语言多得多的语言。人类的评估表明，大多数这样的样本在语法上是正确和合理的。实验结果表明，我们提出的方法可以在XQuAD数据集上获得较大的增益，减少了不同语言上较小QA模型的零拍和监督性能之间的差距。</pre></li>
<li><a href="https://arxiv.org/abs/2010.12643">Synthetic Data Augmentation for Zero-Shot Cross-Lingual Question Answering</a>
<pre>再加上大规模数据集的可用性，深度学习体系结构使问答任务取得了快速进展。然而，这些数据集大部分是英文的，在非英文数据上进行评估时，最先进的多语言模型的性能要低得多。由于数据收集成本很高，为希望支持的每种语言获取带注释的数据是不现实的。我们提出了一种在不需要附加注释数据的情况下提高跨语言问答性能的方法，利用问题生成模型以跨语言方式生成合成样本。我们表明，所提出的方法允许显著优于仅在英语数据上训练的基线。我们报告了四个多语言数据集的最新技术：MLQA、XQuAD、SHADIT和PIAF（fr）。</pre></li>
<li><a href="https://arxiv.org/abs/2010.14271">Cross-lingual Machine Reading Comprehension with Language Branch Knowledge Distillation</a> (COLING2020)
<pre>跨语言机器阅读理解（CLMRC）仍然是一个具有挑战性的问题，因为缺乏大规模的低源语言注释数据集，如阿拉伯语、印地语和越南语。以前的许多方法使用翻译数据，将丰富的源语言（如英语）翻译为低源语言，作为辅助监督。然而，如何有效地利用翻译数据并减少翻译带来的噪音的影响仍然是一项艰巨的任务。在本文中，我们通过一种称为语言分支机器阅读理解（LBMRC）的新的增强方法来应对这一挑战并提高跨语言迁移性能。语言分支是一种语言的一组段落，与所有目标语言的问题成对出现。在LBMRC的基础上，我们训练了精通个人语言的多机器阅读理解（MRC）模型。然后，我们设计了一种多语言提取方法，将多语言分支模型中的知识融合到所有目标语言的单一模型中。将LBMRC与多语言蒸馏相结合，可以更好地抵抗数据噪声，从而提高模型的跨语言能力。同时，生成的单个多语言模型适用于所有目标语言，从而节省了多个模型的训练、推理和维护成本。在两个CLMRC基准上的大量实验清楚地表明了我们提出的方法的有效性。</pre></li>
<li><a href="https://arxiv.org/abs/2007.15207">MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering</a> [<a href="https://github.com/apple/ml-mkqa">github</a>]
<pre>跨语言建模的进展取决于具有挑战性、现实性和多样性的评估集。我们介绍了多语言知识问答（MKQA），这是一个开放领域的问答评估集，包含10k个问答对，跨26种类型多样的语言排列（总共260k个问答对）。答案基于精心策划的、独立于语言的数据表示，使得结果在不同语言之间具有可比性，并且独立于特定语言的段落。该数据集拥有26种语言，提供了迄今为止最广泛的用于评估问答的语言。我们为生成式和抽取式问答测试了各种最先进的方法和基线，并在零镜头和翻译环境中接受了自然问题培训。结果表明，即使在英语中，该数据集也具有挑战性，但在资源不足的语言中尤其如此</pre></li>
<li><a href="https://arxiv.org/abs/2105.14115">Towards More Equitable Question Answering Systems: How Much More Data Do You Need?</a> (ACL2021)
<pre>英语问答（QA）已被广泛探索，但多语言数据集相对较新，有几种方法试图通过翻译和跨语言传输的数据扩充来弥合高资源语言和低资源语言之间的差距。在这个项目中，我们后退一步，研究哪些方法使我们能够最大限度地利用现有资源，以生产多种语言的QA系统。具体而言，我们进行了广泛的分析，以衡量通过自动翻译和上下文问答对排列增强的少数镜头方法的有效性。此外，我们对未来的数据集开发工作提出建议，更好地利用固定的注释预算，目标是增加QA数据集和系统的语言覆盖率。复制我们实验的代码和数据可在此处获得：https://github.com/NavidRajabi/EMQA.</pre></li>
<li><a href="https://arxiv.org/abs/2104.09696">X-METRA-ADA: Cross-lingual Meta-Transfer Learning Adaptation to Natural Language Understanding and Question Answering</a> (NAACL2021)
<pre>多语言模型，如M-BERT和XLM-R，由于其零次跨语言迁移学习能力而越来越受欢迎。然而，对于不同类型的语言和不同的基准，它们的泛化能力仍然不一致。最近，元学习作为一种在低资源情景下促进迁移学习的有前途的技术受到了关注：特别是自然语言理解中的跨语言迁移。在这项工作中，我们提出了X-METRA-ADA，一种适用于NLU的跨语言元迁移学习适应方法。我们的方法采用MAML，一种基于优化的元学习方法，来学习适应新的语言。我们在两个具有挑战性的跨语言NLU任务上广泛评估了我们的框架：面向多语言任务的对话和类型多样的问题回答。我们表明，我们的方法优于简单的微调，在大多数语言的两个任务上都达到了有竞争力的性能。我们的分析表明，X-METRA-ADA可以利用有限的数据进行更快的适应。</pre></li>
<li><a href="https://arxiv.org/abs/1904.09679">Investigating Prior Knowledge for Challenging Chinese Machine Reading Comprehension</a> (TACL)
<pre>机器阅读理解任务要求机器读者回答与给定文档相关的问题。在本文中，我们提出了第一个自由形式的多项选择中文机器阅读理解数据集（C^3），其中包含13369个文档（对话或更正式的混合体裁文本）及其相关的19577个自由形式选择题，这些问题是从汉语作为第二语言的考试中收集的。我们对这些现实问题所需的先验知识（即语言、特定领域和一般世界知识）进行了全面分析。我们实施了基于规则和流行的神经方法，发现在最佳性能模型（68.5%）和人类读者（96.0%）之间仍然存在显著的性能差距，特别是在需要先验知识的问题上。我们进一步研究了基于翻译的英语相关数据集的干扰因素合理性和数据扩充对模型性能的影响。我们预计C^3将给现有系统带来巨大挑战，因为回答86.8%的问题需要随附文档内外的知识，我们希望C^3可以作为一个平台，研究如何利用各种先验知识更好地理解给定的书面或口头文本。C^3可在https://dataset.org/c3/.</pre></li>
<li><a href="https://arxiv.org/abs/1912.09723">SberQuAD - Russian Reading Comprehension Dataset: Description and Analysis</a>
<pre>斯伯夸德——一个用俄语大规模模拟斯坦福队的组织——是一个有价值的资源，但尚未正确地呈现给科学界。我们通过提供描述、全面分析和基线实验结果来填补这一空白。</pre></li>
<li><a href="https://arxiv.org/abs/2004.11142">DuReaderrobust: A Chinese Dataset Towards Evaluating the Robustness of Machine Reading Comprehension Models</a>
<pre>机器阅读理解（MRC）是自然语言处理中的一项重要任务，并取得了显著的进展。然而，大多数神经MRC模型仍然远远不够健壮，在实际应用中不能很好地推广。为了全面验证MRC模型的鲁棒性和泛化性，我们引入了一个真实的中文数据集——DuReader_robust。本文从三个方面对MRC模型进行了评价：过敏感、过稳定和泛化。与以前的工作相比，DuReader_robust中的实例是自然文本，而不是经过修改的非自然文本。它提出了将MRC模型应用于实际应用时的挑战。实验结果表明，MRC模型在挑战测试集上表现不佳。此外，我们还分析了现有模型在挑战测试集上的行为，这可能为未来的模型开发提供建议。数据集和代码可在https://github.com/baidu/DuReader.</pre></li>
<li><a href="https://arxiv.org/abs/1909.00109">Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension</a> (EMNLP2019)
<pre>阅读理解模型已成功应用于抽取文本答案，但如何将这些模型推广到抽象数字答案尚不清楚。我们使基于伯特的阅读理解模型能够进行轻量级的数值推理。我们用一组预定义的可执行“程序”来扩充模型，这些程序包括简单的算法和提取。模型可以选择一个程序并执行它，而不必学习直接操纵数字。在最近的段落离散推理（DROP）数据集上，我们通过添加浅层程序显示了33%的绝对改进，该数据集旨在挑战阅读理解模型。该模型可以在数学单词问题设置中学习预测新操作（Roy和Roth，2015），只需很少的训练示例。</pre></li>
<li><a href="https://arxiv.org/abs/2101.00438">Few-Shot Question Answering by Pretraining Span Selection</a> (ACL2021)
<pre>在几个问答基准中，经过预训练的模型通过微调100000个带注释的问题和答案，达到了人类平等。我们探索了更现实的少数镜头设置，只有几百个训练示例可用，并观察到标准模型表现不佳，突出了当前训练前目标和问答之间的差异。我们提出了一种新的针对问答的预训练方案：重复跨度选择。给定一个具有多组重复跨距的通道，我们在每组中屏蔽除一个以外的所有重复跨距，并要求模型在通道中为每个屏蔽跨距选择正确的跨距。遮罩的范围将替换为一个特殊的标记，该标记被视为问题表示，稍后在微调过程中用于选择答案范围。由此产生的模型在多个基准上获得了令人惊讶的好结果（例如，72.7 F1在班上，只有128个训练示例），同时在高资源环境下保持了竞争力。</pre></li>
<li><a href="https://arxiv.org/abs/2009.13570">DialoGLUE: A Natural Language Understanding Benchmark for Task-Oriented Dialogue</a> [<a href="https://evalai.cloudcv.org/web/challenges/challenge-page/708/overview">website</a>]
<pre>以任务为导向的对话研究的一个长期目标是能够灵活地使对话模式适应新的领域。为了推进这一方向的研究，我们引入了DialoGLUE（对话语言理解评估），这是一个公共基准，由7个面向任务的对话数据集组成，涵盖4个不同的自然语言理解任务，旨在鼓励基于表征的迁移、领域适应和，和样本有效的任务学习。我们发布了几个强大的基线模型，通过在大型开放域对话语料库上进行预训练和任务自适应自我监督训练，展示了在香草伯特体系结构上的性能改进以及7项任务中5项任务的最新结果。通过DialoGLUE基准、基线方法和我们的评估脚本，我们希望促进朝着开发更通用的面向任务的对话模型的目标取得进展。</pre></li>
<li><a href="https://arxiv.org/abs/2104.10810">A Short Survey of Pre-trained Language Models for Conversational AI-A NewAge in NLP</a>
<pre>构建一个能够与人类自然通信的对话系统是基于agent计算的一个具有挑战性但有趣的问题。这一领域的快速发展通常受到长期存在的数据匮乏问题的阻碍，因为这些系统需要从数量不足的任务特定数据集中学习语法、语法、决策和推理。最近引入的预训练语言模型有可能解决数据稀缺的问题，并通过生成上下文化的单词嵌入带来相当大的优势。这些模型被认为是NLP中ImageNet的对应模型，并已证明能够捕获语言的不同方面，如层次关系、长期依赖性和情感。在这篇简短的调查报告中，我们讨论了预训练语言模型领域的最新进展。我们还讨论了如何利用这些语言模型的优势来设计更具吸引力和更雄辩的会话代理。因此，本文旨在确定这些预先训练的模型是否能够克服与对话系统相关的挑战，以及如何利用它们的架构来克服这些挑战。还审议了对话系统领域的公开挑战。</pre></li>
<li><a href="https://arxiv.org/abs/2106.01541">MPC-BERT: A Pre-Trained Language Model for Multi-Party Conversation Understanding</a> (ACL2021)
<pre>近年来，用于多方会话（MPC）的各种神经模型在收件人识别、说话人识别和反应预测等任务上取得了令人印象深刻的改进。然而，现有的MPC方法通常都是将对话者和话语分别表示出来，而忽略了MPC固有的复杂结构，这种结构可以提供关键的对话者和话语语义，从而增强会话理解过程。为此，我们提出了MPC-BERT，这是一个预训练的MPC理解模型，它考虑了在一个统一的模型中学习谁对谁说什么，以及几个精心设计的自我监督任务。具体地说，这些任务一般可分为：（1）对话者结构建模，包括应答话语识别、同一说话人搜索和指针一致性区分；（2）话语语义建模，包括屏蔽共享话语恢复和共享节点检测。我们评估了MPC-BERT的三个下游任务，包括收件人识别、说话人识别和响应选择。实验结果表明，MPC-BERT在两个基准测试中，在所有三个下游任务上都取得了新的最新性能，大大优于以前的方法。</pre></li>
<li><a href="https://arxiv.org/abs/1907.03040">BERT-DST: Scalable End-to-End Dialogue State Tracking with Bidirectional Encoder Representations from Transformer</a> (Interspeech2019)
<pre>对话状态跟踪（DST）中一个重要但很少解决的问题是动态本体（如电影、餐厅）和不可见的时隙值的可伸缩性。我们关注一个特定的情况，其中本体对于状态跟踪器是未知的，但是目标时隙值（除了none和dontcare）在训练期间可能是不可见的，可以在对话上下文中作为词段找到。以前的方法通常依赖于从n-gram枚举或时隙标记器输出生成候选项，这可能是低效的或受到错误传播的影响。我们提出了BERT-DST，一种端到端对话状态跟踪器，它直接从对话上下文中提取时隙值。我们使用BERT作为对话上下文编码器，其上下文化语言表示适用于可伸缩DST，以从其语义上下文中识别时隙值。此外，我们在所有时隙中使用编码器参数共享，具有两个优点：（1）参数数量不会随本体线性增长。（2） 语言表示知识可以在插槽之间传递。实证评估表明，具有跨时隙参数共享的BERT-DST在基准可扩展DST数据集Sim-M和Sim-R上的性能优于先前的工作，并在标准DSTC2和WOZ 2.0数据集上实现了具有竞争力的性能。</pre></li>
<li><a href="https://arxiv.org/abs/1908.01946">Dialog State Tracking: A Neural Reading Comprehension Approach</a>
<pre>对话状态跟踪用于估计给定所有先前对话的对话的当前信任状态。另一方面，机器阅读理解侧重于构建阅读文本段落和回答需要理解段落的问题的系统。我们将对话状态跟踪描述为阅读理解任务，以在阅读会话上下文后回答问题$what\是\当前\对话的\状态\是什么？$。与传统的状态跟踪方法不同，在传统的状态跟踪方法中，对话状态通常被预测为本体中所有可能时隙值的闭合集合上的分布，我们的方法使用一个简单的基于注意的神经网络来指向对话中的时隙值。在MultiWOZ-2.0跨域对话数据集上的实验表明，与以前更复杂的方法相比，我们的简单系统可以获得类似的精度。通过利用上下文单词嵌入方面的最新进展，添加一个明确跟踪插槽值是否应转入下一轮的模型，并将我们的方法与依赖于封闭集词汇表的传统联合状态跟踪方法相结合，我们可以在标准测试分割中获得47.33\%$的联合目标准确度，超过当前最先进水平11.75\%$**。</pre></li>
<li><a href="https://arxiv.org/abs/1910.12995">A Simple but Effective BERT Model for Dialog State Tracking on Resource-Limited Systems</a> (ICASSP2020)
<pre>在面向任务的对话系统中，对话状态跟踪（DST）的目标是从对话历史记录中监控对话的状态。最近，许多基于深度学习的方法被提出用于此任务。尽管DST的当前神经架构具有令人印象深刻的性能，但它们通常都经过了大量的工程设计，而且概念复杂，因此很难在生产环境中实现、调试和维护它们。在这项工作中，我们提出了一个简单而有效的基于BERT的DST模型。除了简单之外，我们的方法还具有许多其他优点：（a）参数的数量不随本体大小而增长（b）模型可以在领域本体可能动态变化的情况下运行。实验结果表明，我们的基于伯特的模型在很大程度上优于以前的方法，在标准WoZ 2.0数据集上获得了最新的结果。最后，为了使模型对于资源受限的系统足够小和快速，我们应用了知识提取方法来压缩我们的模型。最终的压缩模型可获得与原始模型相当的结果，同时更小8倍，更快7倍。</pre></li>
<li><a href="https://arxiv.org/abs/2002.00181">Fine-Tuning BERT for Schema-Guided Zero-Shot Dialogue State Tracking</a>
<pre>我们将在对话系统技术挑战8（DSTC8）的第4轨道上介绍我们的工作。DSTC8 Track 4旨在在零快照设置下执行对话状态跟踪（DST），其中模型需要在给定这些目标API的模式定义的不可见服务API上进行泛化。DST作为许多虚拟助手（如Siri、Alexa和Google Assistant）的核心，跟踪用户的目标和对话历史中发生的事情，主要包括意图预测、时隙填充和用户状态跟踪，测试模型的自然语言理解能力。最近，预训练语言模型已经取得了最新的成果，并在各种NLP任务上表现出令人印象深刻的泛化能力，这为语言理解提供了一种很有希望的零镜头学习方法。在此基础上，我们提出了一种模式引导的零镜头对话状态跟踪（SGP-DST）模式，该模式通过微调BERT（最流行的预训练语言模型之一）实现。SGP-DST系统包含四个模块，分别用于意图预测、时隙预测、时隙传输预测和用户状态汇总。根据官方评估结果，我们的SGP-DST（团队12）在联合目标准确性（排名提交的主要评估指标）方面排名第三，在25个参赛团队中，在规定的F1时段排名第一。</pre></li>
<li><a href="https://arxiv.org/abs/2002.02450">Goal-Oriented Multi-Task BERT-Based Dialogue State Tracker</a>
<pre>对话状态跟踪（DST）是Alexa或Siri等虚拟助理的核心组件。为了完成各种任务，这些助理需要支持越来越多的服务和API。第八次对话系统技术挑战的模式引导状态跟踪跟踪强调了不可见服务的DST问题。组织者介绍了带有多域对话的模式引导对话（SGD）数据集，并发布了零镜头对话状态跟踪模型。在这项工作中，我们提出了一个面向目标的基于多任务的对话状态跟踪器（GOLOMB），其灵感来源于阅读理解问答系统的体系结构。该模型“查询”对话历史，其中包含插槽和服务的描述以及插槽的可能值。这允许在多域对话中传输插槽值，并能够扩展到看不见的插槽类型。我们的模型在SGD数据集上实现了53.97%的联合目标准确率，优于基线模型。</pre></li>
<li><a href="https://arxiv.org/abs/2004.10663">Dialogue State Tracking with Pretrained Encoder for Multi-domain Trask-oriented Dialogue Systems</a>
<pre>我们提出了一种称为显式模块分解（EMD）的快速可扩展体系结构，其中我们结合了基于分类和基于提取的方法，并设计了四个模块（用于分类和序列标记）来联合提取对话状态。基于MultiWoz 2.0数据集的实验结果验证了我们提出的模型在复杂度和可扩展性方面的优越性，特别是在多域对话和多回合话语的情况下。</pre></li>
<li><a href="https://arxiv.org/abs/2005.00891">Zero-Shot Transfer Learning with Synthesized Data for Multi-Domain Dialogue State Tracking</a> (ACL2020)
<pre>用于多域对话状态跟踪的零镜头转移学习可以允许我们处理新的域，而不会导致数据采集的高成本。该文提出了一种新的用于对话状态跟踪的零短转移学习技术，该技术将领域内的训练数据全部由一个抽象的对话模型和领域本体合成。我们证明，在MultiWOZ 2.1数据集上，通过合成数据增加数据可以提高交易模型和基于伯特的SUMBT模型的零炮学习精度。我们表明，在SUMBT模型上仅使用合成的域内数据进行训练可以达到使用完整训练数据集获得的精度的2/3左右。我们将各领域的零射击学习水平平均提高了21%。</pre></li>
<li><a href="https://arxiv.org/abs/2008.12335">A Fast and Robust BERT-based Dialogue State Tracker for Schema-Guided Dialogue Dataset</a> (KDD2020 WS)
<pre>对话状态跟踪（DST）是面向目标对话系统中最关键的模块之一。在本文中，我们介绍了FastSGT（Fast Schema Guided Tracker，快速模式引导跟踪器），一种用于面向目标对话系统中状态跟踪的快速、鲁棒的基于BERT的模型。该模型是为模式引导对话（SGD）数据集设计的，该数据集包含所有实体的自然语言描述，包括用户意图、服务和插槽。该模型包含两个遗留程序，用于处理当前用户话语中未明确提及的值的提取。它还在一些解码器中使用多头注意投影，以便更好地模拟编码器输出。在进行的实验中，我们将FastSGT与SGD数据集的基线模型进行了比较。我们的模型在计算和内存消耗方面保持了效率，同时显著提高了准确性。此外，我们还进行了烧蚀研究，测量模型不同部分对其性能的影响。我们还展示了数据扩充在不增加计算资源量的情况下提高精度的有效性。</pre></li>
<li><a href="https://arxiv.org/abs/2104.04466">Knowledge-Aware Graph-Enhanced GPT-2 for Dialogue State Tracking</a>
<pre>对话状态跟踪是面向多领域任务的对话系统的核心，负责从用户话语中提取信息。我们提出了一种新的混合体系结构，该结构通过从图形注意网络派生的表示来增强GPT-2，从而允许对时隙值进行因果顺序预测。模型体系结构捕获跨域的插槽间关系和依赖关系，否则这些关系和依赖关系可能会在顺序预测中丢失。我们报告了MultiWOZ 2.0在强GPT-2基线下状态跟踪性能的改进，并研究了一种简化的稀疏训练场景，其中DST模型仅在会话级注释上进行训练，但在回合级进行评估。我们进一步报告了详细的分析，以证明DST中图形模型的有效性，通过显示所提出的图形模块捕获时隙间的依赖关系，并改进对多个域通用值的预测。</pre></li>
<li><a href="https://arxiv.org/abs/2106.08723">Coreference Augmentation for Multi-Domain Task-Oriented Dialogue State Tracking</a> (Interspeech2021)
<pre>对话状态跟踪（DST）是通过估计给定对话历史的信念状态来推断用户目标的过程，在面向任务的对话系统中起着至关重要的作用。在多轮会话中观察到的共指现象没有被现有的DST模型所解决，导致了次优性能。在本文中，我们提出了共指对话状态跟踪器（CDST），它显式地模拟了共指特征。特别是，在每一轮中，所提出的模型联合预测共引用域时隙对，并从对话上下文中提取共引用值。在MultiWOZ 2.1数据集上的实验结果表明，该模型达到了56.47%的联合目标准确率。</pre></li>
<li><a href="https://arxiv.org/abs/2004.06871">ToD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogues</a> (EMNLP2020)
<pre>一般文本和任务型对话之间语言模式的潜在差异使得现有的预先训练的语言模式在实践中不太有用。在这项工作中，我们统一了九个面向人类和多回合任务的对话数据集，用于语言建模。为了在培训前更好地建模对话行为，我们将用户和系统标记合并到屏蔽语言建模中。我们提出了一个对比目标函数来模拟反应选择任务。我们预先训练的面向任务的对话BERT（TOD-BERT）在四个下游面向任务的对话应用程序（包括意图识别、对话状态跟踪、对话行为预测和反应选择）上优于类似BERT的强基线。我们还表明，TOD-BERT具有更强的少镜头能力，可以缓解面向任务对话的数据稀缺问题。</pre></li>
<li><a href="https://arxiv.org/abs/2106.02227">Conversations Are Not Flat: Modeling the Dynamic Information Flow across Dialogue Utterances</a> (ACL2021)
<pre>目前，基于大规模预训练语言模型的开放域对话模型能够根据历史语境生成可接受的反应。然而，他们通常将对话历史直接连接起来作为模型输入来预测反应，我们称之为平面模式，忽略了对话话语间的动态信息流。在这项工作中，我们提出了DialoFlow模型，在该模型中，我们引入了一种动态流机制来模拟语境流，并设计了三个训练目标，通过解决大规模预训练中每个话语所带来的语义影响来捕获对话话语中的信息动态。在多参考Reddit数据集和DailyDialog数据集上的实验表明，我们的DialoFlow在对话生成任务上明显优于DialoGPT。此外，我们提出了Flow score，这是一种基于预先训练的DialoFlow的交互式人机对话质量评估的有效自动度量，它与11个聊天机器人之间的人工评分呈现出高度的聊天机器人水平相关性（$r=0.9$）。代码和预先培训的模型将公开\脚注{\url{https://github.com/ictnlp/DialoFlow}}</pre></li>
<li><a href="https://arxiv.org/abs/1908.04812">Domain Adaptive Training BERT for Response Selection</a>
<pre>我们主要研究基于检索的对话系统中的多轮应答选择。在本文中，我们将Transformer（BERT）强大的预训练语言模型双向编码器表示用于多回合对话系统，并提出了一种在特定领域语料库上高效的后训练方法。虽然BERT很容易被各种NLP任务采用，并且优于每个任务的先前基线，但如果任务语料库过于关注某个领域，它仍然有局限性。对特定领域语料库（如Ubuntu语料库）的后期培训有助于模型培训一般语料库（如英语维基百科）中未出现的上下文表示和单词。实验结果表明，我们的方法在两个响应选择基准（即Ubuntu语料库V1，Advisory Corpus V1）上实现了新的技术水平，在两个基准上的性能分别提高了5.9%和6%R@1.</pre></li>
<li><a href="https://arxiv.org/abs/2004.03588">Speaker-Aware BERT for Multi-Turn Response Selection in Retrieval-Based Chatbots</a>
<pre>在本文中，我们研究了在基于检索的聊天机器人中使用预先训练的语言模型进行多轮反应选择的问题。提出了一种新的说话人感知BERT（SA-BERT）模型，使该模型能够感知说话人的变化信息，这是多话轮对话的一个重要的固有特性。此外，还提出了一种说话人感知的解纠缠策略来处理纠缠对话。该策略根据说话人的信息选择少量最重要的话语作为过滤语境。最后，进行领域自适应，将领域内的知识整合到预先训练的语言模型中。在五个公共数据集上的实验表明，我们提出的模型在所有指标上都比现有模型有很大的优势，并且在多轮响应选择方面取得了新的最先进的性能。</pre></li>
<li><a href="https://arxiv.org/abs/1912.08555">Curriculum Learning Strategies for IR: An Empirical Study on Conversation Response Ranking</a> (ECIR2020)
<pre>传统上，神经排序模型是在一系列随机批次上进行训练，从整个训练集中均匀取样。最近的研究表明，课程学习可以通过在训练过程中从易到难的批次进行非均匀采样，从而提高神经模型的有效性。在神经信息检索（IR）的背景下，课程学习尚未探索，因此（1）如何衡量训练实例的难度，以及（2）如何在训练过程中从容易的实例过渡到困难的实例尚不清楚。为了解决这两个挑战并确定课程学习是否有利于神经排名模型，我们需要大规模的数据集和检索任务，以便我们进行广泛的实验。为此，我们求助于会话响应排序的任务：根据会话历史对响应进行排序。为了应对挑战（1），我们探索了基于不同输入空间的评分函数来衡量对话的难度。为了应对挑战（2），我们评估了不同的起搏功能，这些功能决定了我们从容易的情况到困难的情况的速度。我们发现，总的来说，通过对训练数据进行智能排序（即通过执行课程学习），我们可以将检索效率提高2%。</pre></li>
<li><a href="https://arxiv.org/abs/2004.04494">MuTual: A Dataset for Multi-Turn Dialogue Reasoning</a> (ACL2020)
<pre>近年来，非任务型对话系统取得了巨大成功，这得益于大量可访问的对话数据和深度学习技术的发展。在给定的环境下，当前的系统能够产生相关且流畅的响应，但有时由于推理能力较弱而出现逻辑错误。为了促进对话推理的研究，我们引入了一个新的多回合对话推理数据集MuTual，该数据集由8860个基于中国学生英语听力理解考试的人工注释对话组成。与以前的非任务导向对话系统基准相比，MuTual更具挑战性，因为它需要一个能够处理各种推理问题的模型。实证结果表明，最先进的方法仅达到71%，远远落后于94%的人类绩效，这表明推理能力还有很大的提高空间。互惠保险可在https://github.com/Nealcly/MuTual.</pre></li>
<li><a href="https://arxiv.org/abs/2004.03760">DialBERT: A Hierarchical Pre-Trained Model for Conversation Disentanglement</a>
<pre>解纠缠是一个多个对话同时发生在同一频道的问题，听者应该决定哪一个话语是他将回应的对话的一部分。我们提出了一个新的模型，名为DialBERT（DialBERT），它将本地和全局语义集成到单个消息流中，以分离混合在一起的会话。我们使用BERT在话语层捕获每个话语对中的匹配信息，并使用BiLSTM聚合和合并上下文层信息。根据F1成绩，与BERT相比，参数仅增加了3%，提高了12%。该模型在IBM提出的一个新数据集上实现了最先进的结果，大大超过了以前的工作。</pre></li>
<li><a href="https://arxiv.org/abs/2010.11140">Generalized Conditioned Dialogue Generation Based on Pre-trained Language Model</a>
<pre>条件性对话一代缺乏标记的回应。在这项工作中，我们利用标记的非对话文本数据相关的条件，这是更容易收集。我们提出了一种多任务学习方法来利用标记对话和文本数据。这三个任务共同优化了同一个预先训练好的转换器——标记对话数据上的条件对话生成任务、标记文本数据上的条件语言编码任务和条件语言生成任务。实验结果表明，通过利用标记文本，我们的方法优于最先进的模型，并且与以前利用文本数据的方法相比，它在性能上也得到了更大的改进。</pre></li>
<li><a href="https://arxiv.org/abs/2106.06169">BoB: BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data</a> (ACL2021)
<pre>保持一致的人物角色对于对话代理至关重要。尽管已经取得了巨大的进步，但有限的标注角色密集型数据仍然是训练健壮且一致的基于角色的对话模型的障碍。在这项工作中，我们展示了如何通过一个新的BERT-over-BERT（BoB）模型将基于人物角色的对话生成分解为两个子任务来解决这些挑战。具体来说，该模型由一个基于伯特的编码器和两个基于伯特的解码器组成，其中一个解码器用于响应生成，另一个用于一致性理解。特别是，为了从大规模非对话推理数据中学习一致性理解能力，我们以一种不可能的方式训练第二个解码器。在不同的有限数据设置下，自动和人工评估都表明，该模型在响应质量和角色一致性方面优于强基线。</pre></li>
<li><a href="https://arxiv.org/abs/2012.00958">Interactive Teaching for Conversational AI</a> (NeurIPS2020 WS)
<pre>当前的对话式人工智能系统旨在理解一组预先设计的请求并执行相关动作，这限制了它们自然进化并基于人类交互进行调整。受儿童如何与成人互动学习第一语言的启发，本文描述了一种新的可教人工智能系统，该系统能够直接从终端用户使用实时交互式教学会话学习新的语言知识，称为概念。建议的设置使用三种模型：a）在现场对话互动过程中自动识别理解上的差距，b）从与用户的现场互动中学习对此类未知概念的各自解释，以及c）管理专为互动教学环节定制的课堂子对话。我们提出了最先进的基于变压器的模型神经结构，在预训练模型的基础上进行了微调，并显示了各个组件的精度改进。我们证明了该方法在构建更具适应性和个性化的语言理解模型方面具有很好的应用前景。</pre></li>
<li><a href="https://arxiv.org/abs/1911.00473">BERT Goes to Law School: Quantifying the Competitive Advantage of Access to Large Legal Corpora in Contract Understanding</a>
<pre>在特定领域语料库上微调语言模型（如BERT）已被证明在科学论文和生物医学文本等领域很有价值。在本文中，我们证明了对法律文档进行微调同样可以对法律领域中的NLP任务提供有价值的改进。证明这一结果对于分析商业协议具有重要意义，因为获取大型法律语料库由于其保密性质而具有挑战性。因此，我们表明，对于商业应用和分析合同的学术研究而言，访问大型法律语料库是一种竞争优势。</pre></li>
</ul>
<h2 id="slot-filling-and-intent-detection">Slot filling and Intent Detection</h2>
<ul>
<li><a href="https://www.aclweb.org/anthology/D19-1214/">A Stack-Propagation Framework with Token-Level Intent Detection for Spoken Language Understanding</a> (EMNLP2019)</li>
<li><a href="https://arxiv.org/abs/1902.10909">BERT for Joint Intent Classification and Slot Filling</a>
<pre>意图分类和时隙填充是自然语言理解的两个基本任务。它们经常受到小规模的人工标注训练数据的影响，导致泛化能力差，特别是对于稀有词。最近，一种新的语言表示模型BERT（来自变形金刚的双向编码器表示）促进了大规模未标记语料库上深度双向表示的预训练，并在简单微调后为各种自然语言处理任务创建了最先进的模型。然而，还没有太多的努力探索自然语言理解的伯特。在这项工作中，我们提出了一个基于BERT的联合意图分类和时隙填充模型。实验结果表明，与基于注意的递归神经网络模型和时隙选通模型相比，我们提出的模型在几个公共基准数据集上的意图分类精度、时隙填充F1和句子级语义框架精度都有显著提高。</pre></li>
<li><a href="https://arxiv.org/abs/2010.03880">A Co-Interactive Transformer for Joint Slot Filling and Intent Detection</a> (ICASSP2021)
<pre>意图检测和时隙填充是构建口语理解（SLU）系统的两个主要任务。这两项任务密切相关，一项任务的信息可以用于另一项任务。以往的研究要么单独地对两个任务进行建模，要么只考虑从意图到时隙的单个信息流。以前的方法都没有同时对两个任务之间的双向连接建模。在本文中，我们提出了一个共同的互动变压器，以考虑两个任务之间的交叉影响。在VANILA变压器中没有采用自关注机制，而是提出了一种共同交互模块，通过在两个相关任务之间建立双向连接来考虑交叉影响。此外，所提出的交互模块可以堆叠起来，以增加相互之间的特征。在两个公共数据集（SNIPS和ATIS）上的实验结果表明，我们的模型实现了最先进的性能，并有显著的改进（+3.4%和+0.9%）。大量的实验证明，我们的模型成功地捕捉到了相互作用的知识。</pre></li>
<li><a href="https://arxiv.org/abs/2104.05763">Few-shot Intent Classification and Slot Filling with Retrieved Examples</a> (NAACL2021)
<pre>很少有镜头学习出现在重要的实际场景中，例如当自然语言理解系统需要为新兴的、资源稀缺的领域学习新的语义标签时。在本文中，我们探讨了在少数镜头设置下基于检索的意图分类和插槽填充任务的方法。基于检索的方法基于检索索引中与输入相似的标记示例进行预测，因此只需更改索引即可适应新领域，而无需重新训练模型。然而，将此类方法应用于具有复杂标签空间（如插槽填充）的任务并非易事。为此，我们提出了一种跨度级检索方法，该方法通过一个新的批处理softmax目标学习具有相同标签的跨度的相似上下文表示。在推断时，我们使用检索到的跨度的标签来构造具有最高聚合分数的最终结构。在CLINC和SNIPS基准上，我们的方法在不同的少数镜头设置下优于以前的系统。</pre></li>
<li><a href="https://arxiv.org/abs/1907.02884">Multi-lingual Intent Detection and Slot Filling in a Joint BERT-based Model</a>
<pre>意图检测和时隙填充是自然语言口语理解的两大支柱任务。常见的方法是在基于注意力的循环框架中采用联合深度学习架构。在这项工作中，我们的目标是利用成功的“无重复”模型来完成这些任务。我们介绍了Bert-Joint，即一个多语言联合文本分类和序列标记框架。在两个著名的英语基准测试上的实验评估表明，即使只有很少的注释数据可用，该模型也可以获得很好的性能。此外，我们为意大利语注释了一个新的数据集，我们观察到了类似的性能，而无需更改模型。</pre></li>
<li><a href="https://www.isca-speech.org/archive/Interspeech_2019/abstracts/1262.html">A Comparison of Deep Learning Methods for Language Understanding</a> (Interspeech2019)</li>
<li><a href="https://arxiv.org/abs/2004.13952">Data Augmentation for Spoken Language Understanding via Pretrained Models</a>
<pre>口语理解（SLU）模型的训练经常面临数据匮乏的问题。在本文中，我们提出了一种使用预训练语言模型的数据增强方法，以提高生成话语的可变性和准确性。此外，我们研究并提出了解决SLU中两个以前被忽视的数据稀缺的半监督学习场景的方案：i）丰富的本体：给出了具有大量有效对话行为的本体信息；ii）丰富的话语：大量未标记的话语可用。实验结果表明，我们的方法可以生成综合训练数据，提高语言理解模型在各种场景下的性能。</pre></li>
<li>[Few-Shot Intent Detection via Contrastive Pre-Training and Fine-Tuning] (EMNLP2021)</li>
<li><a href="https://arxiv.org/abs/2010.00760">STIL – Simultaneous Slot Filling, Translation, Intent Classification, and Language Identification: Initial Results using mBART on MultiATIS++</a> (AACL-IJCNLP2020) [<a href="https://github.com/amazon-research/stil-mbart-multiatis">github</a>]
<pre>时隙填充、翻译、意图分类和语言识别（STIL）是一项新提出的多语言自然语言理解（NLU）任务。通过同时执行插槽填充和翻译为单一输出语言（本例中为英语），下游系统组件的某些部分可以是单语的，从而降低开发和维护成本。使用多语言BART模型（Liu等人，2020）给出了结果，该模型使用MultiATIS++数据集对7种语言进行了微调。在未进行翻译的情况下，对于测试的语言，mBART的性能与当前最先进的系统（Xu等人（2020年）的跨语言BERT）相当，具有更好的平均意图分类准确率（96.07%对95.50%），但较差的平均时隙F1（89.87%对90.81%）。当执行同声翻译时，平均意图分类精度相对仅降低1.7%，平均时隙F1相对仅降低1.2%。</pre></li>
</ul>
<h2 id="analysis">Analysis</h2>
<ul>
<li><a href="https://arxiv.org/abs/1908.04755">Fine-grained Information Status Classification Using Discourse Context-Aware Self-Attention</a>
<pre>先前关于桥接回指识别的工作（Hou等人，2013a）将该问题视为学习细粒度信息状态（IS）的子任务。然而，这些系统严重依赖于许多手工制作的语言特征。在本文中，我们提出了一个用于细粒度IS分类的语篇上下文感知自我注意神经网络模型。在ISNotes语料库（Markert等人，2012年）上，我们使用上下文编码单词表示法（BERT）的模型（Devlin等人，2018年）在细粒度IS分类方面取得了新的最先进的性能，与Hou等人（2013a）相比，获得了4.1%的绝对整体准确度改进。更重要的是，我们还展示了桥接回指识别的3.9%F1改进，而不使用任何复杂的手工制作的语义特征来捕捉桥接现象。</pre></li>
<li><a href="https://arxiv.org/abs/1907.03750">Neural Aspect and Opinion Term Extraction with Mined Rules as Weak Supervision</a> (ACL2019)
<pre>缺乏标记的训练数据是基于神经网络的产品评论方面和观点术语提取的主要瓶颈。为了缓解这个问题，我们首先提出了一种基于依赖解析结果从现有训练样本中自动挖掘提取规则的算法。然后将挖掘出的规则用于标记大量辅助数据。最后，我们研究了训练过程来训练一个神经模型，该模型既可以从规则自动标注的数据中学习，也可以从人类精确标注的少量数据中学习。实验结果表明，尽管挖掘的规则本身由于灵活性有限而不能很好地执行，但人工标注的数据和规则标记的辅助数据的组合可以改进神经模型，使其实现优于或可与当前最新技术相媲美的性能。</pre></li>
<li><a href="https://www.aclweb.org/anthology/P19-1328">BERT-based Lexical Substitution</a> (ACL2019)</li>
<li><a href="https://arxiv.org/abs/1901.05287">Assessing BERT’s Syntactic Abilities</a>
<pre>我使用（1）自然发生的主谓一致刺激来评估最近引入的BERT模型捕捉英语句法现象的程度；（2） “无色绿色想法”主谓一致刺激，自然句子中的内容词被随机替换为具有相同词性和屈折变化的词；（3）为主谓一致和反身回指现象手工制作的刺激。伯特模型在所有情况下都表现出色。</pre></li>
<li><a href="https://arxiv.org/abs/2011.02417">Investigating Novel Verb Learning in BERT: Selectional Preference Classes and Alternation-Based Syntactic Generalization</a> (EMNLP2020 WS)
<pre>以往对深度学习模型句法能力的研究并没有针对语法泛化的强度与模型在训练过程中暴露的证据数量之间的关系。我们通过部署一种新的词汇学习范式来测试BERT在英语动词的两个方面的少量学习能力：交替和选择偏好。对于前者，我们在动词交替对中的单个框架上微调BERT，并询问模型是否期望新动词出现在其姊妹框架中。对于后者，我们在动词宾语的不完全选择网络上微调BERT，并询问它是否期望未经测试但似乎合理的动词/宾语对。我们发现，BERT在对一个新词进行微调后，仅在一两个实例之后就做出了强有力的语法概括。对于动词交替测试，我们发现该模型表现出与及物性偏见一致的行为：很少看到的动词预期会使用直接宾语，但看到直接宾语的动词预期不会出现不及物性。</pre></li>
<li><a href="https://arxiv.org/abs/1908.09892">Does BERT agree? Evaluating knowledge of structure dependence through agreement relations</a>
<pre>学习能够准确建模语义的表示是自然语言处理研究的一个重要目标。许多语义现象依赖于句法结构。最近的工作考察了训练前表征的最新模型（如BERT）在多大程度上捕捉了这种依赖结构的现象，但主要局限于英语中的一种现象：主语和动词之间的数字一致性。我们在一个新的跨26种语言的半自动管理数据集中评估了BERT对四种类型的结构相关协议关系的敏感性。我们表明，单语言和多语言的BERT模型总体上都很好地捕获了语法敏感的协议模式，但我们也强调了它们性能下降的特定语言环境。</pre></li>
<li><a href="https://arxiv.org/abs/1904.05255">Simple BERT Models for Relation Extraction and Semantic Role Labeling</a>
<pre>我们提出了简单的基于BERT的关系抽取和语义角色标注模型。近年来，通过结合词法和句法特征（如词性标记和依赖树），使用神经模型实现了最先进的性能。在本文中，对这两个任务的数据集进行的大量实验表明，在不使用任何外部特征的情况下，一个简单的基于BERT的模型可以实现最先进的性能。据我们所知，我们是第一个以这种方式成功应用BERT的公司。我们的模型为未来的研究提供了强有力的基线。</pre></li>
<li><a href="https://aclanthology.org/2020.coling-main.120/">Bridging the Gap in Multilingual Semantic Role Labeling: a Language-Agnostic Approach</a> (COLING2020)</li>
<li><a href="https://arxiv.org/abs/1910.14296">LIMIT-BERT : Linguistic Informed Multi-Task BERT</a> (EMNLP2020 Findings)
<pre>在这篇文章中，我们提出了一种语言信息多任务伯特（LIMIT-BERT），用于通过多任务学习（MTL）跨多个语言任务学习语言表示。LIMIT-BERT包括五个关键的语言语法和语义任务：词性（POS）标记、成分和依存句法分析、广度和依存语义角色标记（SRL）。此外，LIMIT-BERT还采用了语言学掩蔽策略：句法和语义短语掩蔽，掩蔽了与句法/语义短语对应的所有标记。与最近的多任务深层神经网络（MT-DNN）（Liu等人，2019）不同，我们的LIMIT-BERT是一种半监督方法，它提供了大量的语言任务数据，与BERT学习语料库一样。因此，LIMIT-BERT不仅提高了语言任务的性能，而且还受益于正则化效应和语言信息，从而产生更通用的表示，以帮助适应新的任务和领域。LIMIT-BERT在Propbank基准上的广度和依赖语义分析以及Penn Treebank上的依赖和成分句法分析方面都获得了最新的或具有竞争力的结果。</pre></li>
<li><a href="https://arxiv.org/abs/2010.05567">Joint Semantic Analysis with Document-Level Cross-Task Coherence Rewards</a>
<pre>共指消解和语义角色标记是NLP任务，它们捕获语义的不同方面，分别指示哪些表达式引用同一实体，以及表达式在句子中充当哪些语义角色。然而，它们通常是紧密相互依赖的，并且通常都需要自然语言理解。它们是否形成文档的连贯抽象表示？我们提出了一种用于英语联合共指消解和语义角色标注的神经网络结构，并训练图神经网络来模拟组合浅层语义图的“连贯性”。使用得到的一致性得分作为我们的联合语义分析器的奖励，我们使用强化学习来鼓励文档和语义注释之间的全局一致性。这导致了来自不同领域的多个数据集中的两项任务的改进，以及不同表现力的一系列编码器的改进，我们相信，这需要NLP中更全面的语义方法。</pre></li>
<li><a href="https://arxiv.org/abs/1907.06226">A Simple BERT-Based Approach for Lexical Simplification</a>
<pre>词汇简化（LS）的目的是将给定句子中的复杂单词替换为具有同等意义的简单替代词。近年来，无监督的词汇简化方法仅依赖于复杂词本身而不考虑给定的句子来生成候选替换，这将不可避免地产生大量虚假候选。我们提出了一个简单的LS方法，利用双向编码器表示从变压器（伯特），可以考虑给定的句子和复杂的词在生成候选替换复杂的词。具体地说，我们将原始句子中的复合词隐藏起来，以便输入到BERT中，从而预测隐藏的标记。预测结果将用作候选替换。尽管完全不受监督，但实验结果表明，与这些利用语言数据库和平行语料库的基线相比，我们的方法取得了明显的改进，在三个著名的基准上超过了最新水平12个精度点。</pre></li>
<li><a href="https://arxiv.org/abs/1909.04181">BERT-Based Arabic Social Media Author Profiling</a>
<pre>我们报告了在阿拉伯语作者分析和欺骗检测共享任务（APDA）背景下从社交媒体数据中检测年龄、语言多样性和性别的模型。我们建立了基于预先训练的变压器双向编码器（BERT）的简单模型。我们首先使用共享的任务发布数据对三个数据集中的每一个数据集上预先训练的BERT模型进行微调。然后，我们用内部的性别和方言数据来扩充共享任务数据，展示了扩充训练数据的效用。我们在共享任务测试数据上的最佳模型是通过在不同数据条件下训练的各种BERT模型的多数投票获得的。在这三项任务中，我们获得了54.72%的年龄准确率、93.75%的方言准确率、81.67%的性别准确率和40.97%的联合准确率。</pre></li>
<li><a href="https://arxiv.org/abs/1911.00637">Sentence-Level BERT and Multi-Task Learning of Age and Gender in Social Media</a>
<pre>社交媒体目前提供了一个了解我们生活的窗口，使我们能够了解来自不同地方、不同背景、年龄和性别的人如何使用语言。在这项工作中，我们利用一个新创建的带有基本真相年龄和性别标签的阿拉伯文数据集，在句子层面上单独或在多任务设置中学习这些属性。我们的模型基于深度双向神经网络的变化。更具体地说，我们使用选通循环单元和来自变压器的双向编码器表示（BERT）构建模型。我们展示了多任务学习（MTL）在这两项任务上的效用，并确定任务特定注意在这种情况下是一种更好的选择。我们还发现，单任务伯特模型在这两个任务上的表现优于我们最好的MTL模型。我们报告，年龄任务（三向）和性别任务（二元）的推特准确率分别为51.43%和65.30%，两者都大大超过了我们的基线。我们的模型与语言无关，因此可以应用于其他语言。</pre></li>
<li><a href="https://arxiv.org/abs/1910.12840">Evaluating the Factual Consistency of Abstractive Text Summarization</a>
<pre>目前用于评估摘要算法的指标没有考虑摘要是否与源文档事实上一致。我们提出了一种弱监督、基于模型的方法来验证事实的一致性，并识别源文档和生成的摘要之间的冲突。通过对源文档的句子应用一系列基于规则的转换来生成训练数据。然后，将事实一致性模型联合训练用于三项任务：1）识别句子在转换后是否保持事实一致，2）提取源文档中的跨度以支持一致性预测，3）提取摘要句子中存在不一致的跨度（如果存在）。将此模型转换为几个最先进的模型生成的摘要表明，这种高度可扩展的方法大大优于以前的模型，包括那些使用标准数据集进行自然语言推理和事实检查的经过严格监督的模型。此外，人工评估表明，辅助跨度提取任务在验证事实一致性的过程中提供了有用的帮助。</pre></li>
<li><a href="https://arxiv.org/abs/2004.05773">Generating Fact Checking Explanations</a> (ACL2020)
<pre>关于自动事实检查的大多数现有工作都是基于元数据、社交网络传播、索赔中使用的语言以及最近支持或否认索赔的证据来预测索赔的准确性。目前还没有解决的一个关键问题是理解如何自动化流程中最复杂的部分——为索赔裁决生成理由。本文首次研究了如何根据可用索赔上下文自动生成这些解释，以及如何将此任务与准确性预测联合建模。我们的结果表明，同时优化这两个目标，而不是单独培训它们，可以提高事实检查系统的性能。人工评估的结果进一步表明，在多任务模型中，生成的解释的信息量、覆盖率和总体质量也得到了提高。</pre></li>
<li><a href="https://arxiv.org/abs/1911.04211">NegBERT: A Transfer Learning Approach for Negation Detection and Scope Resolution</a>
<pre>否定是语言的一个重要特征，也是文本信息提取的重要组成部分。该子任务对生物医学领域具有相当重要的意义。多年来，人们探索了多种方法来解决这个问题：基于规则的系统、机器学习分类器、条件随机场模型、CNN和最近的BILSTM。在本文中，我们着眼于将迁移学习应用于这个问题。首先，我们广泛回顾了过去关于否定检测和范围解析的文献，这三个数据集在过去几年中得到了广泛的应用：BioScope语料库、Sherlock语料库和SFU评论语料库。然后，我们探讨了使用BERT（一种流行的迁移学习模型）完成此任务所涉及的决策选择，并报告了所有3个数据集范围解析的最新结果。我们的模型被称为NegBERT，在Sherlock数据集上，范围分辨率达到了标记级F1分数92.36，在BioScope Abstracts subcorpus上达到了95.68，在BioScope Full Papers subcorpus上达到了91.24，在SFU Review Corpus上达到了90.95，大大超过了之前最先进的系统。我们还分析了该模型对未经训练的数据集的可推广性。</pre></li>
<li><a href="https://arxiv.org/abs/1911.03663">xSLUE: A Benchmark and Analysis Platform for Cross-Style Language Understanding and Evaluation</a>
<pre>每一篇自然的文章都是以某种风格写成的。风格是由不同风格因素的复杂组合形成的，包括形式标记、情感、隐喻等。如果不考虑这些因素，就无法对文本形成完整的理解。这些因素以复杂的方式组合和共同变化，形成风格。研究共变组合的本质可以揭示一般的文体语言，有时称为跨文体语言理解。本文提供了基准语料库（benchmark corpus，xSLUE），该语料库结合了现有的数据集，并收集了一个新的数据集，用于句子级跨风格语言理解和评估。该基准包含15种不同风格的文本，分为四个理论组：比喻组、个人组、情感组和人际组。为了有效评估，我们通过在同一文本上注释所有15种样式来收集额外的诊断集。使用xSLUE，我们在分类、关联和生成方面提出了三个有趣的跨样式应用程序。首先，我们提出的跨风格分类器与多个风格一起训练，有助于提高整体分类性能，而不是单独训练的风格分类器。第二，我们的研究表明，在人类书写的文本中，某些风格是高度依赖的。最后，我们发现一些矛盾风格的组合可能会产生风格上不太合适的文本。我们相信，我们的基准和案例研究有助于探索跨风格研究有趣的未来方向。预处理的数据集和代码是公开的。</pre></li>
<li><a href="https://arxiv.org/abs/1909.02164">TabFact: A Large-scale Dataset for Table-based Fact Verification</a> (ICLR2020)
<pre>在自然语言理解和语义表征的研究中，基于给定的证据验证语篇假设是否成立的问题，也称为事实验证，起着重要的作用。然而，现有的研究主要局限于处理非结构化证据（如自然语言句子和文档、新闻等），而结构化证据（如表格、图表和数据库）下的验证仍有待探索。本文旨在研究半结构化数据作为证据的事实验证问题。为此，我们构建了一个名为TabFact的大规模数据集，其中16k个Wikipedia表作为118k个人类注释自然语言语句的证据，这些语句被标记为包含或反驳。TabFact具有挑战性，因为它涉及软语言推理和硬符号推理。为了解决这些推理难题，我们设计了两种不同的模型：表BERT和潜在程序算法（LPA）。Table BERT利用最先进的预训练语言模型将线性化的表格和语句编码为连续向量以进行验证。LPA将语句解析为程序，并针对表执行语句，以获取返回的二进制值进行验证。这两种方法都达到了相似的精度，但仍远远落后于人的表现。我们还进行了全面分析，以展示未来的巨大机遇。数据集的数据和代码在\url中提供{https://github.com/wenhuchen/Table-Fact-Checking}.</pre></li>
<li><a href="https://arxiv.org/abs/2002.01861">Rapid Adaptation of BERT for Information Extraction on Domain-Specific Business Documents</a>
<pre>从业务文档（如合同、报表和文件）中自动提取重要内容元素的技术有可能提高业务运营的效率。这个问题可以表述为一个序列标记任务，我们演示了BERT对两种类型的业务文档的适应性：监管文件和财产租赁协议。这个问题的某些方面使其比“标准”信息提取任务更容易，而其他方面使其更困难，但总的来说，我们发现适量的注释数据（少于100个文档）足以达到合理的准确性。我们将我们的模型集成到一个端到端的云平台中，该平台提供了一个易于使用的注释界面和一个推理界面，允许用户上传文档和检查模型输出。</pre></li>
<li><a href="https://arxiv.org/abs/2004.14519">A Focused Study to Compare Arabic Pre-training Models on Newswire IE Tasks</a>
<pre>多语言预先培训的变形金刚，如mBERT（Devlin等人，2019年）和XLM RoBERTa（Conneau等人，2020a），已被证明能够实现有效的跨语言零炮转移。然而，他们在阿拉伯语信息提取（IE）任务中的表现还没有得到很好的研究。在本文中，我们预先训练了一个定制的双语BERT，称为GigaBERT，它是专为阿拉伯语NLP和英语到阿拉伯语零镜头迁移学习而设计的。我们研究了GigaBERT在命名实体识别、词性标注、论元角色标注和关系提取四项IE任务中的零短迁移效果。我们的最佳模型在监督和零炮转移设置方面均显著优于mBERT、XLM RoBERTa和AraBERT（Antoun等人，2020）。我们已经在网站上公开了我们经过预培训的模型https://github.com/lanwuwei/GigaBERT.</pre></li>
<li><a href="https://arxiv.org/abs/2002.08087">LAMBERT: Layout-Aware (Language) Modeling for information extraction</a> (ICDAR2021)
<pre>我们介绍了一种简单的新方法来理解非平凡布局影响局部语义的文档。为此，我们修改Transformer编码器体系结构，使其能够使用从OCR系统获得的布局特征，而无需从头开始重新学习语言语义。我们只使用标记边界框的坐标来增加模型的输入，这样就避免了使用原始图像。这导致了一个布局感知语言模型，然后可以对下游任务进行微调。该模型使用四个公开可用的数据集（Kleister NDA、Kleister Charity、SROIE和CORD）进行端到端信息提取任务评估。我们表明，我们的模型在由视觉丰富的文档组成的数据集上实现了优异的性能，同时在平面布局文档（NDA\（F_{1}\）上也优于基线RoBERTa（从78.50增加到80.42）。我们的解决方案在从SROIE数据集中提取关键信息的公共排行榜上排名第一，将SOTA \（F{1}\）分数从97.81提高到98.17。</pre></li>
<li><a href="https://arxiv.org/abs/1910.08840">Keyphrase Extraction from Scholarly Articles as Sequence Labeling using Contextualized Embeddings</a> (ECIR2020) [<a href="https://github.com/midas-research/keyphrase-extraction-as-sequence-labeling-data">github</a>]
<pre>在本文中，我们将从学术文章中提取关键词描述为使用BiLSTM CRF解决的序列标记任务，其中输入文本中的单词使用深层上下文嵌入表示。我们在三个不同的基准数据集（Inspec、SemEval 2010、SemEval 2017）上使用上下文化和固定词嵌入模型评估拟议的体系结构，并与现有流行的无监督和监督技术进行比较。我们的研究结果量化了（a）使用语境化嵌入（如BERT）比固定词嵌入（如Glove）的好处；（b） 使用具有语境化词语嵌入的BiLSTM CRF架构直接微调语境化词语嵌入模型，以及（c）使用特定体裁的语境化嵌入（SciBERT）。通过误差分析，我们还提供了一些关于为什么特定模型比其他模型工作得更好的见解。最后，我们提出了一个案例研究，分析了两个最佳模型（BERT和SciBERT）的不同自我注意层，以更好地理解每个模型对关键短语提取任务的预测。</pre></li>
<li><a href="https://arxiv.org/abs/2002.05407">Keyphrase Extraction with Span-based Feature Representations</a>
<pre>关键短语能够提供描述文档特征的语义元数据，并生成文档内容的概述。由于关键词提取能够方便信息的管理、分类和检索，近年来受到了广泛的关注。有三种方法来解决关键短语提取：（i）传统的两步排序方法，（ii）序列标记和（iii）使用神经网络生成。两步排序方法是基于特征工程的，这是劳动密集型和领域相关的。序列标签无法处理重叠短语。生成方法（即序列到序列的神经网络模型）克服了这些缺点，因此得到了广泛的研究并获得了最先进的性能。然而，生成方法不能有效地利用上下文信息。在本文中，我们提出了一种新颖的Span关键字短语提取模型，该模型直接从所有内容标记中提取关键字短语的基于Span的特征表示。通过这种方式，我们的模型获得每个关键短语的表示，并进一步学习在一个文档中捕获关键短语之间的交互，以获得更好的排名结果。此外，在标记的帮助下，我们的模型能够提取重叠的关键短语。在基准数据集上的实验结果表明，我们提出的模型在很大程度上优于现有的方法。</pre></li>
<li><a href="https://arxiv.org/abs/2004.10462">Keyphrase Prediction With Pre-trained Language Model</a>
<pre>最近，由于生成方法能够生成出现在源文本中的当前关键短语和与任何源文本都不匹配的缺失关键短语，因此在关键短语预测中得到了广泛的应用。然而，由于以前的工作主要使用依赖复制机制的生成模型并逐步选择单词，因此，缺少的关键短语是以当前关键短语预测的性能为代价生成的。此外，直接提取文本跨度的提取模型更适合预测当前关键词。考虑到提取方法和生成方法的不同特点，我们建议将关键短语预测分为两个子任务，即当前关键短语提取（PKE）和缺席关键短语生成（AKG），以充分发挥各自的优势。在此基础上，提出了一种联合推理框架，以充分利用两个子任务中的BERT。对于PKE，我们使用预先训练的语言模型BERT将此任务作为序列标记问题来处理。对于AKG，我们介绍了一种基于转换器的架构，该架构充分集成了通过微调BERT从PKE学习到的现有关键短语知识。实验结果表明，我们的方法可以在基准数据集上实现两种任务的最新结果。</pre></li>
<li><a href="https://www.preprints.org/manuscript/201908.0073/v1">Self-Supervised Contextual Keyword and Keyphrase Retrieval with Self-Labelling</a> [<a href="https://github.com/MaartenGr/KeyBERT">github</a>]</li>
<li><a href="https://arxiv.org/abs/2004.13639">Joint Keyphrase Chunking and Salience Ranking with BERT</a>
<pre>开放域关键短语提取（KPE）旨在从没有域或质量限制的文档中提取关键短语，例如具有不同域和质量的网页。最近，由于神经方法对给定文档的上下文语义建模的强大能力，在许多KPE任务中显示了有希望的结果。然而，我们的经验表明，大多数神经KPE方法更倾向于从开放域文档中提取具有良好措辞的关键短语，如简短和实体风格的n-gram，而不是全局信息的关键短语。本文介绍了JointKPE，一种基于预先训练的语言模型的开放域KPE体系结构，它可以在提取关键短语时同时捕获局部短语和全局信息。JointKPE通过评估关键短语在整个文档中的信息量来学习对关键短语进行排名，并在关键短语组块任务中接受联合培训，以确保关键短语候选词的措辞准确。在两个具有不同领域的大型KPE数据集（OpenKP和KP20k）上的实验表明，在开放领域场景中，JointKPE对不同预训练变量的有效性。进一步的分析揭示了JointKPE在预测长的和非实体的关键短语方面的显著优势，这对以前的神经KPE方法是一个挑战。我们的代码在https://github.com/thunlp/BERT-KPE.</pre></li>
<li><a href="https://arxiv.org/abs/1911.03822">Generalizing Natural Language Analysis through Span-relation Representations</a> (ACL2020) [<a href="https://github.com/neulab/cmu-multinlp">github</a>]
<pre>自然语言处理涵盖了预测语法、语义和信息内容的各种任务，通常每种类型的输出都是通过专门设计的体系结构生成的。在本文中，我们提供了一个简单的见解，即各种各样的任务可以用一个统一的格式表示，该格式由标记范围和范围之间的关系组成，因此单个任务独立模型可以用于不同的任务。我们在10个不同的任务上进行了大量的实验来测试这一洞察力，这些任务包括依赖项解析（语法）、语义角色标记（语义）、关系提取（信息内容）、基于方面的情感分析（情感）和许多其他任务，实现了与最先进的专门模型相当的性能。我们进一步证明了多任务学习的好处，同时也表明所提出的方法可以很容易地分析模型处理不同任务的异同。最后，我们将这些数据集转换成统一的格式来构建基准测试，它为评估广义自然语言分析的未来模型提供了一个全面的测试平台。</pre></li>
<li><a href="https://arxiv.org/abs/1911.05758">What do you mean, BERT? Assessing BERT as a Distributional Semantics Model</a>
<pre>语境化词语嵌入，即语境中词语的向量表示，自然被视为先前非文本分布语义模型的扩展。在这项工作中，我们重点研究了BERT，这是一种产生语境化嵌入并在若干语义任务中创造了最新水平的深层神经网络，并研究了其嵌入空间的语义一致性。虽然呈现出连贯性的趋势，但伯特并没有完全达到对语义向量空间的自然期望。特别是，我们发现单词出现的句子位置虽然没有意义关联，但在单词嵌入上留下了明显的痕迹，扰乱了相似关系。</pre></li>
<li><a href="https://www.aclweb.org/anthology/2020.acl-main.630/">tBERT: Topic Models and BERT Joining Forces for Semantic Similarity Detection</a> (ACL2020)</li>
<li><a href="https://www.aclweb.org/anthology/D19-6109/">Domain Adaptation with BERT-based Domain Classification and Data Selection</a> (EMNLP2019 WS)</li>
<li><a href="https://arxiv.org/abs/2006.09075">PERL: Pivot-based Domain Adaptation for Pre-trained Deep Contextualized Embedding Models</a> (TACL2020)
<pre>基于枢轴的神经表示模型在NLP领域自适应方面取得了重大进展。然而，以前采用这种方法的工作仅利用来自源域的标记数据和来自源域和目标域的未标记数据，而忽略了合并不一定来自这些域的大量未标记语料库。为了缓解这种情况，我们提出了PERL：一种表示学习模型，它通过基于枢轴的微调扩展了上下文化的单词嵌入模型，如BERT。PERL在22个情绪分类域自适应设置中优于强基线，提高了域内模型性能，生成有效的缩减模型并提高了模型稳定性。</pre></li>
<li><a href="https://arxiv.org/abs/2106.00948">Unsupervised Out-of-Domain Detection via Pre-trained Transformers</a> (ACL2021) [<a href="https://github.com/rivercold/BERT-unsupervised-OOD">github</a>]
<pre>已部署的真实世界机器学习应用程序通常会受到不受控制甚至潜在恶意输入的影响。这些域外输入可能会导致不可预测的输出，有时还会导致灾难性的安全问题。先前关于域外检测的研究需要域内任务标签，并且仅限于监督分类场景。我们的工作解决了仅使用无监督的域内数据检测域外样本的问题。我们利用预先训练的变换器的潜在表示，提出了一种简单而有效的方法来转换所有层的特征，从而有效地构造域外检测器。进一步提出了两种特定于领域的微调方法来提高检测精度。我们对两个数据集上的相关方法进行了实证评估，验证了我们的方法在更一般的情况下大大提高了域外检测能力。</pre></li>
<li><a href="https://arxiv.org/abs/2010.11478">Knowledge Distillation for BERT Unsupervised Domain Adaptation</a>
<pre>经过预训练的语言模型BERT在一系列自然语言处理任务中带来了显著的性能改进。由于该模型是在大量不同主题的语料库上训练的，因此对于训练（源数据）和测试（目标数据）中的数据分布不同而共享相似性的领域转移问题，它表现出稳健的性能。尽管与以前的模型相比，它有了很大的改进，但由于域转移，它仍然存在性能下降的问题。为了缓解这些问题，我们提出了一种简单而有效的无监督领域自适应方法，即竞争性区分领域自适应与知识提取相结合的竞争性领域自适应（AAD）。我们在30个领域对的跨领域情感分类任务中评估了我们的方法，提高了文本情感分类中无监督领域适应的最新性能。</pre></li>
<li><a href="https://arxiv.org/abs/2003.03106">Sensitive Data Detection and Classification in Spanish Clinical Text: Experiments with BERT</a> (LREC2020)
<pre>海量数字数据处理提供了广泛的机会和好处，但代价是危及个人数据隐私。匿名化包括从数据中删除或替换敏感信息，使其能够用于不同目的，同时保护个人隐私。多年来，人们提出了许多自动匿名系统；然而，根据数据类型、目标语言或培训文件的可用性，这项任务仍然具有挑战性。在过去的两年中，新的深度学习模型的出现为自然语言处理领域的最新发展带来了巨大的进步。这些进步最引人注目的是由BERT（谷歌在2018年提出的一种模型）和在数百万文档上预先训练的共享语言模型。在本文中，我们使用基于BERT的序列标记模型对几个西班牙语临床数据集进行了一系列匿名实验。我们还将BERT算法与其他算法进行了比较。实验表明，一个简单的基于BERT的通用领域预训练模型在没有任何特定领域特征工程的情况下获得了具有竞争力的结果。</pre></li>
<li><a href="https://arxiv.org/abs/2104.07762">Does BERT Pretrained on Clinical Notes Reveal Sensitive Data?</a> (NAACL2021)
<pre>对电子健康记录（EHR）中的临床记录进行预培训的大型变压器在预测性临床任务方面的性能有了显著提高。培训此类模型的成本（以及数据访问的必要性）加上它们的实用性，促使参数共享，即发布预培训模型，如。虽然大多数研究都使用了未识别的EHR，但许多研究人员可以获得大量敏感的、未识别的EHR，他们可以使用这些EHR来训练BERT模型（或类似模型）。如果他们这样做，释放这样一个模型的重量是否安全？在这项工作中，我们设计了一组方法，旨在从经过训练的BERT恢复个人健康信息（PHI）。具体来说，我们尝试恢复患者姓名以及与之相关的条件。我们发现，简单的探测方法不能有意义地提取敏感信息从BERT训练超过模仿III语料库的EHR。然而，更复杂的“攻击”可能会成功：为了促进此类研究，我们在https://github.com/elehman16/exposing_patient_data_release</pre></li>
<li><a href="https://arxiv.org/abs/2004.12617">On the Importance of Word and Sentence Representation Learning in Implicit Discourse Relation Classification</a> (IJCAI2020)
<pre>隐性语篇关系分类是浅层语篇分析中最困难的部分之一，因为没有显性连接词的关系预测需要在语篇广度和句子两个层面上进行语言理解。以往的研究主要集中在两个论点之间的相互作用。我们认为，一个强大的语境化表达模块、一个双边多视角匹配模块和一个全局信息融合模块对内隐语篇分析都很重要。我们提出了一种新的模型来将这些模块组合在一起。大量实验表明，我们提出的模型在PDTB数据集上的性能优于BERT和其他最先进的系统约8%，在CoNLL 2016数据集上的性能优于16%。我们还分析了内隐语篇关系分类任务中不同模块的有效性，并展示了不同水平的表征学习如何影响结果。</pre></li>
<li><a href="http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.144.pdf">Adapting BERT to Implicit Discourse Relation Classification with a Focus on Discourse Connectives</a> (LREC2020)</li>
<li><a href="https://arxiv.org/abs/2006.11852">Labeling Explicit Discourse Relations using Pre-trained Language Models</a> (TSD2020)
<pre>标记显式语篇关系是浅层语篇分析中最具挑战性的子任务之一，其目标是识别语篇连接词及其论点的边界。通过使用手工制作的功能，最先进的模型达到略高于45%的F分数。本文研究了预先训练的语言模型在这项任务中的有效性。我们发现，经过精细调整的预先训练的语言模型足以取代语言特征。我们在PDTB 2.0上评估了我们的模型，并报告了提取完整关系的最新结果。这是第一次在不使用任何语言特征的情况下，模型的性能优于知识密集型模型。</pre></li>
<li><a href="https://arxiv.org/abs/2012.05453">Causal-BERT : Language models for causality detection between events expressed in text</a>
<pre>事件之间的因果关系理解是一项关键的自然语言处理任务，在许多领域都很有帮助，包括医疗保健、商业风险管理和金融。仔细研究，你可以发现大量文本内容，无论是正式文件的形式，还是Twitter等社交媒体的内容，都致力于交流和探索现实世界中的各种因果关系。认识到自然语言事件之间的这些“因果”关系仍然是一个挑战，因为它通常是隐式表达的。隐式因果关系很难通过文献中使用的大多数技术来发现，有时也会被认为是模棱两可或模糊的。此外，尽管针对这一问题的知名数据集确实存在，但其中的示例在其所描述的因果关系的范围和复杂性方面是有限的，尤其是在与隐式关系相关时。目前大多数方法要么基于词汇语义模式匹配，要么是基于特征驱动的监督方法。因此，正如预期的那样，这些方法更倾向于处理显式因果关系，导致对隐式关系的覆盖范围有限，并且难以推广。在本文中，我们使用句子上下文结合事件信息，并利用域内和域外数据分布的掩蔽事件上下文，研究了语言模型在自然语言文本中表达的事件之间的因果关联能力。我们提出的方法在三种不同的数据分布中实现了最先进的性能，可用于提取因果图和/或从非结构化文本中构建事件链。</pre></li>
<li><a href="https://arxiv.org/abs/2103.13584">BERT4SO: Neural Sentence Ordering by Fine-tuning BERT</a>
<pre>句子排序的目的是按照正确的顺序排列给定文本中的句子。最近的工作将其视为一个排序问题，并将深度神经网络应用于此。在这项工作中，我们提出了一种新的方法，名为BERT4SO，通过微调BERT进行句子排序。我们使用多个特殊标记和精心设计的段（区间）嵌入连接所有句子并计算它们的表示。跨多个句子的标记可以互相关注，这大大增强了它们的交互作用。我们还提出了一种基于ListMLE的基于边际的listwise排序损失，以促进优化过程。在五个基准数据集上的实验结果证明了该方法的有效性。</pre></li>
<li><a href="https://arxiv.org/abs/2104.05919">Document-Level Event Argument Extraction by Conditional Generation</a> (NAACL2021)
<pre>在IE社区中，事件提取一直被视为一项句子级任务。我们认为，这种设置与人类的信息寻求行为不匹配，并导致不完整和无信息的提取结果。我们提出了一个文档级的神经事件参数提取模型，通过将任务描述为事件模板后的条件生成。我们还编译了一个新的文档级事件提取基准数据集WikiEvents，其中包括完整的事件和引用注释。在参数提取任务中，我们在RAMS和WikiEvents数据集上分别获得了7.6%F1和5.7%F1的绝对增益。在更具挑战性的信息论元提取任务中，需要隐式共指推理，我们在最佳基线上获得了9.3%的F1增益。为了证明我们模型的可移植性，我们还创建了第一个端到端的零镜头事件提取框架，并在ACE上仅访问33种类型中的10种的情况下，实现了97%的完全监督模型的触发器提取性能和82%的参数提取性能。</pre></li>
<li><a href="https://arxiv.org/abs/2004.13850">Cross-lingual Zero- and Few-shot Hate Speech Detection Utilising Frozen Transformer Language Models and AXEL</a>
<pre>检测仇恨言论，特别是在低资源语言中，是一项非常艰巨的挑战。为了解决这个问题，我们开发了一个基于冻结的、预先训练过的变形金刚的定制架构，以检查HatEval challenge数据集上的跨语言零射击和少射击学习，以及单语言学习。通过我们新颖的基于注意的分类块AXEL，我们在英语和西班牙语子集上展示了极具竞争力的结果。我们还对英语子集进行了重新采样，以便将来进行更多有意义的比较。</pre></li>
<li><a href="https://arxiv.org/abs/2004.11163">Same Side Stance Classification Task: Facilitating Argument Stance Classification by Fine-tuning a BERT Model</a>
<pre>关于计算论证的研究目前正在深入进行。该社区的目标是为用户给定的主题找到最佳的赞成和反对论据，以形成自己的观点，或说服他人采纳某一观点。虽然现有的参数挖掘方法可以为一个主题找到合适的参数，但正确的正反分类仍然不可靠。“同侧立场分类”任务提供了一个论点对数据集，根据两个论点是否共享同一立场进行分类，不需要区分特定主题的赞成和反对词汇，只需要评估立场中的论点相似性。我们对任务的贡献的结果是建立在基于BERT体系结构的设置之上的。我们对三个时期的预训练伯特模型进行了微调，并使用每个参数的前512个标记来预测两个参数是否具有相同的立场。</pre></li>
<li><a href="https://arxiv.org/abs/2004.13432">Kungfupanda at SemEval-2020 Task 12: BERT-Based Multi-Task Learning for Offensive Language Detection</a>
<pre>如今，社交媒体中的攻击性内容已经成为一个严重的问题，自动检测攻击性语言是一项必不可少的任务。在本文中，我们构建了一个攻击性语言检测系统，该系统将多任务学习与基于BERT的模型相结合。使用预先训练好的语言模型（如BERT），我们可以有效地学习社交媒体中嘈杂文本的表示。此外，为了提高攻击性语言检测的性能，我们利用了来自其他相关任务的监控信号。在OffensEval-2020比赛中，我们的模型在英语子任务A中获得91.51%的F1分数，与第一名（92.23%F1）相当。实证分析被用来解释我们的方法的有效性。</pre></li>
<li><a href="https://arxiv.org/abs/2005.07820">KEIS@JUST at SemEval-2020 Task 12: Identifying Multilingual Offensive Tweets Using Weighted Ensemble and Fine-Tuned BERT</a>
<pre>这项研究展示了我们的团队KEIS@JUST参加SemEval-2020任务12，代表多语言攻击性语言的共享任务。我们参与了所有子任务提供的所有语言，但英语子任务A除外。已经开发了两种主要方法。第一种方法用于处理阿拉伯语和英语两种语言，加权集成包括Bi GRU和CNN，然后是高斯噪声和全局池层乘以权重，以提高整体性能。第二种是针对其他语言执行的，在Bi LSTM和Bi GRU等递归神经网络的旁边从BERT进行转移学习，然后是一个全局平均池层。单词嵌入和上下文嵌入被用作特征，此外，数据扩充仅用于阿拉伯语。</pre></li>
<li><a href="https://www.aclweb.org/anthology/2020.figlang-1.17/">ALBERT-BiLSTM for Sequential Metaphor Detection</a> (ACL2020 WS)</li>
<li><a href="https://arxiv.org/abs/2104.13615">MelBERT: Metaphor Detection via Contextualized Late Interaction using Metaphorical Identification Theories</a> (NAACL2021)
<pre>自动隐喻检测是识别句子中词语隐喻表达的一项具有挑战性的任务。为了解决这个问题，我们采用了预先训练好的情境化模型，例如BERT和RoBERTa。为此，我们提出了一种新的隐喻检测模型，即基于BERT的隐喻感知后期交互（MelBERT）。我们的模型不仅利用了语境化的词语表征，而且还得益于语言隐喻识别理论来区分词语的语境意义和字面意义。我们的实证结果表明，MelBERT在四个基准数据集（即VUA-18、VUA-20、MOH-X和TroFi）上的表现优于几个强基线。</pre></li>
<li><a href="https://arxiv.org/abs/2011.02378">A BERT-based Dual Embedding Model for Chinese Idiom Prediction</a> (COLING2020)
<pre>汉语习语是一种特殊的固定短语，通常来源于古代故事，其含义往往具有高度的习语性和非组合性。汉语习语预测任务是从给定上下文的一组候选习语中选择正确的习语。我们提出了一个基于BERT的双重嵌入模型来对上下文词进行编码，并学习习语的双重嵌入。具体而言，我们首先匹配每个候选成语的嵌入与对应于上下文中的空白的隐藏表示。然后，我们通过上下文池将每个候选习语的嵌入与上下文中所有标记的隐藏表示相匹配。我们进一步建议使用两种不同的习语嵌入来进行两种匹配。在最近发布的汉语习语完形填空测试数据集上的实验表明，我们提出的方法比现有的方法性能更好。烧蚀实验还表明，上下文池和双重嵌入都有助于提高性能。</pre></li>
<li><a href="https://www.aclweb.org/anthology/2020.bea-1.15/">Should You Fine-Tune BERT for Automated Essay Scoring?</a> (ACL2020 WS)</li>
<li><a href="https://arxiv.org/abs/2009.02252">KILT: a Benchmark for Knowledge Intensive Language Tasks</a> (NAACL2021) [<a href="https://github.com/facebookresearch/KILT">github</a>]
<pre>具有挑战性的问题，如开放领域问题回答、事实检查、插槽填充和实体链接，需要访问大型外部知识源。虽然有些模型在单个任务上表现良好，但开发通用模型是困难的，因为除了专用的基础设施之外，每个任务可能还需要对自定义知识源进行昂贵的计算索引。为了促进对以大型文本资源中的特定信息为条件的模型的研究，我们提出了一个知识密集型语言任务（KILT）的基准。KILT中的所有任务都基于Wikipedia的同一快照，通过重用组件减少了工程周期，并加快了对任务无关内存体系结构的研究。我们测试特定任务和一般基线，除了评估模型提供来源的能力外，还评估下游性能。我们发现，共享稠密向量索引与seq2seq模型相结合是一个强大的基线，在事实检查、开放领域问题回答和对话方面优于更多定制方法，并通过生成消除歧义的文本在实体链接和插槽填充方面产生有竞争力的结果。KILT数据和代码可在https://github.com/facebookresearch/KILT.</pre></li>
<li><a href="https://arxiv.org/abs/2009.05387">IndoNLU: Benchmark and Resources for Evaluating Indonesian Natural Language Understanding</a> (AACL-IJCNLP2020)
<pre>尽管印尼语是互联网上第四大最常用语言，但由于缺乏可用资源，自然语言处理（NLP）中对该语言的研究进展缓慢。作为回应，我们引入了第一个庞大的资源，用于印尼自然语言理解（IndoNLU）任务的培训、评估和基准测试。IndoNLU包括12项任务，从单句分类到不同复杂程度的成对句子序列标记。任务的数据集位于不同的域和样式中，以确保任务的多样性。我们还提供了一组印度尼西亚预培训模型（IndoBERT），这些模型来自于从社交媒体文本、博客、新闻和网站等公开来源收集的大型印尼数据集Indob4b。我们发布了所有12项任务的基线模型，以及基准评估框架，从而使每个人都能够对其系统性能进行基准测试。</pre></li>
<li><a href="https://arxiv.org/abs/2010.02246">MedFilter: Improving Extraction of Task-relevant Utterances through Integration of Discourse Structure and Ontological Knowledge</a> (EMNLP2020)
<pre>从会话数据中提取信息尤其具有挑战性，因为会话以任务为中心的性质允许人类有效地交流隐含信息，但对于机器来说则具有挑战性。根据说话人在对话中的角色，不同话语之间的挑战可能有所不同，特别是当相关专业知识在不同角色之间不对称分布时。此外，在对话过程中，这些挑战也可能增加，因为在对话的早期，通过隐式传达的信息建立了更多的共享环境。在本文中，我们提出了一种新的建模方法MedFilter，它解决了这些问题，以提高识别和分类任务相关话语的性能，并在这样做的过程中，积极影响下游信息提取任务的性能。我们在近7000个医患对话的语料库中评估了这种方法，其中使用MedFilter来确定对讨论的医学相关贡献（就PR曲线下的面积而言，比SOTA基线提高10%）。识别任务相关话语有利于下游医疗处理，在提取症状、药物和投诉方面分别提高15%、105%和23%。</pre></li>
<li><a href="https://arxiv.org/abs/2012.12350">ActionBert: Leveraging User Actions for Semantic Understanding of User Interfaces</a> (AAAI2021)
<pre>随着移动设备变得无处不在，与各种用户界面（UI）定期交互是许多人日常生活的一个常见方面。为了提高这些设备的可访问性并使其能够在各种设置中使用，构建能够帮助用户并通过UI完成任务的模型至关重要。然而，要实现这一目标还面临着一些挑战。首先，外观相似的UI组件可以具有不同的功能，因此理解它们的功能比分析它们的外观更重要。其次，特定于领域的功能，如网页中的文档对象模型（DOM）和移动应用程序中的视图层次结构（VH），提供了有关UI元素语义的重要信号，但这些功能不是自然语言格式。第三，由于用户界面的巨大多样性以及缺乏标准的DOM或VH表示，构建具有高覆盖率的用户界面理解模型需要大量的培训数据。受NLP中基于预训练的方法以数据高效方式解决各种问题的成功启发，我们引入了一种新的预训练UI表示模型ActionBert。我们的方法旨在利用用户交互跟踪中的视觉、语言和领域特定功能，预先训练UI及其组件的通用功能表示。我们的关键直觉是，用户操作（例如，对不同UI组件的一系列单击）揭示了有关其功能的重要信息。我们对所提出的模型进行了广泛的下游任务评估，从图标分类到基于其自然语言描述的UI组件检索。实验表明，所提出的ActionBert模型在所有下游任务中的性能比多模式基线高达15.5%。</pre></li>
<li><a href="https://openreview.net/forum?id=zmgJIjyWSOw">UserBERT: Self-supervised User Representation Learning</a></li>
<li><a href="https://arxiv.org/abs/2109.01274">UserBERT: Contrastive User Model Pre-training</a>
<pre>用户建模对于个性化web应用程序至关重要。现有的用户建模方法通常使用特定于任务的标记数据从用户行为中训练用户模型。但是，目标任务中的标记数据可能不足以训练准确的用户模型。幸运的是，通常有丰富的未标记用户行为数据，这些数据编码了用户特征和兴趣的丰富信息。因此，在未标记的用户行为数据上预先训练用户模型有可能改进许多下游任务的用户建模。本文提出了一种对比用户模型预训练方法UserBERT。UserBERT中包含两个自我监督任务，用于对未标记的用户行为数据进行用户模型预训练，以增强用户建模能力。第一种是掩蔽行为预测，旨在对用户行为之间的相关性进行建模。第二种是行为序列匹配，其目的是捕获在不同时期一致的用户固有兴趣。此外，我们提出了一个中硬否定抽样框架来选择信息性否定样本，以便更好地进行对比预训练。我们维护一个同步更新的候选行为池和一个异步更新的候选行为序列池，以高效地选择本地最难的负面行为和行为序列。在两个不同任务的真实数据集上进行的大量实验表明，UserBERT可以有效地改进各种用户模型。</pre></li>
<li><a href="https://arxiv.org/abs/2012.02462">Fine-tuning BERT for Low-Resource Natural Language Understanding via Active Learning</a> (COLING2020)
<pre>最近，在下游利用预先训练好的基于转换器的语言模型，任务特定模型在自然语言理解任务方面取得了先进的成果。然而，只有很少的研究探讨了这种方法在资源不足、训练数据点少于1000个的环境中的适用性。在这项工作中，我们探索了BERT（一种预训练的基于转换器的语言模型）的微调方法，通过使用基于池的主动学习来加速训练，同时保持标记新数据的成本不变。我们在GLUE数据集上的实验结果表明，当从未标记数据池中查询时，通过最大化模型的近似知识增益，在模型性能方面具有优势。最后，我们演示并分析了在微调过程中冻结语言模型层的好处，以减少可训练参数的数量，使其更适合低资源设置。</pre></li>
<li><a href="https://arxiv.org/abs/2101.07343">Automatic punctuation restoration with BERT models</a>
<pre>针对英语和匈牙利语，我们提出了一种基于BERT模型的标点符号自动恢复方法。对于英语，我们在Ted Talks上进行实验，Ted Talks是标点符号恢复的常用基准，而对于匈牙利语，我们在Szeged Treebank数据集上评估我们的模型。我们最好的车型在英语和匈牙利语中的宏观平均分数分别为$F_1$和$F_1$分别为79.8和82.2。我们的代码是公开的。</pre></li>
</ul>
<h2 id="word-segmentation-parsing-ner">Word segmentation, parsing, NER</h2>
<ul>
<li><a href="https://arxiv.org/abs/1909.09292">BERT Meets Chinese Word Segmentation</a>
<pre>中文分词是汉语理解的一项基本任务。最近，基于神经网络的模型在解决领域内CWS任务方面取得了优异的性能。去年，Transformers的双向编码器表示（BERT）作为一种新的语言表示模型被提出作为许多自然语言任务的主干模型，并重新定义了相应的性能。BERT的出色性能促使我们将其应用于解决CWS任务。通过在第二届国际汉语分词大赛的基准数据集中进行深入的实验，我们获得了一些敏锐的观察结果。即使数据集包含标签不一致的问题，BERT也可以稍微提高性能。当应用充分学习的特征时，较简单的分类器Softmax可以获得与更复杂的分类器相同的性能，例如条件随机场（CRF）。BERT的性能通常随着模型大小的增加而增加。BERT提取的特征也可以作为其他神经网络模型的候选特征。</pre></li>
<li><a href="https://arxiv.org/abs/2004.05808">Unified Multi-Criteria Chinese Word Segmentation with BERT</a>
<pre>多准则汉语分词（MCCWS）的目的是在多准则分词的情况下，在由连续字符组成的汉语句子中寻找词的边界。该统一框架已在MCCWS中得到广泛应用，并显示了其有效性。此外，在多任务学习框架下，将预先训练好的BERT语言模型引入到MCCWS任务中。本文结合统一框架和预训练语言模型的优点，提出了一种基于BERT的统一MCCWS模型。此外，我们还利用二元图特征和辅助准则分类任务对统一的基于BERT的MCCWS模型进行了扩充。在八个具有不同标准的数据集上进行的实验表明，我们的方法可以实现针对MCCW的最新结果。</pre></li>
<li><a href="https://arxiv.org/abs/2011.06858">RethinkCWS: Is Chinese Word Segmentation a Solved Task?</a> (EMNLP2020) [<a href="https://github.com/neulab/InterpretEval">github</a>]
<pre>随着深度神经网络的快速发展，特别是大型预训练模型的成功应用，中文分词系统的性能逐渐达到了一个稳定的水平。在本文中，我们总结了我们所取得的成就，并重新思考了CWS任务中剩下的内容。在方法上，我们提出了对现有CWS系统的细粒度评估，这不仅使我们能够诊断现有模型（在数据集设置下）的优缺点，而且使我们能够量化不同标准之间的差异，并在进行多标准学习时缓解负迁移问题。从战略上讲，尽管本文并不打算提出一个新的模型，但我们对8个模型和7个数据集的综合实验以及深入的分析，可以为未来的研究寻找一些有希望的方向。我们公开所有代码，并发布一个可以快速评估和诊断用户模型的界面：https://github.com/neulab/InterpretEval.</pre></li>
<li><a href="https://aclanthology.org/2021.findings-acl.383/">Enhancing Chinese Word Segmentation via Pseudo Labels for Practicability</a> (ACL2021 Findings)</li>
<li><a href="https://arxiv.org/abs/2010.00287">Joint Persian Word Segmentation Correction and Zero-Width Non-Joiner Recognition Using BERT</a>
<pre>单词在波斯语书写系统中被正确分割；然而，在实践中，这些书写规则常常被忽略，导致单个单词被断续书写，而多个单词之间没有任何空格。本文讨论了波斯语中的分词和零宽度非连接词（ZWNJ）识别问题，我们将其作为一个序列标记问题进行联合处理。我们在仔细收集的500个句子的语料库中获得了92.40%的宏观平均F1分数，难度很高。</pre></li>
<li><a href="https://arxiv.org/abs/1903.04190">Toward Fast and Accurate Neural Chinese Word Segmentation with Multi-Criteria Learning</a>
<pre>注释标准的模糊性导致了中文分词数据集在不同粒度上的差异。多准则中文分词旨在捕获数据集中的各种注释准则，并利用它们的共同基础知识。在本文中，我们提出了一种域自适应分段器来利用不同数据集的不同标准。我们的模型基于变压器的双向编码器表示（BERT），它负责引入开放领域知识。提出了私有和共享投影层，分别用于获取领域特定知识和公共知识。我们还通过蒸馏、量化和编译器优化来优化计算效率。实验表明，在10个CWS数据集上，我们的分段器比以前的最新模型（SOTA）具有更高的效率。</pre></li>
<li><a href="https://arxiv.org/abs/1908.04943">Establishing Strong Baselines for the New Decade: Sequence Tagging, Syntactic and Semantic Parsing with BERT</a> (FLAIRS-33)
<pre>本文使用最新的上下文嵌入框架BERT，为词性标注、句法分析和语义分析这三项任务提供了最新的模型。对于每个任务，我们首先复制和简化当前最先进的方法，以提高其模型效率。然后，我们使用BERT生成的令牌嵌入来评估这三个任务的简化方法。我们的实验使用了12个中英文数据集。BERT模型的平均表现优于之前表现最好的模型2.5%（最显著的情况下为7.5%）。此外，还利用自我注意对伯特嵌入的影响进行了深入分析，这有助于理解这种丰富的表达方式。所有模型和源代码都可以公开获取，以便研究人员能够改进并利用它们为下一个十年建立强有力的基线。</pre></li>
<li><a href="https://arxiv.org/abs/1908.07448">Evaluating Contextualized Embeddings on 54 Languages in POS Tagging, Lemmatization and Dependency Parsing</a>
<pre>我们对最近提出的三种语境化嵌入方法进行了广泛的评估，这些方法包括词性标记、引理化和依赖性分析三个任务，涉及54种语言中的89个语料库（Universal Dependencies 2.3）。采用BERT、Flair和ELMo作为UDPipe 2.0强大基线中的预训练嵌入输入，UDPipe 2.0是CoNLL 2018共享任务的最佳执行系统之一，也是EPE 2018的总冠军，我们对三种上下文化单词嵌入方法进行了一对一的比较，以及与word2vec类预训练嵌入和端到端字符级单词嵌入的比较。与CoNLL 2018共享任务中UD 2.2的结果相比，我们报告了所有三项任务的最新结果。</pre></li>
<li><a href="https://arxiv.org/abs/2009.08633">fastHan: A BERT-based Joint Many-Task Toolkit for Chinese NLP</a>
<pre>我们介绍了fastHan，一个开源工具包，用于中文自然语言处理的四个基本任务：中文分词（CWS）、词性（POS）标记、命名实体识别（NER）和依赖项解析。fastHan的主干是一个基于删减的BERT的多任务模型，它使用BERT中的前8层。我们还提供了从8层模型压缩而来的4层基础模型。该联合模型在四个任务的13个语料库上进行了训练和评估，在依赖项解析和NER方面获得了接近最新水平的（SOTA）性能，在CWS和POS方面实现了SOTA性能。此外，fastHan的可转移性也很强，在非训练语料库上的性能远远好于流行的切分工具。为了更好地满足实际应用的需要，我们允许用户使用自己的标记数据进一步微调fastHan。除了体积小、性能优异外，fastHan还具有良好的用户友好性。fastHan作为一个python包实现，将用户与内部技术细节隔离开来，使用起来很方便。该项目在Github上发布。</pre></li>
<li><a href="https://arxiv.org/abs/1908.07397">Deep Contextualized Word Embeddings in Transition-Based and Graph-Based Dependency Parsing – A Tale of Two Parsers Revisited</a> (EMNLP2019)
<pre>基于转换和基于图形的依赖关系解析器之前已被证明具有互补的优势和劣势：基于转换的解析器利用丰富的结构特征，但会遭受错误传播，而基于图形的解析器受益于全局优化，但限制了特征范围。在本文中，我们表明，即使在切换到神经网络和连续表示后，图片的某些细节发生了变化，但丰富特征和全局优化之间的基本权衡基本上保持不变。此外，我们还证明了深层语境化的单词嵌入，允许解析器将关于全局句子结构的信息打包到局部特征表示中，使基于转换的解析器比基于图形的解析器受益更多，使得这两种方法在准确性和错误率方面实际上是等价的。我们认为，原因是这些表示有助于防止搜索错误，从而允许基于转换的解析器更好地利用其做出准确局部决策的固有优势。我们通过对13种语言的语法分析实验的错误分析来支持这一解释。</pre></li>
<li><a href="https://arxiv.org/abs/2003.03204">Is POS Tagging Necessary or Even Helpful for Neural Dependency Parsing?</a>
<pre>在前深度学习时代，词性标记被认为是依赖分析中特征工程不可或缺的组成部分。但有相当多的工作关注于联合标记和解析模型，以避免错误传播。相比之下，最近的研究表明，词性标注对于神经分析来说变得不那么重要，甚至毫无用处，尤其是在使用基于字符的单词表示时。然而，无论是从实证角度还是从语言学角度，都没有足够的研究关注这个问题。为了回答这个问题，我们设计并比较了三个典型的多任务学习框架，即Share-Loose、Share-Tight和Stack，用于基于最先进的biaffine解析器的联合标记和解析。考虑到标注POS标记比解析树便宜得多，我们还研究了大规模异构POS标记数据的利用。我们在英语和汉语数据集上进行了实验，结果清楚地表明，在使用堆栈联合框架时，词性标记（同质和异构）仍然可以显著提高解析性能。我们进行了详细的分析，从语言方面获得了更多的见解。</pre></li>
<li><a href="https://arxiv.org/abs/2002.01685">Parsing as Pretraining</a> (AAAI2020)
<pre>最近的分析表明，为语言建模而预先训练的编码器捕获了某种形态句法结构。然而，单词向量的探测框架仍然不会报告标准设置（如成分和依赖项解析）的结果。本文解决了这个问题，并仅依靠预训练架构（而非解码）进行完整解析（英语）。我们首先将成分和依赖项解析转换为序列标记。然后，我们使用一个前馈层直接将字向量映射到编码线性化树的标签。这是用来：（i）看看我们可以在语法建模方面只使用预训练编码器走多远，（ii）阐明不同词向量的语法敏感性（通过在训练期间冻结预训练网络的权重）。为了进行评估，我们使用括号中的F1分数和LAS，并分析跨度长度和相关性位移表示的深度差异。总体结果超过了PTB（93.5%）和端到端EN-EWT UD（78.8%）上现有的序列标签解析器。</pre></li>
<li><a href="https://arxiv.org/abs/1909.06775">Cross-Lingual BERT Transformation for Zero-Shot Dependency Parsing</a>
<pre>本文研究在语境空间中学习跨语言表征的问题。我们提出了跨语言BERT变换（CLBT），这是一种基于公开的预训练BERT模型生成跨语言语境化单词嵌入的简单有效方法（Devlin等人，2018）。在这种方法中，从上下文词对齐中学习线性变换，以对齐用不同语言独立训练的上下文化嵌入。我们证明了这种方法在零炮跨语言迁移分析中的有效性。实验表明，我们的嵌入大大优于以前使用静态嵌入的最新技术。我们进一步将我们的方法与XLM（Lample和Conneau，2019）进行了比较，XLM是最近提出的一种跨语言语言模型，使用大量并行数据进行训练，并取得了极具竞争力的结果。</pre></li>
<li><a href="https://arxiv.org/abs/2003.13118">Recursive Non-Autoregressive Graph-to-Graph Transformer for Dependency Parsing with Iterative Refinement</a>
<pre>我们提出了递归非自回归图到图转换器架构（RNGTr），通过递归应用非自回归图到图转换器，对任意图进行迭代细化，并将其应用于语法依赖分析。我们通过使用BERT预先训练的细化模型，在几个依赖语料库上展示了RNGTr的威力和有效性。我们还介绍了语法转换器（SynTr），一种类似于我们的细化模型的非递归解析器。RNGTr可以提高来自Universal Dependencies Treebanks、英语和汉语Penn Treebanks以及德语CoNLL2009语料库的13种语言上的各种初始解析器的准确性，甚至比SynTr取得的最新成果还要高，显著提高了所有测试语料库的最新水平。</pre></li>
<li><a href="https://arxiv.org/abs/2012.00857">StructFormer: Joint Unsupervised Induction of Dependency and Constituency Structure from Masked Language Modeling</a>
<pre>自然语言语法有两大类——建立单词之间一一对应关系模型的依赖语法和建立一个或多个对应单词组合模型的选区语法。虽然以前的无监督语法分析方法大多只关注于归纳一类语法，但我们引入了一种新的模型StructFormer，它可以同时归纳依赖结构和选区结构。为了实现这一点，我们提出了一个新的解析框架，可以联合生成选区树和依赖图。然后，我们通过一种新的依赖约束自我注意机制，以可微的方式将诱导依赖关系集成到转换器中。实验结果表明，该模型能够同时在无监督选区分析、无监督依赖分析和掩蔽语言建模方面取得很好的效果。</pre></li>
<li><a href="https://arxiv.org/abs/2005.01306">pyBART: Evidence-based Syntactic Transformations for IE</a> [<a href="https://allenai.github.io/pybart/">github</a>]
<pre>句法依赖可以高精度地预测，并且对于机器学习和基于模式的信息提取任务都很有用。然而，它们的实用性是可以提高的。这些句法依赖关系被设计为准确地反映句法关系，并且它们不会使语义关系显式化。因此，这些表示缺少内容词之间的许多明确连接，这对于下游应用程序很有用。像English Enhanced UD这样的建议通过扩展带有额外显式弧的通用依赖树来改善这种情况。但是，Python用户无法使用它们，而且覆盖范围也有限。我们引入了一个覆盖范围广泛、数据驱动且语言合理的转换集，使事件结构和许多词汇关系显式化。我们介绍pyBART，一个易于使用的开源Python库，用于将英文UD树转换为增强的UD图或表示形式。该库可以作为独立软件包使用，也可以集成在spaCy NLP管道中。在基于模式的关系提取场景中进行评估时，我们的表示比增强的UD获得更高的提取分数，同时需要更少的模式。</pre></li>
<li><a href="https://arxiv.org/abs/1910.02403">Named Entity Recognition – Is there a glass ceiling?</a> (CoNLL2019)
<pre>命名实体识别（NER）的最新发展带来了越来越好的模型。但是，有玻璃天花板吗？我们是否知道哪些类型的错误仍然难以纠正，甚至不可能纠正？在本文中，我们详细分析了最先进的机器学习（ML）方法中的错误类型。我们的研究揭示了斯坦福、CMU、FLAIR、ELMO和BERT模型的优缺点，以及它们共同的局限性。我们还介绍了改进注释、训练过程和检查模型质量和稳定性的新技术。给出的结果基于CoNLL 2003英语数据集。补充资料中附有该数据集和新诊断数据集错误的新丰富语义注释。</pre></li>
<li><a href="https://arxiv.org/abs/1910.11476">A Unified MRC Framework for Named Entity Recognition</a>
<pre>根据命名实体是否嵌套，命名实体识别（NER）任务通常分为嵌套NER和平面NER。通常为这两个任务分别开发模型，因为序列标签模型是平面NER最广泛使用的主干，它只能为特定令牌分配一个标签，这不适用于嵌套的NER，其中一个令牌可以分配多个标签。在本文中，我们提出了一个统一的框架，能够处理平面和嵌套的NER任务。我们建议将NER任务描述为机器阅读理解（MRC）任务，而不是将其视为序列标记问题。例如，将使用\textsc{per}标签提取实体形式化为提取问题“{\it文本中提到了哪个人？}”的答案。该公式自然地解决了嵌套NER中的实体重叠问题：提取不同类别的两个重叠实体需要回答两个独立的问题。此外，由于查询编码了信息丰富的先验知识，因此该策略有助于实体提取过程，从而不仅提高了嵌套NER的性能，而且提高了平面NER的性能。我们在{\em nested}和{\em flat}数据集上进行实验。实验结果证明了该方法的有效性。我们能够在嵌套的NER数据集上，即在ACE04、ACE05、GENIA和KBP17上，分别为+1.28、+2.55、+5.44、+6.37，在当前SOTA模型的基础上，实现大量性能提升，以及在平坦的NER数据集上，分别为+0.24、+1.95、+0.21、+1.49，在English CoNLL 2003、English OntoNotes 5.0、Chinese MSRA上的SOTA结果，中文注释4.0。</pre></li>
<li><a href="https://arxiv.org/abs/2009.01560">Biomedical named entity recognition using BERT in the machine reading comprehension framework</a>
<pre>从生物医学文献中识别生物医学实体是一个具有挑战性的研究热点，它是将大量非结构化文本中存在的生物医学知识提取为结构化格式的基础。利用序列标记框架实现生物医学命名实体识别（BioNER）是目前常用的方法。然而，这种方法往往不能充分利用数据集中的语义信息，并且性能并不总是令人满意的。在这项工作中，我们没有将BioNER任务视为序列标记问题，而是将其表述为机器阅读理解（MRC）问题。该公式可以利用精心设计的查询引入更多的先验知识，并且不再需要解码过程，例如条件随机场（CRF）。我们在六个BioNER数据集上进行了实验，实验结果证明了我们方法的有效性。我们的方法在BC4CHEMD、BC5CDR化学、BC5CDR疾病、NCBI疾病、BC2GM和JNLPBA数据集上实现了最先进的（SOTA）性能，F1得分分别为92.92%、94.19%、87.83%、90.04%、85.48%和78.93%。</pre></li>
<li><a href="https://arxiv.org/abs/1910.06294">Training Compact Models for Low Resource Entity Tagging using Pre-trained Language Models</a>
<pre>低资源命名实体识别任务的培训模型已被证明是一项挑战，特别是在工业应用中，部署更新模型是一项持续的工作，对业务运营至关重要。在这种情况下，通常会有大量未标记的数据，而标记的数据很少或不可用。经过训练从文本中提取上下文特征的预先训练的语言模型显示，通过利用迁移学习，可以改善许多自然语言处理（NLP）任务，包括几乎没有标记的任务。然而，这样的模型带来了沉重的内存和计算负担，因此训练和部署这样的模型用于推理是一个挑战。在这项正在进行的工作中，我们将预先训练的蒙面语言模型提供的迁移学习的有效性与半监督方法相结合，使用标记和未标记的示例来训练快速而紧凑的模型。初步评估表明，与最先进的预先训练的语言模型相比，紧凑型模型能够以36倍的压缩率实现具有竞争力的准确性，并且推理速度显著加快，允许在生产环境或边缘设备上部署此类模型。</pre></li>
<li><a href="https://arxiv.org/abs/1912.07095">Robust Named Entity Recognition with Truecasing Pretraining</a> (AAAI2020)
<pre>尽管现代命名实体识别（NER）系统在标准数据集上表现出令人印象深刻的性能，但当呈现有噪声的数据时，它们的性能很差。特别是，对于许多语言中的实体来说，大写是一个强烈的信号，即使是最先进的模型也过度适应这一特性，在非大写文本上的性能大大降低。在这项工作中，我们使用预测文本中的大小写的预训练目标，或利用未标记数据的truecaser，解决了具有噪声或不确定大小写的数据中NER系统的鲁棒性问题。通过将输出分布附加到字符嵌入，预训练的truecaser与NER的标准BiLSTM CRF模型相结合。在几个不同域和大小写质量的数据集上进行的实验表明，我们的新模型提高了未加密文本的性能，甚至为未加密的BERT嵌入增加了价值。我们的方法在WNUT17共享任务数据集上实现了一种新的技术状态。</pre></li>
<li><a href="https://arxiv.org/abs/2001.02524">LTP: A New Active Learning Strategy for Bert-CRF Based Named Entity Recognition</a>
<pre>近年来，深度学习在包括命名实体识别在内的许多自然语言处理任务中取得了巨大的成功。缺点是通常需要大量手动注释的数据。以往的研究表明，主动学习可以大大降低数据标注的成本，但仍有很大的改进空间。在实际应用中，我们发现现有的基于不确定性的主动学习策略有两个缺点。首先，这些策略倾向于显式或隐式地选择长序列，这增加了注释者的注释负担。其次，一些策略需要入侵模型并进行修改以生成一些额外的样本选择信息，这将增加开发人员的工作量并增加模型的训练/预测时间。在本文中，我们首先在一个特定的BiLstm CRF案例中考察了传统的主动学习策略，该案例已广泛应用于几个典型数据集上的命名实体识别。然后，我们提出了一种基于不确定性的主动学习策略，称为最低令牌概率（LTP），该策略结合CRF的输入和输出来选择信息实例。LTP是一种简单而强大的策略，它不支持长序列，并且不需要侵入模型。我们在多个数据集上对LTP进行了测试，实验表明，LTP在句子层面的准确性和实体层面的F1分数上都略优于传统策略，注释标记明显较少。相关代码已于发布https://github.com/HIT-ICES/AL-NER</pre></li>
<li><a href="https://arxiv.org/abs/2005.07150">Named Entity Recognition as Dependency Parsing</a> (ACL2020)
<pre>命名实体识别（namedentityrecognition，NER）是自然语言处理中的一项基本任务，涉及到识别表示实体引用的文本范围。NER研究通常只关注平面实体（平面NER），忽略实体引用可以嵌套的事实，如[中国银行]（Finkel和Manning，2009）。在本文中，我们使用基于图的依赖解析的思想，通过一个比亚芬模型（Dozat和Manning，2017）为我们的模型提供输入的全局视图。biaffine模型在一个句子中对开始和结束标记进行评分，我们使用它来探索所有跨度，因此该模型能够准确预测命名实体。通过对8个语料库的评估，我们发现该模型对嵌套和平面的NER都很有效，并且在所有语料库上都实现了SoTA性能，准确率提高了2.2个百分点。</pre></li>
<li><a href="https://arxiv.org/abs/2006.01563">Exploring Cross-sentence Contexts for Named Entity Recognition with BERT</a>
<pre>命名实体识别（NER）通常被称为序列分类任务，其中每个输入由一句文本组成。然而，很明显，对于这项任务来说，有用的信息往往可以在单句语境的范围之外找到。最近提出的自我注意模型（如BERT）既能有效地捕捉输入中的远距离关系，又能表示由多个句子组成的输入，为在自然语言处理任务中加入跨句子信息的方法创造了新的机会。在这篇论文中，我们提出了一个系统的研究，探讨了在五种语言中，使用BERT模型，使用跨句信息进行NER。我们发现，在BERT输入中添加额外句子形式的上下文系统地提高了所有测试语言和模型的NER性能。在每种输入中包含多个句子也可以让我们在不同的上下文中研究相同句子的预测。我们提出了一种简单的方法，上下文多数投票（CMV），将不同的句子预测结合起来，并证明了这一点，以进一步提高BERT的NER性能。我们的方法不需要对基础的BERT体系结构进行任何更改，而是依赖于用于训练和预测的重构示例。对已建立的数据集（包括CoNLL'02和CoNLL'03 NER基准）的评估表明，我们提出的方法可以改进英语、荷兰语和芬兰语的最新NER结果，实现基于德语的最佳报告BERT结果，并且与西班牙语中其他基于伯特的方法报告的性能相当。我们在开放许可证下发布本工作中实现的所有方法。</pre></li>
<li><a href="https://arxiv.org/abs/2012.04373">CrossNER: Evaluating Cross-Domain Named Entity Recognition</a> (AAAI2021) [<a href="https://github.com/zliucr/CrossNER">github</a>]
<pre>跨域命名实体识别（NER）模型能够解决目标域内NER样本的稀缺性问题。然而，大多数现有的NER基准缺乏领域专用实体类型，或者不关注某个领域，导致跨领域评估的效率较低。为了解决这些障碍，我们引入了一个跨域NER数据集（CrossNER），这是一个完全标记的NER数据集合，跨越五个不同的域，具有针对不同域的专门实体类别。此外，我们还提供了一个与领域相关的语料库，因为使用它来继续预训练语言模型（领域自适应预训练）对于领域自适应是有效的。然后，我们进行综合实验，探索利用不同层次的领域语料库和预训练策略对跨领域任务进行领域自适应预训练的有效性。结果表明，关注包含领域专门实体的分数语料库并在领域自适应预训练中使用更具挑战性的预训练策略有利于NER领域自适应，并且我们提出的方法能够始终优于现有的跨领域NER基线。然而，实验也说明了这项跨域任务的挑战。我们希望我们的数据集和基线将促进NER领域适应领域的研究。有关代码和数据，请访问https://github.com/zliucr/CrossNER.</pre></li>
<li><a href="https://arxiv.org/abs/2006.01372">Embeddings of Label Components for Sequence Labeling: A Case Study of Fine-grained Named Entity Recognition</a> (ACL2020 SRW)
<pre>通常，序列标签中使用的标签由不同类型的元素组成。例如，IOB格式的实体标签，如B-Person和I-Person，可以分解为span（B和I）和类型信息（Person）。然而，虽然大多数序列标记模型不考虑这样的标签组件，标签之间的共享组件，如人，可以是有益的标签预测。在这项工作中，我们建议将标签组件信息作为嵌入到模型中。通过对英文和日文细粒度命名实体识别的实验，我们证明了该方法提高了性能，特别是对于低频标签的实例。</pre></li>
<li><a href="https://arxiv.org/abs/2006.15509">BOND: BERT-Assisted Open-Domain Named Entity Recognition with Distant Supervision</a> (KDD2020) [<a href="https://github.com/cliang1453/BOND">github</a>]
<pre>研究了远程监控下的开放域命名实体识别问题。远程监控虽然不需要大量的手动注释，但通过外部知识库生成高度不完整和嘈杂的远程标签。为了应对这一挑战，我们提出了一个新的计算框架——BOND，它利用预先训练的语言模型（如BERT和RoBERTa）的能力来提高NER模型的预测性能。具体来说，我们提出了一个两阶段的训练算法：第一阶段，我们使用距离标签将预先训练好的语言模型适应于NER任务，这可以显著提高召回率和准确率；在第二阶段，我们去掉了遥远的标签，并提出了一种自我训练的方法来进一步提高模型的性能。在5个基准数据集上进行的深入实验表明，BOND方法优于现有的远程监督NER方法。代码和远程标记的数据已在中发布https://github.com/cliang1453/BOND.</pre></li>
<li><a href="https://arxiv.org/abs/2004.04564">Interpretability Analysis for Named Entity Recognition to Understand System Predictions and How They Can Improve</a>
<pre>命名实体识别系统在英语新闻等领域取得了显著的性能。很自然地会问：这些模型实际上在学习什么来实现这一点？他们只是自己记住名字吗？或者他们能够解释文本并从语言语境中推断出正确的实体类型？我们通过对比LSTM-CRF体系结构用于命名实体识别的几种变体的性能来研究这些问题，其中一些仅提供上下文作为特征的表示。我们也为伯特做了类似的实验。我们发现上下文表示确实有助于提高系统性能，但驱动高性能的主要因素是学习名称标记本身。我们招募了人工注释员来评估仅从上下文推断实体类型的可行性，并发现，尽管对于仅上下文系统造成的大多数错误，人们也无法推断实体类型，但仍有一些改进空间。系统应该能够正确识别预测上下文中的任何名称，我们的实验表明，当前系统可能会通过这种能力得到进一步改进。</pre></li>
<li><a href="https://arxiv.org/abs/2004.12440">Single-/Multi-Source Cross-Lingual NER via Teacher-Student Learning on Unlabeled Data in Target Language</a> (ACL2020)
<pre>为了更好地解决具有少量/无标记数据的语言的命名实体识别（NER）问题，跨语言的NER必须有效地利用从具有丰富标记数据的源语言学习的知识。以往关于跨语言交际的研究大多基于成对文本的标签投影或直接模式转换。但是，如果源语言中的标记数据不可用，或者不利用目标语言中未标记数据中包含的信息，则此类方法不适用。在本文中，我们提出了一种教师-学生学习方法来解决这些限制，其中源语言中的NER模型被用作教师，用目标语言中的未标记数据训练学生模型。所提出的方法适用于单源和多源跨语言网络。对于后者，我们进一步提出了一种相似性度量方法，以更好地衡量来自不同教师模型的监督。在基准数据集上对3种目标语言进行的大量实验表明，我们的方法在单源和多源跨语言环境下都优于现有的最新方法。</pre></li>
<li><a href="https://arxiv.org/abs/2010.14042">To BERT or Not to BERT: Comparing Task-specific and Task-agnostic Semi-Supervised Approaches for Sequence Tagging</a> (EMNLP2020)
<pre>使用类似转换器的体系结构（如BERT）利用大量未标记的数据在最近得到了广泛的应用，这是因为它们在学习通用表示方面的有效性，然后可以针对下游任务进行进一步的微调，以获得更大的成功。然而，从经济和环境的角度来看，培训这些模型可能成本高昂。在这项工作中，我们研究了如何有效地使用未标记的数据：通过探索任务特定的半监督方法，交叉视图训练（CVT），并将其与任务不可知的BERT在多个环境中进行比较，包括领域和任务相关的英语数据。CVT使用了更轻的模型体系结构，我们证明了它在一组序列标记任务上实现了与BERT类似的性能，对财务和环境的影响较小。</pre></li>
<li><a href="https://arxiv.org/abs/2008.10570">Example-Based Named Entity Recognition</a>
<pre>我们提出了一种新的方法命名实体识别（NER）中存在的稀缺数据，我们称之为基于实例的NER。我们的无训练少数镜头学习方法从问答中获得灵感，以识别新的未知领域中的实体跨度。与目前的最新技术相比，所提出的方法的性能明显更好，尤其是在使用较少的支持示例时。</pre></li>
<li><a href="https://arxiv.org/abs/2011.06993">FLERT: Document-Level Features for Named Entity Recognition</a>
<pre>当前最先进的命名实体识别方法（NER）通常考虑句子级的文本，因此不模拟越过句子边界的信息。但是，基于transformer的NER模型的使用为捕获文档级功能提供了自然的选择。在本文中，我们对文献中常用的两种标准NER体系结构（即“微调”和“基于特征的LSTM-CRF”）中的文档级功能进行了比较评估。我们评估文档级功能的不同超参数，如上下文窗口大小和强制文档位置。我们展示了一些实验，从中我们得出了如何对文档上下文建模的建议，并在几个CoNLL-03基准数据集上展示了新的最先进的分数。我们的方法被集成到Flair框架中，以便于复制我们的实验。</pre></li>
<li><a href="https://arxiv.org/abs/2012.05426">Empirical Analysis of Unlabeled Entity Problem in Named Entity Recognition</a>
<pre>在许多情况下，命名实体识别（NER）模型严重存在未标记实体问题，其中句子的实体可能没有完全注释。通过对合成数据集的实证研究，我们发现了性能下降的两个原因。一种是减少注释实体，另一种是将未标记实体视为负实例。第一个原因的影响小于第二个原因，可以通过采用预培训语言模型来缓解。第二个原因严重误导了一个模型的训练，并极大地影响了它的表现。基于上述观察，我们提出了一种通用方法，几乎可以消除未标记实体带来的误导。关键思想是使用负采样，这在很大程度上避免了使用未标记实体训练NER模型。在合成数据集和真实数据集上的实验表明，我们的模型对未标记实体问题具有鲁棒性，并且优于先前的基线。在注释良好的数据集上，我们的模型与最先进的方法具有竞争力。</pre></li>
<li><a href="https://arxiv.org/abs/2007.06897">What’s in a Name? Are BERT Named Entity Representations just as Good for any other Name?</a> (ACL2020 WS)
<pre>我们通过研究基于BERT的NLP模型对输入中相同类型类的替换的鲁棒性，来评估基于BERT的NLP模型的命名实体表示。我们强调，在一些任务中，虽然这种扰动是自然的，但经过最先进训练的模型却异常脆弱。即使使用最新的实体感知的BERT模型，脆性仍然存在。考虑到标记化和出现频率等因素，我们还试图找出这种非稳健性的原因。然后，我们提供了一种简单的方法，该方法集成来自多个替换的预测，同时联合建模类型注释和标签预测的不确定性。在三个NLP任务上的实验表明，我们的方法在自然数据集和对抗数据集上都增强了鲁棒性并提高了准确性。</pre></li>
<li><a href="https://arxiv.org/abs/2011.06854">Interpretable Multi-dataset Evaluation for Named Entity Recognition</a> (EMNLP2020) [<a href="https://github.com/neulab/InterpretEval">github</a>]
<pre>随着自然语言处理任务模型的激增，理解模型之间的差异及其相对优点变得更加困难。简单地看一下整体度量（如精度、BLEU或F1）之间的差异，并不能告诉我们为什么或如何使用特定方法，以及不同的数据集如何影响模型设计选择。在本文中，我们提出了一种通用的方法，用于命名实体识别（NER）任务的可解释性评估。建议的评估方法使我们能够解释模型和数据集之间的差异，以及它们之间的相互作用，确定当前系统的优缺点。通过提供我们的分析工具，我们使未来的研究人员能够轻松地进行类似的分析，并推动这一领域的进展：https://github.com/neulab/InterpretEval.</pre></li>
<li><a href="https://aclanthology.org/2020.emnlp-main.518/">Entity Enhanced BERT Pre-training for Chinese NER</a> (EMNLP2020)</li>
<li><a href="https://arxiv.org/abs/2105.07148">Lexicon Enhanced Chinese Sequence Labeling Using BERT Adapter</a> (ACL2021)
<pre>由于词汇信息和预训练模型（如BERT）各自的优势，已将它们结合起来探索汉语序列标记任务。然而，现有的方法仅仅通过一个浅层的随机初始化序列层来融合词典特征，而没有将它们集成到BERT的底层。在本文中，我们提出了一种用于中文序列标注的词典增强型BERT（LEBERT），它通过词典适配器层将外部词典知识直接集成到BERT层中。与现有的方法相比，我们的模型有助于在底层进行深层词汇知识融合。在命名实体识别、分词和词性标注三个任务的十个中文数据集上的实验表明，LEBERT达到了最先进的结果。</pre></li>
<li><a href="https://aclanthology.org/2020.acl-main.611/">FLAT: Chinese NER Using Flat-Lattice Transformer</a> (ACL2020)</li>
<li><a href="https://arxiv.org/abs/2009.09223">BioALBERT: A Simple and Effective Pre-trained Language Model for Biomedical Named Entity Recognition</a>
<pre>近年来，随着生物医学文档数量的增加，加上自然语言处理算法的进步，生物医学命名实体识别（BioNER）的研究呈指数级增长。然而，BioNER研究具有挑战性，因为生物医学领域中的NER是：（i）由于训练数据有限，通常受到限制，（ii）实体可以根据其上下文引用多种类型和概念，（iii）严重依赖于特定于子领域的首字母缩略词。现有的BioNER方法往往忽视这些问题，直接采用在一般语料库中训练的最先进（SOTA）模型，结果往往不令人满意。我们提出了生物医学ALBERT（来自生物医学文本挖掘变压器的Lite双向编码器表示）bioALBERT，这是一种在大规模生物医学语料库上训练的有效领域特定语言模型，旨在捕获生物医学上下文相关的NER。我们采用了ALBERT中使用的一种自我监督损失模型，该模型侧重于句子间连贯性建模，以更好地学习上下文相关表示，并结合了参数缩减技术，以降低记忆消耗，提高BioNER中的训练速度。在我们的实验中，BioALBERT在八个具有四种不同实体类型的生物医学NER基准数据集上的表现优于比较SOTA-BioNER模型。我们训练了四种不同的BioALBERT模型，可供研究社区在未来研究中使用。</pre></li>
<li><a href="https://arxiv.org/abs/2001.08904">MT-BioNER: Multi-task Learning for Biomedical Named Entity Recognition using Deep Bidirectional Transformers</a>
<pre>Cortana、Alexa和Siri等会话代理正在通过添加新域不断提高其能力。对新领域的支持包括设计和开发许多NLU组件，用于领域分类、意向分类和插槽标记（包括命名实体识别）。每个组件只有在对大量标记数据进行训练时才能表现良好。其次，这些组件部署在有限的内存设备上，这需要一些模型压缩。第三，对于某些领域，如健康领域，很难找到一个涵盖所有所需插槽类型的单一培训数据集。为了克服上述问题，我们提出了一种基于多任务转换器的插槽标记神经结构。我们考虑使用多个数据集覆盖不同时隙类型作为多任务学习问题的时隙标记器的训练。生物医学领域的实验结果表明，所提出的方法在（时间和内存）效率和有效性方面优于先前最先进的用于不同基准生物医学数据集的时隙标记系统。会话代理可以使用输出槽标记器更好地识别输入话语中的实体。</pre></li>
<li><a href="https://arxiv.org/abs/1911.03869">Knowledge Guided Named Entity Recognition for BioMedical Text</a>
<pre>在这项工作中，我们将NER任务描述为一个多答案知识引导的QA任务（KGQA），该任务仅通过分配B、I和O标记来帮助预测实体，而不将实体类型与标记关联。我们提供不同的知识背景，如实体类型、问题、定义和示例以及文本，并在18个生物医学语料库的组合数据集上进行培训。该公式（a）使系统能够从不同的NER数据集中联合学习NER特定特征，（b）可以使用知识文本注意来识别与提供的知识具有更高相似度的单词，从而提高性能，（c）通过将预测类减少为b、I、O和（d）来减少系统混淆使嵌套实体的检测更容易。我们在18个生物医学NER数据集上对此KGQA配方进行了广泛的实验，通过实验我们注意到知识有助于实现更好的性能。我们的问题公式能够在12个数据集中获得最先进的结果。</pre></li>
<li><a href="https://arxiv.org/abs/2101.11112">Cross-Lingual Named Entity Recognition Using Parallel Corpus: A New Approach Using XLM-RoBERTa Alignment</a>
<pre>我们提出了一种基于平行语料库的跨语言命名实体识别零镜头迁移方法。我们在XLM-RoBERTa的基础上建立了一个实体对齐模型，将平行数据中英文部分检测到的实体投射到目标语言句子中，其准确性超过了所有以前的无监督模型。利用对齐模型，我们可以在目标语言中获得伪标记的NER数据集来训练特定于任务的模型。与使用翻译方法不同，这种方法得益于目标语言原始语料库的自然流畅性和细微差别。我们还提出了一种改进的损失函数，类似于焦点损失，但在相反的方向上分配权重，以进一步改进噪声伪标记数据集上的模型训练。我们在4种目标语言的基准数据集上对该方法进行了评估，并获得了与最新SOTA模型相比具有竞争力的F1分数。我们还额外讨论了平行语料库大小和领域对最终迁移性能的影响。</pre></li>
<li><a href="https://arxiv.org/abs/1909.10649">Portuguese Named Entity Recognition using BERT-CRF</a>
<pre>使用神经网络进行语言表示的最新进展使得将训练模型的学习内部状态转移到下游自然语言处理任务（如命名实体识别（NER）和问答）成为可能。研究表明，利用预先训练好的语言模型可以提高许多任务的总体性能，并且在标记数据稀少时非常有益。在这项工作中，我们训练葡萄牙语的BERT模型，并将BERT-CRF体系结构与葡萄牙语的NER任务相结合，将BERT的传输能力与CRF的结构化预测相结合。我们探索基于特征和微调训练策略的伯特模型。我们的微调方法在HAREM I数据集上获得了新的最先进的结果，在选择性场景（5个NE类）上提高了F1分数1分，在总场景（10个NE类）上提高了4分。</pre></li>
<li><a href="https://arxiv.org/abs/1912.01389">Towards Lingua Franca Named Entity Recognition with BERT</a>
<pre>信息提取是NLP中的一项重要任务，它可以自动提取数据以填充关系数据库。从历史上看，研究和数据是为英文文本而产生的，随后几年又产生了阿拉伯文、中文（ACE/OntoNotes）、荷兰语、西班牙语、德语（CoNLL评估）和许多其他语言的数据集。自然趋势是将每种语言视为不同的数据集，并为每种语言建立优化的模型。在本文中，我们研究了一种基于多语言BERT的单一命名实体识别模型，该模型同时在多种语言上进行联合训练，并且能够比仅在一种语言上训练的模型更准确地解码这些语言。为了改进初始模型，我们研究了正则化策略的使用，如多任务学习和部分梯度更新。除了作为一个可以处理多种语言（包括代码转换）的单一模型外，该模型还可以用于对新语言进行开箱即用的零炮预测，即使是那些没有训练数据的语言。结果表明，该模型不仅具有与单语模型相媲美的性能，而且在CoNLL02荷兰和西班牙数据集、OntoNotes阿拉伯语和汉语数据集上也取得了最新的结果。此外，它在看不见的语言上表现相当好，在三种CoNLL语言上实现了最先进的零拍。</pre></li>
<li><a href="https://arxiv.org/abs/2104.04434">Larger-Context Tagging: When and Why Does It Work?</a> (NAACL2021)
<pre>神经网络和预训练技术的发展催生了许多句子级标记系统，这些系统在典型基准测试中取得了优异的性能。然而，一个相对较少讨论的话题是，如果更多的上下文信息被引入到当前的顶级评分标记系统中会怎样。尽管已有几项工作试图将标记系统从句子级转移到文档级，但对于何时以及为什么有效，仍然没有一致的结论，这限制了更大范围的上下文方法在标记任务中的适用性。在本文中，我们不是通过建筑探索来追求最先进的标签系统，而是着重研究作为一种总体策略的大背景培训何时以及为什么能够起作用。为此，我们对四个提议的上下文信息收集聚合器进行了深入的比较研究，并提出了一种属性辅助评估方法来解释更大的上下文训练带来的改进。实验上，我们建立了一个基于四个标记任务和十三个数据集的测试平台。希望我们的初步观察能够加深对更大规模的语境培训的理解，并启发更多关于语境信息使用的后续工作。</pre></li>
</ul>
<h2 id="pronouncoreference-resolution">Pronoun/coreference resolution</h2>
<ul>
<li><a href="https://arxiv.org/abs/2009.12721">A Brief Survey and Comparative Study of Recent Development of Pronoun Coreference Resolution</a>
<pre>代词共指消解（PCR）的任务是将代词表达分解为所有提及的事物。与一般的共指消解任务相比，PCR的主要挑战是共指关系预测，而不是提及检测。作为自然语言理解（NLU）的一个重要组成部分，代词解析对于许多下游任务来说至关重要，但对于现有的模型来说仍然具有挑战性，这促使我们审视现有的方法并思考如何做得更好。在这篇综述中，我们首先介绍了普通代词共指消解任务的代表性数据集和模型。然后，我们关注硬代词共指消解问题（如Winograd模式挑战）的最新进展，以分析当前模型对常识的理解程度。我们进行了大量的实验，以表明即使当前的模型在标准评估集上取得了良好的性能，它们仍然没有准备好用于实际应用（例如，所有SOTA模型都难以正确地将代词解析为不常见的对象）。所有实验代码可在https://github.com/HKUST-KnowComp/PCR.</pre></li>
<li><a href="https://arxiv.org/abs/1906.01161">Resolving Gendered Ambiguous Pronouns with BERT</a> (ACL2019 WS)
<pre>代词消解是指代消解的一部分，指代消解的任务是将一个表达式与其指代实体配对。这是自然语言理解的一项重要任务，也是机器翻译系统、聊天机器人和助手的必要组成部分。神经机器学习系统在这项任务中的表现远远不理想，在现代基准数据集上F1分数低达73%。此外，男性代词比女性代词表现得更好。因此，这个问题对NLP研究者和实践者来说既具有挑战性又很重要。在这个项目中，我们描述了我们基于BERT的方法来解决性别平衡代词消解问题。在谷歌人工智能语言团队共享的基准数据集上，我们能够达到92%的F1分数和更低的性别偏见。</pre></li>
<li><a href="https://arxiv.org/abs/1905.01780">Anonymized BERT: An Augmentation Approach to the Gendered Pronoun Resolution Challenge</a> (ACL2019 WS)
<pre>我们提出了性别代词解析挑战的第七名解决方案，该方案使用了无微调的BERT和一种新的增强策略，该策略设计用于上下文嵌入标记级任务。我们的方法通过用一组公共占位符名称替换候选名称来匿名引用对象。除了有效增加训练数据大小的通常好处外，这种方法还使姓名中嵌入的特殊信息多样化。使用相同的一组普通名字也可以帮助模型更好地识别名字，缩短标记长度，消除与名字相关的性别和地区偏见。该系统在第2阶段的原木损失得分为0.1947，其中增加的原木损失有助于提高0.04。赛后分析表明，当使用不同的嵌入层时，系统得分为0.1799，排名第三。</pre></li>
<li><a href="https://arxiv.org/abs/1906.03695">Gendered Pronoun Resolution using BERT and an extractive question answering formulation</a> (ACL2019 WS)
<pre>歧义代词的消解是自然语言理解中的一个长期挑战。最近的研究表明，在最先进的共指消解系统中存在性别偏见。例如，谷歌人工智能语言团队最近发布了一个性别平衡的数据集，并表明这些共指解析器在数据集上的性能受到很大限制。在本文中，我们提出了代词消解任务的抽取式问答（QA）公式，它克服了这一限制，并且在数据集上显示出更低的性别偏见（0.99）。该系统使用预先训练好的BERT模型的微调表示，在不使用任何手工设计功能的情况下，其性能显著优于现有基线（F1分数绝对提高22.2%）。即使不知道代词的候选先行词，这个QA框架也同样有效。QA和基于BERT的多项选择和序列分类模型的集成进一步改善了F1（比基线绝对改善23.3%）。该集成模型已提交给第一届ACL自然语言处理性别偏见研讨会的共享任务。它在最终的官方排行榜上排名第九。源代码可在https://github.com/rakeshchada/corefqa</pre></li>
<li><a href="https://arxiv.org/abs/1908.00308">MSnet: A BERT-based Network for Gendered Pronoun Resolution</a> (ACL2019 WS)
<pre>预训练的BERT模型在自然语言处理的广泛任务中达到了卓越的技术水平。为了解决性别代词消解任务中的性别偏见问题，本文提出了一种基于预训练BERT的神经网络模型。该模型是一种提及分数分类器，使用无参数的注意机制计算实体广度的上下文表示，并使用向量表示代词和实体之间的三重语义相似度。在性别代词分辨任务的第1阶段，该模型的一个变体，通过微调方法训练，在训练集的5倍交叉验证中将多类对数损失减少到0.3033，在测试集将多类对数损失减少到0.2795。此外，该变体在任务第二阶段以0.17289分获得第二名。本文中的代码可从以下网址获得：https://github.com/ziliwang/MSnet-for-Gendered-PronounResolution</pre></li>
<li><a href="https://arxiv.org/abs/2006.08881">Scalable Cross Lingual Pivots to Model Pronoun Gender for Translation</a>
<pre>文档理解不足的机器翻译系统在将省略的或中性的代词翻译成带有性别代词的语言（例如英语）时可能会出错。预测这些代词的潜在性别是困难的，因为它不是文本标记的，而必须根据上下文中的相关提及来推断。我们提出了一种新的跨语言旋转技术自动产生高品质的性别标签，并显示，该数据可以用来微调伯特分类器与92% F1的西班牙辍学女性代词，相比之下，30-51%的神经机器翻译模型和54-71%的非精细调谐伯特模型。我们增加了一个神经机器翻译模型与标签从我们的分类器，以提高代词翻译，同时仍然具有并行化翻译模型，翻译一个句子一次。</pre></li>
<li><a href="https://www.aclweb.org/anthology/papers/W/W19/W19-3815/">Fill the GAP: Exploiting BERT for Pronoun Resolution</a> (ACL2019 WS)</li>
<li><a href="https://www.aclweb.org/anthology/W19-3816/">On GAP Coreference Resolution Shared Task: Insights from the 3rd Place Solution</a> (ACL2019 WS)</li>
<li><a href="https://arxiv.org/abs/1905.08868">Look Again at the Syntax: Relational Graph Convolutional Network for Gendered Ambiguous Pronoun Resolution</a> (ACL2019 WS)
<pre>在现有的共指消解者中发现了性别偏见。为了消除性别偏见，发布了一个性别平衡的数据集性别模糊代词（GAP），最佳基线模型仅达到66.9%F1。来自Transformers的双向编码器表示（BERT）打破了几个NLP任务记录，可用于GAP数据集。然而，在特定任务上进行微调在计算上是昂贵的。在本文中，我们提出了一种端到端解析器，它将预训练的BERT与关系图卷积网络（R-GCN）相结合。R-GCN用于消化结构句法信息和学习更好的任务特定嵌入。实证结果表明，在明确的句法监督下，在不需要微调BERT的情况下，R-GCN的嵌入在共指任务上优于原始BERT嵌入。我们的工作将GAP数据集上的代码片段上下文基线F1分数从66.9%显著提高到80.3%。我们参与了2019年GAP共同参考共享任务，我们的代码可在线获取。</pre></li>
<li><a href="https://arxiv.org/abs/2105.12392">Unsupervised Pronoun Resolution via Masked Noun-Phrase Prediction</a> (ACL2021)
<pre>在这项工作中，我们提出了蒙面名词短语预测（MNPP），这是一种在完全无监督的环境下处理代词分解的预训练策略。首先，我们在没有任何微调的情况下，在各种代词分辨率数据集上评估我们预先训练的模型。我们的方法在所有数据集上都比以前的无监督方法有很大的优势。其次，我们继续进行一些镜头设置，分别在WinoGrande-S和XS上微调预先训练的模型。我们的方法比RoBERTa large baseline具有更大的利润，同时，在对WinoGrande剩余的三次官方拆分进行进一步微调后，获得了更高的AUC分数。</pre></li>
<li><a href="https://www.aclweb.org/anthology/papers/W/W19/W19-3811/">BERT Masked Language Modeling for Co-reference Resolution</a> (ACL2019 WS)</li>
<li><a href="https://www.aclweb.org/anthology/P19-1066/">Coreference Resolution with Entity Equalization</a> (ACL2019)</li>
<li><a href="https://arxiv.org/abs/1908.09091">BERT for Coreference Resolution: Baselines and Analysis</a> (EMNLP2019) [<a href="https://github.com/mandarjoshi90/coref">github</a>]
<pre>我们将BERT应用于共指消解，在OntoNotes（+3.9 F1）和GAP（+11.5 F1）基准上实现了显著的改进。对模型预测的定性分析表明，与ELMo和BERT base相比，BERT large在区分相关但不同的实体（如总裁和首席执行官）方面尤其出色。然而，在建模文档级上下文、对话和提及释义方面仍有改进的余地。我们的代码和模型是公开的。</pre></li>
<li><a href="https://arxiv.org/abs/1908.08025">WikiCREM: A Large Unsupervised Corpus for Coreference Resolution</a> (EMNLP2019)
<pre>代词分解是自然语言理解的一个主要领域。然而，大规模的训练集仍然很少，因为手动标记数据的成本很高。在这项工作中，我们介绍了WikiCREM（WikipediaCoreferencesMasked），这是一个大规模但准确的代词消歧实例数据集。我们结合WikiCREM数据集使用基于语言模型的代词解析方法。我们在一系列不同且具有挑战性的共指消解问题上比较了一系列模型，其中我们在7个数据集中的6个数据集（如GAP、DPR、WNLI、PDP、WinoBias和WinGender）上匹配或优于先前的最先进方法。我们发布了我们的模型，用于解决代词消歧问题。</pre></li>
<li><a href="https://arxiv.org/abs/2101.12637">CD2CR: Co-reference Resolution Across Documents and Domains</a> (EACL2021)
<pre>跨文档共同引用解析（CDCR）的任务是在许多文本文档中识别和链接对实体和概念的引用。当前用于此任务的最先进模型假定所有文档都属于同一类型（例如新闻文章）或属于同一主题。但是，还需要跨不同的域（类型或主题）执行CDCR。本文中我们关注的一个特殊用例是在科学工作和讨论它们的报纸文章中提到的实体的解析。在科学文章和新闻中识别相同的实体和相应的概念可以帮助科学家了解他们的工作在主流媒体中的表现。我们提出了一个新的任务和英语数据集，用于跨文档跨域共同引用解析（CD$^2$CR）。该任务旨在跨异构文档类型识别实体之间的链接。我们发现，在这种跨域、跨文档的设置中，现有的CDCR模型性能不佳，我们提供的基线模型优于CD$^2$CR上当前最先进的CDCR模型。我们的数据集，注释工具和指南以及我们的跨文档跨域共同引用模型都是作为开放访问开源资源提供的。</pre></li>
<li><a href="https://arxiv.org/abs/1908.11141">Ellipsis Resolution as Question Answering: An Evaluation</a> (EACL2021)
<pre>大多数情况下，如果不是所有形式的省略（例如Mary）都与阅读理解问题（Mary做什么）相似，那么为了解决这些问题，我们需要在前面的语篇中确定一个适当的语篇跨度。根据这一观察，我们提出了一种基于问答（QA）体系结构的英语省略消解方法。我们提出了单任务模型和在辅助QA和共指消解数据集上训练的联合模型，在水闸省略（从70.00到86.01 F1）和动词短语省略（从72.89到78.66 F1）方面明显优于目前的技术水平。</pre></li>
<li><a href="https://arxiv.org/abs/1911.01746">Coreference Resolution as Query-based Span Prediction</a>
<pre>在这篇文章中，我们提出了一种精确的、可扩展的共指消解方法。我们将这个问题描述为一个跨度预测任务，就像在机器阅读理解（MRC）中一样：使用每个候选词周围的上下文为每个候选词生成一个查询，并使用跨度预测模块使用生成的查询来提取文档中相关指代的文本跨度。该公式具有以下主要优点：（1）跨度预测策略提供了检索在提及建议阶段遗漏的提及的灵活性；（2） 在MRC框架中，在查询中显式地对提及及其上下文进行编码，可以对嵌入在相关提及上下文中的线索进行深入彻底的检查；（3）大量现有的MRC数据集可用于数据扩充，以提高模型的泛化能力。实验表明，与以前的车型相比，性能显著提升，在GAP基准上F1得分为87.5（+2.5），在CoNLL-2012基准上F1得分为83.1（+3.5）。</pre></li>
<li><a href="https://arxiv.org/abs/2004.06870">Coreferential Reasoning Learning for Language Representation</a> (EMNLP2020)
<pre>像BERT这样的语言表示模型能够有效地从纯文本中获取上下文语义信息，并且已经被证明在许多下游NLP任务中通过适当的微调可以获得有希望的结果。然而，大多数现有的语言表征模型不能明确地处理共指现象，这对于连贯理解整个语篇至关重要。为了解决这个问题，我们提出了CorefBERT，一种新的语言表示模型，它可以捕捉上下文中的相关关系。实验结果表明，与现有的基线模型相比，CorefBERT可以在需要协同推理的各种下游NLP任务上取得一致的显著改进，同时在其他常见NLP任务上保持与先前模型相当的性能。本文的源代码和实验细节可以从https://github.com/thunlp/CorefBERT.</pre></li>
<li><a href="https://arxiv.org/abs/2005.00128">Revisiting Memory-Efficient Incremental Coreference Resolution</a>
<pre>我们通过扩展增量聚类算法，利用上下文编码器和神经组件，研究了在固定内存约束下的共指消解建模。给定一个新句子，我们的端到端算法根据从早期文档上下文（如果有）创建的显式实体表示提出并评分每个提及范围。然后使用这些跨距更新实体的表示，以免忘记；我们在整个文档中只保留一组固定的突出实体。在这项工作中，我们成功地转换了一个高性能模型（Joshi et al.，2020），渐进地将其内存使用量减少到恒定空间，在OntoNotes 5.0上F1的相对损失仅为0.3%。</pre></li>
<li><a href="https://arxiv.org/abs/2009.12013">Revealing the Myth of Higher-Order Inference in Coreference Resolution</a> (EMNLP2020)
<pre>本文分析了高阶推理对共指消解任务的影响。近几年来，几乎所有的共指消解模型都采用了HOI，但没有对其在表征学习中的真正有效性进行太多研究。为了进行全面的分析，我们实现了一个端到端的共指系统以及四种HOI方法，参与先行、实体均衡、跨度聚类和聚类合并，其中后两种是我们最初的方法。我们发现，对于SpanBERT这样的高性能编码器，HOI的影响从负面到边缘，为这项任务提供了一个新的HOI视角。我们使用集群合并的最佳模型显示了CoNLL 2012共享任务数据集上80.2的Avg-F1（英文）。</pre></li>
<li><a href="https://arxiv.org/abs/2101.00434">Coreference Resolution without Span Representations</a> (ACL2021)
<pre>预训练语言模型的引入将许多复杂的特定于任务的NLP模型简化为简单的轻量级层。这种趋势的一个例外是共指消解，在预训练的transformer编码器中附加了复杂的特定于任务的模型。该模型虽然非常有效，但内存占用非常大，这主要是由于动态构造的跨距和跨距对表示，这妨碍了完整文档的处理以及在单个批处理中对多个实例进行训练的能力。我们引入了一个轻量级的端到端协同引用模型，该模型消除了对跨度表示、手工制作的特性和启发式的依赖。我们的模型与当前的标准模型相比具有竞争力，同时更简单、更高效。</pre></li>
<li><a href="https://arxiv.org/abs/1907.12524">Neural Mention Detection</a> (LREC2020)
<pre>提及检测是注释和解释的重要预处理步骤，在NER和共指解析等应用中，但很少有独立的神经模型能够处理所有提及。在这项工作中，我们提出并比较了三种基于神经网络的提及检测方法。第一种方法基于最先进的共指消解系统的提及检测部分；第二种方法使用ELMO嵌入、双向LSTM和双仿射分类器；第三种方法使用最近引入的伯特模型。我们的最佳模型（使用双ffine分类器）与高召回率共指注释设置中的强基线相比，在提及召回率上获得高达1.8个百分点的收益。在高F1注释设置下，与CONLL和CRAC共参照数据集上分别报告的最佳提及检测F1相比，同一模型实现了高达5.3和6.2 p.p.的改进。然后，我们评估我们的模型，通过使用我们的最佳模型在最新的协同参考系统中预测的提及来解决协同参考问题。与我们强大的基线系统（管道系统和端到端系统）相比，增强型模型实现了高达1.7和0.7 p.p.的绝对改进。对于nested NER，我们的模型在GENIA语料库上的评估表明，我们的模型与最先进的模型相匹配或优于最先进的模型，尽管我们的模型不是专门为此任务设计的。</pre></li>
<li><a href="https://www.aclweb.org/anthology/2020.acl-main.482/">ZPR2: Joint Zero Pronoun Recovery and Resolution using Multi-Task Learning and BERT</a> (ACL2020)</li>
<li><a href="https://aclanthology.org/2020.coling-main.435/">An Empirical Study of Contextual Data Augmentation for Japanese Zero Anaphora Resolution</a> (COLING2020)</li>
<li><a href="https://aclanthology.org/2020.coling-main.114/">BERT-based Cohesion Analysis of Japanese Texts</a> (COLING2020)</li>
<li><a href="https://arxiv.org/abs/2101.11204">Joint Coreference Resolution and Character Linking for Multiparty Conversation</a>
<pre>角色链接是将对话中提到的人与现实世界联系起来的任务，对于理解对话至关重要。为了提高交流的效率，人们经常选择在口语中使用代词（如“她”）或普通短语（如“那个女孩”）而不是命名实体（如“瑞秋”），这使得将这些提及与真实的人联系起来比常规的实体联系任务更具挑战性。为了应对这一挑战，我们建议将不同提及之间的相互引用关系中更丰富的上下文结合起来，以帮助链接。另一方面，考虑到寻找共指聚类本身并不是一项简单的任务，而且可以从全局字符信息中获益，我们建议联合解决这两项任务。具体来说，我们提出了C$^2$，一种共指消解和字符链接的联合学习模型。实验结果表明，C$^2$在这两项任务上都能显著优于以前的工作。进一步分析了模型中各模块的贡献以及各超参数的影响。</pre></li>
<li><a href="https://www.aclweb.org/anthology/2020.crac-1.5/">Sequence to Sequence Coreference Resolution</a> (COLING2020 WS)</li>
<li><a href="https://arxiv.org/abs/2102.09600">Within-Document Event Coreference with BERT-Based Contextualized Representations</a>
<pre>事件共指仍然是信息提取中一个具有挑战性的问题。由于没有任何外部事件知识库，协同引用成为一项集群任务，它依赖于事件提及出现的上下文的有效表示。语境化语言表征的最新进展已被证明在许多任务中是成功的，但是，它们在事件链接中的应用受到限制。在这里，我们提出了一种由三部分组成的方法：（1）使用来自预训练的伯特模型的表示；（2）训练神经分类器；（3）驱动一个简单的聚类算法来创建共指链。我们使用该模型在两个标准数据集上实现了文档内事件协同引用任务的最新结果，并在第三个较新的数据集上建立了新的标准。</pre></li>
<li><a href="https://arxiv.org/abs/2003.03666">Multi-task Learning Based Neural Bridging Reference Resolution</a>
<pre>我们提出了一个基于多任务学习的神经模型，用于解决桥接参考，解决两个关键挑战。第一个挑战是缺少带有桥接引用的大型语料库。为了解决这个问题，我们使用多任务学习来帮助将参考解析与共指解析联系起来。我们表明，使用此体系结构，在完全桥接分辨率上可以实现高达8 p.p.的实质性改进。第二个挑战是不同语料库中使用的桥接定义不同，这意味着手工编码系统或使用为一个语料库设计的特殊功能的系统不能很好地与其他语料库协同工作。我们的神经网络模型只使用了少量与语料库无关的特征，因此可以应用于不同的语料库。使用非常不同的桥接语料库（ARRAU、ISNOTES、BASHI和SCICORP）进行的评估表明，我们的体系结构在所有语料库上都同样有效，并在所有语料库的完整桥接分辨率上实现了SoTA结果，比最佳报告结果高出36.3 p。。</pre></li>
<li><a href="https://arxiv.org/abs/2004.07898">Bridging Anaphora Resolution as Question Answering</a> (ACL2020)
<pre>以前大多数关于桥接回指消解的研究（Poesio等人，2004；Hou等人，2013b；Hou，2018a）都使用成对模型来解决这个问题，并假设黄金提及信息是给定的。在本文中，我们将桥接回指消解转换为基于语境的问答。这使我们能够在不知道任何黄金提及信息的情况下找到给定回指的先行词（回指本身除外）。我们为这项任务提供了一个问答框架（BARQA），它利用了迁移学习的力量。此外，我们还提出了一种生成大量“准桥接”训练数据的新方法。我们表明，我们的模型在此数据集上预先训练，并在少量域内数据集上进行微调，在两个桥接语料库（ISNotes（Markert et al.，2012）和BASHI（Roesiger，2018））上实现了桥接回指解析的最新结果。</pre></li>
<li><a href="https://arxiv.org/abs/2010.14759">Fine-grained Information Status Classification Using Discourse Context-Aware BERT</a> (COLING2020)
<pre>先前关于桥接回指识别的工作（Hou等人，2013a）将该问题视为学习细粒度信息状态（IS）的子任务。然而，这些系统严重依赖于许多手工制作的语言特征。在本文中，我们提出了一个用于细粒度IS分类的简单语篇上下文感知伯特模型。在ISNotes语料库（Markert et al.，2012）上，我们的模型在细粒度IS分类方面取得了新的最新性能，与Hou et al.（2013a）相比，获得了4.8的绝对总体精度改进。更重要的是，我们还显示了桥接回指识别的改进10.5 F1点，而不使用任何复杂的手工制作的语义特征来捕捉桥接现象。我们进一步分析了训练后的模型，发现每个类别中最受关注的信号与信息状态的语言概念很好地对应。</pre></li>
</ul>
<h2 id="word-sense-disambiguation">Word sense disambiguation</h2>
<ul>
<li><a href="https://arxiv.org/abs/2008.11608">Language Models and Word Sense Disambiguation: An Overview and Analysis</a>
<pre>基于Transformer的语言模型在NLP中占据了很多领域。由于能够捕捉上下文敏感的语义细微差别，BERT及其衍生物主导了大多数现有的评估基准，包括词义消歧（WSD）基准。然而，关于它们在编码和恢复词义方面的能力和潜在局限性，人们仍然知之甚少。在这篇文章中，我们对著名的BERT模型进行了深入的定量和定性分析。我们分析的一个主要结论是，即使每个词的词义只有有限的例子，BERT也能准确地捕捉到高级意义上的区别。我们的分析还表明，在某些情况下，从训练数据和计算资源的可用性来看，语言模型接近于在理想条件下解决粗粒度名词消歧问题。然而，这种情况在现实环境中很少发生，因此，即使在粗粒度环境中也存在许多实际挑战。我们还对两种主要的基于语言模型的WSD策略（即微调和特征提取）进行了深入比较，发现后一种方法在感知偏差方面更为稳健，并且可以更好地利用有限的可用训练数据。事实上，即使每个词义只使用三个训练句子，平均上下文嵌入的简单特征提取策略也被证明是稳健的，通过增加训练数据的大小可以获得最小的改进。</pre></li>
<li><a href="https://arxiv.org/abs/1908.07245">GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge</a> (EMNLP2019)
<pre>词义消歧（WSD）的目的是在特定的语境中找出歧义词的确切含义。传统的监督方法很少考虑像WordNet这样的词汇资源，这些资源在基于知识的方法中被广泛使用。最近的研究表明，将gloss（感官定义）加入到WSD的神经网络中是有效的。然而，与传统的单词专家监督方法相比，它们并没有取得多大的改进。在本文中，我们重点研究如何在有监督的神经WSD系统中更好地利用光泽知识。我们构造了上下文对，并提出了三种基于BERT的WSD模型。我们在SemCor3.0训练语料库上对预训练的BERT模型进行了微调，在几个英语全词WSD基准数据集上的实验结果表明，我们的方法优于最先进的系统。</pre></li>
<li><a href="https://arxiv.org/abs/2009.11795">Adapting BERT for Word Sense Disambiguation with Gloss Selection Objective and Example Sentences</a> (EMNLP2020 Findings)
<pre>使用预先训练好的语言模型（如BERT）进行领域适应或迁移学习已被证明是许多自然语言处理任务的有效方法。在这项工作中，我们提出将词义消歧作为一项相关性排序任务，并对序列对排序任务进行微调，以在给定上下文句子和候选词义定义列表的情况下选择最可能的词义定义。我们还介绍了一种使用WordNet中现有示例语句的WSD数据增强技术。使用提出的训练目标和数据扩充技术，我们的模型能够在英语全词基准数据集上获得最先进的结果。</pre></li>
<li><a href="https://arxiv.org/abs/1910.00194">Improved Word Sense Disambiguation Using Pre-Trained Contextualized Word Representations</a> (EMNLP2019)
<pre>语境化的词表征能够在不同的语境中对同一个词给出不同的表征，并且已经证明它们在下游自然语言处理任务中是有效的，如问答、命名实体识别和情感分析。然而，先前工作中对词义消歧（WSD）的评估表明，使用语境化的单词表示并没有优于使用非语境化单词嵌入的最新方法。在本文中，我们探讨了整合预先训练的上下文化单词表示的不同策略，我们的最佳策略在多个基准WSD数据集上实现了超过先前发布的最佳精度的显著优势。我们在网站上提供源代码https://github.com/nusnlp/contextemb-wsd.</pre></li>
<li><a href="https://arxiv.org/abs/1909.08358">Using BERT for Word Sense Disambiguation</a>
<pre>词义消歧是自然语言处理中一个长期存在的问题，其目的是识别给定多义词的正确词义。在本文中，我们建议使用BERT为WSD提取更好的多义词表示，并探索几种将BERT与分类器相结合的方法。我们还利用语义定义为所有单词训练一个统一的分类器，这使得模型能够消除看不见的多义词的歧义。实验表明，我们的模型在标准英语全词WSD评估中取得了最新的结果。</pre></li>
<li><a href="https://www.aclweb.org/anthology/P19-1569.pdf">Language Modelling Makes Sense: Propagating Representations through WordNet for Full-Coverage Word Sense Disambiguation</a> (ACL2019)</li>
<li><a href="https://arxiv.org/abs/1909.10430">Does BERT Make Any Sense? Interpretable Word Sense Disambiguation with Contextualized Embeddings</a> (KONVENS2019)
<pre>ELMo（Peters et al.，2018）、Flair NLP（Akbik et al.，2018）或BERT（Devlin et al.，2019）提供的语境化单词嵌入（CWE）是NLP的一项重大创新。CWE根据各自的上下文提供单词的语义向量表示。在文本分类、序列标记或机器翻译等任务中，它们比静态单词嵌入更具优势。由于同一单词类型的向量可以根据各自的上下文而变化，因此它们隐式地为词义消歧（WSD）提供了一个模型。我们介绍了一种简单但有效的方法，使用最近邻分类的水务工程。我们比较了不同CWE模型在该任务中的性能，并且可以报告两个标准WSD基准数据集的改进情况，这些改进超过了当前的技术水平。我们进一步表明，预训练的BERT模型能够将多义词放入嵌入空间的不同“意义”区域，而ELMo和Flair NLP似乎不具备这种能力。</pre></li>
<li><a href="https://arxiv.org/abs/2005.01006">An Accurate Model for Predicting the (Graded) Effect of Context in Word Similarity Based on Bert</a>
<pre>近年来，自然语言处理（NLP）在语义分析中得到了广泛的应用。本文主要讨论了一种方法来分析语境对人类对相似词语感知的影响，这是SemEval 2020的第三项任务。我们应用了几种方法来计算由变换器（BERT）产生的双向编码器表示生成的两个嵌入向量之间的距离。我们队will_go在芬兰语subtask1项目中获得第一名，英语subtask1项目中获得第二名。</pre></li>
<li><a href="https://arxiv.org/abs/2101.10448">PolyLM: Learning about Polysemy through Language Modeling</a> (EACL2021)
<pre>为了避免单词嵌入的“意义融合缺陷”，许多模型都旨在嵌入单个单词的意义。这些方法曾在诸如词义归纳（WSI）等任务中表现良好，但后来被利用上下文嵌入的任务特定技术所取代。然而，意义嵌入和语境化不必相互排斥。我们介绍了PolyM，一种将学习意义嵌入作为语言建模问题的方法，允许应用语境化技术。PolyLM基于两个关于词义的基本假设：首先，一个词在给定上下文中出现的概率等于其各个词义出现的概率之和；第二，对于一个词的特定出现，它的一个感官在上下文中比其他感官更可信。我们在WSI上评估了PolyLM，结果表明它的性能比以前的感知嵌入技术要好得多，并且与当前最先进的专门WSI方法相匹配，尽管参数少了六倍。代码和预先培训的模型可在https://github.com/AlanAnsell/PolyLM.</pre></li>
<li><a href="https://www.aclweb.org/anthology/2020.acl-main.369/">CluBERT: A Cluster-Based Approach for Learning Sense Distributions in Multiple Languages</a> (ACL2020)</li>
<li><a href="https://arxiv.org/abs/2012.05300">Cross-lingual Word Sense Disambiguation using mBERT Embeddings with Syntactic Dependencies</a>
<pre>跨语言词义消歧（WSD）解决了在给定语境下跨语言消除歧义词的挑战。经过预训练的BERT嵌入模型已被证明能有效地提取单词的上下文信息，并已作为特征融入到许多最先进的WSD系统中。为了研究如何将语法信息添加到BERT嵌入中，从而产生语义和语法结合的单词嵌入，本项目通过生成依赖解析树并将单词的相对关系编码到输入嵌入中，提出了串联嵌入。还提出了两种方法来减小级联嵌入的大小。实验结果表明，语法嵌入的高维性对分类任务构成了障碍，需要在未来的研究中进一步解决。</pre></li>
<li><a href="https://arxiv.org/abs/2010.03124">VCDM: Leveraging Variational Bi-encoding and Deep Contextualized Word Representations for Improved Definition Modeling</a> (EMNLP2020)
<pre>在本文中，我们处理定义建模的任务，其目标是学习生成单词和短语的定义。现有的这项任务的方法是有区别的，以隐式而非直接的方式将分布语义和词汇语义结合起来。为了解决这个问题，我们为任务提出了一个生成模型，引入一个连续的潜在变量来显式地建模上下文中使用的短语与其定义之间的潜在关系。我们依靠变分推理进行估计，并利用上下文化的单词嵌入来提高性能。我们的方法在四个现有的具有挑战性的基准上进行了评估，添加了两个新的数据集“剑桥”和第一个非英语语料库“罗伯特”，我们发布这两个语料库是为了补充我们的实证研究。我们的变分上下文定义建模器（VCDM）在自动和人工评估指标方面实现了最先进的性能，证明了我们方法的有效性。</pre></li>
</ul>
<h2 id="sentiment-analysis">Sentiment analysis</h2>
<ul>
<li><a href="https://arxiv.org/abs/1903.09588">Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence</a> (NAACL2019)
<pre>基于方面的情绪分析（ABSA）是情绪分析（SA）的一个具有挑战性的子任务，旨在识别针对特定方面的细粒度意见极性。在本文中，我们从方面构造了一个辅助句，并将ABSA转化为一个句子对分类任务，如问答（QA）和自然语言推理（NLI）。我们对来自BERT的预训练模型进行了微调，并在SentiHood和SemEval-2014任务4数据集上取得了最新成果。</pre></li>
<li><a href="https://arxiv.org/abs/1904.02232">BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis</a> (NAACL2019)
<pre>问答在电子商务中扮演着重要的角色，因为它允许潜在客户积极寻求有关产品或服务的关键信息，以帮助他们做出购买决策。受最近机器阅读理解（MRC）在正式文档上取得成功的启发，本文探讨了将客户评论转化为可用于回答用户问题的大量知识来源的潜力。我们称之为问题评论阅读理解（RRC）。据我们所知，目前尚未就RRC开展任何工作。在这项工作中，我们首先基于一个流行的基于方面的情绪分析基准构建了一个名为ReviewRC的RRC数据集。由于ReviewRC对RRC（以及基于方面的情感分析）的训练示例有限，因此我们在流行语言模型BERT上探索了一种新的训练后方法，以提高对RBT进行RRC微调的性能。为了说明该方法的通用性，所提出的后训练也被应用于其他一些基于评论的任务，例如基于方面的情绪分析中的方面提取和方面情绪分类。实验结果表明，本文提出的岗位培训方法是有效的。有关数据集和代码，请访问https://www.cs.uic.edu/~hxu/。</pre></li>
<li><a href="https://arxiv.org/abs/1910.00883">Exploiting BERT for End-to-End Aspect-based Sentiment Analysis</a> (EMNLP2019 WS)
<pre>在本文中，我们研究了在E2E-ABSA任务中，来自预先训练的语言模型（如BERT）的语境化嵌入的建模能力。具体来说，我们建立了一系列简单但有洞察力的神经基线来处理E2E-ABSA。实验结果表明，即使有一个简单的线性分类层，我们的基于BERT的体系结构也可以优于最新的工作。此外，我们还通过一致地使用一个保持有效的数据集进行模型选择来标准化比较研究，这在以前的工作中基本上被忽略了。因此，我们的工作可以作为E2E-ABSA基于BERT的基准。</pre></li>
<li><a href="https://arxiv.org/abs/2010.11731">Improving BERT Performance for Aspect-Based Sentiment Analysis</a>
<pre>基于方面的情绪分析（ABSA）研究消费者对市场产品的意见。它包括检查产品评论中表达的情绪类型和情绪目标。分析评论中使用的语言是一项困难的任务，需要对语言有深刻的理解。近年来，诸如BERT\cite{devlin2019bert}等深层语言模型在这方面取得了很大进展。在这项工作中，我们提出了两个简单的模块，称为并行聚合和分层聚合，以用于两个主要的ABSA任务，即方面提取（AE）和方面情感分类（ASC），以提高模型的性能。我们表明，应用所提出的模型消除了进一步训练伯特模型的需要。源代码可在网上获得，以便进一步研究和复制结果。</pre></li>
<li><a href="https://arxiv.org/abs/2010.07523">Context-Guided BERT for Targeted Aspect-Based Sentiment Analysis</a>
<pre>基于方面的情绪分析（ABSA）和目标ASBA（TABSA）允许根据上下文从同一文本中得出更细粒度的情绪推断。例如，给定的文本可以有不同的目标（例如，社区）和不同的方面（例如，价格或安全），与每个目标方面对关联的情绪不同。在本文中，我们研究了在自我注意模型中添加上下文是否可以提高（T）ABSA的性能。我们提出了两种不同的上下文引导的BERT（CG-BERT），它们学习在不同的上下文下分配注意力。我们首先采用上下文感知转换器来生成CG-BERT，它使用上下文引导的softmax注意。接下来，我们提出了一个改进的准注意CG-BERT模型，该模型学习支持减法注意的合成注意。我们在两个（T）ABSA数据集上使用预训练的BERT对两个模型进行训练：SentiHood和SemEval-2014（任务4）。两种模型都取得了最新的成果，我们的QACG-BERT模型具有最好的性能。此外，我们还分析了我们提出的模型中上下文的影响。我们的工作为在基于上下文的自然语言任务中，在预先训练的基于自我注意的语言模型中添加上下文依赖提供了更多的证据。</pre></li>
<li><a href="https://arxiv.org/abs/2011.00169">Understanding Pre-trained BERT for Aspect-based Sentiment Analysis</a> (COLING2020)
<pre>针对基于方面的情绪分析（ABSA）中的任务，分析了从BERT评论中学习到的预训练隐藏表示。我们的工作受到ABSA基于BERT语言模型的最新进展的推动。然而，目前尚不清楚在未标记语料库上训练的（蒙面）语言模型的一般代理任务（proxy task of）如何为ABSA中的下游任务提供重要特征。通过利用ABSA中的注释数据集，我们研究了在评论中预先训练的BERT的注意和学习表示。我们发现，BERT很少使用自我注意头来编码上下文词（如表示方面的介词或代词）和观点词。方面表示中的大多数特性都致力于领域（或产品类别）和方面本身的细粒度语义，而不是从其上下文中携带总结的观点。我们希望这项研究能对改进ABSA的自监督学习、无监督学习和微调的未来研究有所帮助。预先培训的模型和代码可在https://github.com/howardhsu/BERT-for-RRC-ABSA.</pre></li>
<li><a href="https://arxiv.org/abs/2104.04986">Does syntax matter? A strong baseline for Aspect-based Sentiment Analysis with RoBERTa</a> (NAACL2021)
<pre>基于方面的情绪分析（ABSA）是情绪分析领域的一项细粒度任务，旨在预测方面的极性。以前的工作表明，句法信息（例如依赖树）可以有效地提高ABSA性能。最近，预训练模型（PTM）也在ABSA上显示了其有效性。因此，问题自然而然地出现了：ptm是否包含足够的ABSA语法信息，这样我们就可以仅基于ptm获得一个好的ABSA模型。在本文中，我们首先比较了来自PTMs的诱导树和ABSA任务的几种流行模型上的依赖解析树，表明来自微调RoBERTa（FT-RoBERTa）的诱导树优于解析器提供的树。进一步的分析实验表明，FT-RoBERTa诱导树更倾向于情感词，有利于ABSA任务的完成。实验还表明，基于纯RoBERTa的模型在四种语言的六个数据集上的性能优于或接近于以前的SOTA，因为它隐含了面向任务的语法信息。</pre></li>
<li><a href="https://arxiv.org/abs/1908.11860">Adapt or Get Left Behind: Domain Adaptation through BERT Language Model Finetuning for Aspect-Target Sentiment Classification</a> (LREC2020)
<pre>方面目标情绪分类（ATSC）是基于方面的情绪分析（ABSA）的子任务，ABSA有许多应用，例如在电子商务中，可以利用来自评论的数据和见解为企业和客户创造价值。近年来，深度迁移学习方法已经成功地应用于自然语言处理（NLP）任务，包括ATSC。在突出的伯特语言模型的基础上，我们使用两步程序来处理ATSC：自监督领域特定的伯特语言模型微调，然后是监督任务特定的微调。我们关于如何最好地利用特定领域语言模型微调的发现使我们能够在SemEval 2014 Task 4数据集上产生最新的性能。此外，为了探索我们模型的真实鲁棒性，我们进行了跨域评估。我们表明，跨域适应的BERT语言模型的性能明显优于强基线模型，如香草BERT基和XLNet基。最后，我们进行了一个案例研究来解释模型预测误差。</pre></li>
<li><a href="https://arxiv.org/abs/1905.09642">An Investigation of Transfer Learning-Based Sentiment Analysis in Japanese</a> (ACL2019)
<pre>文本分类方法通常需要特定于任务的模型体系结构和巨大的标记数据集。最近，由于基于文本的迁移学习技术的兴起，可以以无监督的方式预先训练语言模型，并利用它们有效地执行下游任务。在这项工作中，我们关注日语，并展示了迁移学习技术在文本分类中的潜在应用。具体而言，我们在乐天产品评论和雅虎电影评论数据集上执行二进制和多类情感分类。我们发现，基于迁移学习的方法比在3倍数据上训练的任务特定模型表现更好。此外，这些方法对于仅在1/30的数据上预先训练的语言建模同样有效。我们将预先培训的模型和代码作为开源发布。</pre></li>
<li><a href="https://arxiv.org/abs/1908.08039">“Mask and Infill” : Applying Masked Language Model to Sentiment Transfer</a>
<pre>本文主要研究非平行文本中的情感迁移问题，即在保留句子的属性无关内容的同时，修改句子的情感属性（如积极或消极）。由于基于RNN的编译码器结构捕获单词之间深度和长距离依赖关系的能力有限，以前的工作很难从头生成令人满意的句子。当人们转换一个句子的情感属性时，一个简单而有效的方法是只使用目标情感表达替换句子中的原始情感标记，而不是从头构建一个新的句子。这一过程与文本填充或完形填空的任务非常相似，可以通过深层双向掩蔽语言模型（例如BERT）来处理。因此，我们提出了“遮罩和填充”两步方法。在遮罩步骤中，我们通过遮罩情感标记的位置将风格与内容分开。在填充步骤中，我们对传销进行了改进，使其具有条件传销属性，通过预测上下文1和目标情绪条件下的单词或短语来填充蒙面位置。我们在两个回顾数据集上评估我们的模型，包括定量、定性和人为评估。实验结果表明，我们的模型提高了最先进的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2001.11316">Adversarial Training for Aspect-Based Sentiment Analysis with BERT</a>
<pre>基于方面的情绪分析（ABSA）处理情绪及其目标的提取。为这项任务收集标记数据以帮助神经网络更好地泛化可能既费力又耗时。作为替代方案，可以通过在嵌入空间中执行的对抗过程人工生成与真实世界示例类似的数据。虽然这些例子不是真实的句子，但它们被证明是一种正则化方法，可以使神经网络更加健壮。在这项工作中，我们将Goodfelle et al.（2014）提出的对抗性训练应用于Xu et al.（2019）提出的训练后BERT（BERT-PT）语言模型，用于情感分析中的方面提取和方面情感分类这两项主要任务。在通过消融研究改进后训练BERT的结果后，我们提出了一种称为BERT对抗性训练（BAT）的新架构，以利用ABSA中的对抗性训练。该模型在两个任务上都优于训练后的BERT。据我们所知，这是关于ABSA中对抗性训练应用的首次研究。</pre></li>
<li><a href="https://www.aclweb.org/anthology/2020.acl-main.370/">Adversarial and Domain-Aware BERT for Cross-Domain Sentiment Analysis</a> (ACL2020)</li>
<li><a href="https://arxiv.org/abs/2002.04815">Utilizing BERT Intermediate Layers for Aspect Based Sentiment Analysis and Natural Language Inference</a>
<pre>基于方面的情感分析旨在识别文本中某一方面的情感倾向。预训练BERT的微调在这项任务中表现出色，并实现了最先进的性能。现有的基于BERT的工作只利用了BERT的最后一个输出层，而忽略了中间层的语义知识。本文探讨了利用BERT中间层提高BERT微调性能的潜力。据我们所知，目前还没有关于这项研究的工作。为了展示通用性，我们还将这种方法应用到自然语言推理任务中。实验结果证明了该方法的有效性和通用性。</pre></li>
<li><a href="https://arxiv.org/abs/2004.13816">DomBERT: Domain-oriented Language Model for Aspect-based Sentiment Analysis</a>
<pre>本文主要研究由终端任务驱动的面向领域的语言模型的学习，其目的是将通用语言模型（如ELMo和BERT）和特定领域的语言理解结合起来。我们提出了DomBERT，它是BERT的一个扩展，用于学习领域内语料库和相关领域语料库。这有助于学习资源较少的领域语言模型。在基于方面的情绪分析中，对各种各样的任务进行了实验，证明了有希望的结果。</pre></li>
<li><a href="https://arxiv.org/abs/2012.14541">YASO: A New Benchmark for Targeted Sentiment Analysis</a>
<pre>当前跨域设置中的TSA评估仅限于现有数据集中可用的一小部分审查域。这样的评估是有限的，可能无法反映亚马逊或Yelp等网站的真实表现，这些网站拥有来自许多领域的不同评论。为了解决这一差距，我们提出了YASO——一个新的开放域用户评论TSA评估数据集。YASO包含来自数十个评论领域的2215个英语句子，用目标术语和它们的情感进行注释。我们的分析验证了这些注释的可靠性，并探索了所收集数据的特征。使用五个当代TSA系统的基准测试结果表明，在这个具有挑战性的新数据集上有足够的改进空间。YASO可在https://github.com/IBM/yaso-tsa.</pre></li>
<li><a href="https://arxiv.org/abs/2005.04114">SentiBERT: A Transferable Transformer-Based Architecture for Compositional Sentiment Semantics</a> (ACL2020)
<pre>我们提出SentiBERT，它是BERT的一个变体，可以有效地捕获合成情感语义。该模型将上下文表示与二元选区解析树相结合，以捕获语义成分。综合实验表明，SentiBERT在短语级情感分类方面取得了很好的性能。我们进一步证明，从SST上的短语级注释中学习到的情绪成分可以转移到其他情绪分析任务以及相关任务，例如情绪分类任务。此外，我们进行消融研究并设计可视化方法来理解SentiBERT。我们发现SentiBERT方法在捕捉否定和对比关系方面优于基线方法，并对合成情感语义进行建模。</pre></li>
</ul>
<h2 id="relation-extraction">Relation extraction</h2>
<ul>
<li><a href="https://arxiv.org/abs/1906.03158">Matching the Blanks: Distributional Similarity for Relation Learning</a> (ACL2019)
<pre>通用关系抽取器可以对任意关系进行建模，是信息抽取的核心目标。人们已经努力构建通用的提取器，这些提取器表示与曲面形式的关系，或者将曲面形式与现有知识图中的关系联合嵌入。然而，这两种方法的推广能力都有限。在本文中，我们基于哈里斯分布假设对关系的扩展，以及学习文本表示（特别是BERT）的最新进展，仅从实体链接文本构建任务不可知的关系表示。我们表明，即使不使用任何任务的训练数据，这些表示也显著优于以前关于基于范例的关系提取（FewRel）的工作。我们还表明，使用任务不可知表示初始化的模型，然后在有监督的关系提取数据集上进行调整，显著优于SemEval 2010任务8、KBP37和TACRED上的先前方法。</pre></li>
<li><a href="https://arxiv.org/abs/1908.05908">BERT-Based Multi-Head Selection for Joint Entity-Relation Extraction</a> (NLPCC2019)
<pre>在本文中，我们报告了2019年语言与智能挑战赛中信息提取任务的方法。我们将BERT纳入多头部选择框架，用于联合实体关系提取。该模型从三个角度扩展了现有方法。首先，采用BERT作为多头选择框架底部的特征提取层。我们通过在训练前引入语义增强任务来进一步优化训练前训练。第二，我们引入了一个大规模的百度白克语料库进行实体识别预训练，由于没有实际的命名实体标签，因此该语料库是每周监督学习的。第三，提出了软标签嵌入方法，有效地在实体识别和关系提取之间传递信息。结合这三个贡献，我们增强了多头选择模型的信息提取能力，并用单个模型在testset-1上实现了F1得分0.876。通过整合我们模型的四个变体，我们最终在testset-1上获得F1得分0.892（第一名），在testset-2上获得F1得分0.8924（第二名）。</pre></li>
<li><a href="https://arxiv.org/abs/1905.08284">Enriching Pre-trained Language Model with Entity Information for Relation Classification</a>
<pre>关系分类是提取实体间关系的一项重要NLP任务。最先进的关系分类方法主要基于卷积或递归神经网络。最近，预训练的BERT模型在许多NLP分类/序列标记任务中取得了非常成功的结果。关系分类不同于那些任务，因为它依赖于句子和两个目标实体的信息。在本文中，我们提出了一个模型，该模型既利用预先训练好的BERT语言模型，又结合来自目标实体的信息来处理关系分类任务。我们定位目标实体并通过预先训练的体系结构传输信息，并合并两个实体的相应编码。我们在SemEval-2010 task 8关系数据集上实现了对最先进方法的显著改进。</pre></li>
<li><a href="https://arxiv.org/abs/1909.07755">Span-based Joint Entity and Relation Extraction with Transformer Pre-training</a>
<pre>我们介绍了SpERT，一个用于基于跨度的关节实体和关系提取的注意模型。我们的主要贡献是对BERT嵌入的轻量级推理，其特征是实体识别和过滤，以及带有本地化、无标记上下文表示的关系分类。该模型使用强大的句子内否定样本进行训练，这些样本在单个BERT过程中有效地提取。这些方面有助于搜索句子中的所有跨度。在消融研究中，我们证明了预训练、强负采样和局部环境的好处。在联合实体和关系提取的多个数据集上，我们的模型的F1分数高达2.6%，优于先前的工作。</pre></li>
<li><a href="https://arxiv.org/abs/1909.11898">Fine-tune Bert for DocRED with Two-step Process</a>
<pre>多实体之间的关系建模近年来受到越来越多的关注，为了加速文档级关系提取的研究，人们收集了一个称为DocRED的新数据集。此任务的当前基线使用BiLSTM对整个文档进行编码，并从头开始进行培训。我们认为，这种简单的基线不足以模拟实体之间的复杂交互。在本文中，我们进一步应用了预训练语言模型（BERT），为这项任务提供了一个更强的基线。我们还发现，分阶段解决此任务可以进一步提高性能。第一步是预测两个实体是否存在关系，第二步是预测特定关系。</pre></li>
<li><a href="https://arxiv.org/abs/2010.04829">Relation Extraction as Two-way Span-Prediction</a>
<pre>当前的监督关系分类（RC）任务使用单个嵌入来表示一对实体之间的关系。我们认为，更好的方法是将RC任务视为跨度预测（SP）问题，类似于问答（QA）。我们提出了一个基于跨度预测的RC系统，并与基于嵌入的系统进行了性能评估。我们证明了监督SP目标比基于标准分类的目标工作得更好。我们在TACRED和SemEval task 8数据集上获得了最新的结果。</pre></li>
<li><a href="https://arxiv.org/abs/1909.03546">Entity, Relation, and Event Extraction with Contextualized Span Representations</a> (EMNLP2019)
<pre>我们研究了一个统一的多任务框架对三个信息提取任务的能力：命名实体识别、关系提取和事件提取。我们的框架（称为DyGIE++）通过枚举、细化和评分文本跨度来完成所有任务，这些文本跨度旨在捕获局部（句子内）和全局（跨句子）上下文。我们的框架在来自不同领域的四个数据集上实现了所有任务的最新结果。我们进行实验，比较不同的技术来构建跨度表示。像BERT这样的上下文嵌入在捕捉相同或相邻句子中实体之间的关系方面表现得很好，而动态跨度图更新则模拟了长期的跨句子关系。例如，通过预测的共指链接传播跨度表示可以使模型消除具有挑战性的实体提及的歧义。我们的代码在https://github.com/dwadden/dygiepp 并且可以轻松地适应新任务或数据集。</pre></li>
<li><a href="https://arxiv.org/abs/1908.07721">Fine-tuning BERT for Joint Entity and Relation Extraction in Chinese Medical Text</a>
<pre>实体和关系抽取是构建医学文本的必要步骤。然而，现有模型中的双向长短时记忆网络的特征提取能力并没有达到最佳效果。同时，该语言模型在越来越多的自然语言处理任务中取得了优异的效果。在本文中，我们提出了一个集中注意模型的联合实体和关系提取任务。该模型通过动态范围注意机制将著名的BERT语言模型集成到联合学习中，从而提高了共享参数层的特征表示能力。在曙光医院采集的冠状动脉造影文本上的实验结果表明，命名实体识别和关系分类任务的F1得分分别达到96.89%和88.51%，分别优于最先进的方法1.65%和1.22%。</pre></li>
<li><a href="https://arxiv.org/abs/2004.03786">Downstream Model Design of Pre-trained Language Model for Relation Extraction Task</a>
<pre>基于深度神经网络的有监督关系抽取方法在当前信息抽取领域发挥着重要作用。但是，由于复杂关系的存在，目前他们的表现仍未能达到良好水平。另一方面，最近提出的预训练语言模型（pre-trained language models，plm）通过与下游任务模型相结合的微调，在自然语言处理的多个任务中取得了巨大的成功。然而，PLM的原始标准任务还不包括关系提取任务。我们认为PLMs也可以用来解决关系抽取问题，但需要建立一个专门设计的下游任务模型，甚至损失函数来处理复杂的关系。本文设计了一种具有特殊损失函数的新型网络结构，作为PLMs的下游模型，用于监督关系提取。实验表明，我们的方法在多个公共关系提取数据集中显著超过了当前的最佳基线模型。</pre></li>
<li><a href="https://arxiv.org/abs/2004.03636">Efficient long-distance relation extraction with DG-SpanBERT</a>
<pre>在自然语言处理中，关系抽取寻求对非结构化文本的合理理解。在这里，我们提出了一种新的基于SpanBERT的图卷积网络（DG SpanBERT），该网络使用预先训练的语言模型SpanBERT和图卷积网络从原始句子中提取语义特征，以汇集潜在特征。我们的DG-SpanBERT模型继承了SpanBERT从大规模语料库中学习丰富词汇特征的优势。由于在依赖树上使用GCN，它还能够捕获实体之间的长期关系。实验结果表明，我们的模型优于其他现有的基于依赖关系和基于序列的模型，并在TACRED数据集上实现了最先进的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2009.10359">Global-to-Local Neural Networks for Document-Level Relation Extraction</a> (EMNLP2020)
<pre>关系抽取（RE）旨在识别文本中命名实体之间的语义关系。近年来，它被提升到文档级别，这需要对实体进行复杂的推理，并在整个文档中提及。在本文中，我们提出了一种新的文档级RE模型，通过将文档信息编码为实体全局和局部表示以及上下文关系表示。实体全局表示对文档中所有实体的语义信息进行建模，实体局部表示对特定实体多次提及的上下文信息进行聚合，上下文关系表示对其他关系的主题信息进行编码。实验结果表明，我们的模型在两个公共数据集上实现了优异的性能。它在提取远距离实体之间的关系和多次提及方面特别有效。</pre></li>
<li><a href="https://arxiv.org/abs/2004.13845">DARE: Data Augmented Relation Extraction with GPT-2</a>
<pre>现实世界中的关系提取（RE）任务具有挑战性，无论是由于训练数据有限还是由于班级不平衡问题。在这项工作中，我们提出了数据增强关系提取（DARE），这是一种通过适当微调GPT-2生成特定关系类型示例来增强训练数据的简单方法。然后将生成的训练数据与gold数据集结合使用，以训练基于BERT的重新分类器。在一系列的实验中，我们展示了我们的方法的优势，这使得在一个强大的基线上F1分数提高了11分。此外，DARE在三个广泛使用的生物医学RE数据集上达到了新的技术水平，平均比以前的最佳结果高出4.7个F1分。</pre></li>
<li><a href="https://arxiv.org/abs/2004.14443">Distantly-Supervised Neural Relation Extraction with Side Information using BERT</a> (IJCNN2020)
<pre>关系抽取（RE）包括对句子中实体之间的关系进行分类。最近开发关系提取器的范例是远程监控（DS），它允许通过文本语料库和知识库（KB）之间的对齐来自动创建新的数据集。KBs有时还可以为RE任务提供附加信息。采用这种策略的方法之一是驻存模型，该模型提出了一种利用知识库边信息的远程监督神经关系提取方法。考虑到这种方法优于最新的基线，在本文中，我们提出了一种相关的方法，也使用附加的边信息来驻留，但用BERT嵌入来简化句子编码。通过实验，我们在Google远程监控和关于BGWA和基线方法的Riedel数据集中证明了该方法的有效性。尽管由于数据集不平衡，曲线下的面积有所减少，P@N结果表明，使用BERT作为句子编码比基线方法具有更好的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2102.01156">Improving Distantly-Supervised Relation Extraction through BERT-based Label &amp; Instance Embeddings</a>
<pre>远程监督关系抽取（RE）是一种有效的大规模语料库扩展方法，但存在标签噪声问题。现有的方法试图通过多实例学习和提供附加信息来减少噪声，但主要设法识别最频繁的关系，而忽略了长尾关系。我们提出了一种新的基于远程监督变压器的RE方法REDSandT（relationship Extraction with Remote Supervision and Transformers），该方法利用BERT的预训练模型以及标签和实体之间的关系，通过高度信息化的实例和标签嵌入，成功地捕捉到更广泛的关系集，分别地我们指导REDSandT通过对结构化输入（包括连接实体对和实体类型的子树）进行微调，将重点放在关系标记上。利用提取的信息向量，我们形成标签嵌入，我们还将其用作实例上的注意机制，以进一步减少噪声。最后，我们通过连接关系和实例嵌入来表示句子。在NYT-10数据集中的实验表明，REDSandT以更高的置信度捕获了更广泛的关系集，实现了最先进的AUC（0.424）。</pre></li>
<li><a href="https://arxiv.org/abs/2102.05980">An End-to-end Model for Entity-level Relation Extraction using Multi-instance Learning</a> (EACL2021)
<pre>我们提出了一个从文档中提取实体级关系的联合模型。与其他方法不同——这些方法侧重于局部句内提及对，因此需要在提及层面上进行注释——我们的模型在实体层面上运行。为此，我们采用了一种基于共指消解的多任务方法，通过结合全局实体和局部提及信息的多级表示的多实例学习来收集相关信号。我们在DocRED数据集上实现了最先进的关系提取结果，并报告了第一个实体级端到端关系提取结果，以供将来参考。最后，我们的实验结果表明，联合方法与任务特定学习是一致的，但由于共享的参数和训练步骤更有效。</pre></li>
<li><a href="https://arxiv.org/abs/2104.04697">ZS-BERT: Towards Zero-Shot Relation Extraction with Attribute Representation Learning</a> (NAACL2021) [<a href="https://github.com/dinobby/ZS-BERT">github</a>]
<pre>虽然关系提取是知识获取和表示中的一项基本任务，并且新生成的关系在现实世界中很常见，但在预测在训练阶段无法观察到的不可见关系方面所做的努力较少。在本文中，我们通过合并可见和不可见关系的文本描述来描述零镜头关系提取问题。我们提出了一种新的多任务学习模型，零镜头伯特（ZS-BERT），可以直接预测不可见的关系，而无需手工制作属性标签和多重成对分类。给定由输入句子及其关系描述组成的训练实例，ZS-BERT学习将句子和关系描述投影到嵌入空间的两个函数，方法是联合最小化它们之间的距离并对所见关系进行分类。通过基于这两个函数生成不可见关系的嵌入和新出现的句子，我们使用最近邻搜索来获得不可见关系的预测。在两个著名数据集上进行的实验表明，ZS-BERT比现有方法的F1分数至少提高13.54\%。</pre></li>
<li><a href="https://arxiv.org/abs/2104.07650">AdaPrompt: Adaptive Prompt-based Finetuning for Relation Extraction</a>
<pre>最近，对于某些少数镜头分类任务，快速调整已经取得了有希望的结果。提示调优的核心思想是将文本片段（即模板）插入到输入中，并将分类任务转换为隐藏的语言建模问题。然而，对于关系提取，确定合适的提示模板需要领域专家的专业知识，并且获取合适的标签词既麻烦又耗时。此外，实体和关系之间存在着不可忽视的丰富语义知识。为此，我们致力于将知识整合到关系提取的提示调优中，并提出了一种基于协同优化的知识感知提示调优方法（KnowPrompt）。具体来说，我们将实体和关系知识注入到可学习的虚拟模板词和答案词的即时构建中，并在知识约束下协同优化它们的表示。在五个具有标准和低资源设置的数据集上的大量实验结果证明了我们方法的有效性。</pre></li>
<li><a href="https://arxiv.org/abs/2004.08056">Dialogue-Based Relation Extraction</a> (ACL2020)
<pre>我们提出了第一个基于人类注释对话的关系提取（RE）数据集DialogRE，旨在支持预测对话中出现的两个参数之间的关系。我们进一步提供DialogRE作为研究跨句RE的平台，因为大多数事实跨越多个句子。基于对基于对话和传统再任务异同的分析，我们认为说话人相关信息在拟议任务中起着关键作用。考虑到对话中交流的及时性，我们设计了一个新的指标来评估对话环境中RE方法的性能，并调查了几种有代表性的RE方法在对话环境中的性能。实验结果表明，在最佳性能模型上进行说话人感知扩展可以在标准和会话评估设置中获得收益。DialogRE可从以下网址获得：https://dataset.org/dialogre/.</pre></li>
<li><a href="https://arxiv.org/abs/2012.13873">An Embarrassingly Simple Model for Dialogue Relation Extraction</a>
<pre>对话关系抽取（RE）是预测对话中提到的两个实体的关系类型。在本文中，我们将对话RE建模为一个多标签分类任务，并提出了一个简单而有效的模型SimpleRE。SimpleRE通过一种新的输入格式BERT Relation Token Sequence（BRS）捕获对话中多个关系之间的相互关系。在BRS中，多个[CLS]令牌用于捕获不同实体对之间的不同关系。设计了一个关系细化门（RRG）自适应地提取特定于关系的语义表示。在DialogRE上的实验表明，SimpleRE以更短的训练时间获得了最佳性能。SimpleRE在不使用外部资源的情况下，在句子级RE上优于所有直接基线。</pre></li>
<li><a href="https://arxiv.org/abs/1909.03227">A Novel Cascade Binary Tagging Framework for Relational Triple Extraction</a> (ACL2020) [<a href="https://github.com/weizhepei/CasRel">github</a>]
<pre>从非结构化文本中提取关系三元组是构建大规模知识图的关键。然而，现有的工作很少能够解决重叠三元组问题，即同一句子中的多个关系三元组共享相同的实体。在这项工作中，我们引入了一个新的视角来重新审视关系型三重提取任务，并提出了一个新的级联二进制标记框架（CasRel），该框架源自一个原则性的问题公式。我们的新框架不再像以前的工作那样将关系视为离散的标签，而是将关系建模为将主语映射到句子中的对象的函数，这自然会处理重叠问题。实验表明，即使其编码器模块使用随机初始化的BERT编码器，CasRel框架的性能也已经超过了最先进的方法，这表明了新标签框架的威力。当使用预先训练好的BERT编码器时，它的性能得到进一步提升，在两个公共数据集NYT和WebNLG上，F1成绩的绝对增益分别比最强的基线高17.5和30.2。对重叠三元组的不同场景的深入分析表明，该方法在所有这些场景中提供了一致的性能增益。源代码和数据在线发布。</pre></li>
<li><a href="https://arxiv.org/abs/2005.01932">ExpBERT: Representation Engineering with Natural Language Explanations</a> (ACL2020) [<a href="https://github.com/MurtyShikhar/ExpBERT">github</a>]
<pre>假设我们想具体说明已婚夫妇为了从文本中提取配偶对的任务通常会进行蜜月的归纳偏见。在本文中，我们允许模型开发人员指定这些类型的归纳偏差作为自然语言解释。我们使用在MultiNLI上微调的BERT来“解释”这些关于输入句子的解释，生成输入的解释引导表示。在三个关系提取任务中，我们的方法ExpBERT匹配一个BERT基线，但标记数据少了3-20倍，并且在标记数据量相同的情况下比基线提高了3-10个F1点。</pre></li>
<li><a href="https://arxiv.org/abs/2009.10680">AutoRC: Improving BERT Based Relation Classification Models via Architecture Search</a>
<pre>尽管基于BERT的关系分类（RC）模型与传统的深度学习模型相比已经取得了显著的改进，但似乎还没有就什么是最佳体系结构达成共识。首先，实体跨度识别有多种选择。其次，有一组池操作，将实体和上下文的表示聚合为固定长度的向量。第三，很难手动确定哪些特征向量（包括它们的交互作用）有利于关系类型的分类。在这项工作中，我们为基于BERT的RC模型设计了一个综合搜索空间，并使用神经结构搜索（NAS）方法自动发现上述设计选择。在七个基准RC任务上的实验表明，与基于基线BERT的RC模型相比，该方法在寻找更好的体系结构方面是有效的。消融研究证明了我们搜索空间设计的必要性和搜索方法的有效性。</pre></li>
<li><a href="https://arxiv.org/abs/2011.00398">Investigation of BERT Model on Biomedical Relation Extraction Based on Revised Fine-tuning Mechanism</a>
<pre>随着生物医学文献的爆炸式增长，设计自动工具从文献中提取信息在生物医学研究中具有重要意义。最近，基于变压器的适合生物医学领域的BERT模型已经产生了领先的结果。然而，现有的所有用于关系分类的BERT模型都只利用了最后一层的部分知识。在本文中，我们将研究在伯特模型的微调过程中利用整个层的方法。据我们所知，我们是第一个探索这种方法的人。实验结果表明，对于不同的关系提取任务，在三个基准数据集上，我们的方法提高了BERT模型的性能，并优于最新的方法。此外，进一步的分析表明，有关关系的关键知识可以从BERT模型的最后一层学习。</pre></li>
<li><a href="https://arxiv.org/abs/2011.12380">Experiments on transfer learning architectures for biomedical relation extraction</a>
<pre>关系抽取（RE）包括从文本中自动识别和构造感兴趣的关系。最近，BERT改进了几个NLP任务的顶级性能，包括RE。然而，在机器学习体系结构和迁移学习策略中使用BERT的最佳方法仍然是一个悬而未决的问题，因为它高度依赖于每个特定的任务和领域。在这里，我们探讨了各种基于BERT的架构和迁移学习策略（即冻结或微调），用于在两个语料库上执行生物医学RE任务。在经过测试的体系结构和策略中，我们的*BERT segMCNN（具有微调功能）在两个语料库上达到了比最先进水平更高的性能（ChemProt和PGxCorpus语料库的绝对改善率分别为1.73%和32.77%）。更一般地说，我们的实验说明了使用BERT进行微调的预期兴趣，但也说明了除了BERT经典利用的上下文之外，使用结构信息（带有句子切分）的未开发优势。</pre></li>
<li><a href="https://arxiv.org/abs/2104.13913">Improving BERT Model Using Contrastive Learning for Biomedical Relation Extraction</a> (BioNLP2021)
<pre>对比学习在计算机视觉中被用来学习图像的高质量表示。然而，由于缺乏一种通用的文本数据扩充方法，对比学习在自然语言处理中的应用并不广泛。在这项工作中，我们探索了使用对比学习的方法来改进关系抽取的BERT模型中的文本表示。该框架的关键在于通过将语言知识无缝集成到数据扩充中，为关系抽取任务定制了独特的对比预训练步骤。此外，我们还研究了从外部知识库构建的大规模数据如何增强BERT对比预训练的通用性。在三个关系提取基准数据集上的实验结果表明，我们的方法可以改进BERT模型的表示，并实现最先进的性能。此外，我们还通过对比预训练的BERT更依赖于预测的基本原理来探索模型的可解释性。我们的代码和数据可在以下网站公开获取：https://github.com/udel-biotm-lab/BERT-CLRE.</pre></li>
<li><a href="https://arxiv.org/abs/2010.08652">Cross-Lingual Relation Extraction with Transformers</a>
<pre>关系抽取是信息抽取中最重要的任务之一，它为许多NLP应用提供了必要的信息。在本文中，我们提出了一种跨语言再研究方法，它不需要目标语言中的任何人工注释或任何跨语言资源。基于无监督的跨语言表征学习框架，我们开发了几种基于深度变换的RE模型，采用了一种新的编码方案，可以有效地对实体位置和实体类型信息进行编码。当使用英语数据进行训练时，我们的RE模型优于几种基于深度神经网络的英语RE模型。更重要的是，我们的模型可用于执行零炮跨语言再培训，在两个数据集上实现最先进的跨语言再培训性能（监督目标语言再培训模型准确度的68-89%）。在不需要额外的训练数据或跨语言资源的情况下，我们的跨语言迁移效率很高，这表明我们的RE模型对于资源较少的语言特别有用。</pre></li>
<li><a href="https://arxiv.org/abs/2004.06153">Improving Scholarly Knowledge Representation: Evaluating BERT-based Models for Scientific Relation Classification</a>
<pre>随着研究出版物的快速增长，数字图书馆需要组织大量的学术知识。为了应对这一挑战，人们提倡采用基于知识图结构的技术。在这种基于图形的管道中，推断相关科学概念之间的关系类型是一个关键步骤。近年来，基于大规模语料库预训练语言模型的高级关系自动分类技术得到了广泛的应用。尽管做出了显著的贡献，但许多方法都是在不同的情景下进行评估的，这限制了它们的可比性。为此，我们对八个基于伯特的分类模型进行了全面的实证评估，重点关注两个关键因素：1）伯特模型变量和2）分类策略。在三个语料库上的实验表明，特定领域的预训练语料库有助于基于Bert的分类模型识别科学关系的类型。虽然每次预测一个关系的策略比同时识别多个关系类型的策略具有更高的分类精度，但后一种策略在标注量较大或较小的语料库中表现出更一致的性能。我们的研究旨在为数字图书馆的利益相关者提供建议，以选择合适的技术来构建基于知识图的系统，增强学术信息组织。</pre></li>
<li><a href="https://arxiv.org/abs/2004.06216">Robustly Pre-trained Neural Model for Direct Temporal Relation Extraction</a>
<pre>背景：识别临床事件和时间表达之间的关系是有意义地分析临床文本以用于高级人工智能应用的关键挑战。虽然已有先前的研究，但最先进的性能仍有很大的改进空间。方法：我们研究了BERT（使用变压器的双向编码器表示）的几种变体，其中一些涉及临床领域定制，另一些涉及改进的架构和/或培训策略。我们使用直接时态关系数据集评估了这些方法，该数据集是2012年i2b2时态关系挑战数据集的语义集中子集。结果：我们的结果表明，RoBERTa采用了更好的预训练策略，包括使用10倍大的语料库，使总体F测量提高了0.0864绝对分数（1.00分），因此相对于之前使用SVM（支持向量机）模型实现的最先进性能，错误率降低了24%。结论：在大型语料库上预先训练的现代语境语言建模神经网络，即使在高度精细的临床时间关系任务中也能取得令人印象深刻的性能。</pre></li>
<li><a href="https://www.aclweb.org/anthology/2020.bionlp-1.7/">A BERT-based One-Pass Multi-Task Model for Clinical Temporal Relation Extraction</a> (ACL2020 WS)</li>
<li><a href="https://arxiv.org/abs/2004.14577">Exploring Contextualized Neural Language Models for Temporal Dependency Parsing</a>
<pre>提取事件和时间表达式之间的时间关系有许多应用，例如构建事件时间线和与时间相关的问题回答。这是一个具有挑战性的问题，需要句子或语篇层面的句法和语义信息，这些信息可以通过深层语境化语言模型（LMs）如BERT（Devlin et al.，2019）捕获。在本文中，我们开发了基于BERT的时态依赖解析器的几种变体，并表明BERT显著改进了时态依赖解析（Zhang和Xue，2018a）。我们还详细分析了为什么深层情境化神经LMs有帮助，以及它们在哪些方面可能不足。源代码和资源可在https://github.com/bnmin/tdp_ranking.</pre></li>
<li><a href="https://arxiv.org/abs/2010.12753">Temporal Reasoning on Implicit Events from Distant Supervision</a>
<pre>我们提出了TRACIE，一个新的时态推理数据集，用于评估系统理解隐式事件的程度——这些事件在自然语言文本中没有明确提及，但可以从中推断出来。这给时态推理研究带来了一个新的挑战，之前的工作主要集中在明确提到的事件上。人类读者可以通过常识推理推断出隐含事件，从而更全面地理解情况，从而更好地推理时间。然而，我们发现，最先进的模型在预测内隐事件和外显事件之间的时间关系时存在困难。为了解决这个问题，我们提出了一个神经符号时间推理模型SYMTIME，该模型利用来自大规模文本的远程监控信号，并使用时间规则结合开始时间和持续时间来推断结束时间。SYMTIME在TRACIE上的性能优于强基线系统5%，在零先验知识培训环境下的性能优于强基线系统11%。我们的方法也推广到其他时态推理任务，在显式事件基准MATRES上增加了1%-9%。</pre></li>
<li><a href="https://arxiv.org/abs/2005.08178">IMoJIE: Iterative Memory-Based Joint Open Information Extraction</a> (ACL2020)
<pre>虽然传统的开放式信息提取系统是基于统计和规则的，但最近已经为这项任务引入了神经模型。我们的工作基于CopyAttention，一种序列生成OpenIE模型（Cui等人，2018年）。我们的分析表明，CopyAttention在每个句子中产生恒定数量的抽取，其抽取的元组通常表示冗余信息。我们介绍了IMoJIE，它是CopyAttention的一个扩展，它根据所有先前提取的元组生成下一个提取。这种方法克服了CopyAttention的这两个缺点，导致每个句子的不同抽取数量可变。我们对IMoJIE进行训练，训练数据是从几个非神经系统的提取中引导出来的，这些数据已经被自动过滤以减少冗余和噪声。IMoJIE比CopyAttention高出约18个F1点，基于BERT的强基线高出2个F1点，为任务建立了一个新的技术水平。</pre></li>
<li><a href="https://arxiv.org/abs/2010.03147">OpenIE6: Iterative Grid Labeling and Coordination Analysis for Open Information Extraction</a> (EMNLP2020) [<a href="https://github.com/dair-iitd/openie6">github</a>]
<pre>最近最先进的神经开放信息提取（OpenIE）系统以迭代方式生成提取，需要对部分输出进行重复编码。这需要大量的计算成本。另一方面，OpenIE的序列标记方法速度更快，但提取质量更差。在本文中，我们提出了一个基于迭代标记的系统，为OpenIE建立了一个新的技术状态，同时提取速度提高了10倍，从而消除了这种权衡。这是通过一种新的迭代网格标记（IGL）架构实现的，该架构将OpenIE视为二维网格标记任务。我们通过在训练时对网格应用覆盖（软）约束来进一步提高其性能。此外，在观察到最好的OpenIE系统在处理协调结构方面犹豫不决时，我们的OpenIE系统还集成了一个使用相同IGL架构构建的新协调分析器。这种基于IGL的协调分析器有助于我们的OpenIE系统处理复杂的协调结构，同时也建立了协调分析任务的最新状态，F1比以前的分析器提高了12.3分。我们的OpenIE系统OpenIE6在F1中比以前的系统快了4分，同时速度也快得多。</pre></li>
<li><a href="https://arxiv.org/abs/2009.08128">Multi2OIE: Multilingual Open Information Extraction Based on Multi-Head Attention with BERT</a> (EMNLP2020 Findings)
<pre>在本文中，我们提出了Multi$^2$OIE，它通过结合BERT和多头注意来执行开放信息提取（openie）。我们的模型是一个序列标记系统，具有高效的参数提取方法。我们使用一个由多模转换器启发的查询、键和值设置，用多头注意取代以前使用的双向长-短期内存体系结构。Multi$^2$OIE在两个基准评估数据集Re-OIE2016和CaRB上的计算效率优于现有序列标记系统。此外，我们还将该方法应用于使用多语言BERT的多语言开放IE。在两种语言（西班牙语和葡萄牙语）的新基准数据集上的实验结果表明，在没有目标语言的训练数据的情况下，我们的模型优于其他多语言系统。</pre></li>
</ul>
<h2 id="knowledge-base">Knowledge base</h2>
<ul>
<li><a href="https://arxiv.org/abs/1909.03193">KG-BERT: BERT for Knowledge Graph Completion</a>
<pre>知识图是许多人工智能任务的重要资源，但往往存在不完全性。在这项工作中，我们建议使用预先训练的语言模型来完成知识图。我们将知识图中的三元组视为文本序列，并提出了一种新的框架，称为知识图双向编码器表示（KG-BERT），用于对这些三元组进行建模。该方法以三元组的实体描述和关系描述为输入，用KG-BERT语言模型计算三元组的评分函数。在多个基准知识图上的实验结果表明，我们的方法在三重分类、链接预测和关系预测任务中都能达到最先进的性能。</pre></li>
<li><a href="https://openreview.net/forum?id=025X0zPfn">How Context Affects Language Models’ Factual Predictions</a> (AKBC2020)</li>
<li><a href="https://arxiv.org/abs/1911.12753">Inducing Relational Knowledge from BERT</a> (AAAI2020)
<pre>单词嵌入最显著的特性之一是，它们捕获了某些类型的语义和句法关系。最近，诸如BERT之类的预训练语言模型在广泛的自然语言处理任务中取得了突破性的成果。然而，这些模型在多大程度上捕捉到了标准单词嵌入已经捕捉到的关系知识还不清楚。为了探索这个问题，我们提出了一种从预先训练的语言模型中提取关系知识的方法。从给定关系的几个种子实例开始，我们首先使用大型文本语料库来查找可能表达这种关系的句子。然后，我们使用这些提取的句子的子集作为模板。最后，我们对语言模型进行了微调，以预测给定的词对是否可能是某个关系的实例，并将该关系的实例化模板作为输入。</pre></li>
<li><a href="https://arxiv.org/abs/1908.07690">Latent Relation Language Models</a> (AAAI2020)
<pre>在本文中，我们提出了潜在关系语言模型（LRLMs），这是一类通过知识图关系参数化文档中单词和其中出现的实体的联合分布的语言模型。该模型具有许多吸引人的特性：它不仅提高了语言建模性能，而且能够通过关系注释给定文本实体跨度的后验概率。实验证明，与基于单词的基线语言模型和以前采用知识图信息的方法相比，经验改进的效果更好。定性分析进一步证明了所提出的模型在上下文中学习预测适当关系的能力。</pre></li>
<li><a href="https://openreview.net/forum?id=BJlzm64tDH">Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model</a> (ICLR2020)</li>
<li><a href="https://arxiv.org/abs/1911.03814">Scalable Zero-shot Entity Linking with Dense Entity Retrieval</a> (EMNLP2020) [<a href="https://github.com/facebookresearch/BLINK">github</a>]
<pre>本文介绍了一个概念简单、可扩展且高效的基于BERT的实体链接模型，并对其准确性和速度权衡进行了广泛的评估。我们提出了一种两阶段零镜头链接算法，其中每个实体仅由一个简短的文本描述定义。第一阶段在由bi编码器定义的密集空间中进行检索，bi编码器独立嵌入提及上下文和实体描述。然后，使用交叉编码器对每个候选人重新排序，交叉编码器将提及和实体文本连接起来。实验表明，尽管该方法相对简单（例如，没有明确的实体嵌入或手动设计的提及表），但该方法在最近的零炮基准（6点绝对增益）和更成熟的非零炮评估（例如，TACKBP-2010）上是最先进的。我们还表明，通过最近邻搜索，bi编码器链接非常快（例如，在2毫秒内链接590万个候选对象），并且更昂贵的交叉编码器的大部分精度增益可以通过知识提取转移到bi编码器。我们的代码和模型可在https://github.com/facebookresearch/BLINK.</pre></li>
<li><a href="https://arxiv.org/abs/2010.06065">Zero-shot Entity Linking with Efficient Long Range Sequence Modeling</a> (EMNLP2020 Findings)
<pre>本文考虑了零炮实体链接问题，其中测试时间内的链接可能不存在于训练中。在基于BERT的研究成果的基础上，我们发现一种简单而有效的方法是扩展长程序列建模。与以前的许多方法不同，我们的方法不需要对具有长位置嵌入的BERT进行昂贵的预训练。相反，我们提出了一种有效的位置嵌入初始化方法，称为嵌入重复，它基于BERT基初始化较大的位置嵌入。在Wikia的零炮EL数据集上，我们的方法将SOTA从76.06%提高到79.08%，对于其长数据，相应的提高从74.57%提高到82.14%。我们的实验表明，在不重新训练伯特模型的情况下，长程序列建模是有效的。</pre></li>
<li><a href="https://www.aclweb.org/anthology/K19-1063/">Investigating Entity Knowledge in BERT with Simple Neural End-To-End Entity Linking</a> (CoNLL2019)</li>
<li><a href="https://arxiv.org/abs/2001.01447">Improving Entity Linking by Modeling Latent Entity Type Information</a> (AAAI2020)
<pre>现有的最先进的神经实体链接模型采用基于注意的词袋上下文模型和从词嵌入中引导的预先训练的实体嵌入来评估主题级上下文兼容性。然而，提及的直接上下文中潜在的实体类型信息被忽略，这导致模型经常将提及链接到具有不正确类型的不正确实体。为了解决这个问题，我们提出了基于预训练的BERT将潜在的实体类型信息注入到实体嵌入中。此外，我们将基于伯特的实体相似性评分集成到最先进模型的局部上下文模型中，以更好地捕获潜在的实体类型信息。我们的模型显著优于标准基准上最先进的实体链接模型（AIDA CoNLL）。详细的实验分析表明，我们的模型修正了大多数由直接基线产生的类型错误。</pre></li>
<li><a href="https://arxiv.org/abs/1909.00426">Global Entity Disambiguation with Pretrained Contextualized Embeddings of Words and Entities</a>
<pre>我们提出了一个新的全局实体消歧（ED）模型，该模型基于单词和实体的上下文嵌入。我们的模型基于双向转换器编码器（即BERT），为输入文本中的单词和实体生成上下文嵌入。该模型使用一个新的蒙面实体预测任务进行训练，该任务旨在通过预测从维基百科获得的实体注释文本中的随机蒙面实体来训练该模型。我们进一步扩展了该模型，将ED作为一个连续的决策任务来捕获全局上下文信息。我们使用六个标准ED数据集评估我们的模型，并在除一个数据集外的所有数据集上获得最新的结果。</pre></li>
<li><a href="https://arxiv.org/abs/1911.03834">YELM: End-to-End Contextualized Entity Linking</a>
<pre>我们提出了另一个实体链接模型（YELM），它将单词链接到实体而不是跨度。这克服了与选择好的候选提及跨度相关的任何困难，并且使得提及检测（MD）和实体消歧（ED）的联合训练容易实现。我们的模型基于BERT，生成上下文化的单词嵌入，这些单词嵌入是针对MD和ED的联合目标进行训练的。我们在几个标准实体链接（EL）数据集上实现了最先进的结果。</pre></li>
<li><a href="https://arxiv.org/abs/2005.14253">Empirical Evaluation of Pretraining Strategies for Supervised Entity Linking</a> (AKBC2020)
<pre>在这项工作中，我们提出了一个实体链接模型，它结合了转换器架构和维基百科链接的大规模预训练。我们的模型在两个常用的实体链接数据集上达到了最先进的水平：CoNLL的96.7%和TAC-KBP的94.9%。我们提供了详细的分析，以了解哪些设计选择对实体链接很重要，包括负实体候选、转换器架构和输入扰动的选择。最后，我们在更具挑战性的环境中，如端到端实体链接和没有域内训练数据的实体链接，给出了有希望的结果。</pre></li>
<li><a href="https://arxiv.org/abs/2010.01057">LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention</a> (EMNLP2020) [<a href="https://github.com/studio-ousia/luke">github</a>]
<pre>实体表示在涉及实体的自然语言任务中很有用。在本文中，我们提出了新的基于双向变换的单词和实体的预训练上下文化表示。该模型将给定文本中的单词和实体视为独立的标记，并输出它们的上下文表示。我们的模型是使用一个新的预训练任务来训练的，该任务基于伯特的蒙面语言模型。这项任务涉及预测从维基百科检索的大型实体标注语料库中随机隐藏的单词和实体。我们还提出了一种实体感知的自我注意机制，它是transformer自我注意机制的扩展，并在计算注意分数时考虑标记（单词或实体）的类型。所提出的模型在广泛的实体相关任务中取得了令人印象深刻的实证性能。特别是，它在五个著名数据集上获得了最先进的结果：开放实体（实体类型）、TACRED（关系分类）、CoNLL-2003（命名实体识别）、ReCoRD（完形填空式问答）和1.1班（抽取式问答）。我们的源代码和预训练表示可在https://github.com/studio-ousia/luke.</pre></li>
<li><a href="https://arxiv.org/abs/2010.11333">Linking Entities to Unseen Knowledge Bases with Arbitrary Schemas</a>
<pre>在实体链接中，原始文本中提到的命名实体会根据知识库（KB）消除歧义。这项工作的重点是链接到不可见的知识库，这些知识库没有训练数据，并且在训练期间其模式是未知的。我们的方法依赖于将具有多个属性值对的任意KBs中的实体灵活转换为平面字符串的方法，我们将其与最先进的零快照链接模型结合使用。为了提高模型的通用性，我们使用了两种基于实体属性洗牌和不可见属性处理的正则化方案。在CoNLL数据集上训练模型并在TAC-KBP 2010数据集上测试的英语数据集上的实验表明，我们的模型比基线模型的精度高出12个百分点。与之前的工作不同，我们的方法还允许无缝组合多个训练数据集。我们通过添加完全不同的数据集（Wikia）以及增加TAC-KBP 2010培训集中的培训数据量来测试这一能力。我们的模特在各方面都表现出色。</pre></li>
<li><a href="https://arxiv.org/abs/2101.09969">CHOLAN: A Modular Approach for Neural Entity Linking on Wikipedia and Wikidata</a> (EACL2021)
<pre>在本文中，我们提出了CHOLAN，这是一种模块化方法，用于在知识库上实现端到端实体链接（EL）。CHOLAN由两个基于变压器的模型组成的管道依次集成，以完成EL任务。第一个transformer模型标识给定文本中的曲面形式（实体提及）。对于每次提及，使用第二个转换器模型在预定义的候选列表中对目标实体进行分类。后一个转换器由从句子中获取的丰富上下文（即本地上下文）和从维基百科获得的实体描述提供。这种外部环境在最先进的EL方法中没有使用。我们的实证研究是在两个著名的知识库（即维基数据和维基百科）上进行的。实证结果表明，CHOLAN在标准数据集（如CoNLL AIDA、MSNBC、AQUAINT、ACE2004和T-REx）上的表现优于最先进的方法。</pre></li>
<li><a href="https://arxiv.org/abs/2002.00744">PEL-BERT: A Joint Model for Protocol Entity Linking</a>
<pre>诸如BERT之类的预训练模型广泛应用于自然语言处理任务中，并经过微调以持续改进各种自然语言处理任务的性能。然而，在我们的协议语料库上训练的经过微调的BERT模型在实体链接（EL）任务上仍然表现不佳。在本文中，我们提出了一个模型，该模型将微调语言模型与RFC域模型相结合。首先，我们设计了一个协议知识库作为协议EL的指南。其次，我们提出了一种新的模型PEL-BERT，将协议中的命名实体链接到协议知识库中的类别。最后，我们对预先训练的语言模型在描述性文本和抽象概念上的表现进行了全面的研究。实验结果表明，我们的模型在带注释的数据集上实现了最先进的EL性能，优于所有基线。</pre></li>
<li><a href="https://arxiv.org/abs/2104.10493">End-to-end Biomedical Entity Linking with Span-based Dictionary Matching</a>
<pre>疾病名称识别和规范化通常称为生物医学实体链接，是生物医学文本挖掘的一个基本过程。最近，两个任务的神经联合学习已经被提出，以利用双方的利益。虽然这种方法实现了高性能，但无法准确预测训练数据集中未出现的疾病概念。本研究引入了一种新颖的端到端方法，该方法将跨度表示与字典匹配功能相结合来解决这个问题。我们的模型通过引用字典处理看不见的概念，同时以端到端的方式保持基于神经网络的模型的性能。使用两个主要数据集进行的实验表明，我们的模型在强大的基线下取得了有竞争力的结果，特别是对于训练期间看不见的概念。</pre></li>
<li><a href="https://arxiv.org/abs/2010.02413">Efficient One-Pass End-to-End Entity Linking for Questions</a> (EMNLP2020) [<a href="https://github.com/belindal/BLINK/tree/master/elq">github</a>]
<pre>我们提出了ELQ，一种用于问题的快速端到端实体链接模型，它使用biencoder在一个过程中联合执行提及检测和链接。在WebQSP和GraphQuestions上进行评估，扩展注释涵盖每个问题的多个实体，ELQ的表现分别比以前的技术水平高出+12.7%和+19.6%。ELQ具有非常快的推理时间（单个CPU上的推理时间为1.57个示例/秒），可用于下游问答系统。在概念验证实验中，我们证明使用ELQ可以显著提高GraphRetriever的下游QA性能（arXiv:1911.03868）。代码和数据可在https://github.com/facebookresearch/BLINK/tree/master/elq</pre></li>
<li><a href="https://arxiv.org/abs/2010.09828">Cross-Lingual Transfer in Zero-Shot Cross-Language Entity Linking</a>
<pre>跨语言实体将多种语言中的背景信息链接到单一语言知识库。我们为这项任务提出了一个神经排序架构，它使用神经网络中提及和上下文的多语言表示。我们发现，在单语和多语环境下，BERT的多语言能力导致了稳健的性能。此外，我们探索了零镜头语言迁移，发现了令人惊讶的鲁棒性能。我们研究了零镜头退化，发现它可以通过建议的辅助训练目标得到部分缓解，但剩余的错误最好归因于域转移，而不是语言迁移。</pre></li>
<li><a href="https://arxiv.org/abs/2011.02690">Entity Linking in 100 Languages</a> (EMNLP2020) [<a href="https://github.com/google-research/google-research/tree/master/dense_representations_for_entity_retrieval/mel">github</a>]
<pre>我们提出了一种新的多语言实体链接公式，其中特定语言的引用解析为语言不可知的知识库。我们在此新设置中培训了一个双编码器，在先前工作的基础上，改进了特征表示、否定挖掘和辅助实体配对任务，以获得覆盖100多种语言和2000万个实体的单一实体检索模型。该模型的性能优于更有限的跨语言链接任务的最新结果。稀有实体和低资源语言在这种大规模的情况下带来了挑战，因此我们主张更多地关注零镜头和少量镜头的评估。为此，我们提供了一个新的大型多语言数据集Mewsli-9(http://goo.gle/mewsli-dataset)与我们的设置相匹配，并展示基于频率的分析如何为我们的模型和培训增强提供关键见解。</pre></li>
<li><a href="https://arxiv.org/abs/2010.03295">COMETA: A Corpus for Medical Entity Linking in the Social Media</a> (EMNLP2020) [<a href="https://github.com/cambridgeltl/cometa">github</a>]
<pre>虽然通用语言的实体链接（EL）取得了越来越大的进展，但现有数据集无法解决外行语言中健康术语的复杂性。与此同时，越来越需要能够理解公众在健康领域的声音的应用程序。为了解决这个问题，我们引入了一个名为COMETA的新语料库，该语料库由Reddit expert提供的20k个英语生物医学实体组成，并附有到SNOMED CT的链接，SNOMED CT是一个广泛使用的医学知识图。我们的语料库满足了从规模和覆盖到多样性和质量的理想特性的组合，就我们所知，该领域的任何现有资源都无法满足这些特性。通过对20个EL基线（从基于字符串到基于神经的模型）的基准实验，我们揭示了这些系统在2个具有挑战性的评估场景下对实体和概念执行复杂推理的能力。我们在COMETA上的实验结果表明，不存在黄金子弹，即使是最好的主流技术也有很大的性能差距需要填补，而最好的解决方案依赖于组合不同的数据视图。</pre></li>
<li><a href="https://arxiv.org/abs/1911.12543">How Can We Know What Language Models Know?</a> (TACL2020) [<a href="https://github.com/jzbjyb/LPAQA">github</a>]
<pre>最近的工作通过让语言模型（LM）填补诸如“Obama是一个职业”等提示的空白，对语言模型（LM）中包含的知识进行了研究，得出了有趣的结果。这些提示通常是手动创建的，很可能是次优的；另一个提示，如“奥巴马作为一个![3]”可能会导致更准确地预测正确的职业。因此，如果给出不适当的提示，我们可能无法检索LM确实知道的事实，因此任何给定的提示都只能提供LM中包含的知识的下限估计。在本文中，我们试图通过自动发现在查询过程中使用的更好提示来更准确地估计LMs中包含的知识。具体来说，我们提出了基于挖掘和基于解释的方法来自动生成高质量和多样的提示，以及集成方法来组合来自不同提示的答案。在用于从LMs中提取关系知识的LAMA基准上进行的大量实验表明，我们的方法可以将准确率从31.1%提高到39.6%，从而为LMs的知识提供更严格的下限。我们已经在发布了代码和生成的LM提示符和查询归档（LPAQA）https://github.com/jzbjyb/LPAQA.</pre></li>
<li><a href="https://arxiv.org/abs/2108.01928">How to Query Language Models?</a>
<pre>大型预训练语言模型（LMs）不仅能够恢复语言知识，而且能够恢复事实和常识知识。为了访问存储在基于掩模的LMS中的知识，我们可以使用完形风格的问题，并让模型填入空白。与结构化知识库相比，它具有灵活性优势，但缺点是无法为特定的信息需求找到正确的查询。受人类消除歧义行为的启发，我们建议通过示例查询LMs。为了澄清矛盾的问题“诺伊尔为谁效力？”，一个成功的策略是使用另一个主题来证明这种关系，例如，“罗纳尔多为葡萄牙效力。诺伊尔为谁效力？”。我们将这种通过示例查询的方法应用于LAMA探针，在仅提供10个演示的情况下，T-REx数据上的BERT large获得了高达37.8%的实质性改进，甚至超过了使用多达40个问题复述查询模型的基线。示例是通过模型的上下文提供的，因此既不需要微调，也不需要额外的前向传递。这表明，如果我们以正确的方式查询模型，LMs比以前假设的包含更多的事实和常识知识。</pre></li>
<li><a href="https://arxiv.org/abs/2004.00584">Deep Entity Matching with Pre-Trained Language Models</a>
<pre>我们提出了同上，一个新的实体匹配系统的基础上预先训练变压器为基础的语言模型。我们将EM作为一个序列对分类问题进行微调和转换，以利用具有简单体系结构的此类模型。我们的实验表明，直接应用在大型文本语料库上预训练的语言模型，如BERT、DistilBERT或RoBERTa，已经显著提高了匹配质量，并优于以前的最新技术（SOTA），在基准数据集上的F1分数高达29%。我们还开发了三种优化技术，以进一步提高同上的匹配能力。同上，通过突出显示在做出匹配决策时可能感兴趣的重要输入信息片段，可以注入领域知识。同上，还总结了过长的字符串，以便仅保留基本信息并用于EM。最后，同上采用了SOTA技术，将文本增强为EM，用（困难的）示例增强训练数据。通过这种方式，同上，为了提高模型的匹配能力，我们不得不“更加努力地”学习。我们开发的优化进一步将同上的性能提高了9.8%。也许更令人惊讶的是，我们发现同上可以用最多一半的标记数据实现之前的SOTA结果。最后，我们在一个真实的大规模EM任务中演示了同上的有效性。在匹配由789K和412K记录组成的两个公司数据集时，同上，F1得分高达96.5%。</pre></li>
<li><a href="https://arxiv.org/abs/2106.04098">Ultra-Fine Entity Typing with Weak Supervision from a Masked Language Model</a> (ACL2021)
<pre>最近，有一种扩展细粒度实体类型的努力，即使用更丰富和超精细的类型集，标记名词短语，包括代词和名词性名词，而不是仅命名实体提及。这种超精细实体类型任务的一个关键挑战是，人工标注的数据极其稀少，现有远程或弱监控方法的标注能力非常有限。为了解决这个问题，在本文中，我们提出了使用伯特蒙面语言模型（MLM）来获取超精细实体类型的训练数据。如果一个句子中有一个提及，我们的方法为BERT MLM构造一个输入，以便它预测提及的上下文相关的超词，它可以用作类型标签。实验结果表明，借助这些自动生成的标签，可以显著提高超精细实体类型化模型的性能。我们还表明，在执行简单的类型映射之后，我们的方法可以应用于改进传统的细粒度实体类型。</pre></li>
<li><a href="https://arxiv.org/abs/2010.12813">Constructing Taxonomies from Pretrained Language Models</a> (NAACL2021)
<pre>我们提出了一种使用预先训练的语言模型构建分类树（例如WordNet）的方法。我们的方法由两个模块组成，一个模块预测亲子关系，另一个模块将这些预测协调到树中。亲子关系预测模块为每个潜在的父子关系对生成可能性分数，创建父子关系分数图。树协调模块将任务视为一个图优化问题，并输出该图的最大生成树。我们在从WordNet采样的子树上训练我们的模型，并在不重叠的WordNet子树上进行测试。我们表明，合并web检索的gloss可以进一步提高性能。在构建英语WordNet子树的任务中，该模型达到了66.7%，比之前在该任务中发布的最佳结果相对增加了20.0%。此外，我们使用开放式多语言WordNet将原始英语数据集转换为其他九种语言，并将结果扩展到这些语言。</pre></li>
<li><a href="https://arxiv.org/abs/2010.11967">Language Models are Open Knowledge Graphs</a>
<pre>本文展示了如何从预先训练好的语言模型（如BERT、GPT-2/3）构建知识图（KGs），而无需人工监督。流行的KG（如Wikidata、NELL）是以监督或半监督的方式构建的，需要人类创造知识。最近的深层语言模型通过预训练自动从大规模语料库中获取知识。存储的知识使语言模型能够改进下游NLP任务，例如回答问题、编写代码和文章。在本文中，我们提出了一种无监督的方法来将语言模型中包含的知识转换为KGs。我们证明了KGs是由预先训练好的语言模型（无需微调）在语料库上向前传递一次而构建的。我们通过与人类创建的两个KG（Wikidata，TAC KBP）进行比较来证明构建KG的质量。我们的KG还提供现有KG中新的开放事实知识。我们的代码和KG将公开提供。</pre></li>
<li><a href="https://arxiv.org/abs/2106.01561">Can Generative Pre-trained Language Models Serve as Knowledge Bases for Closed-book QA?</a> (ACL2021)
<pre>最近的研究使用预先训练好的语言模型（PLM）作为回答开放性问题的知识库来研究这个有趣的问题。然而，现有的工作在使用具有高测试序列重叠的小型基准方面受到限制。我们使用团队构建了一个新的闭卷QA数据集，并研究了BART的性能。实验表明，高精度记忆训练事实对BART来说是一个挑战，即使保留了相关知识，回答闭卷问题也是一个挑战。发现了一些有希望的方向，包括解耦知识记忆过程和QA微调过程，迫使模型在回答问题时回忆相关知识。</pre></li>
<li><a href="https://arxiv.org/abs/2010.14660">DualTKB: A Dual Learning Bridge between Text and Knowledge Base</a> (EMNLP2020) [<a href="https://github.com/IBM/dualtkb">github</a>]
<pre>在这项工作中，我们提出了一种用于常识知识库（KBs）中无监督文本到路径和路径到文本传输的双重学习方法。我们通过创建一个弱监督数据集来研究弱监督的影响，结果表明，即使是少量的监督也可以显著提高模型性能并实现更好的传输质量。我们研究了不同的模型架构和评估指标，提出了一种新的常识知识库完成指标，该指标是为生成模型量身定制的。大量的实验结果表明，与现有的基线相比，该方法具有非常好的性能。这种方法是朝着更先进的系统迈出的可行一步，该系统用于自动构建/扩展知识库，并将知识库转换为连贯的文本描述。</pre></li>
<li><a href="https://arxiv.org/abs/2104.08610">Zero-shot Slot Filling with DPR and RAG</a>
<pre>从给定的文档集合中自动提取知识图（KG）的能力是人工智能中一个长期存在的问题。评估这种能力的一种方法是通过插槽填充任务。给定一个[entity，Slot，？]形式的实体查询，要求系统通过从相关段落生成或提取缺失值来“填充”该插槽。这种能力对于创建用于自动知识库填充的系统至关重要，这一需求日益增长，尤其是在企业应用程序中。最近，在评估语言模型方面出现了一个很有前途的方向，就像我们评估知识库一样，而时隙填充任务最适合这种目的。该领域的最新进展试图使用基于检索的语言模型以端到端的方式解决这一任务。像检索增强生成（RAG）这样的模型在不涉及复杂信息提取管道的情况下表现出令人惊讶的良好性能。然而，这些模型在KILT基准中的两个插槽填充任务上取得的结果仍然不符合真实世界信息提取系统的要求。在本文中，我们描述了几种改进RAG回收器和生成器的策略，以使其成为更好的插槽填充器。我们的KGI0系统（可在https://github.com/IBM/retrieve-write-slot-filling)在T-REx和zsRE数据集上都以较大的利润率在方格呢布排行榜上排名第一。</pre></li>
<li><a href="https://arxiv.org/abs/2006.07409">How to Avoid Being Eaten by a Grue: Structured Exploration Strategies for Textual Worlds</a> [<a href="https://github.com/rajammanabrolu/Q-BERT">github</a>]
<pre>基于文本的游戏是一种很长的谜题或任务，其特点是一系列稀疏且可能具有欺骗性的奖励。它们提供了一个理想的平台，可以开发使用组合大小的自然语言状态-动作空间感知世界并对其采取行动的代理。标准强化学习代理在有效探索此类空间方面的能力很差，并且常常难以克服瓶颈——代理无法通过的状态仅仅是因为他们没有看到足够的时间来充分强化正确的动作序列。我们介绍了Q*BERT，一个通过回答问题来学习构建世界知识图的代理，这将提高样本效率。为了克服瓶颈，我们进一步引入了MC！Q*BERT是一个代理，它使用基于知识图的内在动机来检测瓶颈，并使用一种新的探索策略来有效地学习一系列策略模块以克服瓶颈。我们展示了一项消融研究和结果，展示了我们的方法在九个文本游戏（包括流行游戏Zork）上的表现如何优于目前最先进的水平，Zork是学习代理第一次突破玩家被Grue吃掉的瓶颈。</pre></li>
<li><a href="https://arxiv.org/abs/2009.07058">MLMLM: Link Prediction with Mean Likelihood Masked Language Model</a>
<pre>知识库（KBs）易于查询、验证和解释。但是，它们可以根据工时和高质量数据进行扩展。蒙蔽语言模型（MLM），如BERT，随着计算能力以及非结构化原始文本数据的增加而扩展。然而，这些模型中包含的知识无法直接解释。我们建议使用MLMs执行链路预测，以解决KBs可伸缩性问题和MLMs可解释性问题。为此，我们引入了MLM，即平均似然屏蔽语言模型，这是一种比较生成不同实体的平均似然的方法，以便于处理的方式执行链路预测。我们在WN18RR数据集上获得了最新的结果（SotA），在FB15k-237数据集上获得了最佳的基于非实体嵌入的结果。我们还获得了令人信服的结果，链接预测对以前看不见的实体，使传销一个合适的方法，以引入新的实体到知识库。</pre></li>
<li><a href="https://arxiv.org/abs/2011.07743">Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases</a>
<pre>现有的知识库问答（KBQA）研究主要采用标准的i.i.d假设，即问题的训练分布与测试分布相同。然而，i.i.d在大规模KBs上可能既不合理也不可取，因为1）真实的用户分布很难捕获，2）从巨大空间中随机抽样的训练示例数据效率很低。相反，我们建议KBQA模型应该具有三个级别的内置泛化：i.i.d、合成和零炮。为了促进具有更强泛化能力的KBQA模型的开发，我们构建并发布了一个包含64331个问题的大规模高质量数据集GrailQA，并为所有三个泛化级别提供评估设置。此外，我们还提出了一种新的基于BERT的KBQA模型。我们的数据集和模型的结合使我们能够第一次彻底地检查和演示像BERT这样的预先训练的上下文嵌入在KBQA泛化中的关键作用。</pre></li>
</ul>
<h2 id="text-classification">Text classification</h2>
<ul>
<li><a href="https://arxiv.org/abs/2004.03705">Deep Learning Based Text Classification: A Comprehensive Review</a>
<pre>在各种文本分类任务中，包括情感分析、新闻分类、问答和自然语言推理，基于深度学习的模型已经超越了经典的基于机器学习的方法。在本文中，我们全面回顾了近年来开发的150多个基于深度学习的文本分类模型，并讨论了它们的技术贡献、相似性和优势。我们还总结了40多个广泛用于文本分类的流行数据集。最后，我们对不同深度学习模型在流行基准上的性能进行了定量分析，并讨论了未来的研究方向。</pre></li>
<li><a href="https://arxiv.org/abs/2008.00364">A Text Classification Survey: From Shallow to Deep Learning</a>
<pre>文本分类是自然语言处理中最基本、最基本的任务。由于深度学习取得了前所未有的成功，在过去十年中，这一领域的研究激增。文献中提出了许多方法、数据集和评估指标，因此需要进行全面更新的调查。本文通过回顾1961年至2020年的最新方法填补了这一空白，重点关注从浅层到深层的学习模式。我们根据所涉及的文本以及用于特征提取和分类的模型创建文本分类法。然后，我们详细讨论这些类别中的每一个，处理支持预测测试的技术开发和基准数据集。本调查还对不同技术进行了全面比较，并确定了各种评估指标的优缺点。最后，我们总结了研究的主要意义、未来的研究方向以及研究领域面临的挑战。</pre></li>
<li><a href="https://arxiv.org/abs/1905.05583">How to Fine-Tune BERT for Text Classification?</a>
<pre>语言模型预训练在学习通用语言表征方面已被证明是有用的。作为一种最先进的语言模型预训练模型，BERT（来自变压器的双向编码器表示）在许多语言理解任务中取得了惊人的成果。在本文中，我们进行了详尽的实验来研究文本分类任务中不同的BERT微调方法，并为BERT微调提供了一个通用的解决方案。最后，提出的解决方案在八个广泛研究的文本分类数据集上获得了最新的结果。</pre></li>
<li><a href="https://arxiv.org/abs/1905.02331">X-BERT: eXtreme Multi-label Text Classification with BERT</a>
<pre>我们考虑极端多标签文本分类（XMC）问题：给定一个输入文本，从一个大标签集合返回最相关的标签。例如，输入文本可以是Amazon.com上的产品描述，标签可以是产品类别。XMC是NLP社区中一个重要但具有挑战性的问题。最近，深度预训练的transformer模型在许多NLP任务（包括句子分类）上取得了最先进的性能，尽管标签集很小。然而，由于大输出空间和标签稀疏性问题，天真地将深度转换器模型应用于XMC问题会导致次优性能。在本文中，我们提出了X-Transformer，这是第一个用于微调XMC问题的深度转换器模型的可伸缩方法。该方法在四个XMC基准数据集上获得了最新的结果。特别是，在一个有大约50万个标签的Wiki数据集上prec@1X-Transformer的性能为77.28%，与最先进的XMC方法Parabel（线性）和AttentionXML（神经）相比有了很大的改进，分别达到68.70%和76.95%precision@1分别地我们进一步将X-Transformer应用于来自Amazon的product2query数据集，并获得了10.7%的相对改进prec@1在寓言之上。</pre></li>
<li><a href="https://arxiv.org/abs/2010.01653">An Empirical Study on Large-Scale Multi-Label Text Classification Including Few and Zero-Shot Labels</a> (EMNLP2020)
<pre>大规模多标签文本分类（LMTC）有着广泛的自然语言处理（NLP）应用，并提出了有趣的挑战。首先，由于LMTC数据集的标签集非常大，且标签分布不均匀，因此并非所有标签都能很好地表示在训练集中。此外，标签层次结构和人类标签指南的差异可能会影响图形感知注释接近度。最后，标签层次结构会定期更新，要求LMTC模型能够进行零炮概括。当前最先进的LMTC模型采用标签式注意网络（LWAN），其（1）通常将LMTC视为平面多标签分类；（2） 可以使用标签层次结构来改进零镜头学习，尽管这一实践尚未得到充分研究；和（3）未与预先培训的变压器（如BERT）结合，这导致了几个NLP基准的最新结果。在这里，我们首次对一组LMTC方法进行了实证评估，从普通LWANs到分层分类方法和转移学习，对来自不同领域的三个数据集的频繁、少量和零炮学习进行了评估。我们证明了基于概率标签树（PLT）的分层方法优于LWAN。此外，我们还证明了基于变压器的方法在两个数据集中的性能优于最先进的方法，并且我们提出了一种结合BERT和LWAN的最先进方法。最后，我们提出了利用标签层次结构改进少镜头和零镜头学习的新模型，在每个数据集上考虑了我们引入的图形感知注释邻近度量。</pre></li>
<li><a href="https://arxiv.org/abs/1905.02331">Taming Pretrained Transformers for Extreme Multi-label Text Classification</a> (KDD2020)
<pre>我们考虑极端多标签文本分类（XMC）问题：给定一个输入文本，从一个大标签集合返回最相关的标签。例如，输入文本可以是Amazon.com上的产品描述，标签可以是产品类别。XMC是NLP社区中一个重要但具有挑战性的问题。最近，深度预训练的transformer模型在许多NLP任务（包括句子分类）上取得了最先进的性能，尽管标签集很小。然而，由于大输出空间和标签稀疏性问题，天真地将深度转换器模型应用于XMC问题会导致次优性能。在本文中，我们提出了X-Transformer，这是第一个用于微调XMC问题的深度转换器模型的可伸缩方法。该方法在四个XMC基准数据集上获得了最新的结果。特别是，在一个有大约50万个标签的Wiki数据集上prec@1X-Transformer的性能为77.28%，与最先进的XMC方法Parabel（线性）和AttentionXML（神经）相比有了很大的改进，分别达到68.70%和76.95%precision@1分别地我们进一步将X-Transformer应用于来自Amazon的product2query数据集，并获得了10.7%的相对改进prec@1在寓言之上。</pre></li>
<li><a href="https://arxiv.org/abs/2010.05763">Layer-wise Guided Training for BERT: Learning Incrementally Refined Document Representations</a> (EMNLP2020 WS)
<pre>虽然BERT被NLP社区广泛使用，但对其内部工作原理知之甚少。人们曾多次试图阐明伯特的某些方面，但往往得出相反的结论。一个备受关注的问题集中在BERT的过度参数化和利用不足问题上。为此，我们提出了一种新的方法来以结构化的方式对BERT进行微调。具体来说，我们关注大规模多标签文本分类（LMTC），其中文档被分配一个或多个来自一组预定义的分层组织标签的标签。我们的方法指导特定的BERT层从特定的层次结构级别预测标签。通过对两个LMTC数据集的实验，我们发现这种结构化的微调方法不仅可以产生更好的分类结果，而且可以提高参数利用率。</pre></li>
<li><a href="https://arxiv.org/abs/1904.08398">DocBERT: BERT for Document Classification</a>
<pre>据我们所知，我们介绍了BERT在文档分类中的首次应用。任务的一些特征可能会让人认为BERT不是最合适的模型：语法结构对内容类别的影响较小，文档通常比典型的BERT输入更长，文档通常有多个标签。尽管如此，我们还是证明了一个使用BERT的简单分类模型能够在四个流行的数据集上达到最先进的水平。为了解决与BERT推理相关的计算开销，我们从BERT大到小的双向lstm中提取知识，在多个数据集上使用少30倍的参数达到BERT基奇偶校验。本文的主要贡献是改进的基线，为以后的工作提供了基础。</pre></li>
<li><a href="https://arxiv.org/abs/1909.08402">Enriching BERT with Knowledge Graph Embeddings for Document Classification</a>
<pre>在本文中，我们重点讨论使用简短描述性文本（封面简介）和附加元数据对书籍进行分类。基于深层神经语言模型BERT，我们演示了如何将文本表示与元数据和知识图嵌入相结合，从而对作者信息进行编码。与标准的BERT方法相比，我们在分类任务中获得了更好的结果。对于使用八个标签的更粗粒度分类，我们的F1分数为87.20，而使用343个标签的详细分类的F1分数为64.70。我们公开实验的源代码和经过训练的模型</pre></li>
<li><a href="https://arxiv.org/abs/1906.09821">Classification and Clustering of Arguments with Contextualized Word Embeddings</a> (ACL2019)
<pre>我们在开放域参数搜索的上下文中使用了两种最新的上下文化单词嵌入方法（ELMo和BERT）。这是第一次，我们展示了如何利用上下文化单词嵌入的功能对主题相关的参数进行分类和聚类，在任务和多个数据集上都取得了令人印象深刻的结果。对于论点分类，我们将UKP句子论点挖掘语料库的最新技术提高了20.8个百分点，将IBM辩手证据句子数据集的最新技术提高了7.4个百分点。对于待研究的参数聚类任务，我们提出了一个预训练步骤，在新数据集上比强基线提高了7.8个百分点，在参数方面相似性（AFS）语料库中提高了12.3个百分点。</pre></li>
<li><a href="https://arxiv.org/abs/1910.02655">BERT for Evidence Retrieval and Claim Verification</a>
<pre>受预先训练的语言模型的良好性能的激励，我们在证据检索和索赔验证管道中研究了BERT，以应对发烧事实提取和验证挑战。为此，我们建议使用两个伯特模型，一个用于检索支持或拒绝索赔的潜在证据句子，另一个用于根据预测的证据集验证索赔。为了训练BERT检索系统，我们使用逐点和成对损失函数，并检查硬负挖掘的效果。第二个伯特模型被训练来将样本分类为支持、反驳和不充分的信息。我们的系统实现了87.1的最新召回率，从包含50K维基百科页面的发烧文档中检索前五名句子，并以69.7的发烧分数在官方排行榜中排名第二。</pre></li>
<li><a href="https://arxiv.org/abs/2001.00137">Stacked DeBERT: All Attention in Incomplete Data for Text Classification</a>
<pre>在本文中，我们提出了堆叠德伯特，从变压器堆叠去噪双向编码器表示的缩写。与现有系统相比，这种新模型通过在BERT中设计一种新的编码方案，提高了不完整数据中的鲁棒性。BERT是一种完全基于注意机制的强大语言表示模型。自然语言处理中的不完整数据指的是单词缺失或不正确的文本，它的存在可能会妨碍当前模型的性能，这些模型无法承受此类噪声，但即使在胁迫下也必须表现良好。这是因为当前的方法是为干净完整的数据而构建和训练的，因此无法提取能够充分表示不完整数据的特征。我们提出的方法包括通过将嵌入层应用于输入标记，然后再应用转换来获得中间输入表示。这些中间特征作为新型去噪变压器的输入，负责获得更丰富的输入表示。该方法利用多层感知器堆栈通过提取更抽象和有意义的隐藏特征向量来重建缺失词的嵌入，并利用双向变换来改进嵌入表示。我们考虑两个数据集进行训练和评估：聊天机器人自然语言理解评价语料库和Kaggle的推特情感语料库。我们的模型显示，在情绪和意图分类任务中，在推特中出现的非正式/不正确文本以及存在语音-文本错误的文本中，F1分数提高，鲁棒性更好。</pre></li>
<li><a href="https://arxiv.org/abs/2003.11563">Cost-Sensitive BERT for Generalisable Sentence Classification with Imbalanced Data</a>
<pre>近年来，由于新闻产生和消费方式的技术和社会变化，宣传的自动识别变得越来越重要。使用BERT（一种功能强大的新体系结构，可以对文本分类任务进行微调）可以有效地解决这一任务，这并不奇怪。然而，与处理新闻文档和其他形式的非文本化社会传播（如情绪分析）的其他任务一样，宣传检测本质上是处理类别同时不平衡和不同的数据。我们表明，虽然BERT能够处理不平衡的类，而无需额外的数据扩充，但当训练和测试数据足够不相似时（新闻来源的情况通常如此，其主题随着时间的推移而变化），BERT不能很好地泛化。我们展示了如何通过提供数据集之间相似性的统计度量以及在训练集和测试集不相似时将成本加权纳入BERT的方法来解决这个问题。我们在宣传技巧语料库（PTC）上测试了这些方法，并在句子层面的宣传分类中获得了第二高分。</pre></li>
<li><a href="https://arxiv.org/abs/2004.01970">BAE: BERT-based Adversarial Examples for Text Classification</a> (EMNLP2020)
<pre>现代文本分类模型容易受到对抗性示例的影响，这些示例是人类无法识别的原始文本的扰动版本，这些文本被模型误分类。NLP中最近的工作使用基于规则的同义词替换策略生成对抗性示例。这些策略可能导致上下文之外和非自然复杂的代币替换，这很容易被人类识别。我们介绍了BAE，一种黑盒攻击，用于使用来自伯特蒙面语言模型的上下文干扰生成对抗性示例。BAE通过屏蔽部分文本并利用BET-MLM生成屏蔽令牌的替代方案，在原始文本中替换和插入令牌。通过自动和人工评估，我们发现BAE除了生成语法性和语义连贯性比以前的工作有所提高的对抗性示例外，还具有更强的攻击能力。</pre></li>
<li><a href="https://arxiv.org/abs/2008.04203">FireBERT: Hardening BERT-based classifiers against adversarial attack</a> [<a href="https://github.com/FireBERT-author/FireBERT">github</a>]
<pre>我们介绍了FireBERT，一组三个概念验证NLP分类器，通过生成不同于原始样本的替代方案，增强了对Text傻瓜式单词干扰的抵抗能力。在一种方法中，我们根据训练数据和合成对抗性样本共同调整BERT。在第二种方法中，我们通过替换单词和扰动嵌入向量在评估时生成合成样本。然后通过投票将多样化的评估结果组合在一起。第三种方法用嵌入向量的扰动代替计算时的词替换。我们评估了原始和textwooler生成的对抗性示例中MNLI和IMDB电影评论数据集的FireBERT。我们还测试了在操纵FireBERT时，与使用未硬化分类器相比，TextWilder是否在创建新的对抗性样本方面不太成功。我们表明，在面对对抗性攻击时，可以提高基于BERT的模型的准确性，而不会显著降低常规基准样本的准确性。我们将与合成数据生成器进行联合调优作为一种非常有效的方法，在保持98%原始基准性能的同时，保护95%的预制对抗性样本。我们还证明了评估时间扰动是进一步研究的一个有希望的方向，在TextWiller主动攻击下，评估时间扰动可使预先制造的对手的准确度恢复到基准性能的75%，并使准确度恢复到65%（从75%原始攻击/12%攻击的基线）。</pre></li>
<li><a href="https://www.aclweb.org/anthology/2020.acl-main.191/">GAN-BERT: Generative Adversarial Learning for Robust Text Classification with a Bunch of Labeled Examples</a> (ACL2020)</li>
<li><a href="https://arxiv.org/abs/2002.03067">Description Based Text Classification with Reinforcement Learning</a>
<pre>文本分类的任务通常分为两个阶段：{\it文本特征提取}和{\it分类}。在这种标准的形式化中，类别仅表示为标签词汇表中的索引，模型缺乏关于分类内容的明确说明。受当前将NLP问题形式化为问答任务的趋势的启发，我们提出了一种新的文本分类框架，其中每个类别标签与一个类别描述相关联。描述由手工制作的模板或使用强化学习中的抽象/提取模型生成。描述和文本的连接被提供给分类器，以决定是否应将当前标签分配给文本。建议的策略迫使模型关注与标签相关的最显著文本，这可以被视为注意力的硬版本，从而产生更好的表现。我们观察到在广泛的文本分类任务（包括单标签分类、多标签分类和多方面情感分析）中，强基线显著提高了性能。</pre></li>
<li><a href="https://arxiv.org/abs/2004.05707">VGCN-BERT: Augmenting BERT with Graph Embedding for Text Classification</a>
<pre>近年来，基于神经网络的文本分类方法取得了很大的进展。特别是，使用注意机制的模型，如BERT，已经证明能够捕获句子或文档中的上下文信息。然而，他们获取语言词汇全局信息的能力更为有限。后者是图卷积网络（GCN）的强度。在本文中，我们提出了VGCN-BERT模型，该模型将BERT的能力与词汇表图卷积网络（VGCN）相结合。局部信息和全局信息通过BERT的不同层进行交互，允许它们相互影响并共同构建分类的最终表示。在我们对多个文本分类数据集的实验中，我们的方法优于单独的BERT和GCN，并且比以前的研究中报告的更有效。</pre></li>
<li><a href="https://www.aclweb.org/anthology/2020.acl-main.272/">Zero-shot Text Classification via Reinforced Self-training</a> (ACL2020)</li>
<li><a href="https://arxiv.org/abs/2009.10778">On Data Augmentation for Extreme Multi-label Classification</a>
<pre>在本文中，我们主要研究极端多标签分类（XMC）问题的数据扩充。XMC最具挑战性的问题之一是长尾标签分发，即使是强大的模型也会受到监管不足的影响。为了减轻这种标签偏见，我们提出了一个简单有效的增强框架和一个新的最先进的分类器。我们的扩充框架利用预先训练的GPT-2模型生成输入文本的标签不变扰动，以扩充现有的训练数据。因此，它比基线模型有了实质性的改进。我们的贡献有两个方面：（1）我们引入了一种新的最先进的分类器，它使用RoBERTa的标签注意，并将其与我们的增强框架结合起来进行进一步改进；（2） 我们对XMC任务中不同的增强方法的有效性进行了广泛的研究。</pre></li>
<li><a href="https://arxiv.org/abs/2108.04106">Noisy Channel Language Model Prompting for Few-Shot Text Classification</a>
<pre>本文介绍了一种噪声信道方法，用于小镜头文本分类中的语言模型提示。信道模型不是计算给定输入的标签的可能性（称为直接模型），而是计算给定标签的输入的条件概率，因此需要解释输入中的每个单词。我们为最近提出的一些镜头学习方法使用通道模型，通过上下文演示或提示调优，对语言模型参数没有更新或更新非常有限。我们的实验表明，对于这两种方法，信道模型的性能明显优于直接对应的模型，这是由于它们的稳定性，即更低的方差和更高的最坏情况精度。我们还提供了广泛的消融，为何时使用通道提示调谐而不是其他竞争模型（例如，直接头部调谐）提供了建议：当训练示例数量较少、训练数据中的标签不平衡或需要概括为看不见的标签时，首选通道提示调谐。</pre></li>
<li><a href="https://arxiv.org/abs/2104.01666">Improving Pretrained Models for Zero-shot Multi-label Text Classification through Reinforced Label Hierarchy Reasoning</a> (NAACL2021)
<pre>利用标签层次结构已成为解决零炮多标签文本分类（ZS-MTC）问题的一种很有前途的方法。传统方法旨在学习文本和标签之间的匹配模型，使用图形编码器合并标签层次结构以获得有效的标签表示。最近，诸如BERT\cite{devlin2018bert}之类的预训练模型被用于将分类任务转换为文本蕴涵任务\cite{yin-etal-2019-benchmarking}。这种方法自然适用于ZS-MTC任务。然而，在现有的工作中，预训练模型没有得到充分的研究，因为它们不能为文本或标签生成单独的向量表示，因此无法将它们与传统的图形编码方法结合起来。在本文中，我们探讨了如何在ZS-MTC任务中改进带有标签层次结构的预训练模型。我们提出了一种增强的标签层次推理（RLHR）方法，以鼓励在培训过程中层次中标签之间的相互依赖。同时，为了克服平面预测的缺点，我们设计了一种回滚算法，可以在推理过程中消除预测中的逻辑错误。在三个真实数据集上的实验结果表明，在ZS-MTC任务上，我们的方法取得了更好的性能，并且优于以前的非预训练方法。</pre></li>
<li><a href="https://arxiv.org/abs/2004.03742">Towards Evaluating the Robustness of Chinese BERT Classifiers</a>
<pre>大规模语言表示模型（如BERT）的最新进展改善了许多NLP任务的最新性能。同时，字符级中文NLP模型，包括用于中文的BERT，也证明了它们的性能优于现有的模型。在本文中，我们表明，然而，这种基于伯特的模型在字符级对抗攻击下是脆弱的。我们提出了一种针对基于BERT分类器的中文字符级攻击方法。本质上，我们在嵌入空间中生成字符级的“小”扰动，并指导字符替换过程。大量实验表明，基于该攻击的中文新闻数据集的分类准确率从91.8%下降到0%，平均操作少于2个字符。人类评估也证实，我们生成的中国对抗性示例几乎不会影响人类在这些NLP任务中的表现。</pre></li>
<li><a href="https://arxiv.org/abs/2005.07503">COVID-Twitter-BERT: A Natural Language Processing Model to Analyse COVID-19 Content on Twitter</a> [<a href="https://github.com/digitalepidemiologylab/covid-twitter-bert">github</a>]
<pre>在这项工作中，我们发布了新冠病毒推特伯特（CT-BERT），这是一个基于变压器的模型，对大量关于新冠病毒19的推特消息进行了预训练。在五种不同的分类数据集上，我们的模型与基础模型BERT Large相比有10-30%的边际改进。最大的改进是在目标域上。预训练的变换器模型，如CT-BERT，在特定的目标域上进行训练，可用于各种自然语言处理任务，包括分类、问答和聊天机器人。CT-BERT被优化用于新冠病毒-19的内容，特别是来自Twitter的社交媒体帖子。</pre></li>
<li><a href="https://arxiv.org/abs/2010.12871">Large Scale Legal Text Classification Using Transformer Models</a>
<pre>大型多标签文本分类是一个具有挑战性的自然语言处理（NLP）问题，涉及具有数千个标签的数据集的文本分类。我们在法律领域解决这个问题，在欧盟法律信息系统中创建了JRC Acquis和EURLEX57K等标有EuroVoc词汇表的数据集。EuroVoc分类法包括大约7000个概念。在这项工作中，我们研究了各种最新的基于变压器的模型的性能，并结合生成性预训练、逐渐解冻和区分学习率等策略，以达到竞争性分类性能，并给出了0.661（F1）的最新结果对于JRC Acquis，0.754欧元用于57k欧元。此外，我们量化了单个步骤的影响，如消融研究中的语言模型微调或逐渐解冻，并提供了使用迭代分层算法创建的参考数据集分割。</pre></li>
<li><a href="https://arxiv.org/abs/2104.01782">BBAEG: Towards BERT-based Biomedical Adversarial Example Generation for Text Classification</a> (NAACL2021)
<pre>医疗保健预测分析有助于医疗决策、诊断预测和药物评价分析。因此，预测准确度是一个重要的标准，这也需要稳健的预测语言模型。然而，使用深度学习的模型已被证明容易受到不明显的干扰输入实例的影响，这些输入实例不太可能被人类错误分类。最近使用基于规则的同义词和BERT MLM生成对手的努力在一般领域已得到证实，但不断增加的生物医学文献提出了独特的挑战。我们提出BBAEG（生物医学基于BERT的对抗性示例生成），这是一种用于生物医学文本分类的黑盒攻击算法，它利用了生物医学命名实体的特定领域同义词替换和BERTMLM预测、拼写变化和数字替换的优点。通过对两个数据集的自动和人工评估，我们证明了BBAEG与之前的工作相比，具有更好的语言流畅性和语义连贯性，具有更强的攻击能力。</pre></li>
<li><a href="https://arxiv.org/abs/2009.05451">A Comparison of LSTM and BERT for Small Corpus</a>
<pre>NLP领域的最新进展表明，迁移学习通过调整预先训练的模型而不是从头开始，有助于实现新任务的最新结果。Transformers在为许多NLP任务（包括但不限于文本分类、文本生成和序列标记）创建新的最新结果方面取得了重大改进。这些成功案例大多基于大型数据集。在本文中，我们关注学术界和工业界科学家经常面临的一个现实场景：给定一个小数据集，我们是否可以使用像BERT这样的大型预训练模型，并获得比简单模型更好的结果？为了回答这个问题，我们使用为构建聊天机器人而收集的用于意图分类的小数据集，并比较简单的双向LSTM模型与预先训练的BERT模型的性能。我们的实验结果表明，对于小数据集，双向LSTM模型可以获得比BERT模型更高的结果，并且这些简单模型的训练时间比调整预训练的模型要短得多。我们得出结论，模型的性能取决于任务和数据，因此在选择模型之前，应考虑这些因素，而不是直接选择最流行的模型。</pre></li>
</ul>
<h2 id="wsc-wnli-nli">WSC, WNLI, NLI</h2>
<ul>
<li><a href="https://arxiv.org/abs/1904.09705">Exploring Unsupervised Pretraining and Sentence Structure Modelling for Winograd Schema Challenge</a>
<pre>Winograd Schema Challenge（WSC）被认为是测试计算机常识表示和推理智能的人工智能难题。本文介绍了WSC的最新发展状况，其准确度达到71.1%。我们证明，领先的性能得益于联合建模句子结构，利用从尖端预训练模型学习到的知识，并进行微调。我们进行了详细的分析，表明微调对于实现性能至关重要，但它在更简单的关联问题上帮助更大。然而，句子依赖结构的建模对于WSC中较难的非关联子集始终有帮助。分析还表明，更大的微调数据集可以产生更好的性能，这表明未来注释更多Winograd模式语句的工作可能会带来好处。</pre></li>
<li><a href="https://arxiv.org/abs/1905.06290">A Surprisingly Robust Trick for the Winograd Schema Challenge</a>
<pre>Winograd Schema Challenge（WSC）数据集WSC273及其推理对应物WNLI是自然语言理解和常识推理的常用基准。在本文中，我们证明了当对类似的代词消歧问题数据集（表示为WSCR）进行微调时，WSC273上的三种语言模型的性能有了很大的提高。此外，我们还生成了一个类似WSC的大型无监督数据集。通过在引入的数据集和WSCR数据集上微调BERT语言模型，我们在WSC273和WNLI上实现了72.5%和74.7%的总体准确率，将以前的最先进解决方案分别提高了8.8%和9.6%。此外，我们的微调模型在Trichelair等人（2018）提出的WSC273的“复杂”子集上也始终更加稳健。</pre></li>
<li><a href="https://arxiv.org/abs/1907.10641">WinoGrande: An Adversarial Winograd Schema Challenge at Scale</a> (AAAI2020)
<pre>Winograd Schema Challenge（WSC）（Levesque、Davis和Morgenstern，2011）是常识推理的基准，是一组273个专家精心设计的代词解析问题，最初设计用于依赖选择偏好或单词关联的统计模型无法解决。然而，神经语言模型的最新进展已经达到了WSC变体的90%左右的准确率。这就提出了一个重要的问题：这些模型是否真正获得了强大的常识能力，或者它们是否依赖于数据集中的虚假偏差，从而导致高估机器常识的真实能力。为了研究这个问题，我们引入了WinoGrande，这是一个由44k个问题组成的大规模数据集，其灵感来自原始WSC设计，但经过调整以提高数据集的规模和硬度。数据集构建的关键步骤包括：（1）精心设计的众包程序，然后（2）使用一种新的AfLite算法系统地减少偏差，该算法将人类可检测的单词关联推广到机器可检测的嵌入关联。WinoGrande最先进的方法达到59.4-79.1%，比94.0%的人类绩效低15-35%，具体取决于允许的培训数据量。此外，我们在五个相关基准上建立了最新的最新结果——WSC（90.1%）、DPR（93.1%）、COPA（90.6%）、KnowRef（85.6%）和Winogener（97.1%）。这些结果具有双重含义：一方面，它们证明了WinoGrande作为迁移学习资源的有效性。另一方面，他们提出了一个担忧，即我们可能高估了所有这些基准中机器常识的真实能力。我们强调在现有和未来基准中减少算法偏差的重要性，以减轻这种高估。</pre></li>
<li><a href="https://arxiv.org/abs/2003.08380">TTTTTackling WinoGrande Schemas</a>
<pre>我们应用T5序列到序列模型来解决AI2 WinoGrande挑战，将每个示例分解为两个输入文本字符串，每个字符串包含一个假设，并使用分配给“蕴涵”标记的概率作为假设分数。我们在2020年3月13日首次（也是唯一一次）向官方排行榜提交的结果为0.7673 AUC，这是目前最著名的结果，比之前的最先进水平高出5个百分点。</pre></li>
<li><a href="https://arxiv.org/abs/2005.05763">WinoWhy: A Deep Diagnosis of Essential Commonsense Knowledge for Answering Winograd Schema Challenge</a> (ACL2020)
<pre>在本文中，我们提出了第一个基本常识知识的综合分类，用于回答Winograd模式挑战（WSC）。对于每个问题，我们请注释者首先提供做出正确决策的原因，然后将其分为六大知识类别。通过这样做，我们可以更好地理解现有方法的局限性（即，什么样的知识不能用现有方法有效地表示或推断），并对我们未来需要获得的常识知识提供一些启示，以便更好地进行常识推理。此外，为了调查当前的WSC模型是否能够理解常识，或者它们只是基于数据集的统计偏差来解决WSC问题，我们利用收集到的原因开发了一个新任务，称为WinoWhy，这要求模型区分所有WSC问题的合理原因和非常相似但错误的原因。实验结果证明，尽管预先训练的语言表示模型在原始WSC数据集上取得了有希望的进展，但它们仍然在WinoWhy上苦苦挣扎。进一步的实验表明，尽管监督模型可以获得更好的性能，但这些模型的性能对数据集分布非常敏感。WinoWhy和所有代码可从以下网址获得：https://github.com/HKUST-KnowComp/WinoWhy.</pre></li>
<li><a href="https://arxiv.org/abs/2005.01348">The Sensitivity of Language Models and Humans to Winograd Schema Perturbations</a> (ACL2020)
<pre>大规模的预训练语言模型是最近Winograd Schema Challenge性能改进的主要驱动力，Winograd Schema Challenge是一项广泛使用的常识推理能力测试。然而，我们通过一个新的诊断数据集表明，这些模型对Winograd示例的语言干扰非常敏感，这些干扰对人类理解的影响最小。我们的研究结果突出了人类和语言模型之间有趣的差异：语言模型比人类对数字或性别变化以及同义词替换更敏感，人类的预测更稳定一致，保持更高的绝对性能，并且在非关联实例上的性能优于关联实例。总的来说，人类比开箱即用的模型更为正确，而模型有时因为错误的原因是正确的。最后，我们展示了在大型、特定于任务的数据集上进行微调可以为这些问题提供解决方案。</pre></li>
<li><a href="https://arxiv.org/abs/2010.04043">Precise Task Formalization Matters in Winograd Schema Evaluations</a> (EMNLP2020)
<pre>Winograd Schema Challenge（WSC）是一个备受尊敬的英国常识推理基准，其在SuperGLUE排行榜上的表现最近从偶然准确率飙升至89%，而推理能力相应大幅提高的确凿证据相对较少。我们假设，这种改进很大程度上来自于数据集用户在任务形式化方面的最新变化，即输入规范、损失函数和预训练参数的重用的结合，而不是预训练模型推理能力的改进。我们在两个Winograd模式数据集上进行了一次消融，这两个数据集在这次激增前后使用的形式化之间进行了插值，并发现（i）由于多项选择提高了2-6个点的性能，因此对任务进行了框架化，以及（ii）几种附加技术，包括重新使用预先训练的语言建模头，可以减轻模型对超参数的极端敏感性。我们敦促未来的基准制定者实施额外的结构，以尽量减少正式化决策对报告结果的影响。</pre></li>
<li><a href="https://arxiv.org/abs/2011.12081">Tackling Domain-Specific Winograd Schemas with Knowledge-Based Reasoning and Machine Learning</a>
<pre>Winograd模式挑战（WSC）是一项需要背景知识的常识推理任务。在本文中，我们从四个方面为解决WSC做出了贡献。首先，我们提出了一种关键字方法来定义一个受限域，在该域中可以找到不同的高级语义模式。通过关键词定义了一个感谢域，并将该域中的数据集用于我们的实验。其次，我们在Sharma[2019]方法的基础上，开发了一种基于语义角色的高级知识推理方法。第三，我们提出了一种将基于知识的推理和机器学习相结合的集成方法，在我们的实验中表现出最好的性能。作为一种机器学习方法，我们使用变压器的双向编码器表示法（BERT）[Kocijan等人，2019]。最后，在评估方面，我们建议通过修改Trichelair等人[2018]的测量结果，实现“稳健”的精度测量。与他们的切换方法一样，我们通过考虑模型在测试集中每个句子的平凡变体上的性能来评估模型。</pre></li>
<li><a href="https://arxiv.org/abs/2004.13831">A Review of Winograd Schema Challenge Datasets and Approaches</a>
<pre>Winograd模式挑战是常识推理和自然语言理解挑战，作为图灵测试的替代方案引入。Winograd模式是一对句子，在一个或两个单词中具有高度歧义的代词，在两个句子中以不同的方式解决，这似乎需要正确解决常识知识。这些例子被设计成很容易被人类解决，但对于机器来说很难，原则上需要对文本内容及其描述的情况有深入的理解。本文回顾了自引入以来已发布的现有Winograd模式挑战基准数据集和方法。</pre></li>
<li><a href="https://arxiv.org/abs/1909.08217">Improving Natural Language Inference with a Pretrained Parser</a>
<pre>我们介绍了一种新的方法，将语法整合到自然语言推理（NLI）模型中。我们的方法使用来自预训练依赖项解析器的上下文标记级向量表示。与其他上下文嵌入器一样，我们的方法广泛适用于任何神经模型。我们用四个强NLI模型（可分解注意模型、ESIM、BERT和MT-DNN）进行了实验，并在三个NLI基准测试中显示出一致的准确性优势。</pre></li>
<li><a href="https://arxiv.org/abs/2004.03066">Are Natural Language Inference Models IMPPRESsive? Learning IMPlicature and PRESupposition</a>
<pre>自然语言推理（NLI）是自然语言理解中一项越来越重要的任务，它要求一个人推断一个句子是否包含另一个句子。然而，NLI模型进行语用推理的能力仍有待研究。我们创建了一个含义和预设诊断数据集（IMPPRES），该数据集由>25k个半自动生成的句子对组成，说明了经过充分研究的语用推理类型。我们使用IMPPRES来评估在多重NLI（Williams等人，2018年）上接受培训的BERT、Inferesent和BOW NLI模型是否学习进行语用推理。虽然MultiNLI似乎包含很少的对来说明这些推理类型，但我们发现BERT学习绘制语用推理。它可靠地将“某些”触发的标量含义视为蕴涵。对于某些预设触发器，如“only”，BERT可靠地将预设识别为蕴涵，即使触发器嵌入了蕴涵取消运算符（如否定）下。BOW和InferSent的语用推理证据较弱。我们的结论是NLI培训鼓励模型学习一些但不是全部的语用推理。</pre></li>
<li><a href="https://arxiv.org/abs/2106.09449">DocNLI: A Large-scale Dataset for Document-level Natural Language Inference</a> (ACL2021 Findings)
<pre>自然语言推理（Natural language Reference，NLI）是一个统一的框架，用于解决各种NLP问题，如关系抽取、问题回答、摘要等。由于大规模标记数据集的可用性，近年来对其进行了深入的研究。然而，现有的研究大多集中于句子层面的推理，这限制了自然语言学习在下游自然语言处理问题中的应用范围。这项工作提出了DocNLI——一个新构建的用于文档级NLI的大规模数据集。DocNLI从广泛的NLP问题转化而来，涵盖多种文本类型。前提总是停留在文档粒度上，而假设的长度从单个句子到包含数百个单词的段落不等。此外，DocNLI具有非常有限的工件，不幸的是，这些工件广泛存在于一些流行的句子级NLI数据集中。我们的实验表明，即使没有微调，在DocNLI上预训练的模型在流行的句子级基准测试上显示出良好的性能，并且很好地推广到依赖于文档粒度推理的域外NLP任务。特定于任务的微调可以带来进一步的改进。数据、代码和预训练模型可在https://github.com/salesforce/DocNLI.</pre></li>
<li><a href="https://arxiv.org/abs/1910.14599">Adversarial NLI: A New Benchmark for Natural Language Understanding</a>
<pre>我们介绍了一个新的大规模NLI基准数据集，该数据集通过迭代、对抗性的人和模型在环过程收集。我们表明，在这个新数据集上的训练模型可以在各种流行的NLI基准上获得最先进的性能，同时对其新的测试集提出了更困难的挑战。我们的分析揭示了当前最先进模型的缺点，并表明非专家注释者能够成功地发现他们的缺点。数据收集方法可以应用于永无止境的学习场景中，成为NLU的移动目标，而不是快速饱和的静态基准。</pre></li>
<li><a href="https://arxiv.org/abs/1912.03441">Adversarial Analysis of Natural Language Inference Systems</a> (ICSC2020)
<pre>大型自然语言推理（NLI）数据集（如SNLI和MNLI）的发布导致了该任务的完全神经系统的快速发展和改进。最近，经过大量预训练的基于变压器的模型（如BERT和MT-DNN）在这些数据集上达到了接近人类的性能。然而，这些标准数据集已被证明包含许多注释工件，允许模型使用简单易出错的启发式方法来简化理解，并且在测试集上仍然表现良好。因此，毫不奇怪，许多对抗性（挑战性）数据集的创建导致在标准数据集上训练的模型显著失败。尽管对这些数据进行额外的培训通常可以提高该类型数据的模型性能，但将这种学习转化为看不见的示例充其量只是部分。这项工作评估了现有对抗性数据集上测试不同语言现象的最新模型的失败，并发现即使这些模型在MNLI上表现相似，它们对这些攻击的鲁棒性也有很大差异。特别是，我们发现与语法相关的攻击在所有模型中都特别有效，因此我们对这些示例提供了模型性能的细粒度分析和比较。我们得出关于模型大小和多任务学习的价值的结论（除了比较它们的标准测试集性能之外），并为更有效的训练数据提供建议。</pre></li>
<li><a href="https://arxiv.org/abs/2010.12729">ANLIzing the Adversarial Natural Language Inference Dataset</a>
<pre>我们对对抗性NLI（ANLI）进行了深入的错误分析，ANLI是最近引入的大规模人与模型在环自然语言推理数据集，收集了多轮数据。我们对负责黄金分类标签的推理的不同方面提出了一个细粒度注释方案，并使用它手动编码所有三个ANLI开发集。我们使用这些注释来回答各种有趣的问题：哪种推理类型最常见，哪种模型在每种推理类型上的性能最高，哪种类型对最先进的模型来说最具挑战性？我们希望我们的注释能够对在ANLI上训练的模型进行更细粒度的评估，让我们更深入地理解模型的失败和成功之处，并帮助我们确定未来如何训练更好的模型。</pre></li>
<li><a href="https://arxiv.org/abs/2004.11999">Syntactic Data Augmentation Increases Robustness to Inference Heuristics</a> (ACL2020)
<pre>预训练的神经模型，如BERT，当微调以执行自然语言推理（NLI）时，通常在标准数据集上显示出高精度，但在受控挑战集上显示出对词序的惊人缺乏敏感性。我们假设，这一问题主要不是由预训练模型的局限性引起的，而是由于在微调阶段可能传达句法结构重要性的众包NLI示例的缺乏。我们探索了几种方法，通过对MNLI语料库中的句子进行句法转换来生成语法信息示例，从而增强标准训练集。表现最好的增强方法，主语/宾语倒置，将BERT对受控示例的准确度从0.28提高到0.73，而不影响MNLI测试集的性能。这种改进超越了用于数据扩充的特定构造，表明扩充导致BERT招募抽象语法表示。</pre></li>
<li><a href="https://arxiv.org/abs/2010.08580">Linguistically-Informed Transformations (LIT): A Method for Automatically Generating Contrast Sets</a> (EMNLP2020 WS) [<a href="https://github.com/leo-liuzy/LIT_auto-gen-contrast-set">github</a>]
<pre>尽管大规模的预训练语言模型，如BERT和RoBERTa，在分布内测试集上取得了超人的性能，但在分布外测试集（如对比集）上，它们的性能受到影响。构建对比集通常需要人工专家注释，这既昂贵又难以大规模创建。在这项工作中，我们提出了一种语言信息转换（LIT）方法来自动生成对比集，这使从业者能够探索感兴趣的语言现象以及合成不同的现象。在SNLI和MNLI上用我们的方法进行的实验表明，当前的预训练语言模型尽管被认为包含了足够的语言知识，但在我们自动生成的对比集上仍存在困难。此外，我们在不影响原始数据性能的情况下，通过应用ing LIT增加训练数据，提高了模型在对比度集上的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2003.02756">HypoNLI: Exploring the Artificial Patterns of Hypothesis-only Bias in Natural Language Inference</a> (LREC2020)
<pre>最近的许多研究表明，对于在自然语言推理（NLI）数据集上训练的模型，只需查看假设而完全忽略前提，就可以做出正确的预测。在这项工作中，我们设法从仅假设的偏见中得出对抗性的例子，并探索缓解这种偏见的合适方法。具体来说，我们从训练集中的假设（人工模式）中提取各种短语，并表明它们是特定标签的有力指标。然后，我们从原始测试集中找出“硬”和“易”实例，其标签与这些指示相反或一致。我们还建立了基线，包括预训练模型（BERT、RoBERTa、XLNet）和竞争性非预训练模型（InferSent、DAM、ESIM）。除了基准和基线外，我们还研究了两种利用人工模式建模来缓解这种仅假设偏差的debiasing方法：下采样和对抗性训练。我们相信这些方法可以作为NLI借记任务中的竞争基线。</pre></li>
<li><a href="https://arxiv.org/abs/2004.14963">Use of Machine Translation to Obtain Labeled Datasets for Resource-Constrained Languages</a> (EMNLP2020) [<a href="https://github.com/boun-tabi/NLI-TR">github</a>]
<pre>NLP中的大型注释数据集绝大多数为英语。这是其他语言进步的障碍。不幸的是，为每种语言的每项任务获取新的带注释的资源将花费高昂。与此同时，商业机器翻译系统现在已经非常健壮。我们能利用这些系统自动翻译英语数据集吗？在本文中，我们对土耳其语的自然语言推理（NLI）做出了积极的回应。我们将两个大型英语NLI数据集翻译成土耳其语，并让一组专家验证其翻译质量和对原始标签的忠实性。使用这些数据集，我们解决了土耳其NLI表示的核心问题。我们发现，在语言中嵌入是必要的，并且在训练集较大的情况下可以避免形态分析。最后，我们证明了在机器翻译的数据集上训练的模型在人工翻译的评估集上是成功的。我们公开共享所有代码、模型和数据。</pre></li>
<li><a href="https://arxiv.org/abs/2009.08820">FarsTail: A Persian Natural Language Inference Dataset</a>
<pre>自然语言推理（NLI）是自然语言处理（NLP）的核心任务之一，包含了语言理解的许多基本方面。随着NLP任务中数据饥饿的深度学习方法取得了巨大的成就，人们投入了大量的精力为不同的语言开发更加多样化的数据集。在本文中，我们提出了一个新的数据集的NLI任务波斯语，也被称为波斯，这是在中东的主要语言之一。这个名为FarsTail的数据集包括10367个样本，这些样本以波斯语和索引格式提供，对非波斯语研究人员有用。样本由3539个选择题生成，注释者干预最少，方式与SciTail数据集类似。为了保证数据集的质量，采用了精心设计的多步骤过程。我们还介绍了FarsTail的传统和最先进方法的结果，包括不同的嵌入方法，如word2vec、fastText、ELMo、BERT和LASER，以及不同的建模方法，如DecompAtt、ESIM、HBMP和ULMFiT，为未来的研究提供了坚实的基础。获得的最佳测试准确率为83.38%，这表明现有方法有很大的改进空间，可用于不同语言的实际NLP应用。我们还调查了模型在FarsTail中利用表面线索（也称为数据集偏差）的程度，并根据偏差模型的成功程度将测试集划分为容易和困难的子集。该数据集可在https://github.com/dml-qom/FarsTail</pre></li>
<li><a href="https://www.aclweb.org/anthology/D19-1630/">Evaluating BERT for natural language inference: A case study on the CommitmentBank</a> (EMNLP2019)</li>
<li><a href="https://arxiv.org/abs/2004.14839">Do Neural Models Learn Systematicity of Monotonicity Inference in Natural Language?</a> (ACL2020)
<pre>尽管使用神经网络的语言模型取得了成功，但仍不清楚神经模型在多大程度上具有执行推理的泛化能力。本文介绍了一种评价神经模型是否能够学习自然语言中单调性推理的系统性的方法，即对合成进行泛化的任意推理的规律性。我们考虑单调性推断的四个方面，并检验模型是否能够系统地解释不同训练/测试分裂中的词汇和逻辑现象。一系列实验表明，当训练集和测试集的句子句法结构相似时，三个神经模型系统地对词汇和逻辑现象的不可见组合进行推理。然而，当测试集中的结构发生轻微变化时，模型的性能会显著降低，同时保留训练集中已经出现的所有词汇和成分。这表明神经模型的泛化能力仅限于句法结构与训练集中的句法结构几乎相同的情况。</pre></li>
<li><a href="https://arxiv.org/abs/1908.05739">Abductive Commonsense Reasoning</a> (ICLR2020)
<pre>溯因推理是对最合理解释的推理。例如，如果Jenny下班回来时发现自己的房子乱七八糟，并且记得自己把窗户开着，那么她可以假设是小偷闯入了她的房子，造成了混乱，这是最合理的解释。虽然诱因一直被认为是人们在自然语言中如何理解和阅读字里行间的核心（Hobbs et al.，1988），但支持诱因性自然语言推理和生成的研究相对较少。我们提出了第一项研究，调查的可行性，基于语言的诱因推理。我们介绍了一个挑战数据集ART，它由超过20000个常识性的叙述上下文和200000个解释组成。基于这个数据集，我们概念化了两个新任务——（i）诱因性NLI：选择更可能解释的多项选择问答任务，和（ii）诱因性NLG：用自然语言解释给定观察的条件生成任务。在诱因NLI上，最佳模型达到68.9%的准确率，远低于91.4%的人类表现。在诱因性NLG上，目前最好的语言生成器甚至更加困难，因为它们缺乏对人类来说微不足道的推理能力。我们的分析为深入的预先训练的语言模型无法执行的推理类型提供了新的见解——尽管它们在相关但更狭义的蕴涵NLI任务中表现出色——为未来的研究指明了有趣的途径。</pre></li>
<li><a href="https://arxiv.org/abs/2104.14690">Entailment as Few-Shot Learner</a>
<pre>大型预先训练的语言模型（LMs）已经证明了与少数shot学习者一样的卓越能力。然而，他们的成功在很大程度上取决于模型参数的缩放程度，这使得训练和服务具有挑战性。在本文中，我们提出了一种新的方法，称为EFL，它可以将小LMs转化为更好的少镜头学习者。这种方法的关键思想是将潜在的NLP任务重新表述为包含任务，然后用8个示例对模型进行微调。我们进一步证明了我们提出的方法可以：（i）自然地与基于无监督对比学习的数据扩充方法相结合；（ii）易于扩展到多语言的少数镜头学习。对18项标准NLP任务的系统评估表明，该方法将现有的各种SOTA少数镜头学习方法提高了12%，并产生了500倍大的模型（如GPT-3）具有竞争力的少数镜头性能。</pre></li>
<li><a href="https://arxiv.org/abs/2004.11997">Collecting Entailment Data for Pretraining: New Protocols and Negative Results</a>
<pre>自然语言推理（NLI）数据在基准测试中被证明是有用的，尤其是作为需要语言理解的任务的预训练数据。然而，用于收集这些数据的众包协议存在已知问题，并且没有针对这两个目的进行明确优化，因此它可能远远不够理想。我们提出了四个备选方案，每个方案的目的是提高注释员制作声音训练示例的容易程度，或者提高这些示例的质量和多样性。使用这些替代方案和第五个基线协议，我们收集并比较了五个新的8.5k示例训练集。在以迁移学习应用为重点的评估中，我们的结果完全是负面的，在我们的基线数据集上训练的模型产生了良好的下游任务迁移性能，但我们的四种新方法（以及最近的ANLI）都没有显示出比基线有任何改进。有一点值得庆幸的是，我们发现所有四个新协议，特别是注释者编辑预填充文本框的协议，都减少了以前观察到的注释工件问题。</pre></li>
<li><a href="https://arxiv.org/abs/2010.01239">Mining Knowledge for Natural Language Inference from Wikipedia Categories</a> (EMNLP2020 Findings)
<pre>准确的词汇蕴涵（LE）和自然语言推理（NLI）通常需要大量昂贵的注释。为了减少对标记数据的需求，我们引入了WikiNLI：一种用于改进NLI和LE任务的模型性能的资源。它包含428899对短语，这些短语是根据维基百科中自然注释的类别层次结构构建的。我们表明，我们可以通过在WikiNLI上预先培训BERT和RoBERTa等强基线，并将模型转移到下游任务上，从而改进它们。我们对从其他知识库（如WordNet和Wikidata）中提取的短语进行了系统的比较，发现在WikiNLI上进行预培训的效果最好。此外，我们还用其他语言构建了WikiNLI，并表明对它们进行预训练可以提高相应语言NLI任务的性能。</pre></li>
</ul>
<h2 id="commonsense">Commonsense</h2>
<ul>
<li><a href="https://arxiv.org/abs/1811.00937">CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge</a> (NAACL2019)
<pre>当回答一个问题时，除了特定的背景之外，人们还经常利用他们丰富的世界知识。最近的工作主要集中在回答给定一些相关文档或上下文的问题，并且只需要很少的一般背景。为了研究具有先验知识的问答，我们提出了常识问答：一个具有挑战性的常识问答新数据集。为了捕捉关联之外的常识，我们从ConceptNet（Speer et al.，2017）中提取了与单个源概念具有相同语义关系的多个目标概念。人群工作者被要求撰写多项选择题，其中提到源概念，并依次区分每个目标概念。这鼓励工作人员创建具有复杂语义的问题，这些问题通常需要事先了解。我们通过这一过程提出了12247个问题，并通过大量强有力的基线证明了我们任务的难度。我们的最佳基线基于BERT large（Devlin et al.，2018），获得了56%的准确率，远远低于人的表现，即89%。</pre></li>
<li><a href="https://arxiv.org/abs/2112.03254">Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention</a>
<pre>当今大多数人工智能系统都专注于在大量不同数据上使用自我关注机制和转换器架构，以获得令人印象深刻的性能提升。在本文中，我们建议使用外部注意机制来增强transformer体系结构，以带来外部知识和上下文。通过将外部信息集成到预测过程中，我们希望减少对更大模型的需求，并提高人工智能系统的民主化程度。我们发现，所提出的外部注意机制可以显著改善现有的人工智能系统的性能，允许从业者容易地将基础AI模型定制到许多不同的下游应用。特别是，我们关注于常识推理的任务，证明所提出的外部注意机制可以增强现有的变压器模型，并显著提高模型的推理能力。所提议的系统，知识外部注意推理（KEAR），在开放常识QA研究基准上达到人类平等，准确率为89.4\%，而人类准确率为88.9\%。</pre></li>
<li><a href="https://arxiv.org/abs/1905.07830">HellaSwag: Can a Machine Really Finish Your Sentence?</a> (ACL2019) [<a href="https://rowanzellers.com/hellaswag/">website</a>]
<pre>Zellers等人（2018年）最近的工作引入了一项新的常识自然语言推理任务：给定一个事件描述，如“一个女人坐在钢琴前”，机器必须选择最可能的后续事件：“她将手指放在键上。”随着BERT的引入，达到了接近人类水平的性能。这是否意味着机器可以执行人类水平的常识推理？在本文中，我们展示了一个新的挑战数据集HellaSwag，证明即使是最先进的模型也难以进行常识推理。尽管它的问题对人类来说微不足道（>95%的准确率），但最先进的模型却难以解决（<48%）。我们通过对抗性过滤（AF）实现这一点，这是一种数据收集范式，其中一系列鉴别器迭代选择一组机器生成的错误答案。事实证明，AF具有惊人的健壮性。关键的洞察是将数据集示例的长度和复杂性扩展到一个关键的“金发”区域，其中生成的文本对人类来说是荒谬的，但往往被最先进的模型错误分类。我们对HellaSwag的构造及其带来的困难，揭示了深层预训练模型的内部工作原理。更广泛地说，它为NLP研究提供了一条新的前进道路，基准与不断发展的最新技术以对抗的方式共同发展，从而提出更严峻的挑战。</pre></li>
<li><a href="https://www.aclweb.org/anthology/2020.emnlp-main.192/">A Method for Building a Commonsense Inference Dataset Based on Basic Events</a> (EMNLP2020) [<a href="http://nlp.ist.i.kyoto-u.ac.jp/EN/?KUCI">website</a>]</li>
<li><a href="https://arxiv.org/abs/1905.07504">Story Ending Prediction by Transferable BERT</a> (IJCAI2019)
<pre>最近的一些进展，如GPT和BERT，已经表明成功地结合了预先训练的transformer语言模型和微调操作，以改进下游NLP系统。然而，该框架在有效整合来自其他相关任务的监督知识方面仍然存在一些基本问题。在本研究中，我们研究了一个可转移的BERT（TransBERT）训练框架，该框架不仅可以从大规模未标记数据中转移一般语言知识，还可以从各种语义相关的监督任务中转移特定类型的知识，用于目标任务。特别地，我们建议利用三种迁移任务，包括自然语言推理、情感分类和下一个动作预测，在预训练模型的基础上进一步训练BERT。这使模型能够更好地初始化目标任务。我们以故事结尾预测为目标任务进行实验。最终结果的准确率为91.8%，大大优于以前最先进的基线方法。几个对比实验为如何选择迁移任务提供了一些有益的建议。错误分析显示了基于伯特的故事结局预测模型的优缺点。</pre></li>
<li><a href="https://arxiv.org/abs/1906.02361">Explain Yourself! Leveraging Language Models for Commonsense Reasoning</a> (ACL2019)
<pre>深度学习模型在需要常识推理的任务上表现不佳，这通常需要某种形式的世界知识或对输入中不立即出现的信息进行推理。我们以自然语言序列的形式收集人类对常识推理的解释，并在称为常识解释（CoS-E）的新数据集中突出显示注释。我们使用CoS-E来训练语言模型，以便在一个新的常识自动生成解释（CAGE）框架中自动生成可在训练和推理期间使用的解释。在具有挑战性的常识QA任务中，CAGE将最新技术提高了10%。我们进一步研究DNN中的常识推理，使用人工和自动生成的解释，包括转移到域外任务。实证结果表明，我们可以有效地利用语言模型进行常识推理。</pre></li>
<li><a href="https://arxiv.org/abs/2004.14074">Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning</a> (ACL2020)
<pre>对预先训练好的变压器模型进行微调已成为解决常见NLP任务的标准方法。现有的大多数方法都依赖于这种网络上的随机初始化分类器。我们认为，这种微调过程是次优的，因为预先训练的模型没有特定分类器标签的先验知识，而它可能已经学习了任务的内在文本表示。在本文中，我们介绍了一种新的评分方法，该方法以全文格式对合理性排序任务进行评分，并利用在训练前阶段调整的蒙面语言建模头。我们研究常识推理任务，其中模型必须对给定前提的一组假设进行排序，重点关注COPA、Swag、HellaSwag和常识QA数据集。通过利用我们的评分方法而不进行微调，我们能够产生与监督方法相当的强基线（例如，COPA测试准确率为80%）。此外，当直接对建议的评分函数进行微调时，我们表明，我们的方法在随机重新启动过程中提供了更稳定的训练阶段（例如，COPA测试精度的$\乘以10$标准偏差减少），并且需要比标准分类器方法更少的注释数据才能达到同等性能。</pre></li>
<li><a href="https://arxiv.org/abs/1908.06725">Align, Mask and Select: A Simple Method for Incorporating Commonsense Knowledge into Language Representation Models</a>
<pre>最先进的预先训练的语言表示模型，例如变压器的双向编码器表示（BERT），很少明确地包含常识知识或其他知识。我们提出了一种将常识知识融入语言表示模型的预训练方法。我们构建了一个与常识相关的多选问答数据集，用于预训练神经语言表示模型。数据集是由我们提出的“对齐、屏蔽和选择”（AMS）方法自动创建的。我们还调查了不同的培训前任务。实验结果表明，在两个与常识相关的基准测试（包括commonsense QA和Winograd Schema Challenge）上，使用所提出的方法并进行微调的预训练模型比以前的最新模型取得了显著的改进。我们还观察到，与原始的BERT模型相比，在提出的预训练方法之后的微调模型在其他自然语言处理任务（如句子分类和自然语言推理任务）上保持了相当的性能。这些结果验证了所提出的方法在显著改善与常识相关的NLP任务的同时，不会降低通用语言表示能力。</pre></li>
<li><a href="https://arxiv.org/abs/1909.02339">Informing Unsupervised Pretraining with External Linguistic Knowledge</a>
<pre>无监督预训练模型已被证明有助于广泛的下游NLP应用。然而，这些模型保留了传统静态单词嵌入的一些局限性。特别是，它们只编码原始文本语料库中可用的分布知识，通过语言建模目标合并。在这项工作中，我们用外部词汇知识来补充这种分布知识，也就是说，我们将单词级语义相似性的离散知识集成到预训练中。为此，我们将标准的BERT模型推广到多任务学习环境中，将BERT的蒙面语言建模和下一句预测目标与二元词关系分类的辅助任务相结合。我们的实验表明，我们的“词汇知情”伯特（LIBERT），专门用于单词级语义相似性，在一些语言理解任务上比词汇盲的“香草”伯特产生更好的性能。具体地说，在胶粘基准的10个任务中，9个优于伯特，并且在剩下的一个任务中与伯特相媲美。此外，我们在词汇简化的3个基准上取得了一致的成果，在这项任务中，关于单词级语义相似性的知识是至关重要的。</pre></li>
<li><a href="https://arxiv.org/abs/1909.03415">Commonsense Knowledge + BERT for Level 2 Reading Comprehension Ability Test</a>
<pre>常识知识在我们阅读时起着重要作用。在班数据集上的性能表明，该算法的准确率优于人工用户。然而，这并不意味着计算机在阅读理解上可以超越人类。CommonsenseQA是基于常识知识设计的大规模数据集。伯特在这方面的准确率只有55.9%。结果表明，计算机不能像人类一样应用常识知识来回答问题。理解能力测试（CAT）将阅读理解能力分为四个层次。我们可以一层一层地达到人的理解能力。伯特在不需要常识的1级考试中表现良好。在这项研究中，我们提出了一个系统，其目的是让计算机阅读文章和回答相关问题的常识知识，就像人类的猫2级。该系统由三部分组成。首先，我们构建了一个常识知识图；然后根据它自动构造常识知识问题集。最后，将BERT与常识知识相结合，达到CAT 2级的阅读理解能力。实验表明，只要知识库中包含所需的公共知识，该算法就可以通过CAT。</pre></li>
<li><a href="https://arxiv.org/abs/1910.07713">BIG MOOD: Relating Transformers to Explicit Commonsense Knowledge</a>
<pre>我们介绍了一种简单而有效的方法，将上下文嵌入与常识图嵌入相结合，称为BERT注入图：与其他嵌入相匹配。首先，我们引入一种预处理方法来提高查询知识库的速度。然后，我们开发了一种从每个知识库创建知识嵌入的方法。我们介绍了一种在两种未对齐的标记化方法之间对齐标记的方法。最后，结合知识库嵌入，提出了一种基于知识库嵌入的文本化方法。我们还显示了BERTs倾向于纠正精度较低的问题类型。我们的模型实现了比BERT更高的准确性，我们在共享任务的官方排行榜上排名第五，在没有任何额外语言模型预训练的情况下得分最高。</pre></li>
<li><a href="https://arxiv.org/abs/1909.00505">Commonsense Knowledge Mining from Pretrained Models</a> (EMNLP2019)
<pre>推理常识知识是自然语言处理中的一个关键挑战，但由于训练数据的稀疏性，以前的工作表明，有监督的常识知识挖掘方法在对新数据进行评估时表现不佳。在这项工作中，我们开发了一种使用预先训练的大型双向语言模型生成常识知识的方法。通过将关系三元组转换为掩蔽句，我们可以使用该模型根据估计的两个实体之间的逐点互信息对三元组的有效性进行排序。由于我们不更新双向模型的权重，因此我们的方法不受任何常识知识库覆盖范围的影响。虽然该方法在测试集上的性能比在相应的训练集上明确训练的模型差，但在从新的来源挖掘常识知识时，它的性能优于这些方法，这表明无监督技术可能比现有的有监督方法具有更好的泛化能力。</pre></li>
<li><a href="https://arxiv.org/abs/1909.02151">KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning</a> (EMNLP2019)
<pre>常识推理的目的是赋予机器以人类对日常生活中的普通情况做出假设的能力。在本文中，我们提出了一个用于回答常识问题的文本推理框架，该框架有效地利用外部的结构化常识知识图来执行可解释的推理。该框架首先将问题-答案对从语义空间导出到基于知识的符号空间，作为一个模式图，一个外部知识图的相关子图。它使用一个新的知识感知图网络模块KagNet来表示模式图，并最终使用图表示对答案进行评分。我们的模型基于图卷积网络和LSTM，具有基于层次路径的注意机制。中等注意分数使其透明和可解释，从而产生可信的推论。使用ConceptNet作为基于Bert模型的唯一外部资源，我们在CommonsenseQA（用于常识推理的大规模数据集）上实现了最先进的性能。</pre></li>
<li><a href="https://www.aclweb.org/anthology/D19-6001/">Cracking the Contextual Commonsense Code: Understanding Commonsense Reasoning Aptitude of Deep Contextual Representations</a> (EMNLP2019 WS)</li>
<li><a href="https://arxiv.org/abs/1909.10705">Do Massively Pretrained Language Models Make Better Storytellers?</a> (CoNLL2019)
<pre>在大量文本上训练的大型神经语言模型已经成为自然语言理解任务的一种强大策略。然而，这些模型作为自然语言生成器的优势还不太清楚。尽管轶事证据表明，这些模型生成的文本质量更好，但尚未对其生成能力进行详细研究。在这项工作中，我们比较了广泛预训练模型OpenAI GPT2-117（Radford et al.，2019）和最先进的神经故事生成模型（Fan et al.，2018）的性能。通过在各种自动度量中评估生成的文本，我们描述了预训练模型是否能够成为更好的故事讲述者的方式。我们发现，尽管GPT2-117的语境条件更强，对事件的顺序更敏感，并且使用了更多不寻常的词，但在使用最大似然解码算法时，它同样可能产生重复的和不完全不同的文本。</pre></li>
<li><a href="https://arxiv.org/abs/1911.11641">PIQA: Reasoning about Physical Commonsense in Natural Language</a> (AAAI2020)
<pre>不用刷子涂眼影，我应该用棉签还是牙签？需要这种物理常识的问题对今天的自然语言理解系统提出了挑战。虽然最近的预训练模型（如BERT）在更抽象的领域（如文本丰富的新闻文章和百科全书条目）的问答方面取得了进展，但在更多的物理领域，文本由于报道偏见而受到了固有的限制。人工智能系统能在不经历物理世界的情况下可靠地回答物理常识问题吗？在本文中，我们介绍了物理常识推理的任务和相应的基准数据集物理交互：问答或PIQA。尽管人类发现数据集很容易（准确率为95%），但大型预训练模型却很难（准确率为77%）。我们提供现有模型所缺乏的知识维度的分析，这为未来的研究提供了重要的机会。</pre></li>
<li><a href="https://arxiv.org/abs/1911.11931">Evaluating Commonsense in Pre-trained Language Models</a> (AAAI2020)
<pre>在大量原始文本数据上训练的语境化表征对自然语言处理任务（包括问答和阅读理解）有显著的改善。有研究表明，句法、语义和词义知识包含在这些表示中，这解释了为什么它们有利于这些任务。然而，对语境化表征中包含的常识知识的研究相对较少，语境化表征对于人类的问题回答和阅读理解至关重要。我们通过在七个具有挑战性的基准上测试GPT、BERT、XLNet和RoBERTa的常识能力，发现语言建模及其变体是提升模型常识能力的有效目标，而双向上下文和更大的训练集是额外的。此外，我们还发现，目前的模型在需要更多必要推理步骤的任务上表现不佳。最后，我们通过制作双重测试用例来测试模型的稳健性，这些测试用例相互关联，因此一个样本的正确预测应该导致另一个样本的正确预测。有趣的是，这些模型显示了对这些测试用例的混淆，这表明他们从表面而不是深层次学习常识。我们公开发布了一个名为CATs的测试集，用于未来的研究。</pre></li>
<li><a href="https://arxiv.org/abs/1911.03024">Why Do Masked Neural Language Models Still Need Common Sense Knowledge?</a>
<pre>目前，语境化的单词表示是通过复杂的神经网络模型来学习的，如蒙面神经语言模型（MNLMs）。新的表示法通过阅读段落显著提高了自动问答的性能。然而，由于参数众多且混杂，识别在MNLMs中训练的详细知识是困难的。本文从常识知识的角度对预先培训的MNLMs进行了实证分析。首先，我们提出了一个测试来衡量接受过训练的MNLMs理解哪些类型的常识知识。通过测试，我们观察到MNLMs部分理解各种类型的常识知识，但不能准确理解关系的语义。此外，基于问答任务问题的难度，我们观察到基于传销的预训练模型仍然容易受到需要常识知识的问题的影响。我们还通过实验证明，我们可以通过组合来自外部常识库的知识来提升现有的基于MNLM的模型。</pre></li>
<li><a href="https://arxiv.org/abs/2008.03945">Does BERT Solve Commonsense Task via Commonsense Knowledge?</a>
<pre>BERT已被用于解决常识性任务，如CommonsenseQA。虽然先前的研究发现，在某种程度上，BERT确实包含常识信息，但已有研究表明，在解决情绪分类和其他问题时，预先训练的模型可以依赖虚假关联（例如，数据偏差），而不是关键线索。我们定量地研究了在解决常识任务时，结构常识线索在BERT中的存在，以及这些线索对模型预测的重要性。使用两种不同的测量方法，我们发现伯特确实使用相关知识来解决任务，并且常识知识的存在与模型的准确性正相关。</pre></li>
<li><a href="https://arxiv.org/abs/2004.05483">Unsupervised Commonsense Question Answering with Self-Talk</a> (EMNLP2020)
<pre>自然语言理解包括字里行间的阅读和隐含的背景知识。当前的系统要么依赖预先训练的语言模型作为世界知识的唯一隐式来源，要么借助外部知识库（KBs）来整合其他相关知识。我们提出了一个基于自我对话的无监督框架，作为多项选择常识任务的新替代方案。受基于探究的发现学习（Bruner，1961）的启发，我们的方法通过一些信息查询问题（如“$\textit{what is definition of…}$”）来查询语言模型，以发现更多的背景知识。实证结果表明，自对话程序在六分之四的常识基准上显著提高了零镜头语言模型基线的性能，并与从外部知识库获取知识的模型相竞争。虽然我们的方法在几个基准上提高了性能，但人类法官并不总是认为自言自语诱导的知识有用，这就提出了关于常识推理预先训练的语言模型的内部工作原理的有趣问题。</pre></li>
<li><a href="https://arxiv.org/abs/2011.03863">Knowledge-driven Data Construction for Zero-shot Evaluation in Commonsense Question Answering</a> (AAAI2021)
<pre>最近在预先训练的神经语言建模方面的发展已经导致了常识性问答基准的准确性飞跃。然而，人们越来越担心模型过于适合特定任务，而没有学习利用外部知识或执行一般语义推理。相比之下，零炮评估显示了作为模型一般推理能力更稳健的衡量标准的前景。在本文中，我们提出了一个新的神经符号框架，用于跨常识任务的零炮问答。在一系列假设的指导下，该框架研究如何将各种预先存在的知识资源转化为对预培训模型最有效的形式。我们改变语言模型、培训制度、知识来源和数据生成策略，并衡量它们在任务中的影响。在前人工作的基础上，我们设计并比较了四种限制性干扰取样策略。我们提供了五个常识性问答任务的实证结果，数据来自五个外部知识资源。我们发现，虽然单个知识图更适合于特定任务，但全局知识图在不同任务中带来一致的收益。此外，保留任务结构以及生成公平且信息丰富的问题都有助于语言模型更有效地学习。</pre></li>
<li><a href="https://arxiv.org/abs/2004.11546">G-DAUG: Generative Data Augmentation for Commonsense Reasoning</a>
<pre>常识推理的最新进展依赖于大规模人工标注的训练数据来实现最高性能。然而，人工整理训练示例的成本很高，并且已经证明会引入注释伪影，神经模型可以很容易地利用和过度拟合这些伪影。我们研究了G-DAUG^C，这是一种新的生成性数据增强方法，旨在在低资源环境下实现更精确、更稳健的学习。我们的方法使用预先训练的语言模型生成合成示例，并选择信息量最大、种类最多的示例集进行数据扩充。在多个常识推理基准测试的实验中，G-DAUG^C始终优于现有的基于反译的数据增强方法，并在WinoGrande、CODAH和CommonsenseQA上建立了一个新的最新技术。此外，除了提高分布精度外，G-DAUG^C增强训练还增强了分布外泛化，显示出更强的对抗性或扰动示例的鲁棒性。我们的分析表明，G-DAUG^C产生了一组不同的流利培训示例，其选择和培训方法对绩效非常重要。我们的发现鼓励了未来对生成性数据增强的研究，以增强分布内学习和分布外泛化。</pre></li>
<li><a href="https://arxiv.org/abs/2005.00669">Contrastive Self-Supervised Learning for Commonsense Reasoning</a> (ACL2020)
<pre>我们提出了一种自监督方法来解决代词消歧和Winograd模式挑战问题。我们的方法利用了与所谓的“触发”词相关的训练语料库的特征结构，这些词负责在代词消歧中翻转答案。我们通过构建成对对比辅助预测来实现这种常识推理。为此，我们利用一种通过对比保证金调整的互斥损失。我们的架构基于最近引入的变压器网络BERT，它在许多NLP基准测试中表现出强大的性能。实验结果表明，我们的方法减轻了现有常识推理监督方法的局限性。这项研究为利用廉价的自我监督在常识推理任务中获得绩效增益开辟了途径。</pre></li>
<li><a href="https://arxiv.org/abs/2010.14439">Differentiable Open-Ended Commonsense Reasoning</a>
<pre>当前的常识推理研究侧重于开发使用常识知识回答多项选择题的模型。然而，设计用于回答多项选择题的系统在没有提供少量候选答案供选择的应用程序中可能没有用处。作为使常识推理研究更加现实的一步，我们建议研究开放式常识推理（OpenCSR）——在没有任何预定义选择的情况下回答常识问题的任务——仅使用自然语言编写的常识事实语料库作为资源。OpenCSR具有挑战性，因为它有很大的决策空间，而且许多问题需要隐式多跳推理。作为OpenCSR的一种方法，我们提出了DrFact，这是一种有效的可微模型，用于对知识事实进行多跳推理。为了评估OpenCSR方法，我们采用了几种流行的常识推理基准，并通过众包为每个测试问题收集多个新答案。实验表明，DrFact的性能大大优于强基线方法。</pre></li>
<li><a href="https://arxiv.org/abs/2005.08156">Adversarial Training for Commonsense Inference</a> (ACL2020 WS)
<pre>我们提出了一种对抗性的常识推理训练算法（ALICE）。我们将小扰动应用于单词嵌入，并最小化由此产生的对抗风险以规范化模型。我们利用两种不同方法的新组合来估计这些扰动：1）使用真标签，2）使用模型预测。我们的模型不依赖任何人工构建的功能、知识库或目标数据集以外的其他数据集，提高了RoBERTa的微调性能，在需要常识推理的多个阅读理解数据集上取得了竞争性的结果。</pre></li>
<li><a href="https://arxiv.org/abs/2011.09159">Do Fine-tuned Commonsense Language Models Really Generalize?</a>
<pre>最近，基于变换器的方法（如RoBERTa和GPT-3）在自然语言处理任务（如问答和常识推理）方面取得了重大的实验进展。后者通常通过多个基准进行评估，作为前者的多项选择实例。根据艾伦研究所（Allen Institute）主持的有影响力的排行榜（在常识推理基准上评估最先进的性能），基于这种转换方法的模型正在接近人类的性能，在许多基准上的平均准确率远远超过80%。由于这些都是常识性基准测试，因此在常识性推理基础上进行概括的模型在多个常识性基准测试中不会出现太多性能损失。在本文中，我们通过设计和进行严格的科学研究，详细研究了泛化问题。通过使用五种常见的基准测试、多重控制和统计分析，我们发现了明确的证据，即即使实验设置有适度的变化，经过微调的常识语言模型仍然不能很好地概括，事实上，可能容易受到数据集偏差的影响。我们还进行选择性研究，包括定性和一致性分析，以深入了解问题。</pre></li>
<li><a href="https://arxiv.org/abs/2106.11533">Do Language Models Perform Generalizable Commonsense Inference?</a> (ACL2021 Findings)
<pre>受预训练语言模型（LMs）编码常识知识的证据启发，最近的工作已经应用LMs自动填充常识知识图（CKG）。然而，对于它们对多个CKG、看不见的关系和新实体的泛化缺乏理解。本文从知识能力、可转移性和归纳性三个方面分析了LMs进行广义常识推理的能力。我们在这三个方面的实验表明：（1）LMs能够适应由多个CKG定义的不同模式，但不能重用知识来概括新的关系。（2） 适应的LMs很好地概括了看不见的主题，但对新奇的对象则不太适用。未来的工作应该研究如何提高LMs中常识挖掘的可转移性和归纳性。</pre></li>
<li><a href="https://arxiv.org/abs/2012.06236">Improving Zero Shot Learning Baselines with Commonsense Knowledge</a>
<pre>零射击学习——在一组完全不相交的类上进行训练和测试的问题——在很大程度上依赖于它将知识从训练类转移到测试类的能力。传统上，由人类定义的属性（HA）或分布式单词嵌入（DWE）组成的语义嵌入通过改善视觉和语义嵌入之间的关联来促进这种转换。在本文中，我们利用常识知识图ConceptNet中定义的节点之间的显式关系，使用基于图卷积网络的自动编码器生成类标签的常识嵌入。我们在三个标准基准数据集上进行的实验超过了强基线，我们将常识性嵌入与现有语义嵌入（即HA和DWE）融合在一起。</pre></li>
<li><a href="https://ducdauge.github.io/files/xcopa.pdf">XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning</a> [<a href="https://github.com/cambridgeltl/xcopa">github</a>]</li>
<li><a href="https://arxiv.org/abs/1908.02899">Do Neural Language Representations Learn Physical Commonsense?</a> (CogSci2019)
<pre>人类理解语言的基础是关于物质世界如何运作的丰富背景知识，这反过来又使我们能够通过语言对物质世界进行推理。除了物体的属性（例如，船只需要燃料）及其可承受性，即适用于它们的动作（例如，船只可以被驱动），我们还可以推断物体属性之间的推论是否意味着适用于它们的动作类型（例如，如果我们能驾驶某个东西，那么它可能需要燃料）在本文中，我们研究了在大量自然语言文本上训练的最先进的神经语言表征在多大程度上展示了物理常识推理。虽然神经语言模型的最新进展在各种自然语言推理任务中表现出了强大的性能，但我们的研究根据一个超过20万条新收集的注释数据集，神经语言表征仍然只学习明确记录下来的关联。</pre></li>
</ul>
<h2 id="extractive-summarization">Extractive summarization</h2>
<ul>
<li><a href="https://arxiv.org/abs/1905.06566">HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization</a> (ACL2019)
<pre>神经提取摘要模型通常采用分层编码器进行文档编码，并使用基于规则的启发式方法创建的句子级标签进行训练。使用这些\emph{不准确}标签训练分层编码器是一项挑战。受最近关于变压器句子编码器预训练工作的启发，我们提出了{\sc-Hibert}（作为{\bf-HI}erachical{\bf-B}方向{\bf-E}ncoder{\bf-R}表示来自{\bf-T}变压器的缩写）用于文档编码，并提出了一种使用未标记数据对其进行预训练的方法。我们将预先训练好的{\sc Hibert}应用到我们的摘要模型中，它在CNN/Dailymail数据集上的性能比随机初始化的模型好1.25个ROUGE，在纽约时报数据集上的性能比随机初始化的模型好2.0个ROUGE。我们还在这两个数据集上实现了最先进的性能。</pre></li>
<li><a href="https://arxiv.org/abs/1909.03223">Deleter: Leveraging BERT to Perform Unsupervised Successive Text Compression</a>
<pre>文本压缩在摘要、阅读理解和文本编辑等方面有着广泛的应用。然而，几乎所有现有的方法都需要手工制作的特性、语法标签或并行数据。即使是在无监督的环境下完成此任务，其体系结构也需要特定于任务的自动编码器。此外，这些模型只为每个源输入生成一个压缩句子，因此适应最终输出的不同风格要求（例如长度）通常意味着从头开始重新训练模型。在这项工作中，我们提出了一个完全无监督的模型Deleter，该模型能够发现任意句子的“最佳删除路径”，其中路径上的每个中间序列都是前一个序列的相干子序列。该方法完全依赖于预训练的双向语言模型（BERT），根据生成句子的平均复杂度对每个候选删除进行评分，并执行渐进式贪婪前瞻搜索，为每个步骤选择最佳删除。我们将Deleter应用于抽取句子压缩的任务中，发现我们的模型与在102万个具有相似压缩比的领域示例上训练的最新监督模型具有竞争力。定性分析以及自动和人工评估都验证了我们的模型能够产生高质量的压缩。</pre></li>
<li><a href="https://arxiv.org/abs/1910.14142">Discourse-Aware Neural Extractive Text Summarization</a> (ACL2020) [<a href="https://github.com/jiacheng-xu/DiscoBERT">github</a>]
<pre>最近，在最先进的文本摘要模型中，BERT被用于文档编码。然而，基于句子的抽取模型通常会导致抽取的摘要中出现冗余或无信息的短语。此外，整个文档中的长期依赖关系不能很好地由BERT捕获，它是在句子对而不是文档上进行预训练的。为了解决这些问题，我们提出了一个语篇感知的神经摘要模型——DiscoBert。DiscoBert提取次句子的话语单位（而不是句子）作为更细粒度提取选择的候选。为了捕获语篇单元之间的长期依赖关系，基于RST树和共指提及构建结构语篇图，并用图卷积网络编码。实验表明，与其他基于BERT的模型相比，该模型的性能要优于现有的方法，在流行的摘要基准上有显著的优势。</pre></li>
<li><a href="https://arxiv.org/abs/2004.06176">AREDSUM: Adaptive Redundancy-Aware Iterative Sentence Ranking for Extractive Document Summarization</a>
<pre>冗余感知提取摘要系统对要包含在摘要中的句子的冗余度进行评分，可以与它们的显著性信息一起评分，也可以作为额外的句子评分步骤单独评分。先前的研究表明，使用神经序列生成模型联合评分和选择句子是有效的。然而，如果增益是由于更好的编码技术或更好的冗余减少方法造成的，这一点尚不清楚。同样，在所创建的摘要中，显著性与多样性成分的贡献也没有得到很好的研究。基于最先进的摘要编码方法，我们提出了两种自适应学习模型：AREDSUM-SEQ，它在句子选择过程中综合考虑显著性和新颖性；两步AREDSUM-CTX首先对显著性进行评分，然后学会平衡显著性和冗余，从而能够测量每个方面的影响。CNN/DailyMail和NYT50数据集的实证结果表明，通过在单独的步骤中明确建模多样性，AREDSUM-CTX的性能明显优于AREDSUM-SEQ以及最先进的提取摘要基线。</pre></li>
<li><a href="https://arxiv.org/abs/2011.09739">Fact-level Extractive Summarization with Hierarchical Graph Mask on BERT</a> (COLING2020)
<pre>目前大多数提取摘要模型都是通过选择突出的句子来生成摘要的。然而，句子级抽取摘要的一个问题是，人工编写的黄金摘要与甲骨文句子标签之间存在差距。在本文中，我们建议提取事实级语义单元，以便更好地提取摘要。我们还引入了一种层次结构，它将文本信息的多级粒度合并到模型中。此外，我们使用层次图掩码将我们的模型与BERT结合起来。这使我们能够在不增加模型规模的情况下，将伯特在自然语言理解和结构信息方面的能力结合起来。在CNN/DaliyMail数据集上的实验表明，我们的模型达到了最先进的结果。</pre></li>
<li><a href="https://arxiv.org/abs/2012.02144">Do We Really Need That Many Parameters In Transformer For Extractive Summarization? Discourse Can Help !</a> (EMNLP2020 WS)
<pre>在自然语言处理（NLP）中，流行变压器模型的多头自注意被广泛使用，包括用于抽取摘要任务。为了分析和修剪参数重的自我注意机制，有多种方法提出了更多参数轻的自我注意替代方案。在本文中，我们提出了一种新的参数精益自我注意机制使用话语先验。我们的新树自我关注基于文档级的话语信息，用另一个轻量级的替代方案扩展了最近提出的“合成器”框架。我们的实证结果表明，我们的树自我注意方法在抽取摘要任务上取得了有竞争力的胭脂分数。与原始的单头变压器模型相比，尽管注意成分的参数显著减少，但树注意方法在EDU和句子水平上都达到了相似的性能。当应用更平衡的超参数设置时，我们在句子层面上的表现明显优于8头变压器模型，需要的参数数量级更少。</pre></li>
<li><a href="https://arxiv.org/abs/1910.11411">Multi-Document Summarization with Determinantal Point Processes and Contextualized Representations</a> (EMNLP2019 WS)
<pre>行列式点过程作为提取摘要的最佳技术之一，根据句子突出和两两排斥建模定义的概率度量，选择最可能的句子集来形成摘要。传统上，这些方面是使用浅层和语言信息特征建模的，但深层语境化表达的兴起提出了一个有趣的问题，即语境化表达是否以及在多大程度上可以用于改进DPP建模。我们的研究结果表明，尽管深度表征取得了成功，但仍有必要将其与表面指标结合起来，以有效识别摘要句子。</pre></li>
<li><a href="https://arxiv.org/abs/2007.03405">Continual BERT: Continual Learning for Adaptive Extractive Summarization of COVID-19 Literature</a>
<pre>科学界继续每天发表大量与新冠病毒-19相关的新研究，导致大量文献很少或根本没有受到关注。为了帮助社区理解快速流动的新冠病毒-19文献，我们提出了一种新颖的BERT体系结构，该体系结构提供了对长篇论文的简短而新颖的总结。该模型以在线方式不断学习新数据，同时最大限度地减少灾难性遗忘，因此符合社区的需要。对其性能的基准测试和手动检查表明，该模型为新的科学文献提供了良好的总结。</pre></li>
</ul>
<h2 id="grammatical-error-correction">Grammatical error correction</h2>
<ul>
<li><a href="https://www.aclweb.org/anthology/papers/W/W19/W19-4426/">Multi-headed Architecture Based on BERT for Grammatical Errors Correction</a> (ACL2019 WS)</li>
<li><a href="https://arxiv.org/abs/2001.03521">Towards Minimal Supervision BERT-based Grammar Error Correction</a>
<pre>当前的语法错误校正（GEC）模型通常将任务视为序列生成，这需要大量注释的数据并限制应用在数据受限的设置中。我们试图结合来自预先训练的语言模型的上下文信息，以利用注释并使多语言场景受益。结果表明，双向编码器表示从变压器（伯特）在语法错误纠正任务的强大潜力。</pre></li>
<li><a href="https://arxiv.org/abs/1906.03897">Learning to combine Grammatical Error Corrections</a> (EMNLP2019 WS)
<pre>语法错误纠正（GEC）领域产生了各种系统来处理聚焦现象或一般文本编辑。我们提出了一种自动组合黑盒系统的方法。我们的方法自动检测每个错误类型的系统强度或多个系统的组合，提高了精确度和召回率，同时直接优化$F$分数。在所有测试的配置中，我们都显示出与最佳独立系统相比的持续改进。这种方法也优于随机初始化的不同RNN模型的平均加密。此外，我们还分析了BERT在GEC中的应用——报告这方面有希望的结果。我们还提供了为该任务创建的拼写检查器，其性能优于在拼写检查任务中测试的标准拼写检查器。本文描述了一个系统提交给构建教育应用程序的共享任务：语法错误更正。使用我们的方法结合BEA 2019顶级共享任务系统的输出，目前在BEA 2019共享任务开放阶段的报告分数最高，比报告的最佳结果提高了F0.5 3.7分。</pre></li>
<li><a href="https://arxiv.org/abs/2109.06822">LM-Critic: Language Models for Unsupervised Grammatical Error Correction</a> (EMNLP2021) [<a href="https://github.com/michiyasunaga/LM-Critic">github</a>]
<pre>训练语法错误纠正模型（GEC）需要一组标记的非语法/语法句子对，但手动注释这类句子对可能代价高昂。最近，Break-It-Fix-It（BIFI）框架在学习在没有任何标记示例的情况下修复损坏的程序方面显示出了强大的效果，但这依赖于一个完美的批评家（例如编译器），它返回一个示例是否有效，而GEC任务中不存在该示例。在这项工作中，我们展示了如何利用预训练语言模型（LM）来定义LM批评家，如果LM赋予句子比其局部扰动更高的概率，则该模型会判断句子是否符合语法。我们应用LM批评家和BIFI以及大量未标记的句子来引导现实的非语法/语法对，以训练纠正者。我们在多个领域（CoNLL-2014、BEA-2019、GMEG wiki和GMEG yahoo）的GEC数据集上评估了我们的方法，并表明它在无监督设置（+7.7 F0.5）和监督设置（+0.5 F0.5）方面都优于现有方法。</pre></li>
<li><a href="https://arxiv.org/abs/2005.00987">Encoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error Correction</a> (ACL2020)
<pre>本文研究如何有效地将预先训练好的蒙面语言模型（MLM），如BERT，合并到用于语法错误纠正（GEC）的编码器-解码器（EncDec）模型中。这个问题的答案并不像人们想象的那么简单，因为以前将传销合并到EncDec模型中的常用方法在应用于GEC时有潜在的缺点。例如，GEC模型的输入分布可能与用于训练前传销的语料库的分布有很大不同（错误、笨拙等）；然而，在以前的方法中没有解决这个问题。我们的实验表明，我们提出的方法，即首先使用给定的GEC语料库对MLM进行微调，然后使用微调后的MLM输出作为GEC模型中的附加特征，最大限度地提高了MLM的效益。性能最佳的车型在BEA-2019和CoNLL-2014基准上实现了最先进的性能。我们的代码可在以下网站公开获取：https://github.com/kanekomasahiro/bert-gec.</pre></li>
<li><a href="https://arxiv.org/abs/2011.02093">Chinese Grammatical Correction Using BERT-based Pre-trained Model</a> (AACL-IJCNLP2020)
<pre>近年来，预训练模型得到了广泛的研究，一些下游任务也从中受益。在这项研究中，我们验证了两种方法的有效性，这两种方法将Cui等人（2020）开发的基于BERT的预训练模型整合到汉语语法纠错任务的编解码模型中。我们还分析了错误类型，并得出结论，句子层面的错误尚未得到解决。</pre></li>
<li><a href="https://arxiv.org/abs/2005.07421">Spelling Error Correction with Soft-Masked BERT</a> (ACL2020)
<pre>拼写错误纠正是一项重要而富有挑战性的任务，因为要想圆满解决拼写错误，就必须具备人类水平的语言理解能力。在不失一般性的前提下，我们考虑了汉语拼写错误校正（CSC）。该任务的最先进方法是根据语言表示模型BERT从候选字符列表中选择一个字符，以便在句子的每个位置进行纠正（包括非纠正）。然而，该方法的精度可能是次优的，因为BERT没有足够的能力检测每个位置是否存在错误，这显然是由于使用掩码语言建模对其进行预训练的方式。在这项工作中，我们提出了一种新的神经结构来解决上述问题，它包括一个用于错误检测的网络和一个基于BERT的错误校正网络，前者通过我们称之为软掩蔽技术连接到后者。我们使用“软蒙蔽BERT”的方法是通用的，它可以应用于其他语言检测校正问题。在两个数据集上的实验结果表明，我们提出的方法的性能明显优于基线，包括仅基于BERT的方法。</pre></li>
</ul>
<h2 id="ir">IR</h2>
<ul>
<li><a href="https://arxiv.org/abs/2104.08663">BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models</a> [<a href="https://github.com/UKPLab/beir">github</a>]
<pre>现有的神经信息检索（IR）模型通常是在同质和狭窄的环境中进行研究的，这使得对其非分布（OOD）泛化能力的了解非常有限。为了解决这一问题，并便于研究人员广泛评估其模型的有效性，我们引入了Benchmarking IR（BEIR），这是一种用于信息检索的健壮且异构的评估基准。我们从不同的文本检索任务和领域中仔细挑选了18个公开可用的数据集，并在BEIR基准上评估了10个最先进的检索系统，包括词汇、稀疏、密集、后期交互和重新排序体系结构。我们的结果表明，BM25是一个稳健的基线，基于重新排序和后期交互的模型平均实现了最佳的零炮性能，但计算成本较高。相比之下，密集和稀疏检索模型的计算效率更高，但其性能往往低于其他方法，这突出了它们在泛化能力方面的巨大改进空间。我们希望这个框架能让我们更好地评估和理解现有的检索系统，并有助于在未来加速向更好的健壮性和通用性系统的发展。BEIR可在以下网址公开获取：https://github.com/UKPLab/beir.</pre></li>
<li><a href="https://arxiv.org/abs/2010.06467">Pretrained Transformers for Text Ranking: BERT and Beyond</a>
<pre>文本排序的目标是生成从语料库中检索的文本的有序列表，以响应查询。尽管文本排序最常见的公式是搜索，但在许多自然语言处理应用程序中也可以找到该任务的实例。这项调查提供了一个文本排名与神经网络结构称为变压器概述，其中伯特是最著名的例子。变形金刚和自我监督预训练的结合导致了自然语言处理（NLP）、信息检索（IR）等领域的范式转变。在这项调查中，我们提供了现有工作的综合，作为希望更好地了解如何将变形金刚应用于文本排序问题的从业者和希望从事这一领域工作的研究人员的单一切入点。我们涵盖了广泛的现代技术，分为两个高级类别：在多阶段体系结构中执行重新排序的变压器模型和直接执行排序的密集检索技术。在我们的调查中有两个主题：处理长文档的技术（NLP中的典型逐句处理除外），以及处理有效性（即结果质量）和效率（例如查询延迟、模型和索引大小）之间权衡的技术。虽然transformer体系结构和预训练技术是最近的创新，但它们如何应用于文本排序的许多方面都得到了比较好的理解，并代表了成熟的技术。然而，仍然存在许多开放性的研究问题，因此，除了为文本排名奠定预训练变形金刚的基础外，本次调查还试图预测该领域的发展方向。</pre></li>
<li><a href="https://arxiv.org/abs/1901.04085">Passage Re-ranking with BERT</a>
<pre>最近，对语言建模任务进行预训练的神经模型，如ELMo（Peters et al.，2017）、OpenAI GPT（Radford et al.，2018）和BERT（Devlin et al.，2018），在各种自然语言处理任务（如问答和自然语言推理）上取得了令人印象深刻的结果。在本文中，我们描述了一个简单的基于查询的段落重新排序的BERT重新实现。我们的系统在TREC-CAR数据集上是最先进的，并且在MS MARCO通道检索任务排行榜上排名第一，在以下方面比以前的先进水平高出27%（相对）MRR@10. 复制我们结果的代码可在https://github.com/nyu-dl/dl4marco-bert</pre></li>
<li><a href="https://arxiv.org/abs/1905.01758">Investigating the Successes and Failures of BERT for Passage Re-Ranking</a>
<pre>变压器双向编码器表示（BERT）模型最近提高了通道重新排序的技术水平。在本文中，我们分析了一个微调的伯特模型产生的结果，以更好地理解这些实质性改进背后的原因。为此，我们将重点放在MARCO女士文章的重新排序数据集上，并提供BERT检索成功与失败的潜在原因。更详细地说，我们实证研究了一组假设，并提供了额外的分析来解释BERT的成功表现。</pre></li>
<li><a href="https://arxiv.org/abs/1904.07531">Understanding the Behaviors of BERT in Ranking</a>
<pre>本文研究了BERT在排序任务中的性能和行为。我们探索了几种不同的方法来利用预先训练好的BERT，并在两个排名任务上对其进行微调：MARCO女士文章重新排名和TREC Web Track即席文档排名。在MS MARCO上的实验结果证明了BERT在以问答为中心的段落排序任务中的强大有效性，以及BERT是一个基于强交互的seq2seq匹配模型的事实。在TREC上的实验结果表明，预先训练好的基于周围环境的BERT与adhoc文档排序的需求之间存在差距。分析说明了BERT如何在其转换器层中的查询文档标记之间分配注意力，它如何偏好解释标记之间的语义匹配，以及这与通过点击训练的神经ranker学习的软匹配模式有何不同。</pre></li>
<li><a href="https://arxiv.org/abs/1904.08375">Document Expansion by Query Prediction</a>
<pre>提高搜索引擎检索效率的一种技术是使用与文档内容相关或代表文档内容的术语扩展文档。从问答系统的角度来看，这可能包括文档可能回答的问题。根据这一观察结果，我们提出了一种简单的方法，该方法预测给定文档将发出哪些查询，然后使用普通的序列到序列模型（使用由查询和相关文档对组成的数据集进行训练）用这些预测对其进行扩展。通过将我们的方法与一个高效的重新排序组件相结合，我们在两个检索任务中达到了最先进的水平。在延迟临界状态下，单独检索结果（无需重新排序）接近计算成本更高的神经检索的有效性，但速度要快得多。</pre></li>
<li><a href="https://arxiv.org/abs/2105.03599">Improving Document Representations by Generating Pseudo Query Embeddings for Dense Retrieval</a> (ACL2021)
<pre>近年来，基于稠密表示的检索模型逐渐应用于文档检索的第一阶段，表现出比传统稀疏向量空间模型更好的性能。为了获得高效率，在大多数情况下，这些模型的基本结构是双编码器。然而，由于查询是不可知的，这种简单的结构可能会在文档编码过程中导致严重的信息丢失。为了解决这个问题，我们设计了一种方法，通过迭代聚类过程模拟每个文档上的查询，并通过多个伪查询（即聚类质心）表示文档。为了提高使用近似最近邻搜索库的检索过程，我们还通过两步分数计算过程优化了匹配函数。在几个流行的排名和QA数据集上的实验结果表明，我们的模型可以达到最先进的结果。</pre></li>
<li><a href="https://arxiv.org/abs/1904.07094">CEDR: Contextualized Embeddings for Document Ranking</a> (SIGIR2019)
<pre>尽管最近对神经排序结构给予了相当大的关注，但对作为这些模型输入的术语表示的关注却少得多。在这项工作中，我们研究了如何利用两个预先训练的上下文化语言模型（ELMo和BERT）进行临时文档排序。通过在TREC基准测试上的实验，我们发现一些现有的神经排序结构可以受益于上下文化语言模型提供的附加上下文。此外，我们提出了一种联合方法，将BERT的分类向量纳入现有的神经模型中，并表明它优于最先进的ad-hoc排序基线。我们称这种联合方法为CEDR（文档排名的上下文嵌入）。我们还解决了使用这些模型进行排名的实际挑战，包括由BERT施加的最大输入长度以及上下文化语言模型对运行时性能的影响。</pre></li>
<li><a href="https://arxiv.org/abs/1905.09217">Deeper Text Understanding for IR with Contextual Neural Language Modeling</a> (SIGIR2019)
<pre>神经网络为自动学习复杂的语言模式和查询文档关系提供了新的可能性。神经IR模型在学习查询文档相关模式方面取得了很好的效果，但在理解查询或文档的文本内容方面却很少有人做过探索。本文研究利用最近提出的上下文神经语言模型BERT，为IR提供更深入的文本理解。实验结果表明，BERT的上下文文本表示比传统的单词嵌入更有效。与单词袋检索模型相比，上下文语言模型可以更好地利用语言结构，大大改进了用自然语言编写的查询。将文本理解能力与搜索知识相结合，可以得到一个增强的预训练BERT模型，该模型可以在训练数据有限的情况下对相关的搜索任务有所帮助。</pre></li>
<li><a href="https://arxiv.org/abs/1905.02851">FAQ Retrieval using Query-Question Similarity and BERT-Based Query-Answer Relevance</a> (SIGIR2019)
<pre>常见问题（FAQ）检索是一项重要任务，其目标是根据用户的查询从数据库中检索适当的问题-答案（QA）对。我们提出了一个FAQ检索系统，该系统考虑了用户查询与问题之间的相似性以及查询与答案之间的相关性。尽管常见问题解答检索的常见方法是构造用于培训的标记数据，但它需要注释成本。因此，我们使用传统的无监督信息检索系统来计算查询和问题之间的相似度。另一方面，可以通过在FAQ数据库中使用QA对来了解查询和答案之间的相关性。最近提出的伯特模型用于相关性计算。由于FAQ页面中QA对的数量不足以训练一个模型，我们通过利用与所讨论的FAQ集相似的FAQ集来解决这个问题。我们在两个数据集上评估了我们的方法。第一个是localgovFAQ，这是我们在日本行政自治区域中构建的数据集。第二个是StackExchange数据集，它是英语中的公共数据集。我们证明了我们提出的方法在这些数据集上优于基线方法。</pre></li>
<li><a href="https://openreview.net/forum?id=dGOeF3y_Weh">An Analysis of BERT FAQ Retrieval Models for COVID-19 Infobot</a></li>
<li><a href="https://arxiv.org/abs/2010.12800">COUGH: A Challenge Dataset and Models for COVID-19 FAQ Retrieval</a>
<pre>我们提供了一个大的、具有挑战性的数据集，COGURG，用于新冠病毒-19常见问题的检索。与标准FAQ数据集类似，COUGH由三部分组成：FAQ库、查询库和关联集。常见问题库包含了从55个可信网站（如CDC和WHO）收集的约16K个常见问题条目。对于评估，我们引入了查询库和关联集，其中前者包含1236个人工解释的查询，而后者包含每个查询的32个人工注释的FAQ项。我们通过测试基于BM25和BERT构建的不同FAQ检索模型来分析咳嗽，其中最佳模型在以下条件下达到48.8P@5，这表明咳嗽带来了巨大的挑战，并鼓励未来的研究进一步改进。我们的咳嗽数据集可在https://github.com/sunlab-osu/covid-faq.</pre></li>
<li><a href="https://www.aclweb.org/anthology/2020.acl-main.74/">Unsupervised FAQ Retrieval with Question Generation and BERT</a> (ACL2020)</li>
<li><a href="https://arxiv.org/abs/1910.14424">Multi-Stage Document Ranking with BERT</a>
<pre>通过语言建模任务预训练的深层神经网络的出现，刺激了自然语言处理中的许多成功应用。这项工作探讨了一个流行的模型，伯特，在文件排名的背景下。我们提出了两种变体，称为monoBERT和duoBERT，分别将排序问题表述为点分类和成对分类。这两个模型被安排在一个多阶段的排名架构中，以形成一个端到端的搜索系统。这种设计的一个主要优点是能够通过控制每个管道阶段的候选准入来权衡质量和延迟，通过这样做，我们能够找到在这两个竞争指标之间提供良好平衡的操作点。在两个大型数据集，即MARCO女士和TREC CAR上，实验表明，我们的模型产生的结果与最新技术相当。消融研究显示了每个成分的贡献，并描述了延迟/质量权衡空间。</pre></li>
<li><a href="https://arxiv.org/abs/2004.08476">Learning-to-Rank with BERT in TF-Ranking</a>
<pre>本文描述了一种用于文档（re）排序的机器学习算法，该算法首先使用BERT[1]对查询和文档进行编码，并在此基础上使用TF-ranking（TFR）[2]构造的学习排序（LTR）模型进一步优化排序性能。这种方法在公共MS MARCO基准测试中被证明是有效的[3]。截至2020年4月10日，我们提交的前两份报告在文章重新排名任务[4]中取得了最好的成绩，在文章完整排名任务[5]中取得了第二好的成绩。为了利用预训练语言模型的最新发展，我们最近集成了RoBERTa[6]和ELECTRA[7]。我们最新提交的意见将我们之前最先进的重新排名绩效提高了4.3%[8]，并在2020年6月8日的全面排名任务[9]中取得了第三好的绩效。它们都证明了将排序损失与用于文档排序的BERT表示相结合的有效性。</pre></li>
<li><a href="https://arxiv.org/abs/2005.04588">Transformer-Based Language Models for Similar Text Retrieval and Ranking</a>
<pre>大多数使用长自然语言查询进行相似文本检索和排序的方法在某种程度上依赖于具有共同单词的查询和响应。最近，基于变换器的神经语言模型在文本检索和排序问题上的应用非常有前景，但仍然涉及一个两步过程，在这个过程中，首先通过基于词包的方法获得候选结果，然后通过神经变换器重新排序。在本文中，我们介绍了一种新的方法来有效地将神经变压器模型应用于相似文本的检索和排序，而无需基于初始词包的步骤。通过消除基于单词包的步骤，我们的方法能够准确地检索和排序结果，即使它们与查询没有共同的非停止词。我们通过使用来自转换器的双向编码器表示（BERT）来创建句子长度文本的矢量化表示，以及矢量最近邻搜索索引来实现这一点。我们演示了使用BERT来完成此任务的有监督和无监督方法。</pre></li>
<li><a href="https://arxiv.org/abs/2008.02460">DeText: A Deep Text Ranking Framework with BERT</a>
<pre>排名是搜索系统中最重要的组成部分。大多数搜索系统处理大量的自然语言数据，因此一个有效的排名系统需要对文本语义有深刻的理解。最近，基于深度学习的自然语言处理（deep-NLP）模型在排名系统上产生了有希望的结果。BERT是学习上下文嵌入最成功的模型之一，它已被应用于捕获复杂的查询文档关系以进行搜索排名。然而，这通常是通过彻底地将每个查询词与每个文档词交互来完成的，这对于搜索产品系统中的在线服务来说是低效的。在本文中，我们研究了如何为行业用例构建一个有效的基于BERT的排名模型，并将该解决方案进一步扩展到一个通用的排名框架DeText，该框架是开源的，可应用于各种排名产品。在三个真实世界的搜索系统上进行的离线和在线DeText实验表明，夸大艺术的方法有了显著的改进。</pre></li>
<li><a href="https://arxiv.org/abs/2004.12832">ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT</a> (SIGIR2020)
<pre>自然语言理解（NLU）的最新进展推动了信息检索（IR）的快速发展，这在很大程度上归功于用于文档排名的微调深度语言模型（LMs）。虽然非常有效，但基于这些LMs的排名模型比以前的方法增加了几个数量级的计算成本，特别是因为它们必须通过大规模神经网络为每个查询文档对提供数据，以计算单个相关性得分。为了解决这个问题，我们提出了ColBERT，一种新的排序模型，它采用了深度LMs（特别是BERT）来实现高效检索。ColBERT引入了一种后期交互体系结构，该体系结构使用BERT对查询和文档进行独立编码，然后采用一种廉价但功能强大的交互步骤对它们的细粒度相似性进行建模。通过延迟并保留这种细粒度的交互，ColBERT可以利用深度LMs的表达能力，同时获得离线预计算文档表示的能力，从而大大加快查询处理。除了降低对传统模型检索的文档重新排序的成本外，ColBERT的修剪友好交互机制还可以利用向量相似性索引直接从大型文档集合进行端到端检索。我们使用两个最近的文章搜索数据集对科尔伯特进行了广泛的评估。结果表明，ColBERT的有效性与现有的基于BERT的模型具有竞争性（并且优于每个非BERT基线），同时执行速度快两个数量级，每个查询所需的失败次数少四个数量级。</pre></li>
<li><a href="https://arxiv.org/abs/2006.15498">RepBERT: Contextualized Text Embeddings for First-Stage Retrieval</a> [<a href="https://github.com/jingtaozhan/RepBERT-Index">github</a>]
<pre>虽然查询和文档之间的精确术语匹配是执行第一阶段检索的主要方法，但我们提出了一种称为RepBERT的不同方法，用固定长度的上下文嵌入来表示文档和查询。查询和文档嵌入的内积被视为相关性得分。在MARCO女士文章排名任务中，RepBERT在所有初始检索技术中取得了最先进的结果。其效率可与文字袋法相媲美。</pre></li>
<li><a href="https://arxiv.org/abs/2007.00808">Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval</a>
<pre>与稀疏检索相比，在密集的学习表示空间中进行文本检索具有许多有趣的优点。然而，密集检索（DR）的有效性通常需要与稀疏检索相结合。在本文中，我们发现主要的瓶颈在于训练机制，其中训练中使用的否定实例不能代表测试中的无关文档。本文提出了近似最近邻否定对比估计（ANCE），这是一种训练机制，它从语料库的近似最近邻（ANN）索引中构造否定，并随学习过程并行更新，以选择更真实的否定训练实例。这从根本上解决了DR训练和测试中使用的数据分布之间的差异。在我们的实验中，ANCE增强了BERT-Siamese DR模型，使其优于所有竞争密集和稀疏检索基线。它几乎与稀疏检索和使用点积在表示空间中重新排序的精度相匹配，并提供了近100倍的加速。</pre></li>
<li><a href="https://arxiv.org/abs/2009.01938">Multi-Perspective Semantic Information Retrieval</a>
<pre>信息检索（IR）是从大型信息库中获取与特定查询或需求相关的数据片段（如文档或文本片段）的任务。虽然在最近的工作中，传统的基于关键词的方法和现代的基于BERT的方法的组合被证明是有效的，但在识别与特定查询“相关”的信息时，往往存在细微差别，使用这些系统很难正确捕获这些信息。这项工作引入了多视角信息检索系统的概念，这是一种将多种深度学习和传统信息检索模型相结合的新方法，可以更好地预测查询语句对的相关性，并提供了调整该系统的标准化框架。这项工作是根据BioASQ生物医学IR+QA挑战进行评估的。</pre></li>
<li><a href="https://arxiv.org/abs/2004.14245">Expansion via Prediction of Importance with Contextualization</a> (SIGIR2020)
<pre>在文本语境较少的情况下识别相关性是篇章检索的主要挑战。我们用一种基于表示的排序方法来解决这个问题，该方法：（1）使用上下文化语言模型明确地建模每个术语的重要性；（2） 通过将重要性传播到相似的术语来执行通道扩展；（3）使词典中的表述合理化，使其具有可解释性。可以在索引时预先计算通道表示，以减少查询时间延迟。我们称我们的方法为EPIC（通过上下文化的重要性预测进行扩展）。我们表明，EPIC显著优于先前的重要性建模和文档扩展方法。我们还观察到，性能与当前领先的第一阶段检索方法是相加的，进一步缩小了廉价和成本高昂的通道排序方法之间的差距。具体来说，EPIC实现了MRR@10在MS-MARCO通道排名数据集上为0.304，在商品硬件上的平均查询延迟为78ms。我们还发现，通过修剪文档表示，延迟被进一步减少到68ms，而有效性几乎没有差异。</pre></li>
<li><a href="https://arxiv.org/abs/2009.07258">BERT-QE: Contextualized Query Expansion for Document Re-ranking</a> (EMNLP2020 Findings)
<pre>查询扩展旨在减少查询中使用的语言与文档中使用的语言之间的不匹配。但是，查询扩展方法在扩展查询时可能会引入不相关的信息。为了弥合这一差距，受将诸如BERT之类的上下文化模型应用于文档检索任务的最新进展的启发，本文提出了一种新的查询扩展模型，该模型利用BERT模型的优势选择相关文档块进行扩展。在对标准TREC Robust04和GOV2测试集的评估中，提出的BERT-QE模型显著优于BERT大型模型。</pre></li>
<li><a href="https://arxiv.org/abs/2010.03073">Beyond [CLS] through Ranking by Generation</a> (EMNLP2020)
<pre>信息检索的生成模型将文档的排序视为从文档的语言模型生成查询的任务，在过去的各种IR任务中都非常成功。然而，随着现代深层神经网络的出现，人们的注意力已经转移到区分性排序函数上，该函数对文档和查询的语义相似性建模。最近，GPT2和BART等深层生成模型被证明是优秀的文本生成器，但它们作为RANKER的有效性尚未得到证明。在这项工作中，我们重新审视了信息检索的生成框架，并表明我们的生成方法与基于语义相似性的最新判别模型在答案选择任务中同样有效。此外，我们还证明了IR不太可能损失的有效性。</pre></li>
<li><a href="https://arxiv.org/abs/2004.14255">Efficient Document Re-Ranking for Transformers by Precomputing Term Representations</a> (SIGIR2020)
<pre>深度预训练变压器网络在各种排序任务中都是有效的，例如问答和即席文档排序。然而，他们的计算费用认为他们的成本在实践中令人望而却步。我们提出的方法称为PreTTR（预计算转换器术语表示法），它大大减少了深度转换器网络的查询时间延迟（web文档排名的加速比高达42倍），使这些网络在实时排名场景中更实用。具体来说，我们在索引时预计算部分文档术语表示（无需查询），并在查询时将它们与查询表示合并以计算最终排名分数。由于令牌表示的规模很大，我们还提出了一种有效的方法，通过训练压缩层以匹配注意分数来减少存储需求。我们的压缩技术将所需的存储空间减少了95%，并且可以在不大幅降低排名性能的情况下进行应用。</pre></li>
<li><a href="https://arxiv.org/abs/2004.14269">Training Curricula for Open Domain Answer Re-Ranking</a> (SIGIR2020)
<pre>在答案排序这样的精确任务中，对许多相关答案进行高度排序比检索所有相关答案更为重要。因此，一个好的排名策略是先学习如何识别最简单的正确答案（即，将高排名分数分配给具有通常表明相关性的特征的答案，将低排名分数分配给不具有相关性的特征的答案），然后再结合更复杂的逻辑来处理困难的案例（例如，语义匹配或推理）.在这项工作中，我们将这一思想应用到使用课程学习的神经回答等级的训练中。我们提出了几种启发式方法来估计给定训练样本的难度。我们表明，所提出的启发式方法可用于构建训练课程，在训练过程的早期降低难度样本的权重。作为随着时间的推移，我们的方法逐渐转向平均加权所有样本，而不考虑难度。我们在三个答案排序数据集上对我们提出的想法进行了综合评估。结果表明，我们的方法使两种领先的神经排序架构（即BERT和ConvKNRM）在使用p当应用于一个基于伯特的ranker时，我们的方法在MRR上提高了4%，在ranker上提高了9%P@1（与未经课程培训的模型相比）。这使得模型可以实现与更昂贵的最新技术相当的性能。</pre></li>
<li><a href="SIGIR2021">Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling</a></li>
<li><a href="https://arxiv.org/abs/2003.06713">Document Ranking with a Pretrained Sequence-to-Sequence Model</a>
<pre>这项工作提出了一种新的适应于文档排序任务的预训练序列到序列模型。我们的方法从根本上不同于通常采用的基于分类的排名公式，它基于仅编码器的预训练变压器架构，如BERT。我们展示了如何训练序列到序列模型以生成作为“目标词”的相关标签，以及如何将这些目标词的基本逻辑解释为用于排序的相关概率。在流行的MARCO通道排序任务中，实验结果表明，我们的方法至少与以前的基于分类的模型相一致，并且可以通过更大、更近的模型来超越它们。在TREC 2004 Robust Track的测试集合上，我们展示了一种基于零镜头传输的方法，其性能优于先前需要数据集交叉验证的最先进模型。此外，我们发现我们的方法在数据贫乏的情况下（即，很少有训练示例）显著优于仅编码器模型。我们通过改变目标词来探索模型对潜在知识的使用，从而进一步研究这一观察结果。</pre></li>
<li><a href="https://arxiv.org/abs/2104.07186">COIL: Revisit Exact Lexical Match in Information Retrieval with Contextualized Inverted List</a> (NAACL2021)
<pre>BM25等经典信息检索系统依赖于精确的词汇匹配，并通过倒排列表索引高效地进行搜索。最近的神经网络红外模型转向软语义匹配所有查询文档词，但它们失去了精确匹配系统的计算效率。本文介绍了COIL，一个上下文化的精确匹配检索体系结构，它带来了语义词汇匹配。线圈评分基于重叠查询文档标记的上下文表示。新架构将上下文化的标记表示存储在倒排列表中，将精确匹配的效率和深层语言模型的表示能力结合在一起。我们的实验结果表明，COIL在延迟相似或更小的情况下优于经典词汇检索器和最先进的深度LM检索器。</pre></li>
<li><a href="https://arxiv.org/abs/2006.07548">Guided Transformer: Leveraging Multiple External Sources for Representation Learning in Conversational Search</a> (SIGIR2020)
<pre>对于各种信息检索系统，特别是具有有限带宽接口的会话搜索系统，在回答模糊或分面查询时提出澄清问题已被认为是一种有用的技术。分析和生成澄清问题最近得到了研究，但用户回答澄清问题的准确利用相对较少。在本文中，我们使用一种新的注意机制来丰富变压器网络从外部信息源学习到的表示，该外部信息源对会话中的每个术语进行加权。我们在会话搜索场景中评估这个引导变压器模型，其中包括澄清问题。在我们的实验中，我们使用了两个独立的外部源，包括顶级检索的文档和一组不同的可能的澄清问题。我们为会话搜索中的两个下游任务实现了所提出的表示学习模型；文献检索和下一步澄清问题选择。我们的实验使用一个公共数据集进行搜索澄清，并证明与竞争基线相比有显著改进。</pre></li>
<li><a href="https://arxiv.org/abs/2008.09689">Fine-tune BERT for E-commerce Non-Default Search Ranking</a>
<pre>电子商务平台上非默认排名的质量，例如基于商品价格上升或历史销售额下降的排名，往往存在严重的相关性问题，因为不相关的商品更容易在排名结果的顶部暴露出来。在这项工作中，我们提出了一个两阶段的排序方案，该方案首先通过精确的查询/标题关键字匹配召回大量候选项，然后使用对人类标签数据进行微调的BERT Large对召回项进行分类。我们还实现了并行预测多个GPU主机和C++令牌定制OP的ToSoFLoad。在这次数据挑战中，我们的模型在监督阶段（基于F1总得分）获得第一名，在最后阶段（基于每次查询的F1平均得分）获得第二名。</pre></li>
<li><a href="https://arxiv.org/abs/2007.12603">IR-BERT: Leveraging BERT for Semantic Search in Background Linking for News Articles</a>
<pre>这项工作描述了TREC 2020新闻跟踪的背景链接任务的两种方法。这项任务的主要目标是推荐一个相关文章的列表，读者应该参考这些文章，以便理解上下文并获得查询文章的背景信息。我们的第一种方法侧重于通过组合从查询文档中提取的加权关键字并使用BM25进行检索来构建有效的搜索查询。第二种方法利用SBERT（Nils-Reimers等人）的能力来学习查询的上下文表示，以便在语料库上执行语义搜索。我们的经验表明，使用语言模型有助于我们理解查询文章的上下文和背景。在TREC 2018《华盛顿邮报》数据集上对所提出的方法进行了评估，我们的最佳模型在成本方面优于TREC中位数以及2018年得分最高的模型nDCG@5米制的我们进一步提出了一个多样性度量来评估各种方法在检索不同文档集时的有效性。这可能会激发研究人员在推荐名单中引入多样性。我们已经在Github上公开了我们的实现，并计划在TREC 2020中提交后台链接任务的运行情况。</pre></li>
<li><a href="https://arxiv.org/abs/2010.10789">ProphetNet-Ads: A Looking Ahead Strategy for Generative Retrieval Models in Sponsored Search Engine</a> (NLPCC2020)
<pre>在赞助搜索引擎中，最近提出了生成检索模型来挖掘用户输入查询的相关广告关键词。生成性检索模型在目标库前缀树（Trie）的路径上逐个令牌生成输出，这保证了所有生成的输出都是合法的，并且由目标库覆盖。在实际应用中，我们发现了Trie约束搜索长度引起的几个典型问题。在本文中，我们分析了这些问题，并提出了一种前瞻性的生成式检索模型ProphetNet-Ads策略。ProphetNet-Ads通过直接优化Trie约束的搜索空间来提高检索能力。我们从一个真实的词赞助的搜索引擎中构建了一个数据集，并进行了实验来分析不同的生成检索模型。与最近提出的基于Trie的LSTM生成检索模型相比，我们的单模型结果和综合结果在波束大小为5的情况下，召回率分别提高了15.58%和18.8%。案例研究进一步说明了ProphetNet广告是如何缓解这些问题的。</pre></li>
<li><a href="https://openreview.net/forum?id=PlUA_mgGaPq">Rapidly Deploying a Neural Search Engine for the COVID-19 Open Research Dataset: Preliminary Thoughts and Lessons Learned</a> (ACL2020 WS)</li>
<li><a href="https://arxiv.org/abs/2010.05987">SLEDGE-Z: A Zero-Shot Baseline for COVID-19 Literature Search</a> (EMNLP2020)
<pre>随着世界范围内对严重急性呼吸综合征冠状病毒2（SARS-CoV-2）的关注，有关该病毒的科学文献迅速增多。临床医生、研究人员和决策者需要能够有效地搜索这些文章。在这项工作中，我们提出了一种适用于新冠病毒相关科学文献的零射击排序算法。我们的方法将来自另一个集合的训练数据过滤到与医学相关的查询，使用在科学文本上预先训练的神经重新排序模型（SciBERT），并过滤目标文档集合。这种方法在TREC-COVID第1轮排行榜上的零射击方法中名列前茅，并展示了P@50.80和nDCG@10在第1轮和第2轮判断中进行评估时为0.68。尽管不依赖TREC-COVID数据，但我们的方法优于依赖TREC-COVID数据的模型。作为第一批全面评估新冠病毒-19搜索的搜索方法之一，我们希望这能成为一个强有力的基线，有助于应对全球危机。</pre></li>
<li><a href="https://www.aclweb.org/anthology/D19-1171/">Neural Duplicate Question Detection without Labeled Training Data</a> (EMNLP2019)</li>
<li><a href="https://arxiv.org/abs/2011.11090">Cross-Domain Generalization Through Memorization: A Study of Nearest Neighbors in Neural Duplicate Question Detection</a>
<pre>重复问题检测（DQD）对于提高社区和自动问答系统的效率非常重要。不幸的是，在域中收集受监督的数据既耗时又昂贵，而且我们跨域利用注释的能力非常有限。在这项工作中，我们利用神经表示和研究最近邻在DQD的跨域泛化。我们首先在丰富的表示空间中对源域和目标域的问题对进行编码，然后使用基于k近邻检索的方法，将邻域的标签和距离聚合为排序对。我们在StackExchange、Spring和Quora数据集的不同跨域场景中观察到该方法的鲁棒性能，在多种情况下优于交叉熵分类。</pre></li>
<li><a href="https://arxiv.org/abs/2008.13546">Effective Transfer Learning for Identifying Similar Questions: Matching User Questions to COVID-19 FAQs</a>
<pre>人们越来越多地在网上搜索他们的医疗问题的答案，但在线询问医疗问题的速度大大超过了合格人员的回答能力。这使得许多问题没有得到回答或回答不充分。这些问题中有许多不是唯一的，对类似问题的可靠识别将使问答模式更加高效。新冠病毒-19只会加剧这个问题。几乎每个政府机构和医疗保健组织都试图通过建立在线常见问题解答来满足用户的信息需求，但人们无法询问他们的问题并知道这些页面中是否有答案。虽然许多研究工作集中在一般问题相似性问题上，但这些方法不能很好地推广到需要专家知识来确定语义相似性的领域，例如医学领域。在本文中，我们展示了一种双重微调方法，即在医学问题-答案对上预先训练神经网络，然后在医学问题-问题对上进行微调，对于确定医学问题相似性的最终目标来说，这是一项特别有用的中间任务。当其他预训练任务的准确率低于78.7%时，我们的模型在相同数量的训练样本下达到了82.6%的准确率，在更小的训练集下达到了80.0%的准确率，在使用完整的医学问答数据集时达到了84.5%的准确率。我们还描述了一个当前的实时系统，该系统使用经过培训的模型将用户问题与新冠病毒相关的常见问题进行匹配。</pre></li>
<li><a href="https://arxiv.org/abs/2004.13005">Cross-lingual Information Retrieval with BERT</a>
<pre>最近发展了多种神经语言模型，如BERT和XLNet，并在各种自然语言处理任务（包括句子分类、问答和文档排序）中取得了令人印象深刻的结果。在本文中，我们探讨了在跨语言信息检索任务中使用流行的双向语言模型BERT来建模和学习英语查询与外语文档之间的相关性。介绍了一种基于BERT的深度相关匹配模型，并利用来自平行语料库的CLIR训练数据，对一个具有弱监督的预训练多语言BERT模型进行了微调。立陶宛文文档检索的实验结果表明，我们的模型是有效的，优于竞争性基线方法。</pre></li>
<li><a href="https://arxiv.org/abs/2006.09526">Cross-lingual Retrieval for Iterative Self-Supervised Training</a> (NeurIPS2020)
<pre>最近的研究已经证明了多语言预训练语言模型的跨语言对齐能力。在这项工作中，我们发现跨语言对齐可以通过在使用自己的编码器输出挖掘的句子对上训练seq2seq模型来进一步改进。我们利用这些发现开发了一种新的方法——迭代自监督训练跨语言检索（CRIS），其中挖掘和训练过程被迭代应用，同时提高跨语言对齐和翻译能力。使用这种方法，我们在9种语言方向上获得了最先进的无监督机器翻译结果，平均提高了2.4 BLEU，在16种语言的XTREME基准测试中，在Tatoeba句子检索任务上获得了21.5%的绝对准确率平均提高。此外，当对有监督机器翻译下游任务进行微调时，CRIS与mBART相比，平均带来额外1.8 BLEU改进。</pre></li>
<li><a href="https://aclanthology.org/2021.naacl-industry.19/">Graph-based Multilingual Product Retrieval in E-Commerce Search</a> (NAACL2021 Industry)</li>
<li><a href="https://arxiv.org/abs/1912.13080">Teaching a New Dog Old Tricks: Resurrecting Multilingual Retrieval Using Zero-shot Learning</a> (ECIR2020)
<pre>虽然每天有数十亿非英语用户依赖搜索引擎，但对于非英语语言的即席信息检索问题却鲜有研究。这主要是由于缺乏适合训练排名算法的数据集。在本文中，我们通过利用预先训练好的多语言模型来解决数据不足的问题，将一个经过英语集合训练的检索系统转换为非英语查询和文档。我们的模型是在零镜头设置下评估的，这意味着我们使用它们来预测训练期间从未见过的语言中的查询文档对的相关性分数。我们的结果表明，所提出的方法可以显著优于阿拉伯语、汉语普通话和西班牙语的无监督检索技术。我们还表明，使用目标语言中的一些示例来扩充英语培训集有时可以提高性能。</pre></li>
<li><a href="https://arxiv.org/abs/2010.10137">PROP: Pre-training with Representative Words Prediction for Ad-hoc Retrieval</a> (WSDM2021)
<pre>最近，当对下游任务（包括信息检索（IR））进行微调时，预训练的语言表示模型（如BERT）已显示出巨大的成功。然而，为特别检索定制的训练前目标尚未得到很好的探索。在本文中，我们提出了使用代表词预测（PROP）进行预训练的方法来进行ad-hoc检索。PROP的灵感来源于经典的IR统计语言模型，特别是查询可能性模型，该模型假设查询是作为代表“理想”文档的文本片段生成的。基于这一思想，我们构建了用于预训练的代表词预测（ROP）任务。给定一个输入文档，我们根据文档语言模型对一对词集进行采样，其中可能性较高的词集被认为更能代表文档。然后，我们结合蒙面语言模型（MLM）的目标，预先训练Transformer模型来预测两个词集之间的成对偏好。通过对各种具有代表性的下游即席检索任务进行进一步微调，PROP在没有预训练或使用其他预训练方法的情况下实现了基线的显著改进。我们还表明，在零和低资源红外设置下，PROP都可以实现令人兴奋的性能。代码和预先培训的模型可在https://github.com/Albert-Ma/PROP.</pre></li>
<li><a href="https://arxiv.org/abs/2104.09791">B-PROP: Bootstrapped Pre-training with Representative Words Prediction for Ad-hoc Retrieval</a> (SIGIR2021)
<pre>预训练和微调在许多下游自然语言处理（NLP）任务中取得了显著的成功。最近，为信息检索（IR）量身定制的预训练方法也得到了探索，最新的成功是PROP方法，它在各种特定检索基准上达到了新的SOTA。PROP的基本思想是构造\textit{representative words prediction}（ROP）任务，用于受查询似然模型启发的预训练。尽管PROP的性能令人兴奋，但其有效性可能受到ROP任务构造过程中采用的经典单语言模型的限制。为了解决这个问题，我们提出了一种基于BERT的自举预训练方法（即B-PROP），用于adhoc检索。其关键思想是使用强大的上下文语言模型BERT来代替经典的单语言模型来构建ROP任务，并将BERT自身重新训练到为IR定制的目标上。具体来说，我们引入了一种新的对比方法，受随机性思想的启发，利用BERT的自我注意机制从文档中抽取代表性单词。通过对下游ad-hoc检索任务的进一步微调，我们的方法在没有预训练或使用其他预训练方法的情况下实现了基线的显著改进，并进一步推动了SOTA在各种ad-hoc检索任务上的应用。</pre></li>
<li><a href="https://arxiv.org/abs/2104.08253">Condenser: a Pre-training Architecture for Dense Retrieval</a> (EMNLP2021)
<pre>预先训练的Transformer语言模型（LM）已经成为文本表示编码器。先前的研究对深层LMs进行了微调，以将文本序列（如句子和段落）编码为单个密集向量表示，从而实现高效的文本比较和检索。然而，密集编码器需要大量的数据和复杂的技术，才能在低数据情况下有效地进行训练。本文发现一个关键原因是标准LMs的内部注意结构不适合密集编码器，需要将文本信息聚合到密集表示中。我们建议使用一种新的变压器结构——电容器对密集编码器进行预训练，其中LM预测条件基于密集表示。我们的实验表明，Concerter在各种文本检索和相似性任务上比标准LM有很大的提高。</pre></li>
<li><a href="https://arxiv.org/abs/2203.07735">Augmenting Document Representations for Dense Retrieval with Interpolation and Perturbation</a> (ACL2022)
<pre>稠密检索模型以在稠密表示空间上检索与输入查询最相关的文档为目标，因其显著的成功而备受关注。然而，密集模型需要大量标记的训练数据才能获得显著的性能，而获取由人类注释的查询文档对往往是一项挑战。为了解决这个问题，我们提出了一个简单而有效的文档增强密集检索（DAR）框架，该框架通过插值和扰动来增强文档的表示。我们使用两个基准数据集验证了DAR在检索任务上的性能，表明所提出的DAR在标记和未标记文档的密集检索上显著优于相关基线。</pre></li>
<li><a href="https://arxiv.org/abs/2108.08787">Mr. TyDi: A Multi-lingual Benchmark for Dense Retrieval</a> (EMNLP2021 WS) [<a href="https://github.com/castorini/mr.tydi">github</a>]
<pre>我们介绍了TyDi先生，这是一个多语言基准数据集，用于11种类型多样的语言中的单语检索，旨在使用所学的密集表示来评估排名。本资源的目标是促进非英语语言中密集检索技术的研究，其动机是最近观察到，现有的表征学习技术在应用于分布外数据时表现不佳。作为一个起点，我们基于DPR的多语言改编（我们称之为“mDPR”）为这个新数据集提供零炮基线。实验表明，尽管mDPR的有效性远低于BM25，但密集表示似乎提供了有价值的相关信号，改善BM25会产生稀疏密集的杂种。除了分析我们的结果，我们还讨论了未来的挑战，并提出了多语言密集检索的研究议程。TyDi先生可从以下网址下载：https://github.com/castorini/mr.tydi.</pre></li>
</ul>
<h1 id="generation">Generation</h1>
<ul>
<li><a href="https://arxiv.org/abs/2105.10311">Pretrained Language Models for Text Generation: A Survey</a> (IJCAI2021 Survey Track)
<pre>文本生成已成为自然语言处理（NLP）中最重要但最具挑战性的任务之一。深度学习的复兴通过神经生成模型，特别是预训练语言模型（PLM）的范例，极大地推进了这一领域。在本文中，我们概述了在文本生成的PLMs主题中取得的主要进展。作为开场白，我们给出了一般的任务定义，并简要描述了用于文本生成的PLMs的主流体系结构。作为核心内容，我们讨论了如何使现有的PLM适应不同的输入数据并满足生成文本中的特殊属性。我们进一步总结了几种重要的文本生成微调策略。最后，我们提出了未来的几个方向，并对本文进行了总结。我们的调查旨在为文本生成研究人员提供一个综合和相关研究的指针。</pre></li>
<li><a href="https://arxiv.org/abs/2201.05273">A Survey of Pretrained Language Models Based Text Generation</a>
<pre>文本生成的目的是从输入数据中生成人类语言中的可信和可读文本。深度学习的复兴通过神经生成模型，特别是预训练语言模型（PLM）的范例，极大地推进了这一领域。基于PLMs的文本生成在学术界和工业界都被视为一个有前途的方向。在这篇综述中，我们介绍了文本生成的PLMs主题的最新进展。具体而言，我们首先介绍了将PLMs应用于文本生成的三个关键点：1）如何将输入数据编码为保留输入语义的表示，并将其融合到PLMs中；2） 如何设计作为发电模型的PLMs的通用性和性能架构；3）如何在给定参考文本的情况下优化PLMs，并确保生成的文本满足特殊的文本属性。然后，我们在每个关键点中找出几个挑战和未来方向。接下来，我们将总结各种有用的资源和用于PLMs的典型文本生成应用程序。最后，我们总结了本次调查的贡献。</pre></li>
<li><a href="https://arxiv.org/abs/2011.11928">GLGE: A New General Language Generation Evaluation Benchmark</a> [<a href="https://github.com/microsoft/glge">github</a>]
<pre>GLUE和SuperGLUE等多任务基准测试推动了自然语言处理（NLP）中预训练和迁移学习的巨大发展。这些基准主要关注一系列自然语言理解（NLU）任务，而不考虑自然语言生成（NLG）模型。在本文中，我们提出了通用语言生成评估（GLGE），这是一个新的多任务基准，用于评估NLG模型在八种语言生成任务中的泛化能力。对于每个任务，我们继续根据任务难度设计三个子任务（GLGE Easy、GLGE Medium和GLGE Hard）。这将引入24个子任务来全面比较模型性能。为了鼓励对NLG模型的预培训和转移学习进行研究，我们公开了GLGE，并建立了一个具有强大基线的排行榜，包括MASS、BART和ProphetNet（源代码和数据集在https://github.com/microsoft/glge).</pre></li>
<li><a href="https://arxiv.org/abs/1902.04094">BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model</a> (NAACL2019 WS)
<pre>我们表明，BERT（Devlin等人，2018年）是一个马尔可夫随机场语言模型。这个公式让位于一个自然的过程，从伯特的句子样本。我们从BERT生成，发现它可以生成高质量、流畅的代。与传统的从左到右的语言模型相比，伯特生成的句子更加多样化，但质量稍差。</pre></li>
<li><a href="https://arxiv.org/abs/1902.09243">Pretraining-Based Natural Language Generation for Text Summarization</a>
<pre>在本文中，我们提出了一种新的基于预训练的编码器-解码器框架，它能够以两级方式基于输入序列生成输出序列。对于我们模型的编码器，我们使用BERT将输入序列编码为上下文表示。对于解码器，在我们的模型中有两个阶段，第一阶段，我们使用基于转换器的解码器来生成草稿输出序列。在第二阶段，我们屏蔽草稿序列中的每个单词并将其提供给BERT，然后通过组合输入序列和BERT生成的草稿表示，我们使用基于转换器的解码器来预测每个屏蔽位置的细化单词。据我们所知，我们的方法是第一种将BERT应用于文本生成任务的方法。作为这一方向的第一步，我们在文本摘要任务中评估了我们提出的方法。实验结果表明，我们的模型在CNN/每日邮报和纽约时报的数据集上都达到了最新的水平。</pre></li>
<li><a href="https://arxiv.org/abs/1908.08345">Text Summarization with Pretrained Encoders</a> (EMNLP2019) [<a href="https://github.com/nlpyang/PreSumm">github (original)</a>] [<a href="https://github.com/huggingface/transformers/tree/master/examples/summarization">github (huggingface)</a>]
<pre>来自Transformers的双向编码器表示（BERT）代表了预训练语言模型的最新体现，这些模型最近促进了广泛的自然语言处理任务。在本文中，我们展示了如何将BERT有效地应用于文本摘要，并为抽取模型和抽象模型提出了一个通用框架。我们介绍了一种新的基于BERT的文档级编码器，它能够表达文档的语义并获得其句子的表示。我们的提取模型是建立在这个编码器之上的，通过堆叠几个句子间转换层。对于抽象总结，我们提出了一种新的微调调度方案，该方案对编码器和解码器采用不同的优化器，以缓解两者之间的不匹配（前者预训练，而后者不训练）。我们还证明了两阶段微调方法可以进一步提高生成摘要的质量。在三个数据集上的实验表明，我们的模型在提取和抽象环境中都获得了最先进的结果。我们的代码可在https://github.com/nlpyang/PreSumm</pre></li>
<li><a href="https://arxiv.org/abs/1909.10599">Multi-stage Pretraining for Abstractive Summarization</a>
<pre>抽象摘要的神经模型往往在高度专业化、摘要特定的建模附加组件（如指针生成器、覆盖率建模和推断时间启发式）的存在下实现最佳性能。我们在这里展示了预训练可以补充这些建模的进步，从而使用两个关键概念：全网络初始化和多阶段预训练，在短形式和长形式的抽象摘要中产生改进的结果。我们的方法允许模型从多个预训练任务中获得过渡利益，从通用语言任务到专门的摘要任务，再到更专门的任务，例如基于项目符号的摘要。使用这种方法，我们证明了与随机初始化的基线相比，Gigaword基准上的1.05胭脂-L点和CNN/DailyMail基准上的1.78胭脂-L点的改进。</pre></li>
<li><a href="https://arxiv.org/abs/1912.08777">PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization</a>
<pre>最近的工作预训练变压器与自我监督的目标在大型文本语料库已显示出巨大的成功时，微调下游NLP任务，包括文本摘要。然而，为抽象文本摘要量身定制的训练前目标尚未探索。此外，缺乏跨不同领域的系统评估。在这项工作中，我们提出了一个新的自我监督目标，在海量文本语料库上预训练基于变压器的大型编解码器模型。在PEGASUS中，重要的句子从输入文档中删除/隐藏，并作为一个输出序列从剩余的句子中一起生成，类似于提取摘要。我们在12个下游摘要任务中评估了我们最好的PEGASUS模型，这些任务涵盖新闻、科学、故事、说明、电子邮件、专利和立法法案。实验表明，它在所有12个下游数据集上都达到了最先进的性能，这些数据集是通过ROUGE分数测量的。我们的模型在低资源汇总方面也显示出令人惊讶的性能，超过了之前在6个数据集上的最新结果，只有1000个示例。最后，我们使用人工评估验证了我们的结果，并表明我们的模型总结在多个数据集上实现了人工绩效。</pre></li>
<li><a href="https://arxiv.org/abs/2003.13028">Abstractive Summarization with Combination of Pre-trained Sequence-to-Sequence and Saliency Models</a>
<pre>预先训练的序列到序列（seq-to-seq）模型显著提高了一些语言生成任务的准确性，包括抽象摘要。尽管通过对这些模型进行微调，抽象摘要的流畅性得到了极大的提高，但尚不清楚它们是否能够识别出摘要中要包含的源文本的重要部分。在这项研究中，我们通过大量实验研究了将识别源文本重要部分的显著性模型与预先训练的seq-to-seq模型相结合的有效性。我们还提出了一种新的组合模型，包括从源文本中提取标记序列的显著性模型和将该序列作为额外输入文本的seq-to-seq模型。实验结果表明，即使seq-to-seq模型在大规模语料库上预先训练，大多数组合模型在CNN/DM和XSum数据集上都优于简单的微调seq-to-seq模型。此外，对于CNN/DM数据集，建议的组合模型在ROUGE-L上比之前的最佳执行模型高1.33个点。</pre></li>
<li><a href="https://arxiv.org/abs/2010.08014">GSum: A General Framework for Guided Neural Abstractive Summarization</a> (NAACL2021) [<a href="https://github.com/neulab/guided_summarization">github</a>]
<pre>神经抽象摘要模型是灵活的，可以产生连贯的摘要，但有时不可靠，难以控制。虽然以前的研究试图提供不同类型的指导来控制输出和提高忠诚度，但不清楚这些策略之间如何进行比较和对比。在本文中，我们提出了一个通用的、可扩展的引导式摘要框架（GSum），该框架可以有效地将不同类型的外部引导作为输入，并在不同的种类上进行了实验。实验表明，该模型是有效的，当使用突出显示的句子作为指导时，在4个流行的摘要数据集上根据ROUGE实现了最先进的性能。此外，我们还展示了我们的指导模型可以生成更可靠的摘要，并展示了不同类型的指导如何生成质量不同的摘要，从而为学习模型提供了一定程度的可控性。</pre></li>
<li><a href="https://arxiv.org/abs/2004.01853">STEP: Sequence-to-Sequence Transformer Pre-training for Document Summarization</a>
<pre>摘要文档摘要通常被建模为序列对序列（Seq2Seq）学习问题。不幸的是，在有限的监督摘要数据上训练基于Seq2Seq的大型摘要模型是一项挑战。本文提出了三个预训练目标，允许我们在未标记文本上预训练基于Seq2Seq的抽象摘要模型。其主要思想是，给定从文档人工构建的输入文本，对模型进行预训练以恢复原始文档。这些目标包括句子重新排序、下一个句子生成和隐藏文档生成，它们与抽象文档摘要任务密切相关。在两个基准摘要数据集（即CNN/DailyMail和纽约时报）上的实验表明，所有三个目标都可以提高基线的性能。与在大规模数据（超过160GB）上预训练的模型相比，我们的方法只需19GB的预训练文本，就可以获得类似的结果，这证明了它的有效性。</pre></li>
<li><a href="https://arxiv.org/abs/2004.15011">TLDR: Extreme Summarization of Scientific Documents</a> [<a href="https://github.com/allenai/scitldr">github</a>]
<pre>我们将介绍TLDR生成，一种新的科技论文极端总结形式。TLDR生成涉及到高源代码压缩，需要专家背景知识和对复杂领域特定语言的理解。为了促进这项任务的研究，我们介绍了SciTLDR，一个新的多目标数据集，包含5.4K TLDR和3.2K篇论文。SciTLDR包含作者编写的TLDR和专家派生的TLDR，后者使用一种新的注释协议收集，该协议可以生成高质量的摘要，同时最大限度地减少注释负担。我们提出了CATTS，这是一种简单而有效的学习策略，用于生成TLDR，利用标题作为辅助训练信号。CATTS改进了自动度量和人工评估下的强基线。数据和代码可在https://github.com/allenai/scitldr.</pre></li>
<li><a href="https://arxiv.org/abs/2007.11768">Product Title Generation for Conversational Systems using BERT</a>
<pre>通过最近语音技术的进步和智能设备的引入，如Amazon Alexa和Google Home，越来越多的用户通过语音与应用程序交互。电子商务公司通常会在其网页上显示简短的产品标题，无论是人工策划的还是算法生成的，如果需要简洁，但这些标题与自然语言不同。例如，“Lucky Charms无麸质速食麦片，20.5盎司一盒Lucky Charms无麸质麦片”可以显示在网页上，但“20.5盎司一盒Lucky Charms无麸质麦片”通过对话系统更容易理解。与可以向用户显示图像和详细产品信息的显示设备相比，在与语音助手交互时，产品的简短标题是必要的。我们提出了一种序列到序列的方法，使用BERT从输入的web标题生成简短、自然的口语标题。我们在真实的行业数据集上进行了大量实验，并对模型输出进行了人工评估，结果表明，BERT摘要优于可比较的基线模型。</pre></li>
<li><a href="https://arxiv.org/abs/2011.01421">WSL-DS: Weakly Supervised Learning with Distant Supervision for Query Focused Multi-Document Abstractive Summarization</a> (COLING2020)
<pre>在以查询为中心的多文档摘要（QF-MDS）任务中，给出了一组文档和一个查询，目标是根据给定的查询从这些文档生成摘要。然而，这项任务的一个主要挑战是缺少标记的训练数据集。为了克服这个问题，本文提出了一种新的基于远程监督的弱监督学习方法。特别是，我们使用与目标数据集相似的数据集作为训练数据，利用预先训练的句子相似性模型从多文档黄金参考摘要生成文档集中每个文档的弱参考摘要。然后，我们在每个文档上迭代地训练我们的摘要模型，以减轻同时在多个文档（即长序列）中训练神经摘要模型时出现的计算复杂性问题。在文档理解会议（DUC）数据集中的实验结果表明，我们提出的方法在各种评估指标方面建立了一个新的最先进的结果。</pre></li>
<li><a href="https://arxiv.org/abs/2010.12723">Constrained Abstractive Summarization: Preserving Factual Consistency with Constrained Generation</a>
<pre>抽象摘要生成的摘要应该只包含源文档所包含的语句。然而，最先进的抽象方法仍然容易产生与源文档不一致的幻觉。在本文中，我们提出了约束抽象摘要（constrained abstractive Summation，CAS），这是一种通过指定标记作为摘要中必须存在的约束来保持抽象摘要的事实一致性的通用设置。我们探讨了使用词汇约束解码（一种适用于任何具有束搜索解码的抽象方法）来实现CAS的可行性，并在两种场景中进行了实验：（1）无需人工参与的标准摘要，其中关键短语提取用于从源文档中提取约束；（2） 具有人工反馈的交互式摘要，以参考摘要中缺失的标记作为约束进行模拟。对两个基准数据集的自动和人工评估表明，CAS提高了抽象摘要的质量，特别是在事实一致性方面。特别是，我们观察到，在交互摘要场景中，当几个地面真相标记用作约束时，高达11.2 ROUGE-2增益。</pre></li>
<li><a href="https://arxiv.org/abs/2012.14774">Abstractive Query Focused Summarization with Query-Free Resources</a>
<pre>大规模数据集的可用性推动了神经模型的发展，神经模型可以从单个或多个文档中创建通用摘要。在这项工作中，我们考虑查询聚焦总结（QFS），一个任务，以查询，文档和摘要的形式训练数据是不容易获得的。我们建议将QFS分解为（1）查询建模（即在一组文档中查找支持查询的证据）和（2）条件语言建模（即摘要生成）。我们引入了MaRGE，一个用于证据估计和排序的蒙面ROUGE回归框架，它依赖于摘要和查询的统一表示，因此通用数据中的摘要可以转换为代理查询，以学习查询模型。跨QFS基准测试和查询类型的实验表明，我们的模型实现了最先进的性能，尽管从薄弱的监督中学习。</pre></li>
<li><a href="https://arxiv.org/abs/2008.09676">Abstractive Summarization of Spoken and Written Instructions with BERT</a>
<pre>语音摘要是一个困难的问题，因为语音流的自发性、不流畅性以及书面语篇中通常不会遇到的其他问题。我们的工作介绍了BERTSum模型在会话语言中的首次应用。我们生成了从园艺和烹饪到软件配置和运动等各种主题的叙述式教学视频的抽象摘要。为了丰富词汇，我们使用迁移学习，并在几个大型跨领域的书面和口语数据集上预训练该模型。我们还对抄本进行预处理，以恢复ASR系统输出中的句子切分和标点符号。结果通过How2和WikiHow数据集的ROUGE和Content-F1评分进行评估。我们聘请人工评委对从HowTo100M和YouTube策划的数据集中随机选择的一组摘要打分。基于盲评估，我们的文本流畅性和实用性达到了与人类内容创作者撰写的摘要相近的水平。当应用于风格和主题差异很大的WikiHow文章时，该模型优于当前的SOTA，同时在规范的CNN/DailyMail数据集上没有显示性能回归。由于该模型在不同风格和领域的高度通用性，它在提高互联网内容的可访问性和可发现性方面具有巨大潜力。我们将此功能集成到智能虚拟助理中，使他们能够根据要求总结书面和口头教学内容。</pre></li>
<li><a href="https://arxiv.org/abs/2105.12544">Language Model as an Annotator: Exploring DialoGPT for Dialogue Summarization</a> (ACL2021)
<pre>当前的对话摘要系统通常使用一些通用语义特征（例如关键字和主题）对文本进行编码，以获得更强大的对话建模功能。然而，这些特性是通过开放域工具包获得的，这些工具包与对话框无关或严重依赖于人工注释。在本文中，我们展示了DialoGPT，一个预先训练的会话反应生成模型，如何利用DialoGPT中编码的对话背景知识，开发成一个无监督的对话注释器。我们使用DialoGPT在两个对话摘要数据集SAMSum和AMI上标记三种类型的特征，并使用预训练和非预训练模型作为我们的摘要。实验结果表明，我们提出的方法可以在这两个数据集上获得显著的改进，并在SAMSum数据集上实现了新的最新性能。</pre></li>
<li><a href="https://arxiv.org/abs/2106.08556">Coreference-Aware Dialogue Summarization</a> (SIGDIAL2021)
<pre>最近，通过神经方法总结对话越来越受到研究的关注，但要获得切实可行的解决方案仍然具有挑战性。这些挑战的例子包括对话中的非结构化信息交换、演讲者之间的非正式互动以及随着对话的发展演讲者角色的动态变化。许多这样的挑战导致复杂的共指链接。因此，在这项工作中，我们研究了不同的方法来明确地将共指信息纳入神经抽象对话摘要模型，以应对上述挑战。实验结果表明，所提出的方法达到了最先进的性能，这意味着在对话摘要中利用共指信息是有用的。对事实正确性的评估结果表明，这种共指感知模型能够更好地跟踪对话者之间的信息流，并将准确的状态/行为与相应的对话者和个人提及联系起来。</pre></li>
<li><a href="https://arxiv.org/abs/2106.13822">XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages</a> (ACL2021 Findings) [<a href="https://github.com/csebuetnlp/xl-sum">github</a>]
<pre>当代关于抽象文本摘要的工作主要集中在高资源语言（如英语）上，这主要是由于低/中资源语言的数据集可用性有限。在这项工作中，我们介绍了XL Sum，这是一个综合性和多样性的数据集，由来自BBC的100万对专业注释文章摘要组成，使用一组精心设计的启发式方法提取。该数据集涵盖从低到高资源的44种语言，其中许多语言目前没有可用的公共数据集。XL Sum是高度抽象、简洁和高质量的，这一点可以从人的评价和内在评价中看出。我们使用XL Sum微调了mT5，这是一种最先进的预训练多语言模型，并对多语言和低资源摘要任务进行了实验。与使用类似单语数据集获得的结果相比，XL Sum得出的结果具有竞争性：我们在10种语言上的得分高于11分，其中一些超过15分，这是通过多语言培训获得的。此外，针对低资源语言的单独培训也提供了有竞争力的表现。就我们所知，就从单一来源收集的样本数量和涵盖的语言数量而言，XL Sum是最大的抽象摘要数据集。我们正在发布数据集和模型，以鼓励未来对多语言抽象摘要的研究。可以在\url中找到这些资源{https://github.com/csebuetnlp/xl-sum}.</pre></li>
<li><a href="https://arxiv.org/abs/2004.14135">BERT Fine-tuning For Arabic Text Summarization</a> (ICLR2020 WS)
<pre>微调预训练的BERT模型是提取/抽象文本摘要的最先进方法，在本文中，我们展示了如何将这种微调方法应用于阿拉伯语，以构建第一个文档化的抽象阿拉伯语文本摘要模型，并展示其在阿拉伯语摘要提取中的性能。我们的模型适用于多语言的BERT（因为阿拉伯语本身没有经过预训练的BERT）。我们首先在英语语料库中展示了它的性能，然后将其应用于阿拉伯文语料库的抽取和抽象任务中。</pre></li>
<li><a href="https://arxiv.org/abs/2006.01997">Automatic Text Summarization of COVID-19 Medical Research Articles using BERT and GPT-2</a>
<pre>随着新冠病毒-19大流行，医学界越来越迫切地需要跟上新冠病毒相关文献的快速增长。因此，新冠病毒-19开放研究数据集挑战发布了一系列学术文章，并呼吁采用机器学习方法来帮助弥合研究人员与快速增长的出版物之间的差距。在这里，我们利用预训练NLP模型（BERT和OpenAI GPT-2）的最新进展，通过在此数据集上执行文本摘要来解决这一挑战。我们使用胭脂评分和目视检查来评估结果。我们的模型基于从原始文章中提取的关键词提供抽象和全面的信息。我们的工作可以为医学界提供简洁的摘要，这些摘要目前还没有。</pre></li>
<li><a href="https://arxiv.org/abs/2010.08892">Mixed-Lingual Pre-training for Cross-lingual Summarization</a> (AACL-IJCNLP2020)
<pre>跨语言摘要（CLS）旨在以目标语言为源语言的文章生成摘要。传统的解决方案采用两步法，即翻译后总结或总结后翻译。近年来，端到端模型取得了较好的效果，但这些方法主要受到对大规模标记数据依赖性的限制。我们提出了一种基于混合语言预训练的解决方案，该方案利用了跨语言任务（如翻译）和单语任务（如蒙面语言模型）。因此，我们的模型可以利用大量的单语数据来增强对语言的建模。此外，该体系结构没有特定于任务的组件，这节省了内存并提高了优化效率。实验表明，这种预训练方案可以有效地提高跨语言摘要的性能。在神经跨语言摘要（NCLS）数据集中，我们的模型实现了2.82（英文对中文）和1.15（中文对英文）的ROUGE-1分数比最先进的结果提高。</pre></li>
<li><a href="https://arxiv.org/abs/2010.04191">PoinT-5: Pointer Network and T-5 based Financial NarrativeSummarisation</a> (COLING2020 WS)
<pre>公司在财政年度结束时向股东提供年度报告，说明其运营和财务状况。这些报告的平均长度为80页，最长可达250页。在本文中，我们提出了我们在财务叙述总结（FNS）2020任务中使用的方法点5（指针网络和T-5（测试到文本传输转换器）算法的组合）。该方法使用指针网络从报告中提取重要的叙述性句子，然后使用T-5将提取的句子改写为简洁而信息丰富的句子。我们使用ROUGE-N（1,2）、L和SU4来评估我们的方法。所提出的方法在所有度量中获得最高的精度分数，在ROUGE 1和LCS中获得最高的F1分数，并且是在ROUGE-LCS度量中跨越MUSE解决方案基线的唯一解决方案。</pre></li>
<li><a href="https://arxiv.org/abs/1905.02450">MASS: Masked Sequence to Sequence Pre-training for Language Generation</a> (ICML2019) [<a href="https://github.com/microsoft/MASS">github</a>], [<a href="https://github.com/microsoft/MASS/tree/master/MASS-fairseq">github</a>]
<pre>预训练和微调，例如BERT，通过将知识从丰富资源的预训练任务转移到低/零资源的下游任务，在语言理解方面取得了巨大成功。受BERT成功的启发，我们提出了基于编解码器的语言生成任务的掩蔽序列到序列预训练（MASS）。MASS采用编码器-解码器框架来重构给定句子剩余部分的句子片段：其编码器以随机屏蔽片段（几个连续标记）作为输入的句子，其解码器尝试预测该屏蔽片段。通过这种方式，MASS可以联合训练编码器和解码器，以发展表示提取和语言建模的能力。通过进一步细化各种零/低资源语言生成任务，包括神经机器翻译、文本摘要和会话响应生成（3个任务和总共8个数据集），在没有预先训练或其他预训练方法的情况下，质量实现了基线的显著改进。特别是，我们在无监督的英法翻译中达到了最先进的准确性（BLEU分数为37.5），甚至超过了早期基于注意的监督模型。</pre></li>
<li><a href="https://arxiv.org/abs/2005.03361">JASS: Japanese-specific Sequence to Sequence Pre-training for Neural Machine Translation</a> (LREC2020)
<pre>神经机器翻译（NMT）需要大量的并行语料库来提供最先进的翻译质量。低资源NMT通常通过转移学习解决，转移学习利用大型单语或平行语料库进行预培训。单语预训练方法，如MASS（掩蔽序列到序列）对于提高具有小型平行语料库的语言的NMT质量非常有效。然而，它们不考虑使用句法分析器获得的语言信息，而句法分析器对于一些自然语言处理（NLP）任务来说是非常宝贵的。为此，我们提出JASS，日语特定序列到序列，作为一种新的NMT预训练替代方案，将日语作为源语言或目标语言。JASS是联合BMASS（Bunsetsu MASS）和BRSS（Bunsetsu Reordering Sequence to Sequence）预培训，重点是日语语言单位bunsetsus。在我们对ASPEC日语——英语和新闻评论日语——俄语翻译的实验中，我们表明JASS可以给出与MASS相比甚至更好的结果。此外，我们首次表明，联合MASS和JASS预训练的结果显著优于单独的方法，表明了它们的互补性。我们将发布我们的代码、预先训练的模型和bunsetsu注释数据，作为研究人员在自己的NLP任务中使用的资源。</pre></li>
<li><a href="https://arxiv.org/abs/1905.03197">Unified Language Model Pre-training for Natural Language Understanding and Generation</a> [<a href="https://github.com/microsoft/unilm">github</a>] (NeurIPS2019)
<pre>本文提出了一种新的统一预训练语言模型（UniLM），该模型可以对自然语言理解和生成任务进行微调。该模型使用三种类型的语言建模任务进行预训练：单向、双向和序列到序列预测。统一建模是通过使用共享变压器网络和使用特定的自我注意掩码来控制预测条件的上下文来实现的。UniLM在GLUE基准测试、2.0班和CoQA答疑任务方面均优于BERT。此外，UniLM在五个自然语言生成数据集上取得了最新成果，包括将CNN/DailyMail摘要ROUGE-L提高到40.51（绝对提高2.04），将Gigaword摘要ROUGE-L提高到35.75（绝对提高0.86），CoQA生成性问题回答F1得分为82.5（绝对提高37.1），团队问题生成BLEU-4得分为22.12（绝对提高3.75），DSTC7文档固定对话响应生成NIST-4得分为2.67（人因绩效为2.65）。代码和预先培训的模型可在https://github.com/microsoft/unilm.</pre></li>
<li><a href="https://arxiv.org/abs/2002.12804">UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training</a> [<a href="https://github.com/microsoft/unilm">github</a>]
<pre>我们建议使用一种新的训练过程，即伪掩蔽语言模型（PMLM），为自动编码和部分自回归语言建模任务预先训练一个统一的语言模型。给定一个带有屏蔽标记的输入文本，我们依靠传统的掩码通过自动编码来学习损坏的标记和上下文之间的相互关系，依靠伪掩码通过部分自回归建模来学习屏蔽范围之间的内部关系。通过精心设计的位置嵌入和自我注意掩码，可重用上下文编码以避免冗余计算。此外，用于自动编码的传统掩码提供全局掩码信息，因此在部分自回归语言建模中可以访问所有位置嵌入。此外，这两个任务分别将统一的语言模型预训练为双向编码器和序列到序列解码器。我们的实验表明，使用PMLM预先训练的统一语言模型在多个广泛使用的基准测试中，在广泛的自然语言理解和生成任务上取得了新的最新成果。</pre></li>
<li><a href="https://arxiv.org/abs/2010.04246">Dual Inference for Improving Language Understanding and Generation</a> (EMNLP2020 Findings)
<pre>自然语言理解（NLU）和自然语言生成（NLG）任务具有很强的双重关系，NLU的目标是基于自然语言话语预测语义标签，而NLG的目标恰恰相反。以前的工作主要集中在利用模型训练中的对偶性，以获得性能更好的模型。然而，考虑到当前NLP领域模型规模的快速增长，有时我们可能难以重新培训整个NLU和NLG模型。为了更好地解决这个问题，本文建议在不需要再培训的情况下利用推理阶段的对偶性。在三个基准数据集上的实验证明了该方法在NLU和NLG中的有效性，具有很大的实用潜力。</pre></li>
<li><a href="https://arxiv.org/abs/2103.10360">All NLP Tasks Are Generation Tasks: A General Pretraining Framework</a>
<pre>存在各种类型的预训练架构，包括自回归模型（例如，GPT）、自动编码模型（例如，BERT）和编解码器模型（例如，T5）。另一方面，NLP任务的性质不同，主要有三类：分类、无条件生成和条件生成。然而，没有一个预训练框架对所有任务都表现得最好，这给模型开发和选择带来了不便。我们提出了一个新的预培训框架GLM（通用语言模型）来应对这一挑战。与以前的工作相比，我们的体系结构有三个主要优点：（1）它在分类、无条件生成和条件生成任务上表现良好，只使用一个预训练模型；（2） 由于改进的预训练细调一致性，它在分类上优于类BERT模型；（3） 它自然地处理可变长度的空白填充，这对于许多下游任务至关重要。从经验上看，在相同数量的预训练数据下，GLM在SuperGLUE自然语言理解基准上显著优于BERT。此外，具有1.25x BERT-Large参数的GLM在NLU、条件生成和无条件生成中同时达到最佳性能，这表明了其对不同下游任务的可推广性。</pre></li>
<li><a href="https://arxiv.org/abs/2001.04063">ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training</a> (EMNLP2020 Findings) [<a href="https://github.com/microsoft/ProphetNet">github</a>]
<pre>提出了一种新的序列到序列预训练模型ProphetNet，该模型引入了一种新的自监督目标future n-gram prediction和提出的n流自注意机制。在传统的序列到序列模型中，ProphetNet不是优化一步超前预测，而是通过n步超前预测进行优化，n步超前预测基于每个时间步的先前上下文令牌同时预测下一个n令牌。未来n-gram预测明确地鼓励模型为未来令牌进行规划，并防止对强局部相关性的过度拟合。我们分别使用基本规模数据集（16GB）和大规模数据集（160GB）对ProphetNet进行预训练。然后，我们在CNN/DailyMail、Gigaword和1.1班的抽象摘要和问题生成任务基准上进行实验。实验结果表明，与使用相同规模的预训练语料库的模型相比，ProphetNet在所有这些数据集上都取得了最新的结果。</pre></li>
<li><a href="https://arxiv.org/abs/2104.08006">ProphetNet-X: Large-Scale Pre-training Models for English, Chinese, Multi-lingual, Dialog, and Code Generation</a>
<pre>目前，预训练技术已广泛应用于自然语言处理领域。ProphetNet是一种基于预训练的自然语言生成方法，在英语文本摘要和问题生成任务中表现出强大的性能。本文将ProphetNet扩展到其他领域和语言，提出了ProphetNet族预训练模型ProphetNet-X，其中X可以是英语、汉语、多语言等。我们预先训练了一个跨语言生成模型ProphetNet Multi、一个中文生成模型ProphetNet Zh、两个开放域对话生成模型ProphetNet dialog En和ProphetNet dialog Zh。此外，我们还提供了一个PLG（programminglanguagegeneration）模型和一个net代码来显示除自然语言生成（NLG）任务外的生成性能。在我们的实验中，ProphetNet-X模型在10个基准上实现了最新的性能。ProphetNet-X的所有模型都共享相同的模型结构，允许用户在不同的模型之间轻松切换。我们公开了代码和模型，并将不断更新更多的培训前模型和微调脚本。</pre></li>
<li><a href="https://arxiv.org/abs/1908.05672">Towards Making the Most of BERT in Neural Machine Translation</a>
<pre>GPT-2和BERT证明了在各种自然语言处理任务中使用预训练语言模型（LMs）的有效性。然而，LM微调在应用于资源丰富的任务时，往往会遭遇灾难性遗忘。在这项工作中，我们引入了协调训练框架（\方法），这是将预先训练的LMS集成到神经机器翻译（NMT）的关键。我们提出的Cnmt包括三种技术：a）渐进蒸馏，以确保NMT模型能够保留先前预先训练的知识；b） 动态切换门，避免预训练知识的灾难性遗忘；c）根据预定策略调整学习节奏的策略。我们在机器翻译方面的实验表明，WMT14英语-德语语言对的BLEU分数高达3，甚至比之前最先进的训练前辅助NMT的BLEU分数高出1.4。对于包含4000万个句子对的大型WMT14英语-法语任务，我们的基本模型仍比最先进的Transformer大模型显著提高了1个BLEU分数以上。</pre></li>
<li><a href="https://arxiv.org/abs/1908.07688">Improving Neural Machine Translation with Pre-trained Representation</a>
<pre>单语数据已被证明有助于提高翻译质量的神经机器翻译（NMT）。目前的方法主要停留在单词级知识的使用上，如生成合成的并行数据或从单词嵌入中提取信息。相比之下，句子层面的语境知识在自然语言生成中扮演着重要角色，其功能更加复杂多样，尚未得到充分利用。在本文中，我们提出了一种新的结构，它可以利用单语数据获取句子层面的上下文表示。然后，我们设计了一个框架，将源语句和目标语句级表达整合到NMT模型中，以提高翻译质量。在汉英、德英机器翻译任务上的实验结果表明，我们提出的模型在强变压器基线上取得了改进，而在英语土耳其语上的实验进一步证明了我们的方法在低资源场景下的有效性。</pre></li>
<li><a href="https://arxiv.org/abs/2109.04588">BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation</a> (EMNLP2021)
<pre>在许多自然语言处理任务中，使用掩模语言模型（如伯特）的双向编码器的成功促使研究人员尝试将这些预先训练的模型合并到神经机器翻译（NMT）系统中。然而，所提出的合并预训练模型的方法并不简单，主要集中在BERT上，这缺乏对其他预训练模型可能对翻译性能产生的影响的比较。在本文中，我们证明了简单地使用定制和合适的双语预训练语言模型（称为BiBERT）的输出（语境化嵌入）作为NMT编码器的输入可以实现最先进的翻译性能。此外，我们还提出了随机层选择方法和双向翻译模型的概念，以确保充分利用语境嵌入。在不使用反向翻译的情况下，我们的最佳模型在IWSLT'14数据集上的En->De和De->En的BLEU分数分别为30.45和38.61，在WMT'14数据集上的En->De和De->En的BLEU分数分别为31.26和34.94，超过了所有公布的数字。</pre></li>
<li><a href="https://arxiv.org/abs/1909.12744">On the use of BERT for Neural Machine Translation</a> (EMNLP2019 WS)
<pre>最近，为各种NMT任务开发大型预训练模型获得了很多关注。在这项工作中，我们研究如何伯特预训练模型可以利用监督神经机器翻译。我们比较了各种将预训练的BERT模型与NMT模型相结合的方法，并研究了用于BERT训练的单语数据对最终翻译质量的影响。我们使用WMT-14英语德语、IWSLT15英语德语和IWSLT14英语俄语数据集进行这些实验。除了标准任务测试集评估外，我们还对域外测试集和噪声注入测试集进行评估，以评估伯特预训练表示如何影响模型鲁棒性。</pre></li>
<li><a href="https://openreview.net/forum?id=Hyl7ygStwB">Incorporating BERT into Neural Machine Translation</a> (ICLR2020)</li>
<li><a href="https://www.aclweb.org/anthology/D19-5603/">Recycling a Pre-trained BERT Encoder for Neural Machine Translation</a></li>
<li><a href="https://arxiv.org/abs/2106.05634">Exploring Unsupervised Pretraining Objectives for Machine Translation</a> (ACL2021 Findings)
<pre>无监督的跨语言预训练在神经机器翻译（NMT）中取得了很好的结果，极大地减少了对大型并行数据的需求。大多数方法通过屏蔽部分输入并在解码器中重建它们，使屏蔽语言建模（MLM）适应于序列到序列的体系结构。在这项工作中，我们系统地比较了掩蔽和替代目标，这些目标通过根据上下文重新排序和替换单词，产生类似真实（完整）句子的输入。我们在英语$\leftrightarrow$德语、英语$\leftrightarrow$尼泊尔语和英语$\leftrightarrow$僧伽罗语单语数据上用不同的方法预训练模型，并在NMT上进行评估。在（半）监督NMT中，改变预训练目标会导致微调性能的微小差异，而非监督NMT对其更为敏感。为了理解这些结果，我们使用一系列探针彻底研究了预训练模型，并验证它们以不同的方式编码和使用信息。我们得出结论，对并行数据的微调对大多数模型共享的少数属性（如强解码器）最为敏感，而无监督NMT也需要具有强跨语言能力的模型。</pre></li>
<li><a href="https://arxiv.org/abs/2009.07610">Reusing a Pretrained Language Model on Languages with Limited Corpora for Unsupervised NMT</a> (EMNLP2020)
<pre>使用一个语言模型（LM）预训练两种语言与大单语数据，以初始化一个无监督的神经机器翻译（UNMT）系统产生最先进的结果。然而，当一种语言的可用数据有限时，这种方法会导致较差的翻译。我们提出了一种有效的方法，重用仅在高资源语言上预训练的LM。单语LM在两种语言上都进行了微调，然后用于初始化UNMT模型。为了重用经过预训练的LM，我们必须修改其预定义的词汇表，以适应新的语言。因此，我们提出了一种新的词汇扩展方法。我们的方法RE-LM在英语马其顿语（En-Mk）和英语阿尔巴尼亚语（En-Sq）方面优于竞争性跨语言预训练模型（XLM），在所有四个翻译方向上都获得了超过+8.3个BLEU分数。</pre></li>
<li><a href="https://arxiv.org/abs/2106.13627">Language Models are Good Translators</a>
<pre>近年来，神经机器翻译（NMT）得到了迅速发展，其核心在于编码器解码器结构。受有限场景下大规模预训练语言模型在机器翻译方面的最新进展的启发，我们首先证明了单语言模型（LM4MT）可以在标准机器翻译基准上实现与强编码器-解码器NMT模型相当的性能，使用相同的训练数据和相似数量的模型参数。LM4MT还可以轻松地利用源端文本作为额外的监督。LM4MT采用相同的机制对源语和目标语文本进行建模，可以为源语和目标语句子提供统一的表示，从而更好地跨语言传递知识。对基于枢轴和零镜头翻译任务的大量实验表明，LM4MT的性能大大优于编码器-解码器NMT模型。</pre></li>
<li><a href="https://arxiv.org/abs/1907.12461">Leveraging Pre-trained Checkpoints for Sequence Generation Tasks</a>
<pre>大型神经模型的无监督预训练最近彻底改变了自然语言处理。通过从公开发布的检查点开始，NLP从业者在多个基准上推动了最先进的技术，同时节省了大量的计算时间。到目前为止，重点主要放在自然语言理解任务上。在本文中，我们证明了预先训练的检查点用于序列生成的有效性。我们开发了一个基于转换器的序列到序列模型，该模型与公开的预训练BERT、GPT-2和RoBERTa检查点兼容，并对使用这些检查点初始化我们的模型（编码器和解码器）的实用性进行了广泛的实证研究。我们的模型在机器翻译、文本摘要、句子分割和句子融合方面取得了最新的成果。</pre></li>
<li><a href="https://arxiv.org/abs/1904.09324">Mask-Predict: Parallel Decoding of Conditional Masked Language Models</a> (EMNLP2019)
<pre>大多数机器翻译系统从左到右自动回归生成文本。相反，我们使用掩蔽语言建模目标来训练模型，以预测目标词的任何子集，条件是输入文本和部分掩蔽的目标翻译。这种方法允许高效的迭代解码，我们首先以非自回归的方式预测所有目标单词，然后重复屏蔽并重新生成模型最不确定的单词子集。通过将此策略应用于恒定的迭代次数，我们的模型将非自回归和并行解码翻译模型的最新性能水平平均提高了4 BLEU以上。它还能够达到一个典型的从左到右变压器模型的大约1 BLEU点，同时解码速度明显加快。</pre></li>
<li><a href="https://arxiv.org/abs/2004.07159">PALM: Pre-training an Autoencoding&amp;Autoregressive Language Model for Context-conditioned Generation</a> (EMNLP2020)
<pre>自监督预训练，如BERT、MASS和BART，已成为自然语言理解和生成的有力技术。现有的预训练技术采用自动编码和/或自回归目标来训练基于转换器的模型，方法是使用一些屏蔽标记从损坏的文本中恢复原始单词标记。现有技术的训练目标通常与许多语言生成任务的目标不一致，例如生成性问题回答和会话反应生成，以便在给定的上下文中生成新的文本。这项工作为PALM提出了一种新方案，该方案在大型未标记语料库上联合预训练自动编码和自回归语言模型，专门用于生成基于上下文的新文本。新方案缓解了现有去噪方案在预训练和微调之间引入的不匹配，其中生成比重建原始文本更多。大量实验表明，PALM在各种语言生成基准上取得了最新的成果，包括生成性问答（在官方MARCO排行榜上排名第一）、CNN/DailyMail上的摘要以及Gigaword、团队上的问题生成、，以及康奈尔大学电影对白中的对话反应生成。</pre></li>
<li><a href="https://arxiv.org/abs/2001.11314">ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation</a>
<pre>目前自然语言生成的预训练工作很少关注下游任务的暴露偏差问题。为了解决这个问题，我们提出了一个增强的多流序列到序列预训练和微调框架ERNIE-GEN，该框架使用填充生成机制和噪声感知生成方法来弥补训练和推理之间的差异。为了使生成更接近人类的书写模式，该框架引入了一个逐跨生成流程，该流程训练模型连续预测语义完整的跨，而不是逐字预测。与现有的预训练方法不同，ERNIE-GEN采用多粒度目标采样来构造预训练数据，增强了编码器和解码器之间的相关性。实验结果表明，ERNIE-GEN在一系列语言生成任务中，包括抽象摘要（Gigaword和CNN/DailyMail）、问题生成（团队）、对话生成（人物角色聊天），只需少量的预训练数据和参数，即可获得最先进的结果和生成性问答（CoQA）。</pre></li>
<li><a href="https://arxiv.org/abs/2102.08220">Non-Autoregressive Text Generation with Pre-trained Language Models</a> (EACL2021)
<pre>非自回归生成（NAG）由于其快速的推理速度，近年来受到了广泛的关注。然而，现有NAG模型的生成质量仍然落后于自回归模型。在这项工作中，我们证明了BERT可以作为NAG模型的主干，从而大大提高性能。此外，我们还设计了一些机制来缓解vanilla NAG模型的两个常见问题：前缀输出长度的不灵活性和单个令牌预测的条件独立性。最后，为了进一步提高该模型的速度优势，我们提出了一种新的解码策略，即比率优先，用于可以预先近似估计输出长度的应用。为了综合评估，我们在三个文本生成任务上测试了该模型，包括文本摘要、句子压缩和机器翻译。实验结果表明，我们的模型显著优于现有的非自回归基线，并且与许多强自回归模型相比具有竞争力。此外，我们还进行了广泛的分析实验，以揭示每个拟议组件的影响。</pre></li>
<li><a href="https://arxiv.org/abs/1909.10481">Cross-Lingual Natural Language Generation via Pre-Training</a> (AAAI2020) [<a href="https://github.com/CZWin32768/XNLG">github</a>]
<pre>在这项工作中，我们重点关注自然语言生成（NLG）任务的监督信号在多种语言之间的传递。我们建议在单语言和跨语言设置下预训练序列到序列模型的编码器和解码器。训练前目标鼓励模型在共享空间中表示不同的语言，这样我们就可以进行零镜头跨语言迁移。在预训练过程之后，我们使用单语数据对下游NLG任务的预训练模型进行微调。然后，用单一语言训练的序列到序列模型可以直接在该语言之外进行评估（即，接受多语言输入并产生多语言输出）。在问题生成和抽象摘要方面的实验结果表明，该模型在零镜头跨语言生成方面优于基于机器翻译的流水线方法。此外，跨语言迁移通过利用丰富的资源语言数据提高了低资源语言的自然语言转换性能。我们的实施和数据可在https://github.com/CZWin32768/xnlg.</pre></li>
<li><a href="https://arxiv.org/abs/1910.07931">PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable</a> (ACL2020)
<pre>预训练模型已被证明对广泛的自然语言处理任务是有效的。受此启发，我们提出了一个新的对话生成预训练框架，以支持各种类型的对话，包括聊天、基于知识的对话和会话问答。在这个框架中，我们采用灵活的注意机制来充分利用双向语境和语言生成的单向特性。我们还引入离散潜在变量来解决响应生成中固有的一对多映射问题。在一个共享网络中，设计并同时执行响应生成和潜在行为识别两个交互任务。在三个公开数据集上的综合实验验证了该框架的有效性和优越性。</pre></li>
<li><a href="https://arxiv.org/abs/2004.13835">A Tailored Pre-Training Model for Task-Oriented Dialog Generation</a>
<pre>最近，大型预训练语言模型（如BERT和GPT-2）的成功表明，在下游对话生成任务中加入语言先验知识是有效的。但是，预先训练的模型在对话任务上的性能并不像预期的那样最佳。在本文中，我们提出了一个预先训练的角色交替语言模型（PRAL），专门为面向任务的会话系统设计。我们采用（Wu等人，2019年）分别为两个扬声器建模。我们还设计了一些技术，如起始位置随机化、知识提取和历史折扣，以提高训练前的性能。通过清理13个现有数据集，我们引入了一个面向任务的对话预训练数据集。我们在三个不同的下游任务上测试PRAL。结果表明，PRAR表现更好或等同于最先进的方法。</pre></li>
<li><a href="https://arxiv.org/abs/2010.07576">Pretrained Language Models for Dialogue Generation with Multiple Input Sources</a> (EMNLP2020 Findings)
<pre>大规模预训练语言模型在自然语言理解任务中取得了优异的性能。然而，如何将它们应用于对话生成任务，特别是那些以多种来源为反应条件的任务，仍在研究之中。以前的工作只是将所有输入源连接起来，或者对来自不同输入源的信息进行平均。在这项工作中，我们研究了基于预训练语言模型GPT2的多输入源对话模型。我们探索了各种方法来融合不同来源的多个独立注意信息。我们的实验结果表明，适当的融合方法比简单的融合基线与对话历史具有更高的相关性。</pre></li>
<li><a href="https://arxiv.org/abs/2010.08824">Knowledge-Grounded Dialogue Generation with Pre-trained Language Models</a> (EMNLP2020)
<pre>我们用预先训练好的语言模型来研究基于知识的对话生成。为了在容量约束下利用冗余的外部知识，我们建议在预先训练的语言模型定义的响应生成中配备一个知识选择模块，以及一种无监督的方法来联合优化知识选择和响应生成，并使用未标记的对话。在两个基准上的实证结果表明，我们的模型在自动评估和人类判断方面都显著优于最先进的方法。</pre></li>
<li><a href="https://arxiv.org/abs/2011.09708">Are Pre-trained Language Models Knowledgeable to Ground Open Domain Dialogues?</a>
<pre>我们用预先训练好的语言模型来研究基于知识的对话生成。我们不是在基准上追求新的技术水平，而是试图了解存储在预先训练的模型参数中的知识是否已经足以进行开放领域的对话，从而使我们在生成过程中摆脱对外部知识源的依赖。通过大量的基准测试实验，我们发现，通过微调包含知识的对话，预先训练的语言模型可以优于在自动评估和人类判断方面需要外部知识的最先进模型，这表明我们提出的问题得到了肯定的答案。</pre></li>
<li><a href="https://arxiv.org/abs/2010.12780">Open-Domain Dialogue Generation Based on Pre-trained Language Models</a>
<pre>预先训练的语言模型已成功用于开放域对话的响应生成。提出了四种主要框架：（1）对源语句和目标语句分别使用变压器编码器和解码器进行变压器编码；（2） Transformer-Dec对源语句和目标语句使用Transformer解码器；（3） Transformer MLM使用Transformer解码器，在源端应用双向注意，在目标端应用从左到右的注意，并使用蒙面语言模型目标；（4）采用自回归目标的变压器AR。在这项研究中，我们在3个数据集上对这些框架进行了比较，我们的比较表明，最好的框架在源端使用双向注意，并且不分离编码器和解码器。我们还检验了模型差异，我们的实验证实了模型的性能直接受到潜在差异的影响。然后，我们提出了两种修正方法，以减少差异，并提高模型性能。这些结果表明，当我们使用预先训练的模型时，差异是一个重要的考虑因素，并且差异的减少可以导致性能的提高。</pre></li>
<li><a href="https://arxiv.org/abs/2105.09235">Retrieval-Augmented Transformer-XL for Close-Domain Dialog Generation</a>
<pre>基于Transformer的模型在自然语言生成中表现出了捕获模式和结构的优异能力，并在许多任务中取得了最新成果。在本文中，我们提出了一个基于变压器的多匝对话响应生成模型。我们的解决方案基于一种混合方法，该方法通过一种新的检索机制来增强基于变压器的生成模型，该检索机制通过k-最近邻搜索来利用训练数据中存储的信息。我们的系统是在两个数据集上进行评估的，这两个数据集是由客户/助理对话框制作的：Taskmaster-1，由谷歌发布，拥有高质量、面向目标的对话数据和从真实客户服务呼叫中心收集的专有数据集。两者都能在强基线上获得更好的BLEU分数。</pre></li>
<li><a href="https://arxiv.org/abs/2107.07566">Internet-Augmented Dialogue Generation</a>
<pre>地球上最大的不断更新的知识库可以通过互联网搜索访问。在这项工作中，我们研究让会话代理访问这些信息。大型语言模型即使在其权重范围内存储了大量知识，但在生成对话时会产生幻觉（Shuster et al.，2021）；此外，这些事实在模型训练时被冻结。相反，我们提出了一种基于上下文学习生成互联网搜索查询的方法，然后对搜索结果进行条件设置以最终生成响应，这种方法可以利用最新的相关信息。我们在一个新收集的人类对话数据集上对这些模型进行培训和评估，其中一位演讲者在知识驱动的讨论期间可以访问互联网搜索，以确定他们的回答。我们发现，与不使用增广或基于FAISS的检索的现有方法相比，基于搜索查询的对话互联网访问提供了更高的性能（Lewis et al.，2020）。</pre></li>
<li><a href="https://arxiv.org/abs/2012.01775">DialogBERT: Discourse-Aware Response Generation via Learning to Recover and Rank Utterances</a> (AAAI2021)
<pre>预训练语言模型的最新进展显著改善了神经反应的产生。然而，现有的方法通常将对话上下文视为一个线性的标记序列，并通过标记级的自我注意来学习生成下一个单词。这种标记层面的编码阻碍了语篇层面连贯性的探索。本文介绍了DialogBERT，一种新的会话响应生成模型，它增强了以前基于PLM的对话模型。DialogBERT采用分层变压器架构。为了有效地获取话语间的语篇连贯性，我们提出了两个训练目标，包括掩蔽话语回归和类似于原始BERT训练的分布式话语顺序排序。在三个多回合会话数据集上的实验表明，我们的方法在定量评估方面明显优于基线，如BART和DialoGPT。人类的评估表明，DialogBERT产生的反应比具有显著裕度的基线更连贯、信息更丰富、更人性化。</pre></li>
<li><a href="https://arxiv.org/abs/2004.01881">CG-BERT: Conditional Text Generation with BERT for Generalized Few-shot Intent Detection</a>
<pre>在本文中，我们为自然语言理解中的意图检测任务制定了一个更为现实和困难的问题设置，即广义少镜头意图检测（GFSID）。GFSID旨在区分一个联合标签空间，该空间由具有足够标记数据的现有意图和每个类只有少量示例的新意图组成。为了解决这个问题，我们提出了一种新的模型，即基于BERT的条件文本生成（CG-BERT）。CG-BERT有效地利用预先训练好的大型语言模型生成以意图标签为条件的文本。CG-BERT通过变分推理对话语分布进行建模，即使只有很少的话语可用，也能为小说意图生成不同的话语。实验结果表明，CG-BERT在两个真实数据集上的单镜头和五镜头设置下，在GFSID任务上实现了最先进的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2004.11026">QURIOUS: Question Generation Pretraining for Text Generation</a>
<pre>使用预训练的自然语言处理的最新趋势已将重点转向文本生成的预训练和微调方法。通常，重点放在概括语言建模目标的任务无关方法上。我们提出问题生成作为一种预训练方法，它更好地符合文本生成的目标。我们用这种方法训练的文本生成模型能够更好地理解输入的本质，并且是目标任务的更好的语言模型。当对两个文本生成任务（抽象摘要和以答案为中心的问题生成）进行评估时，我们的模型在自动度量方面产生了最先进的性能。评估人员还发现，我们的总结和生成的问题更自然、简洁、信息更丰富。</pre></li>
<li><a href="https://arxiv.org/abs/1904.09521">Few-Shot NLG with Pre-Trained Language Model</a> (ACL2020)
<pre>基于神经网络的从结构化数据或知识生成自然语言（NLG）的端到端方法需要大量数据，这使得它们难以在数据有限的情况下用于实际应用。在这项工作中，我们提出了一个新的任务\texit{now shot natural language generation}。受人们如何总结表格数据的启发，我们提出了一种简单而有效的方法，并表明它不仅表现出强大的性能，而且还提供了跨领域的良好泛化。模型结构的设计基于两个方面：从输入数据中选择内容和从先验知识中获取连贯句子的语言建模。通过200个跨多个领域的培训示例，我们证明了我们的方法实现了非常合理的性能，比最强的基线平均提高了8.0个BLEU点。我们的代码和数据可在\url中找到{https://github.com/czyssrs/Few-Shot-NLG}</pre></li>
<li><a href="https://arxiv.org/abs/2005.10433">Text-to-Text Pre-Training for Data-to-Text Tasks</a>
<pre>我们研究了数据到文本任务的预训练+微调策略。我们的实验表明，采用T5形式的文本到文本预训练能够使简单的、基于端到端转换器的模型优于为数据到文本生成定制的流水线神经结构，以及基于语言模型的替代预训练技术，如BERT和GPT-2。重要的是，T5预训练可以带来更好的泛化，这一点可以从域外测试集的大量改进中得到证明。我们希望我们的工作能为未来的研究提供一个有用的基线，因为数据到文本任务中的迁移学习变得越来越普遍。</pre></li>
<li><a href="https://arxiv.org/abs/2010.02307">KGPT: Knowledge-Grounded Pre-Training for Data-to-Text Generation</a> (EMNLP2020)
<pre>数据到文本的生成由于其广泛的应用，最近吸引了大量的兴趣。现有方法在一系列任务上表现出了令人印象深刻的性能。然而，它们依赖于每个任务的大量标记数据，获取成本很高，因此限制了它们在新任务和域中的应用。在本文中，我们建议利用预培训和迁移学习来解决这个问题。我们提出了一种基于知识的预训练（KGPT），它由两部分组成：1）生成知识丰富文本的基于一般知识的生成模型。2） 一个基于网络的海量知识文本语料库的预训练范例。预先训练的模型可以在各种数据到文本生成任务上进行微调，以生成特定于任务的文本。我们采用三种设置，即完全监督、零炮、少炮来评估其有效性。在完全监督的情况下，我们的模型可以在已知的基线上获得显著的增益。在零炮设置下，我们的模型在没有看到任何示例的情况下，在WebNLG上实现了超过30个胭脂-L，而所有其他基线都失败了。在少数镜头设置下，我们的模型只需要大约十五分之一的标记示例就可以达到与基线模型相同的性能水平。这些实验一致证明了我们提出的框架具有很强的泛化能力https://github.com/wenhuchen/KGPT.</pre></li>
<li><a href="https://arxiv.org/abs/2011.10819">Evaluating Semantic Accuracy of Data-to-Text Generation with Natural Language Inference</a> (INLG2020)
<pre>评估数据到文本（D2T）生成的一个主要挑战是测量生成文本的语义准确性，即检查输出文本是否包含输入数据支持的所有且仅包含事实。我们提出了一种新的基于自然语言推理（NLI）预训练神经模型的D2T生成语义准确性评估指标。我们使用NLI模型在两个方向上检查输入数据和输出文本之间的文本蕴涵，从而揭示遗漏或幻觉。使用普通模板将输入数据转换为NLI的文本。我们在两个最新的D2T数据集上的实验表明，我们的度量在识别错误的系统输出时可以达到很高的精度。</pre></li>
<li><a href="https://arxiv.org/abs/2010.12688">Large Scale Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training</a>
<pre>在数据到文本生成之前，将知识图（KG）三元组转换为自然文本的任务主要集中在特定领域的基准数据集上。然而，在本文中，我们描述了整个英文维基百科KG，并讨论了与广泛、开放领域、大规模描述相关的独特挑战。我们进一步表明，将一个全面的、百科全书式的KG（如Wikidata）表达出来可以用来集成结构化KG和自然语言语料库。与为集成这两个源而开发的许多体系结构不同，我们的方法将KG转换为自然文本，允许它无缝集成到现有的语言模型中。它还具有进一步的优点，即提高了事实的准确性，减少了生成的语言模型的毒性。我们通过在检索语言模型中增加检索语料库来评估这种方法，并在开放域QA和LAMA知识探测的知识密集型任务上显示出显著的改进。</pre></li>
<li><a href="https://arxiv.org/abs/2010.12773">Structure-Grounded Pretraining for Text-to-SQL</a>
<pre>学习捕获文本表对齐方式对于文本到SQL这样的任务至关重要。模型需要正确识别对列和值的自然语言引用，并将它们放在给定的数据库模式中。在本文中，我们提出了一种新的基于弱监督结构的文本到SQL预训练框架（StruG），它可以有效地学习基于并行文本表语料库的文本表对齐。我们确定了一组新的预测任务：列基础、值基础和列值映射，并利用它们预训练文本表编码器。此外，为了在更真实的文本表对齐设置下评估不同的方法，我们在Spider dev set的基础上创建了一个新的评估集Spider reality，删除了对列名的显式提及，并采用八个现有的text-to-SQL数据集进行跨数据库评估。STRUG在所有设置中都比BERT-LARGE有显著改进。与现有的预训练方法（如GRAPPA）相比，STRUG在Spider上实现了类似的性能，并且在更真实的集合上优于所有基线。本工作中使用的所有代码和数据可在https://aka.ms/strug.</pre></li>
<li><a href="https://arxiv.org/abs/2010.05243">Data Agnostic RoBERTa-based Natural Language to SQL Query Generation</a>
<pre>关系数据库是现代世界中用于存储大量数据的最广泛的体系结构之一。然而，这些数据库与普通用户之间存在着障碍。用户通常缺乏与数据库交互所需的查询语言（如SQL）知识。NL2SQL任务旨在通过将自然语言问题转换为有效的SQL查询，找到解决此问题的深入学习方法。考虑到某些数据库的敏感性和对数据隐私的日益增长的需求，我们提出了一种以数据隐私为核心的方法。我们将RoBERTa嵌入和数据不可知的知识向量传递到基于LSTM的子模型中，以预测最终查询。虽然我们还没有达到最先进的结果，但是我们已经从模型的训练中消除了对表数据的需要，并且已经实现了76.7%的测试集执行精度。通过在训练时消除表数据依赖性，我们创建了一个能够基于自然语言问题和表模式的零镜头学习模型。</pre></li>
<li><a href="https://arxiv.org/abs/2004.14373">ToTTo: A Controlled Table-To-Text Generation Dataset</a> (EMNLP2020) [<a href="https://github.com/google-research-datasets/ToTTo">github</a>]
<pre>我们介绍了ToTTo，一个开放域英语表到文本数据集，包含120000多个培训示例，它提出了一个受控生成任务：给定一个Wikipedia表和一组突出显示的表单元格，生成一句话描述。为了获得生成的目标是自然的，但也忠实于源表，我们引入了一个数据集构建过程，注释者直接修改维基百科中现有的候选句子。我们对我们的数据集和注释过程进行了系统分析，并通过几个最先进的基线获得了结果。现有方法虽然通常流畅，但往往会产生表格不支持的短语，这表明该数据集可以作为高精度条件文本生成的有用研究基准。</pre></li>
<li><a href="https://arxiv.org/abs/2012.10033">Exploring Fluent Query Reformulations with Text-to-Text Transformers and Reinforcement Learning</a> (AAAI2021 WS)
<pre>查询重组旨在将嘈杂或模棱两可的文本序列转换为更接近自然语言问题的连贯文本序列。这是为了防止错误在面向客户端的管道中传播，并促进与用户的更好通信。此外，当重新措辞的查询作为输入时，在下游环境（如问答）中保持性能是至关重要的。我们表明，在先前的框架（AQA）下，改变RL算法的尝试不会给奖励获取或序列流畅性带来显著的好处。相反，我们利用查询重新格式化文本到文本转换器（QRT5）并应用基于策略的RL算法进一步推动该重新格式化程序，并通过生成奖励获取查询轨迹获得更好的下游答案。QRT5在RL中显示出更好的样本效率，以实现与前一种方法相同水平的QA性能。它可以基于查询格式良好性评估生成更具可读性的重新格式，并且可以推广到样本外数据。我们的框架被证明是灵活的，允许奖励信号来自不同的下游环境，如意图分类。</pre></li>
<li><a href="https://arxiv.org/abs/2001.05139">A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation</a> (TACL2020) [<a href="https://github.com/JianGuanTHU/CommonsenseStoryGen">github</a>]
<pre>故事生成是一项重要但具有挑战性的任务，即从主导上下文生成合理的故事。尽管在流畅性和局部连贯性建模方面取得了成功，但现有的神经语言生成模型（如GPT-2）仍然存在重复、逻辑冲突以及生成的故事缺乏长期连贯性的问题。我们推测，这是因为很难将相关常识知识联系起来，理解因果关系，并以适当的时间顺序规划实体和事件。在本文中，我们设计了一个知识增强的常识故事生成预训练模型。我们建议利用来自外部知识库的常识知识来生成合理的故事。为了进一步捕捉合理故事中句子之间的因果关系和时间依赖关系，我们采用了多任务学习，该学习结合了一个区分目标，在微调过程中区分真假故事。自动和手动评估表明，我们的模型可以生成比最先进的基线更合理的故事，特别是在逻辑和全球一致性方面。</pre></li>
<li><a href="https://arxiv.org/abs/2010.00840">MEGATRON-CNTRL: Controllable Story Generation with External Knowledge Using Large-Scale Language Models</a> (EMNLP2020)
<pre>现有的预先训练过的大型语言模型已经显示出无与伦比的生成能力。然而，它们是不可控的。在本文中，我们提出了MEGATRON-CNTRL，这是一种新的框架，它使用大规模语言模型，并通过合并外部知识库为文本生成添加控制。我们的框架由关键字预测器、知识检索器、上下文知识检索器和条件文本生成器组成。由于我们无法对知识等级者进行基本真理监督，我们利用了句子嵌入的弱监督。实证结果表明，与之前在ROC故事数据集上的工作相比，我们的模型生成了更流畅、一致和连贯的故事，重复更少，多样性更高。我们通过替换用于生成故事的关键字并重新运行生成过程来展示模型的可控性。人类评估结果显示，77.5%的故事成功地被新关键词控制。此外，通过将我们的模型从1.24亿个参数扩展到83亿个参数，我们证明了较大的模型可以改善发电质量（一致性从74.5%提高到93.0%）和可控性（从77.5%提高到91.5%）。</pre></li>
<li><a href="https://arxiv.org/abs/2012.04332">Facts2Story: Controlling Text Generation by Key Facts</a>
<pre>自关注神经网络结构的最新进展提高了开放式文本生成的门槛。然而，尽管目前的方法能够产生几百字长的连贯文本，但控制正在生成的内容以及对其进行评估仍然是一个悬而未决的问题。我们提出了一个控制生成任务，该任务基于将自然语言表达的一系列事实扩展为更长的叙述。我们为这项任务引入了基于人的评估指标，以及一种获取大型训练数据集的方法。基于微调预训练模型，我们评估了这项任务的三种方法。我们发现，尽管像GPT2这样的自回归单向语言模型能产生更好的流利性，但它们很难坚持要求的事实。我们提出了一个计划和完形填空模型（使用微调的XLNet），该模型在保持所需内容的同时，能产生有竞争力的流畅性。</pre></li>
<li><a href="https://arxiv.org/abs/1911.03705">CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning</a> [<a href="https://github.com/INK-USC/CommonGen">github</a>] [<a href="https://inklab.usc.edu/CommonGen/">website</a>] (EMNLP2020 Findings)
<pre>最近，大规模预先训练的语言模型在几个常识推理基准数据集上表现出令人印象深刻的性能。然而，用常识构建机器来组成现实可行的句子仍然具有挑战性。在本文中，我们提出了一个约束文本生成任务，CommonGen与一个基准数据集相关联，用于显式测试机器的生成常识推理能力。给定一组常见概念（例如，{狗、飞盘、接球、投掷}）；任务是使用这些概念生成一个连贯的句子来描述日常场景（例如，“一个人扔飞盘，他的狗抓住了它”）。CommonGen任务具有挑战性，因为它本质上需要1）具有背景常识知识的关系推理，以及2）处理看不见的概念组合的合成概括能力。我们的数据集由众包和现有字幕语料库组合而成，由79k个常识描述和35k个独特概念集组成。实验表明，最先进的文本生成模型（如T5）与人类绩效之间存在很大差距。此外，我们还证明了学习到的生成性常识推理能力可以通过生成额外的上下文来转移，以改进下游任务，如常识问答。</pre></li>
<li><a href="https://arxiv.org/abs/2012.00366">An Enhanced Knowledge Injection Model for Commonsense Generation</a> (COLING2020)
<pre>常识生成旨在基于一组提供的概念生成合理的日常场景描述。从头开始挖掘概念之间的关系并非易事，因此，我们从外部知识中检索原型，以帮助理解场景，从而更好地生成描述。我们将另外两个模块，即位置指示器和缩放模块，集成到预训练编码器-解码器模型中进行原型建模，以增强知识注入过程。我们在CommonGen基准上进行了实验，实验结果表明，我们的方法在所有指标上都显著提高了性能。</pre></li>
<li><a href="https://arxiv.org/abs/2105.11174">Retrieval Enhanced Model for Commonsense Generation</a> (ACL2021 Findings)
<pre>常识生成是一项具有挑战性的任务，需要使用提供的概念生成一个描述日常场景的看似合理的句子。它对常识知识的推理能力和合成泛化能力的要求甚至让强大的预训练语言生成模型感到困惑。我们提出了一个新的框架，使用检索方法来增强常识生成的预训练和微调。我们通过概念匹配检索原型候选句子，并将其作为辅助输入。对于微调，我们使用可训练的句子检索器进一步提高其性能。我们在大规模CommonGen基准上进行了实验，证明我们的方法取得了最新的结果。</pre></li>
<li><a href="https://arxiv.org/abs/2012.10813">Lexically-constrained Text Generation through Commonsense Knowledge Extraction and Injection</a> (AAAI2021WS)
<pre>条件文本生成一直是一项具有挑战性的任务，目前尚未从最先进的模型中看到人类水平的性能。在这项工作中，我们特别关注Commongen基准测试，其目的是为给定的输入概念集生成一个合理的句子。尽管在其他任务方面取得了进展，但在此数据集上进行微调的大型预训练语言模型通常会生成语法正确但质量上偏离人类对常识理解的句子。此外，生成的序列无法满足匹配词性和完整概念覆盖等词汇要求。在本文中，我们探讨了常识知识图如何在常识推理和词汇约束解码方面提高模型性能。我们提出了增强生成文本语义正确性的策略，我们通过以下方式实现：从概念网中提取常识关系，通过注意机制将这些关系注入统一语言模型（UniLM），并通过输出约束强制执行上述词汇要求。通过几次删减，我们发现常识注入能够生成更符合人类理解的句子，同时仍然符合词汇要求。</pre></li>
<li><a href="https://arxiv.org/abs/2011.07956">Pre-training Text-to-Text Transformers for Concept-centric Common Sense</a>
<pre>预训练语言模型（PTLM）在一系列自然语言理解（NLU）和生成（NLG）任务中取得了令人印象深刻的成果。然而，当前的预培训目标，如蒙面代币预测（对于伯特式PTLM）和蒙面跨度填充（对于T5式PTLM），并未明确建模关于日常概念的关系常识知识，这对于许多需要常识来理解或生成的下游任务至关重要。为了用以概念为中心的常识知识增强PTLM，在本文中，我们提出了从文本中学习常识的生成性和对比性目标，并将其用作增量预训练PTLM的中间自我监督学习任务（在对下游数据集进行特定任务微调之前）。此外，我们还开发了一个联合培训前框架，以统一生成目标和对比目标，从而使它们能够相互加强。大量的实验结果表明，我们的方法，概念感知语言模型（CALM），可以在不依赖外部知识图的情况下，将更多的常识知识封装到预先训练好的文本到文本转换器的参数中，从而在NLU和NLG任务中获得更好的性能。我们表明，尽管仅在相对较小的语料库上进行了几个步骤的增量预训练，但CALM的表现优于基线方法，具有一致的优势，甚至与一些较大的PTLM相当，这表明CALM可以作为一种通用的即插即用方法，用于提高PTLM的常识推理能力。</pre></li>
<li><a href="https://arxiv.org/abs/2009.11692">Language Generation with Multi-Hop Reasoning on Commonsense Knowledge Graph</a> (EMNLP2020)
<pre>尽管生成性预训练语言模型在一系列文本生成任务中取得了成功，但在生成过程中需要对基本常识知识进行推理的情况下，它们仍然会受到影响。现有的将常识知识整合到生成性预先训练的语言模型中的方法只是通过对单个知识三元组的后训练来传递关系知识，而忽略了知识图中丰富的联系。我们认为，利用知识图的结构和语义信息有助于常识感知文本生成。在本文中，我们提出了基于多跳推理流（GRF）的生成方法，该方法能够在从外部常识知识图中提取的多关系路径上实现具有动态多跳推理的预训练模型。我们的经验表明，我们的模型在三个文本生成任务上优于现有的基线，这三个任务需要对常识知识进行推理。我们还证明了动态多跳推理模块的有效性，该模型推理出的推理路径为生成提供了理论依据。</pre></li>
<li><a href="https://arxiv.org/abs/2009.12677">KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning</a>
<pre>生成性常识推理（generativecommonsense reasoning，简称generativecommonsense，简称generativecommonsense，简称generativecommonsense，简称generativecommonsense，简称generativecommonsense，简称generativecommonsense，简称generativecommonsense）是文本生成的一。即使是最先进的预先训练过的语言生成模型也难以完成这项任务，并且常常会产生令人难以置信的异常句子。一个原因是，他们很少考虑结合知识图，可以提供丰富的关系信息之间的常识概念。为了提高文本生成的常识推理能力，我们提出了一种新的知识图增强预训练语言生成模型KG-BART，该模型通过知识图包含概念之间的复杂关系，并生成更多逻辑和自然的句子作为输出。此外，KG-BART可以利用图形注意来聚合丰富的概念语义，从而增强模型对不可见概念集的泛化。在基准CommonGen数据集上的实验通过与几个强大的预训练语言生成模型的比较，验证了我们提出的方法的有效性，特别是KG-BART在BLEU-3,4方面的性能比BART好5.80,4.60。此外，我们还表明，由我们的模型生成的上下文可以作为背景场景，以利于下游常识QA任务。</pre></li>
<li><a href="https://arxiv.org/abs/2010.00904">Autoregressive Entity Retrieval</a> (ICLR2021) [<a href="https://github.com/facebookresearch/GENRE">github</a>]
<pre>实体是我们如何表示和聚合知识的中心。例如，维基百科等百科全书是由实体构成的（例如，每一篇维基百科文章一个实体）。在给定查询的情况下检索此类实体的能力对于知识密集型任务（如实体链接和开放域问题回答）是至关重要的。当前的方法可以理解为原子标签中的分类器，每个实体一个。它们的权重向量是通过编码实体元信息（如它们的描述）生成的密集实体表示。这种方法有几个缺点：（i）上下文和实体关联主要通过向量点积捕获，可能缺少细粒度交互；（ii）在考虑大型实体集时，需要大内存占用来存储密集表示；（iii）必须在培训时对一组适当硬的负面数据进行二次抽样。在这项工作中，我们提出了流派，这是第一个系统，它通过从左到右，以自回归方式逐个标记地生成实体的唯一名称来检索实体。这缓解了上述技术问题，因为：（i）自回归公式直接捕获上下文和实体名称之间的关系，有效地对两者进行交叉编码；（ii）由于我们的编码器-解码器体系结构的参数随词汇表大小而不是实体计数进行缩放，因此大大减少了内存占用；（iii）计算softmax损耗时，无需对负数据进行二次采样。我们在实体消歧、端到端实体链接和文档检索任务方面对20多个数据集进行了实验，在使用竞争系统内存占用的极小部分的同时，获得了最新的或极具竞争力的结果。最后，我们演示了只需指定新实体的名称即可添加新实体。代码和预先培训的模型https://github.com/facebookresearch/GENRE.</pre></li>
<li><a href="https://arxiv.org/abs/2103.12528">Multilingual Autoregressive Entity Linking</a>
<pre>我们介绍了mGENRE，这是一个用于多语言实体链接（MEL）问题的序列到序列系统——解决多语言知识库（KB）中特定于语言的引用的任务。对于给定语言中的提及，mGENRE以自回归方式从左到右逐个标记地预测目标实体的名称。自回归公式允许我们有效地交叉编码提及字符串和实体名称，以捕获提及向量和实体向量之间比标准点积更多的交互。它还支持在大KB范围内快速搜索，即使是提及表中没有出现的提及，也不需要大规模向量索引。虽然之前的MEL工作对每个实体使用单一表示，但我们尽可能多地匹配语言的实体名称，这允许利用源输入和目标名称之间的语言连接。此外，在完全没有训练数据的语言的零射击设置中，mGENRE将目标语言视为在预测时被边缘化的潜在变量。这将使平均精度提高50%以上。我们通过广泛的评估（包括在三个流行的MEL基准上的实验）证明了我们的方法的有效性，其中mGENRE建立了新的最先进的结果。代码和预先培训的模型https://github.com/facebookresearch/GENRE.</pre></li>
<li><a href="https://arxiv.org/abs/2010.11764">EIGEN: Event Influence GENeration using Pre-trained Language Models</a>
<pre>对事件进行推理并跟踪其影响是理解过程的基础。在本文中，我们提出了一种利用预先训练好的语言模型生成事件影响的方法，该方法根据上下文、影响的性质以及推理链中的距离来生成事件影响。我们还导出了一个新的数据集，用于研究和评估事件影响生成方法。在自动评估指标（10个胭脂点）和人类对参考和世代相关性的接近性判断方面，EIGEN优于强大的基线。此外，我们还表明，在“假设”问题回答（WIQA）基准测试（超过3%F1）中，EIGEN产生的事件影响提高了性能，特别是对于需要背景知识和多跳推理的问题。</pre></li>
<li><a href="https://arxiv.org/abs/2104.08251">proScript: Partially Ordered Scripts Generation via Pre-trained Language Models</a>
<pre>脚本——描述典型日常活动的标准化事件序列——通过提供期望、解决歧义和填写未说明的信息来帮助理解叙述。然而，到目前为止，他们已经证明很难作者或摘录文本。在这项工作中，我们首次证明，预训练的神经语言模型（LMs）可以进行微调，以在不同粒度级别生成适用于各种日常场景（例如烘焙蛋糕）的高质量脚本。为此，我们收集了大量（6.4k）的众包偏序脚本（名为proScript），该脚本比以前的数据集大得多，并开发了结合语言生成和结构预测生成脚本的模型。我们定义了两个互补任务：（i）边缘预测：给定场景和无序事件，将事件组织成有效（可能是偏序）脚本；（ii）脚本生成：仅给定场景，生成事件并将其组织成（可能是偏序）脚本。我们的实验表明，我们的模型表现良好（例如，任务（i）中的F1=75.7），说明了一种克服以前脚本收集障碍的新方法。我们还表明，在人的水平绩效方面仍有很大的改进空间。总之，我们的任务、数据集和模型为学习脚本知识提供了一个新的研究方向。</pre></li>
<li><a href="https://arxiv.org/abs/2107.13189">Goal-Oriented Script Construction</a> (INLG2021)
<pre>脚本的知识，在常规场景中常见的事件链，是面向任务的自然语言理解系统的宝贵资产。我们提出了面向目标的脚本构建任务，其中模型生成一系列步骤来完成给定的目标。我们在第一个支持18种语言的多语言脚本学习数据集上试验我们的任务，该数据集来自wikiHow，一个包含50万篇how-to文章的网站。对于基线，我们考虑使用基于语言的模型和基于检索的方法，首先从大的候选池中检索相关的步骤，然后对它们进行排序。我们表明，我们的任务是切实可行的，但对于最先进的变压器模型来说是具有挑战性的，并且我们的方法可以很容易地部署到各种其他数据集和领域，具有良好的零炮性能。</pre></li>
<li><a href="https://arxiv.org/abs/2009.06207">Contrastive Triple Extraction with Generative Transformer</a> (AAAI2021)
<pre>三重抽取是自然语言处理和知识图构造中信息抽取的一项重要任务。在本文中，我们回顾了用于序列生成的端到端三重提取任务。由于生成式三重抽取可能难以捕获长期依赖关系并生成不可靠的三重抽取，因此我们引入了一种新模型，即使用生成转换器的对比三重抽取。具体来说，我们介绍了一个用于基于编码器-解码器的生成的单一共享转换器模块。为了得到真实的结果，我们提出了一种新的三重对比训练对象。此外，我们还引入了两种机制来进一步提高模型性能（即批处理动态注意掩蔽和三重校准）。在三个数据集（即NYT、WebNLG和MIE）上的实验结果表明，我们的方法比基线方法获得了更好的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2009.06367">GeDi: Generative Discriminator Guided Sequence Generation</a>
<pre>虽然大规模语言模型（LMs）能够很好地模拟自然语言的分布以生成真实的文本，但很难控制它们生成的分布区域。这一问题尤其严重，因为用于训练大型LMs的数据集通常包含显著的毒性、仇恨、偏见和负面性。我们提出GeDi作为一种有效的方法，使用较小的LMs作为生成鉴别器来引导从较大的LMs生成，从而使它们更安全、更可控。GeDi通过规范化两类条件分布，通过贝叶斯规则计算所有可能的下一个令牌的分类概率，在每一步指导生成；一个以所需属性或控制代码为条件，另一个以不需要的属性或反控制代码为条件。我们发现，GeDi比最先进的方法具有更强的可控性，同时发电速度提高了30倍以上。此外，只对GeDi进行四个主题的培训，使我们能够从一个关键字中可控地生成新主题，从而解锁以前可控生成方法所不具备的新功能。最后，我们证明了GeDi可以使GPT-2（1.5B参数）在不牺牲语言质量的情况下显著降低毒性，使其成为目前最实用的解决大型语言模型问题的方法，同时保持快速生成速度。</pre></li>
<li><a href="https://arxiv.org/abs/2009.08942">Generating similes effortlessly like a Pro: A Style Transfer Approach for Simile Generation</a> (EMNLP2020)
<pre>从诗歌到故事，文学比喻是人类想象和交流的关键。比喻语言，如明喻，超越了简单的表达，给读者新的见解和灵感。在本文中，我们解决了明喻生成的问题。生成明喻需要正确理解两个概念之间属性的有效映射。为此，我们首先提出了一种方法，通过使用结构化常识知识将从Reddit收集的大量明喻转换为它们的字面对应物来自动构建平行语料库。然后，我们建议对字面明喻对上的预训练序列到序列模型BART~\cite{lewis2019bart}进行微调，以获得泛化性，这样我们就可以在给出字面句子的情况下生成新的明喻。实验表明，我们的方法生成了$88\%$新的明喻，这些明喻不与训练数据共享属性。对一组独立的文字陈述进行的人类评估表明，我们的模型生成的明喻比两位文学专家要好，两位人类的平均值分别为32.6%和41.3%，三个基线系统包括最近的隐喻生成模型{我们三条基线的平均值分别为82\%，63\%和68\%。}两两比较时的次数。\footnote{标题中的明喻是由我们的最佳模型生成的。输入：轻松生成明喻，输出：生成明喻\text{像专业人士一样}我们还展示了如何在机器生成的故事中用我们最好的模型中的明喻替换字面句子，从而提高唤起性，并导致更好地被人类法官接受。</pre></li>
<li><a href="https://arxiv.org/abs/1910.10683">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a> (JMLR2020) [<a href="https://github.com/google-research/text-to-text-transfer-transformer">github</a>]
<pre>迁移学习（Transfer learning）是自然语言处理（NLP）中的一种强大技术，它首先对数据丰富的任务进行预训练，然后再对下游任务进行微调。迁移学习的有效性带来了多种方法、方法和实践。在本文中，我们通过引入一个统一的框架，将所有基于文本的语言问题转换为文本到文本的格式，来探索NLP迁移学习技术的前景。我们的系统研究比较了几十项语言理解任务的训练前目标、体系结构、未标记数据集、迁移方法和其他因素。通过结合我们的探索和scale以及我们新的“庞大干净的爬网语料库”，我们在许多基准上取得了最先进的成果，包括摘要、问答、文本分类等。为了促进NLP迁移学习的未来工作，我们发布了我们的数据集、预先训练的模型和代码。</pre></li>
<li><a href="https://arxiv.org/abs/2010.11934">mT5: A massively multilingual pre-trained text-to-text transformer</a> (NAACL2021) [<a href="https://github.com/google-research/multilingual-t5">github</a>]
<pre>最近的“文本到文本传输转换器”（T5）利用统一的文本到文本格式和比例，在各种英语NLP任务中获得最先进的结果。在本文中，我们介绍了mT5，一个T5的多语言变体，它是在一个新的基于爬网的数据集上预先训练的，该数据集涵盖101种语言。我们详细介绍了mT5的设计和改进培训，并在许多多语言基准上展示了其最先进的性能。我们还描述了一种简单的技术，以防止在零镜头设置中出现“意外翻译”，即生成模型选择（部分）将其预测翻译成错误的语言。本工作中使用的所有代码和模型检查点都是公开的。</pre></li>
<li><a href="https://arxiv.org/abs/2106.02171">nmT5 – Is parallel data still relevant for pre-training massively multilingual language models?</a> (ACL2021)
<pre>最近，mT5——T5的大规模多语言版本——利用统一的文本到文本格式，在各种多语言NLP任务上获得最先进的结果。在本文中，我们调查了将并行数据纳入mT5预训练的影响。我们发现，多任务语言建模与目标，如机器翻译在训练前是一个简单的方法，以提高下游的多语言和跨语言任务的性能。然而，随着模型容量的增加，收益开始减少，这表明并行数据对于大型模型可能不那么重要。同时，即使在更大的模型尺寸下，我们发现使用并行数据进行预训练仍然在有限的标记数据区域中提供益处。</pre></li>
<li><a href="https://arxiv.org/abs/2104.08692">mT6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs</a>
<pre>多语言T5（mT5）在大量单语文本上预训练了一个序列到序列的模型，在许多跨语言任务中显示了有希望的结果。在本文中，我们改进了带翻译对的多语言文本到文本传输转换器（mT6）。具体来说，我们探讨了三种跨语言文本到文本的预训练任务，即机器翻译、翻译对跨度损坏和翻译跨度损坏。此外，我们提出了一个文本到文本预训练的部分非自回归目标。我们在八个多语言基准数据集上评估了这些方法，包括句子分类、命名实体识别、问答和摘要。实验结果表明，与mT5相比，mT6提高了跨语言迁移能力。</pre></li>
<li><a href="https://arxiv.org/abs/2004.14546">WT5?! Training Text-to-Text Models to Explain their Predictions</a>
<pre>神经网络最近在各种具有挑战性的自然语言处理（NLP）任务上取得了人类水平的性能，但众所周知，很难理解神经网络为什么会产生特定的预测。在本文中，我们利用Raffel et al.（2019）提出的文本到文本框架来训练语言模型，以便在预测的同时输出自然的文本解释。关键的是，这不需要修改损失函数或训练和解码过程——我们只需训练模型，在生成（自然文本）预测后输出解释。我们表明，这种方法不仅在可解释性基准上获得了最新的结果，而且允许从有限的一组标记解释中学习，并在数据集之间传递合理化能力。为了促进再现性和未来的工作，我们发布了用于训练模型的代码。</pre></li>
<li><a href="https://arxiv.org/abs/2104.07307">NT5?! Training T5 to Perform Numerical Reasoning</a> [<a href="https://github.com/lesterpjy/numeric-t5">github</a>]
<pre>文本数字推理（NRoT）提出了独特的挑战，但现有的培训前目标并未很好地解决这些挑战。我们探索了五种顺序训练计划，它们适用于NRoT的预训练T5模型。我们的最终模型改编自T5，但在对文本离散推理（DROP）数据集进行微调之前，对三个数据集进行了进一步的预训练，旨在加强NRoT和一般阅读理解所需的技能。这项训练将DROP调整后的F1成绩（专注于计算的分数）从45.90提高到70.83。我们的模型接近GenBERT（72.4），这是一个自定义的基于BERT的模型，使用相同的数据集，参数显著增加。我们表明，使用多个难度越来越大的数值推理数据集训练T5多任务框架，可以在分布式模块和符号模块之间不需要手动工程划分功能的情况下实现良好的DROP性能。</pre></li>
<li><a href="https://arxiv.org/abs/1910.13461">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a> (ACL2020)
<pre>我们提出了BART，一种用于预训练序列到序列模型的去噪自动编码器。BART的训练方法是：（1）使用任意噪声函数破坏文本，（2）学习一个模型来重建原始文本。它使用一个标准的基于转换器的神经机器翻译架构，尽管它很简单，可以被看作是概括伯特（由于双向编码器），GPT（与左到右解码器），以及许多其他更近的预训练方案。我们评估了许多去噪方法，通过随机洗牌原始句子的顺序和使用一种新的填充方案，其中文本的跨度被替换为单个掩码标记，找到了最佳性能。当对文本生成进行微调时，BART特别有效，但也适用于理解任务。它将RoBERTa的表现与GLUE和SQuAD上的可比训练资源相匹配，在一系列抽象对话、问答和总结任务上取得了最新成果，最多可获得6个胭脂。BART还为机器翻译提供了比反向翻译系统1.1 BLEU的增加，只有目标语言预培训。我们还报告了在BART框架内复制其他预训练方案的消融实验，以更好地衡量哪些因素对最终任务绩效影响最大。</pre></li>
<li><a href="https://arxiv.org/abs/2102.01672">The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics</a>
<pre>我们将介绍GEM，一个自然语言生成（NLG）的活基准，它的评估和度量。衡量NLG的进展依赖于不断发展的自动化指标、数据集和人类评估标准生态系统。由于这一不断变化的目标，新模型通常仍然基于以盎格鲁语为中心的不同语料库进行评估，这些语料库具有完善但有缺陷的指标。这种脱节使得识别当前模式的局限性和取得进展的机会具有挑战性。为了解决这一局限性，GEM提供了一个环境，在这个环境中，模型可以轻松地应用于一系列广泛的任务，并且可以测试评估策略。定期更新基准将有助于NLG研究变得更加多语言，并与模型一起发展挑战。本文是我们在ACL 2021车间组织共享任务的数据的描述，我们邀请整个NLG社区参与。</pre></li>
<li><a href="https://arxiv.org/abs/2109.01652">Finetuned Language Models Are Zero-Shot Learners</a> [<a href="https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html">blog</a>]
<pre>本文探讨了一种提高语言模型零炮学习能力的简单方法。我们展示了指令调优——对通过指令描述的一组任务进行精细调优的语言模型——在看不见的任务上显著提高了零炮性能。我们采用137B参数预训练语言模型，并在通过自然语言教学模板表达的60多个NLP任务上对其进行教学调整。我们对这个指令优化模型（我们称之为FLAN）进行评估，评估的是看不见的任务类型。FLAN大大提高了其未经修改的同类产品的性能，在我们评估的25项任务中，有20项超过了零炮175B GPT-3。FLAN甚至在ANLI、RTE、BoolQ、AI2-ARC、OpenbookQA和StoryCloze上的表现都大大超过了几杆GPT-3。消融研究表明，任务数量和模型规模是教学调整成功的关键因素。</pre></li>
<li><a href="https://arxiv.org/abs/2110.08207">Multitask Prompted Training Enables Zero-Shot Task Generalization</a>
<pre>大型语言模型最近被证明可以在不同的任务集上实现合理的零炮泛化。有人假设这是语言模型训练中内隐多任务学习的结果。零炮泛化能直接由显式多任务学习产生吗？为了大规模地测试这个问题，我们开发了一个系统，可以轻松地将一般自然语言任务映射为人类可读的提示形式。我们使用不同的自然语言转换一大组有监督的数据集，每个数据集都有多个提示。这些提示数据集允许对模型执行自然语言中指定的完全看不见的任务的能力进行基准测试。我们在此多任务混合上微调了一个预训练编码器-解码器模型，该模型涵盖了各种各样的任务。该模型在多个标准数据集上实现了强大的零炮性能，通常比其尺寸大16倍的模型表现更好。此外，我们的方法在大型基准测试中的一部分任务上获得了很好的性能，超过了6倍于其规模的模型。所有提示和经过培训的模型均可在github.com/bigscience workshop/promptsource/上获得。</pre></li>
<li><a href="https://arxiv.org/abs/2001.08210">Multilingual Denoising Pre-training for Neural Machine Translation</a>
<pre>本文证明了多语言去噪预训练可以在各种机器翻译（MT）任务中产生显著的性能增益。我们提出了mBART——一种使用BART目标在多种语言的大规模单语语料库上预先训练的序列对序列去噪自动编码器。mBART是最早通过去除多语言全文中的噪声来预训练完整序列到序列模型的方法之一，而以前的方法只关注编码器、解码器或重建部分文本。预训练一个完整的模型可以直接对其进行微调，以实现有监督（句子级和文档级）和无监督的机器翻译，而无需进行特定任务的修改。我们证明，添加mBART初始化在除最高资源设置外的所有设置中都会产生性能提升，包括低资源MT高达12个BLEU点，以及许多文档级别和无监督模型超过5个BLEU点。我们还表明，它还可以实现无双文本或不在训练前语料库中的新类型的语言对迁移，并对哪些因素对训练前的有效性贡献最大进行了广泛的分析。</pre></li>
<li><a href="https://arxiv.org/abs/2011.03877">Best Practices for Data-Efficient Modeling in NLG:How to Train Production-Ready Neural Models with Less Data</a> (COLING2020)
<pre>自然语言生成（Natural language generation，NLG）是会话系统中的一个重要组成部分，因为它能够形成正确、自然的文本响应。传统上，NLG组件是使用基于模板的解决方案部署的。尽管最近在研究界开发的神经网络解决方案已被证明提供了一些好处，但由于高延迟、正确性问题和高数据需求，此类基于模型的解决方案的部署一直具有挑战性。在本文中，我们提出了一些方法，帮助我们将会话系统中NLG的数据高效神经解决方案部署到生产环境中。我们描述了一系列采样和建模技术，以通过仅使用少量数据（否则将是必要的）的轻型神经网络模型实现生产质量，并对每种技术进行了彻底的比较。我们的结果表明，域复杂性决定了实现高数据效率的适当方法。最后，我们将从我们的实验结果中总结出的经验教训归纳成生产级NLG模型开发的最佳实践列表，并在一个简短的运行手册中介绍它们。重要的是，所有技术的最终产品都是小序列到序列模型（2Mb），我们可以在生产中可靠地部署这些模型。</pre></li>
<li><a href="https://arxiv.org/abs/2101.00190">Prefix-Tuning: Optimizing Continuous Prompts for Generation</a>
<pre>微调实际上是利用大型预训练语言模型来执行下游任务的方法。但是，它会修改所有语言模型参数，因此需要为每个任务存储完整副本。在本文中，我们提出了前缀调优，这是一种轻量级的自然语言生成任务微调替代方案，它使语言模型参数保持不变，但优化了一个小的连续任务特定向量（称为前缀）。前缀调优从提示中获得灵感，允许后续令牌关注此前缀，就好像它是“虚拟令牌”。我们将前缀调整应用于GPT-2以生成表到文本，并将前缀调整应用于BART以进行摘要。我们发现，通过只学习0.1\%的参数，前缀调整在完整数据设置中获得了可比的性能，在低数据设置中优于微调，并且可以更好地推断训练期间未看到主题的示例。</pre></li>
<li><a href="https://arxiv.org/abs/1911.06171">Unsupervised Pre-training for Natural Language Generation: A Literature Review</a>
<pre>近年来，无监督预训练在计算语言学领域越来越流行，这得益于它在促进自然语言理解（NLU）方面的惊人成功以及有效利用大规模未标记语料库的潜力。然而，尽管NLU取得了成功，但在自然语言生成（NLG）方面，无监督预训练的力量仅得到了部分挖掘。主要障碍源于NLG的一种特殊性质：文本通常是基于特定的上下文生成的，这可能因目标应用程序而异。因此，为NLU场景中的预培训设计通用体系结构是一件棘手的事情。此外，在学习目标任务时保留从预培训中学习到的知识也是一个非常重要的问题。这篇综述总结了最近通过无监督的预训练增强NLG系统的努力，特别侧重于促进将预训练模型集成到下游任务中的方法。根据它们处理上述障碍的方式，它们被分为基于架构的方法和基于策略的方法。还提供了讨论，以进一步深入了解这两条工作线之间的关系，一些信息丰富的经验现象，以及未来工作可以致力于的一些可能方向。</pre></li>
</ul>
<h1 id="quality-evaluator">Quality evaluator</h1>
<ul>
<li><a href="https://arxiv.org/abs/1904.09675">BERTScore: Evaluating Text Generation with BERT</a> (ICLR2020)
<pre>我们提出了BERTScore，一种用于文本生成的自动评估指标。与常见度量类似，BERTScore计算候选句子中每个标记与参考句子中每个标记的相似性分数。然而，我们使用上下文嵌入计算标记相似性，而不是精确匹配。我们使用363个机器翻译和图像字幕系统的输出进行评估。BERTScore更好地与人类判断相关联，并提供比现有指标更强的模型选择性能。最后，我们使用了一个对抗性的释义检测任务，以表明BERTScore与现有指标相比，对具有挑战性的示例更具鲁棒性。</pre></li>
<li><a href="https://arxiv.org/abs/2106.02208">BERTTune: Fine-Tuning Neural Machine Translation with BERTScore</a> (ACL2021)
<pre>神经机器翻译模型往往偏向于在训练过程中看到的有限翻译参考文献。为了修正这种形式的过度拟合，在本文中，我们基于最近提出的BERTScore评估指标，提出了一种新的训练目标来微调模型。BERTScore是一个基于上下文嵌入的评分函数，它克服了基于n-gram的度量标准（例如同义词、释义）的典型限制，允许将与参考不同但在上下文嵌入空间中相近的翻译视为基本正确。为了能够使用BERTScore作为训练目标，我们提出了三种生成软预测的方法，允许网络保持端到端完全可微。在四对不同语言对上进行的实验表明，当微调强基线时，BLEU分数提高了0.58 pp（3.28%），BERTScore分数提高了0.76 pp（0.98%）。</pre></li>
<li><a href="https://arxiv.org/abs/1907.12679">Machine Translation Evaluation with BERT Regressor</a>
<pre>我们介绍了使用BERT（来自变压器的双向编码器表示）（Devlin等人，2019）进行自动机器翻译评估的度量。WMT-2017度量共享任务数据集的实验结果表明，我们的度量在所有英语语言对的段级度量任务中达到了最先进的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2011.01536">TransQuest: Translation Quality Estimation with Cross-lingual Transformers</a> (COLING2020)
<pre>近年来，句子级质量评估（QE）领域取得了巨大进展，这主要是由于使用了基于神经的架构。然而，这些方法中的大多数只适用于他们接受培训的语言对，并且需要为新的语言对进行再培训。从技术角度来看，这一过程可能很困难，而且通常计算成本很高。在本文中，我们提出了一个简单的基于跨语言转换器的QE框架，并用它来实现和评估两种不同的神经结构。我们的评估表明，当对来自WMT的数据集进行训练时，所提出的方法比当前的开源质量评估框架取得了最先进的结果。此外，该框架在迁移学习环境中非常有用，尤其是在处理资源不足的语言时，使我们能够获得非常有竞争力的结果。</pre></li>
<li><a href="https://arxiv.org/abs/1909.00578">SumQE: a BERT-based Summary Quality Estimation Model</a> (EMNLP2019)
<pre>我们提出了一种新的基于BERT的摘要质量估计模型SumQE。该模型解决了语言质量方面的问题，这些问题只能通过基于内容的总结评估方法间接获取，而不涉及与人类参考文献的比较。SumQE与人类评级的相关性非常高，优于处理这些语言方面的简单模型。SumQE模型的预测可用于系统开发，并告知用户自动生成的摘要和其他类型生成文本的质量。</pre></li>
<li><a href="https://arxiv.org/abs/1909.02622">MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance</a> (EMNLP2019) [<a href="https://github.com/AIPHES/emnlp19-moverscore">github</a>]
<pre>稳健的评估指标对文本生成系统的开发有着深远的影响。一个理想的度量是基于语义而不是表面形式将系统输出与引用进行比较。在本文中，我们研究了编码系统和参考文本的策略，以设计一个与人类对文本质量的判断高度相关的度量。我们在许多文本生成任务上验证了我们的新指标，即MoverScore，这些任务包括摘要、机器翻译、图像字幕和数据到文本生成，其中输出由各种神经和非神经系统生成。我们的研究结果表明，结合上下文表示和距离度量的指标表现最好。这些指标还显示了跨任务的强大泛化能力。为了便于使用，我们将指标作为web服务提供。</pre></li>
<li><a href="https://arxiv.org/abs/2003.02738">BERT as a Teacher: Contextual Embeddings for Sequence-Level Reward</a>
<pre>在许多学习框架中，根据一组参考来衡量生成序列的质量是一个中心问题，无论是计算分数、分配奖励还是进行区分。尽管在模型架构方面取得了巨大的进步，但独立于参考数量的度量仍然基于n-gram估计。我们证明了底层操作，计数字和比较计数，可以提升到嵌入字和比较嵌入。对BERT嵌入的深入分析从经验上表明，上下文嵌入可以用来捕获所需的依赖关系，同时通过适当的修剪和平滑技术保持必要的可伸缩性。我们将无条件生成转化为强化学习问题，并表明在这种具有挑战性的环境中，我们的奖励函数确实比n-gram奖励提供了更有效的学习信号。</pre></li>
<li><a href="https://arxiv.org/abs/2108.08485">Language Model Augmented Relevance Score</a> (ACL2021)
<pre>虽然自动度量通常用于评估NLG系统，但它们通常与人类的判断关联性较差。较新的指标（如BERTScore）解决了以前的指标（如BLEU和ROUGE）中的许多弱点，这些指标依赖于n-gram匹配。然而，这些新的方法仍然是有限的，因为它们不考虑生成上下文，所以它们不能正确地奖励生成的文本是正确的，但偏离给定的引用。在本文中，我们提出了语言模型增强相关性评分（MARS），一种新的上下文感知NLG评估指标。火星利用现成的语言模型，在强化学习指导下，创建既考虑生成上下文又考虑可用的人类参考的扩充引用，然后将其用作附加得分的生成文本的引用。与三个常见NLG任务中的七个现有指标相比，MARS不仅实现了与人类参考判断的更高相关性，而且在更大程度上区分了形成良好的候选对象和对抗性样本。</pre></li>
<li><a href="https://arxiv.org/abs/2004.04696">BLEURT: Learning Robust Metrics for Text Generation</a> (ACL2020)
<pre>文本生成在过去几年中取得了重大进展。然而，由于最受欢迎的选择（如BLEU和ROUGE）可能与人类的判断关联性较差，因此评估指标落后。我们提出了BLEURT，这是一种基于BERT的学习评估指标，可以用几千个可能有偏差的训练示例来模拟人类的判断。我们方法的一个关键方面是一个新的预训练方案，它使用数百万个合成示例来帮助模型推广。BLEURT提供了过去三年WMT Metrics共享任务和WebNLG竞争数据集的最新结果。与普通的基于BERT的方法相比，即使在训练数据稀少且分布不均的情况下，它也能产生更好的结果。</pre></li>
<li><a href="https://arxiv.org/abs/2106.11520">BARTScore: Evaluating Generated Text as Text Generation</a> [<a href="https://github.com/neulab/BARTScore">github</a>]
<pre>各种各样的NLP应用程序，如机器翻译、摘要和对话，都涉及文本生成。这些应用程序面临的一个主要挑战是如何评估生成的文本是否真正流畅、准确或有效。在这项工作中，我们将生成文本的评估概念化为一个文本生成问题，使用预先训练的序列到序列模型进行建模。一般的想法是，当生成的文本更好时，经过训练将生成的文本转换成参考输出或源文本的模型将获得更高的分数。我们使用基于编码器-解码器的预训练模型BART将这一想法付诸实施，并提出了一种具有多种变体的度量BARTScore，可在无监督的情况下灵活应用于从不同角度（例如信息性、流利性或真实性）评估文本。BARTScore在概念上简单，在经验上有效。它可以在22个测试设置中的16个测试设置中超越现有的最高评分指标，涵盖16个数据集（例如，机器翻译、文本摘要）和7个不同角度（例如，信息性、真实性）的评估。计算BARTScore的代码可在https://github.com/neulab/BARTScore，我们已经发布了一个交互式的元评估排行榜http://explainaboard.nlpedia.ai/leaderboard/task-meval/ 在ExplainOnboard平台上，它允许我们以交互方式了解每个指标的优势、劣势和互补性。</pre></li>
<li><a href="https://arxiv.org/abs/1910.14659">Masked Language Model Scoring</a> (ACL2020)
<pre>预训练蒙面语言模型（MLM）需要对大多数NLP任务进行微调。相反，我们通过伪对数似然分数（PLL）对MLM进行开箱即用的评估，该分数是通过逐个屏蔽标记计算的。我们发现，PLL在各种任务中的表现都优于自回归语言模型（如GPT-2）的分数。通过重新审视ASR和NMT假设，RoBERTa将端到端LibriSpeech模型的WER相对降低了30%，在低资源翻译对的最新基线上加起来为+1.7 BLEU，并从域适应中进一步获益。我们将这一成功归因于PLL无监督的语言可接受性表达，没有从左到右的偏见，大大提高了GPT-2的分数（+岛效应10分，BLiMP中的NPI许可）。人们可以微调MLM，在不掩蔽的情况下给出分数，从而在单个推理过程中实现计算。总之，锁相环及其相关的伪困惑（PPPL）使得即插即用的使用越来越多的预训练传销；e、 例如，我们使用单一的跨语言模型来重新存储多种语言的翻译。我们发布了我们的语言模型评分库https://github.com/awslabs/mlm-scoring.</pre></li>
<li><a href="https://arxiv.org/abs/2012.12382">Simple-QE: Better Automatic Quality Estimation for Text Simplification</a>
<pre>文本简化系统生成的文本版本更容易为更广泛的受众理解。简化文本的质量通常使用与人类参考文献相比较的指标进行估计，这可能很难获得。我们提出了一个简单的QE模型，这是一个基于BERT的质量评估（QE）模型，该模型改编自先前的总结QE工作，并表明它与人类质量判断有很好的相关性。简单QE不需要人工参考，这使得模型在实际环境中非常有用，用户需要了解生成的简化的质量。我们还表明，我们可以采用这种方法来准确预测人类书面文本的复杂性。</pre></li>
</ul>
<h1 id="modification-multi-task-masking-strategy-etc.">Modification (multi-task, masking strategy, etc.)</h1>
<ul>
<li><a href="https://arxiv.org/abs/1901.11504">Multi-Task Deep Neural Networks for Natural Language Understanding</a> (ACL2019)
<pre>在本文中，我们提出了一种多任务深层神经网络（MT-DNN），用于跨多个自然语言理解（NLU）任务学习表征。MT-DNN不仅利用了大量跨任务数据，而且还受益于正则化效应，该效应导致更通用的表示，以适应新的任务和域。MT-DNN扩展了Liu等人（2015年）提出的模型，加入了预先训练的双向变压器语言模型，称为BERT（Devlin等人，2018年）。MT-DNN在十项NLU任务（包括SNLI、SciTail和九项GLUE任务中的八项）上获得了最新的结果，将GLUE基准推至82.7%（绝对改善2.2%）。我们还使用SNLI和SciTail数据集证明，MT-DNN学习的表示允许域自适应，域内标签明显少于预先训练的BERT表示。代码和预先培训的模型可在https://github.com/namisan/mt-dnn.</pre></li>
<li><a href="https://arxiv.org/abs/2002.07972">The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding</a>
<pre>我们介绍了MT-DNN，这是一个开源的自然语言理解（NLU）工具包，使研究人员和开发人员能够轻松地培训定制的深度学习模型。MT-DNN建立在Pytork和Transformers的基础上，旨在通过使用各种目标（分类、回归、结构化预测）和文本编码器（如RNN、BERT、RoBERTa、UniLM），为广泛的NLU任务提供快速定制。MT-DNN的一个独特功能是其内置支持使用对抗式多任务学习范式进行稳健和可转移的学习。为了实现高效的生产部署，MT-DNN支持多任务知识提取，这可以大幅压缩深层神经模型，而不会造成显著的性能下降。我们证明了MT-DNN在普通和生物医学领域的广泛NLU应用中的有效性。软件和预先培训的模型将在https://github.com/namisan/mt-dnn.</pre></li>
<li><a href="https://arxiv.org/abs/1902.02671">BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning</a> (ICML2019)
<pre>多任务学习在相关任务之间共享信息，有时减少所需参数的数量。GLUE基准中多个自然语言理解任务的最新成果之前使用了单个大型任务的迁移：使用BERT进行无监督预训练，其中为每个任务微调了单独的BERT模型。我们探讨了多任务方法，这些方法共享一个带有少量额外任务特定参数的伯特模型。使用新的适应模块、PAL或“投射注意层”，我们在GLUE基准上匹配单独微调模型的性能，参数大约减少7倍，并在识别文本蕴涵数据集上获得最先进的结果。</pre></li>
<li><a href="https://arxiv.org/abs/2009.03300">Measuring Massive Multitask Language Understanding</a> (ICLR2021) [<a href="https://github.com/hendrycks/test">github</a>]
<pre>我们提出了一个新的测试来衡量文本模型的多任务准确性。该测试涵盖57项任务，包括初等数学、美国历史、计算机科学、法律等。为了在测试中获得高精度，模型必须具备广泛的世界知识和解决问题的能力。我们发现，虽然最新的模型具有接近随机机会的准确性，但最大的GPT-3模型比随机机会平均提高了近20个百分点。然而，在57项任务中的每一项上，最好的模型在达到专家级精度之前仍然需要大量改进。模型也有不平衡的表现，常常不知道什么时候是错的。更糟糕的是，他们在一些重要的社会问题上，如道德和法律，仍然具有近乎随机的准确性。通过全面评估模型的学术和专业理解的广度和深度，我们的测试可以用于跨多个任务分析模型，并识别重要的缺陷。</pre></li>
<li><a href="https://arxiv.org/abs/2106.04489">Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks</a> (ACL2021)
<pre>最先进的参数有效微调方法依赖于在预训练语言模型的层之间引入适配器模块。但是，这些模块针对每个任务分别进行培训，因此无法在任务之间共享信息。在本文中，我们表明，我们可以通过使用共享超网络生成所有层和任务的适配器参数来学习这些参数，而共享超网络是以transformer模型中的任务、适配器位置和层id为条件的。这个参数高效的多任务学习框架允许我们通过超网络在任务之间共享知识，同时通过特定于任务的适配器使模型适应每个单独的任务，从而实现两个世界的最佳效果。在著名的GLUE基准测试上的实验表明，多任务学习的性能有所提高，而每个任务只添加了0.29%的参数。此外，我们还展示了在各种任务中的少数镜头域泛化方面的显著性能改进。我们的代码在https://github.com/rabeehk/hyperformer.</pre></li>
<li><a href="https://arxiv.org/abs/2004.05568">Pre-training Text Representations as Meta Learning</a>
<pre>训练前的文本表示最近被证明可以显著提高许多自然语言处理任务的最新水平。预培训的中心目标是学习对后续任务有用的文本表示。然而，现有的方法是通过最小化代理目标来优化的，例如语言建模的负对数可能性。在这项工作中，我们引入了一种学习算法，它直接优化了模型学习文本表示的能力，从而有效地学习下游任务。我们通过一系列元训练步骤证明了多任务预训练和模型不可知元学习之间存在内在联系。BERT中采用的标准多任务学习目标是我们的学习算法的一个特例，其中元训练的深度为零。我们在两种环境下研究了该问题：无监督预训练和具有不同预训练对象的监督预训练，以验证我们方法的通用性。实验结果表明，我们的算法对各种下游任务都有改进，并能更好地学习初始化。</pre></li>
<li><a href="https://arxiv.org/abs/1904.09286">Unifying Question Answering and Text Classification via Span Extraction</a>
<pre>即使像BERT这样的预先训练过的语言编码器在许多任务中共享，问答、文本分类和回归模型的输出层也有显著的不同。Span解码器通常用于问答、固定类、文本分类的分类层和回归任务的相似性评分层。我们表明，这种区别是不必要的，这三者可以统一为Span提取。在多个问答、文本分类和回归基准测试的辅助监督预训练、低数据和多任务学习实验中，统一的跨度提取方法可以获得优异或可比的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2004.12302">MATINF: A Jointly Labeled Large-Scale Dataset for Classification, Question Answering and Summarization</a> (ACL2020)
<pre>近年来，大规模数据集极大地促进了自然语言处理几乎所有领域的发展。然而，目前NLP中没有跨任务数据集，这阻碍了多任务学习的发展。我们提出了MATINF，这是第一个用于分类、问答和摘要的联合标记大规模数据集。MATINF包含107万个问答对，带有人类标记的类别和用户生成的问题描述。基于如此丰富的信息，MATINF适用于三大NLP任务，包括分类、问答和摘要。我们在MATINF上对现有方法和一个新的多任务基线进行了基准测试，以激励进一步的研究。我们对MATINF和其他数据集的综合比较和实验证明了MATINF的优点。</pre></li>
<li><a href="https://arxiv.org/abs/1905.07129">ERNIE: Enhanced Language Representation with Informative Entities</a> (ACL2019)
<pre>在大规模语料库上预训练的神经语言表示模型（如BERT）可以很好地从纯文本中捕获丰富的语义模式，并可以进行微调以持续改进各种自然语言处理任务的性能。然而，现有的预训练语言模型很少考虑结合知识图（KGS），它可以提供丰富的结构化知识事实，以更好地理解语言。我们认为KGs中的信息实体可以通过外部知识增强语言表达。在本文中，我们利用大规模文本语料库和KGs来训练一个增强的语言表示模型（ERNIE），该模型可以同时充分利用词汇、句法和知识信息。实验结果表明，ERNIE在各种知识驱动任务上都取得了显著的改进，同时在其他常见NLP任务上与最先进的模型BERT相当。本文的源代码可以从https://github.com/thunlp/ERNIE.</pre></li>
<li><a href="https://arxiv.org/abs/1904.09223">ERNIE: Enhanced Representation through Knowledge Integration</a>
<pre>我们提出了一种新的由知识增强的语言表示模型，称为ERNIE（通过知识集成增强表示）。受BERT掩蔽策略的启发，ERNIE旨在学习通过知识掩蔽策略增强的语言表示，包括实体级掩蔽和短语级掩蔽。实体级策略屏蔽了通常由多个单词组成的实体。短语级策略屏蔽了由多个单词组成的作为概念单元的整个短语。实验结果表明，ERNIE优于其他基线方法，在自然语言推理、语义相似度、命名实体识别、情感分析和问答等五项中文自然语言处理任务上取得了最新成果。我们还证明了ERNIE在完形填空测试中具有更强大的知识推理能力。</pre></li>
<li><a href="https://arxiv.org/abs/1907.12412">ERNIE 2.0: A Continual Pre-training Framework for Language Understanding</a> (AAAI2020)
<pre>最近，预训练模型在各种语言理解任务中取得了最新的成果，这表明大规模语料库的预训练可能在自然语言处理中起着至关重要的作用。目前的预训练程序通常侧重于训练模型，通过几个简单的任务来掌握单词或句子的共现情况。然而，除了共现之外，训练语料库中还存在其他有价值的词汇、句法和语义信息，如命名实体、语义贴近度和语篇关系。为了最大限度地从训练语料库中提取词汇、句法和语义信息，我们提出了一个名为ERNIE 2.0的持续训练前框架，该框架通过不断的多任务学习逐步构建和学习训练前任务。实验结果表明，ERNIE2.0在16项任务上优于BERT和XLNet，其中包括GLUE基准上的英语任务和汉语中的几个常见任务。源代码和预先培训的模型已在https://github.com/PaddlePaddle/ERNIE.</pre></li>
<li><a href="https://arxiv.org/abs/2107.02137">ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation</a>
<pre>预先训练的模型在各种自然语言处理（NLP）任务中取得了最先进的结果。最近的工作，如T5和GPT-3表明，扩大预先训练的语言模型可以提高它们的泛化能力。特别是，具有1750亿个参数的GPT-3模型显示了其强大的任务不可知的零炮/少炮学习能力。尽管取得了成功，但这些大规模模型都是在纯文本上训练的，没有引入语言知识和世界知识等知识。此外，大多数大型模型都是以自回归的方式进行训练的。因此，这种传统的微调方法在解决下游语言理解任务时表现出相对较弱的性能。为了解决上述问题，我们提出了一个统一的框架ernie3.0，用于大规模知识增强模型的预训练。它融合了自回归网络和自编码网络，使得训练后的模型可以很容易地适应自然语言理解和生成任务，具有零镜头学习、少量镜头学习或微调功能。我们在一个由纯文本和大规模知识图组成的4TB语料库上用100亿个参数训练了该模型。实证结果表明，该模型在54项中文NLP任务上优于最先进的模型，其英文版在SuperGLUE基准（2021年7月3日）上获得第一名，超过人类绩效+0.8%（90.6%对89.8%）。</pre></li>
<li><a href="https://arxiv.org/abs/2010.12148">ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding</a>
<pre>粗粒度的语言信息，如命名实体或短语，有助于在预训练中进行充分的表征学习。以前的工作主要集中在将BERT的掩蔽语言建模（MLM）的目标从掩蔽单个令牌扩展到n个令牌的连续序列。我们认为这种连续掩蔽方法忽略了对粗粒度语言信息的内部依赖和相互关系的建模。作为替代方案，我们提出了ERNIE-Gram，一种显式n-Gram掩蔽方法，以增强粗粒度信息与预训练的集成。在ERNIE-Gram中，n-Gram直接使用显式n-Gram恒等式而不是n个令牌的连续序列来屏蔽和预测。此外，ERNIE-Gram使用生成器模型对合理的n-Gram身份进行采样，作为可选的n-Gram掩码，并以粗粒度和细粒度方式对其进行预测，以实现全面的n-Gram预测和关系建模。我们对ERNIE Gram进行了中英文文本语料库的预训练，并对19项下游任务进行了微调。实验结果表明，ERNIE-Gram在很大程度上优于以前的预训练模型，如XLNet和RoBERTa，并与最先进的方法取得了相当的结果。源代码和预先培训的模型已在https://github.com/PaddlePaddle/ERNIE.</pre></li>
<li><a href="https://arxiv.org/abs/1906.08237">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a> (NeurIPS2019) [<a href="https://github.com/zihangdai/xlnet">github</a>]
<pre>基于自回归语言建模的预训练方法与基于自回归语言建模的预训练方法相比，基于自编码的预训练方法具有更好的性能。然而，依赖于使用掩码破坏输入，BERT忽略了掩码位置之间的依赖关系，并遭受了预训练微调差异。鉴于这些优点和缺点，我们提出了XLNet，这是一种广义自回归预训练方法，它（1）通过最大化因子分解顺序的所有排列的期望可能性来学习双向上下文，（2）由于其自回归公式，克服了BERT的局限性。此外，XLNet将Transformer XL（最先进的自回归模型）的思想集成到预训练中。从经验上看，在可比较的实验设置下，XLNet在20项任务上都优于BERT，通常相差很大，包括问答、自然语言推理、情绪分析和文档排名。</pre></li>
<li><a href="https://arxiv.org/abs/2004.09297">MPNet: Masked and Permuted Pre-training for Language Understanding</a>
<pre>BERT采用蒙面语言建模（MLM）进行预训练，是最成功的预训练模型之一。由于BERT忽略了预测标记之间的依赖关系，XLNet引入了置换语言建模（PLM）进行预训练来解决这个问题。然而，XLNet没有充分利用句子的位置信息，因此在预训练和微调之间存在位置差异。在本文中，我们提出了一种新的预训练方法MPNet，它继承了BERT和XLNet的优点，避免了它们的局限性。MPNet通过置换语言建模（与BERT中的MLM相比）利用预测标记之间的依赖关系，并将辅助位置信息作为输入，使模型看到完整的句子，从而减少位置差异（与XLNet中的PLM相比）。我们在一个大型数据集（超过160GB的文本语料库）上预先训练MPNet，并对各种下行任务（胶水、团队等）进行微调。实验结果表明，在相同的模型设置下，MPNet的性能大大优于MLM和PLM，并在这些任务上取得了比以前最先进的预训练方法（如BERT、XLNet、RoBERTa）更好的结果。代码和预先培训的模型可从以下网址获得：https://github.com/microsoft/MPNet.</pre></li>
<li><a href="https://arxiv.org/abs/1906.08101">Pre-Training with Whole Word Masking for Chinese BERT</a>
<pre>来自Transformers的双向编码器表示（BERT）在各种NLP任务中显示出惊人的改进。最近，一个升级版的BERT发布了全词屏蔽（WWM），它缓解了在预训练BERT时屏蔽部分词条标记的缺点。在本技术报告中，我们采用了中文文本中的整词掩蔽，即掩蔽整词而不是掩蔽汉字，这可能会给蒙面语言模型（MLM）预训练任务带来另一个挑战。所提出的模型在不同的NLP任务上进行了验证，从句子到文档，包括机器阅读理解（CMRC 2018、DRCD、CJRC）、自然语言推理（XNLI）、情感分类（CHNSTICORP）、句子对匹配（LCQMC、BQ语料库）和文档分类（THUCNews）。在这些数据集上的实验结果表明，全词掩蔽可以带来另一个显著的增益。此外，我们还检验了中国预训练模型的有效性：BERT、ERNIE、BERT wwm、BERT wwm ext、RoBERTa wwm ext和RoBERTa wwm ext。我们发布了所有经过预培训的模型：\url{https://github.com/ymcui/Chinese-BERT-wwm</pre></li>
<li><a href="https://arxiv.org/abs/1907.10529">SpanBERT: Improving Pre-training by Representing and Predicting Spans</a> (TACL2020) [<a href="https://github.com/facebookresearch/SpanBERT">github</a>]
<pre>我们介绍了SpanBERT，这是一种预训练方法，旨在更好地表示和预测文本的跨度。我们的方法通过（1）屏蔽连续的随机跨度，而不是随机令牌，以及（2）训练跨度边界表示来预测屏蔽跨度的整个内容，而不依赖其中的单个令牌表示，从而扩展了BERT。SpanBERT始终优于BERT和我们更好的调整基线，在范围选择任务（如问题回答和共指消解）方面有很大的收益。特别是，在与BERT large相同的训练数据和模型大小的情况下，我们的单一模型在1.1班和2.0班分别获得94.6%和88.7%的F1。我们还实现了OntoNotes共指消解任务（79.6\%F1）的最新发展，在TACRED关系提取基准测试中表现出色，甚至在GLUE方面也取得了进步。</pre></li>
<li><a href="https://arxiv.org/abs/2008.02496">ConvBERT: Improving BERT with Span-based Dynamic Convolution</a>
<pre>像BERT及其变体这样的预先训练的语言模型最近在各种自然语言理解任务中取得了令人印象深刻的性能。然而，BERT严重依赖于全局自我注意块，因此承受着巨大的内存占用和计算成本。虽然它的所有注意头都从全局的角度查询整个输入序列来生成注意图，但是我们观察到一些注意头只需要学习局部依赖，这意味着存在计算冗余。因此，我们提出了一种新的基于广度的动态卷积来代替这些自我注意头来直接建模局部依赖性。新的卷积头与其他自我注意头一起形成了一种新的混合注意块，在全局和局部上下文学习中都更有效。我们用这种混合注意设计装备了伯特，并建立了一个康文伯特模型。实验表明，在各种下游任务中，ConvBERT显著优于BERT及其变体，具有较低的训练成本和较少的模型参数。值得注意的是，ConvBERTbase模型的粘合分数达到了86.4分，比ELECTRAbase高0.7分，而使用的培训成本不到1/4。代码和预先培训的模型将发布。</pre></li>
<li><a href="https://arxiv.org/abs/2109.01819">Frustratingly Simple Pretraining Alternatives to Masked Language Modeling</a> (EMNLP2021) [<a href="https://github.com/gucci-j/light-transformer-emnlp2021">github</a>]
<pre>蒙面语言建模（MLM）是一种自监督的预训练目标，广泛应用于自然语言处理中，用于学习文本表示。MLM训练一个模型来预测在整个词汇表的多类设置中被[MASK]占位符替换的输入标记的随机样本。在预训练时，通常与传销一起使用令牌或序列级别的其他辅助目标，以提高下游性能（例如，下一句预测）。然而，到目前为止，还没有任何工作试图检验其他更简单的语言直观目标是否可以单独用作主要的训练前目标。在本文中，我们探索了五个简单的基于令牌级别分类任务的预训练目标，作为传销的替代。在GLUE和SQuAD上的实验结果表明，我们提出的方法与使用BERT-BASE架构的MLM具有相当或更好的性能。我们使用更小的模型进一步验证了我们的方法，结果表明，对具有41%的BERT-BASE参数的模型进行预训练，BERT-MEDIUM只会使我们的最佳目标的GLUE分数下降1%。</pre></li>
<li><a href="https://aclanthology.org/2020.findings-emnlp.425/">ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations</a> (EMNLP2020 Findings)</li>
<li><a href="https://arxiv.org/abs/2105.01279">ZEN 2.0: Continue Training and Adaption for N-gram Enhanced Text Encoders</a>
<pre>经过预训练的文本编码器在自然语言处理（NLP）中引起了人们的持续关注，并在不同的任务中表现出了获得有希望的结果的能力。最近的研究表明，外部自监督信号（或通过无监督学习提取的知识，如n-grams）有助于为理解语言（如汉语）提供有用的语义证据，从而相应地提高各种下游任务的性能。为了进一步增强编码器，在本文中，我们建议使用大量数据和先进的训练技术对n-gram增强编码器进行预训练。此外，我们尝试将编码器扩展到不同的语言以及不同的领域，在这些领域中，我们确认相同的体系结构适用于这些不同的环境，并且从跨语言和领域的NLP任务的长列表中观察到新的最先进性能。</pre></li>
<li><a href="https://arxiv.org/abs/2011.08539">MVP-BERT: Redesigning Vocabularies for Chinese BERT and Multi-Vocab Pretraining</a>
<pre>尽管预训练语言模型（PLM）的开发显著提高了各种中文自然语言处理（NLP）任务的性能，但这些中文PLM的词汇仍然是由基于汉字的Google Chinese Bert\cite{devlin2018bert}提供的。第二，蒙面语言模型预训练基于单一词汇，这限制了其下游任务的执行。在这项工作中，我们首先提出了一种新的方法，即emph{seg\\\ tok}，借助汉语分词（CWS）和子词标记形成汉语BERT的词汇。然后，我们提出了三个版本的多词汇预训练（MVP）来提高模型的表达能力。实验表明：（1）与基于字符的词汇相比，emph{seg\\\\\\\\\\\\\\\\\ tok}不仅提高了汉语PLMs在句子级任务上的表现，而且提高了效率；（b） MVP提高了PLM的下游性能，特别是它可以提高序列标记任务的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2004.08994">Adversarial Training for Large Neural Language Models</a>
<pre>泛化和鲁棒性都是设计机器学习方法的关键要求。对抗性训练可以增强健壮性，但过去的工作经常发现它会损害泛化能力。在自然语言处理（NLP）中，预训练大型神经语言模型（如BERT）在各种任务的泛化方面取得了令人印象深刻的成果，并从对抗性微调中得到了进一步改进。然而，这些模型仍然容易受到敌对攻击。在本文中，我们证明了对抗性预训练可以提高泛化能力和鲁棒性。我们提出了一种通用算法ALUM（大型神经语言模型的对抗训练），该算法通过在嵌入空间中应用最大对抗损失的扰动来调整训练目标。我们首次对所有阶段的对抗性训练进行了全面研究，包括从头开始的预备训练、在训练有素的模型上持续预备训练以及特定任务的微调。ALUM在常规和对抗场景中的NLP任务中都比BERT获得了巨大的收益。即使对于已经在非常大的文本语料库上接受过良好训练的模型，如RoBERTa，ALUM仍然可以从持续的预训练中获得显著的收益，而传统的非对抗性方法则不能。明矾可以进一步与特定任务的微调相结合，以获得额外的收益。明矾代码公开于https://github.com/namisan/mt-dnn.</pre></li>
<li><a href="https://aclanthology.org/2021.acl-long.164/">BERTAC: Enhancing Transformer-based Language Models with Adversarially Pretrained Convolutional Neural Networks</a> (ACL2021)</li>
<li><a href="https://arxiv.org/abs/2004.09733">Train No Evil: Selective Masking for Task-guided Pre-training</a>
<pre>最近，预训练语言模型大多遵循先训练后微调的范式，并在各种下游任务中取得了优异的性能。然而，由于预训练阶段通常是任务不可知的，而微调阶段通常受到监督数据不足的影响，因此模型不能很好地捕获特定于领域和特定于任务的模式。在本文中，我们提出了一个三阶段的框架，通过在一般预训练和微调之间添加一个具有选择性掩蔽的任务引导预训练阶段。在这一阶段，通过对域内无监督数据进行蒙蔽语言建模来训练模型，以学习特定于域的模式，我们提出了一种新的选择性蒙蔽策略来学习特定于任务的模式。具体来说，我们设计了一种方法来测量序列中每个令牌的重要性，并选择性地屏蔽重要令牌。在两个情绪分析任务上的实验结果表明，我们的方法在计算量不到50%的情况下可以达到相当甚至更好的性能，这表明我们的方法是有效和高效的。本文的源代码可以从https://github.com/thunlp/SelectiveMasking.</pre></li>
<li><a href="https://arxiv.org/abs/2006.05676">Position Masking for Language Models</a>
<pre>蒙面语言建模（MLM）预训练模型（如BERT）通过使用[MASK]替换某些标记来破坏输入，然后训练模型以重建原始标记。这是一种有效的技术，在所有NLP基准测试中都取得了良好的结果。我们建议通过掩蔽一些令牌的位置以及掩蔽的输入令牌ID来扩展这个想法。我们遵循与BERT相同的标准方法，掩蔽标记位置的百分比，然后使用额外的完全连接分类器阶段预测其原始值。这种方法显示了良好的性能提升（.3\%的改进），从而在收敛时间上获得了额外的改进。对于Graphcore IPU，具有位置掩蔽的BERT基的收敛只需要原始BERT论文中50%的标记。</pre></li>
<li><a href="https://arxiv.org/abs/2004.12406">Masking as an Efficient Alternative to Finetuning for Pretrained Language Models</a> (EMNLP2020)
<pre>我们提出了一种利用预训练语言模型的有效方法，其中我们学习用于预训练权重的选择性二进制掩码，而不是通过微调来修改它们。对一系列NLP任务的掩蔽BERT和RoBERTa的广泛评估表明，我们的掩蔽方案产生的性能与微调相当，但当需要同时推断多个任务时，其内存占用要小得多。通过内在评估，我们证明了由屏蔽语言模型计算的表示编码了解决下游任务所需的信息。通过分析损失情况，我们发现掩蔽和微调产生的模型位于极小值，可以通过线段连接，几乎保持恒定的测试精度。这证实了掩蔽可以作为精细调谐的有效替代品。</pre></li>
<li><a href="https://arxiv.org/abs/2008.05333">Variance-reduced Language Pretraining via a Mask Proposal Network</a>
<pre>自我监督学习，又称预训练，在自然语言处理中非常重要。大多数预训练方法首先随机屏蔽句子中的某些位置，然后训练一个模型来恢复被屏蔽位置的标记。通过这种方式，模型可以在没有人为标记的情况下进行训练，并且海量数据可以使用数十亿个参数。因此，优化效率变得至关重要。本文从梯度方差约简的角度来解决这一问题。特别是，我们首先提出了一个有原则的梯度方差分解定理，它表明语言训练前的随机梯度方差可以自然地分解为两项：批量数据样本产生的方差和掩码采样产生的方差。第二个术语是自我监督学习和监督学习之间的关键区别，这使得预训练速度变慢。为了减少第二部分的方差，我们采用了重要性抽样策略，其目的是根据建议分布而不是均匀分布对掩码进行抽样。可以证明，如果建议分布与梯度范数成正比，则抽样方差减小。为了提高效率，我们引入了一个掩码建议网络（MAPNet），该网络近似于最佳掩码建议分布，并与模型一起进行端到端的训练。实验结果表明，该模型收敛速度快，性能优于基准的BERT模型。</pre></li>
<li><a href="https://arxiv.org/abs/2010.02705">Neural Mask Generator: Learning to Generate Adaptive Word Maskings for Language Model Adaptation</a> (EMNLP2020)
<pre>我们提出了一种自动生成给定文本的域和任务自适应掩码的方法，用于自我监督的预训练，这样我们可以有效地使语言模型适应特定的目标任务（例如问答）。具体地说，我们提出了一种新的基于强化学习的框架，该框架学习掩蔽策略，从而使用生成的掩蔽对目标语言模型进行进一步的预训练，有助于提高在看不见文本上的任务性能。我们使用Office策略演员评论家的熵正则化和经验重演的强化学习，并提出了一个基于变压器的政策网络，可以考虑在给定文本中的单词的相对重要性。我们使用BERT和DistilBERT作为语言模型，在几个问答和文本分类数据集上验证了我们的神经掩码生成器（NMG），通过自动学习最佳自适应掩码，它的性能优于基于规则的掩码策略。</pre></li>
<li><a href="https://arxiv.org/abs/2010.06040">Improving Self-supervised Pre-training via a Fully-Explored Masked Language Model</a>
<pre>蒙面语言模型（MLM）框架已被广泛用于自我监督语言预训练。在本文中，我们认为，随机抽样面具传销会导致不必要的大梯度方差。因此，我们在理论上通过将梯度协方差与两个不同遮罩（给定特定文本序列）之间的汉明距离相关联来量化梯度方差。为了减少由于掩模采样而产生的方差，我们提出了一种充分探索的掩模策略，将文本序列划分为一定数量的非重叠段。此后，一个段内的令牌被屏蔽以进行训练。我们从理论的角度证明，从这种新的掩蔽模式得到的梯度具有较小的方差，并且可以导致更有效的自我监督训练。我们从零开始对持续预培训和一般预培训进行了广泛的实验。实证结果证实，这种新的掩蔽策略可以始终优于标准随机掩蔽。详细的效率分析和烧蚀研究进一步验证了我们在传销框架下充分探索的掩蔽策略的优势。</pre></li>
<li><a href="https://arxiv.org/abs/2204.04163">Contextual Representation Learning beyond Masked Language Modeling</a> (ACL2022)
<pre>像伯特这样的蒙面语言模型（MLM）是如何学习上下文表示的？在这项工作中，我们分析了传销的学习动态。我们发现，MLM采用抽样嵌入作为锚来估计和向表示注入上下文语义，这限制了MLM的效率和有效性。为了解决这些问题，我们提出了TACO，一种简单而有效的表示学习方法，用于直接建模全局语义。TACO提取并对齐隐藏在上下文化表示中的上下文语义，以鼓励模型在生成上下文化表示时加入全局语义。在GLUE基准上的实验表明，与现有MLM相比，TACO实现了高达5倍的加速，平均提高了1.2个点。代码位于https://github.com/FUZHIYI/TACO.</pre></li>
<li><a href="https://arxiv.org/abs/2108.02170">Curriculum learning for language modeling</a>
<pre>ELMo和BERT等语言模型提供了自然语言的稳健表示，自然语言作为各种下游任务的语言理解组件。课程学习是一种采用结构化训练机制的方法，它已被用于计算机视觉和机器翻译，以提高模型训练速度和模型性能。虽然语言模型已被证明对自然语言处理社区具有变革性，但这些模型已被证明成本高昂、能耗高、培训难度大。在这项工作中，我们探讨了课程学习对语言模式训练的影响，使用不同的语言动机课程，并在GLUE基准上评估迁移绩效。尽管有各种各样的培训方法和实验，我们没有发现令人信服的证据表明课程学习方法可以改善语言模型培训。</pre></li>
<li><a href="https://arxiv.org/abs/2108.06084">Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training</a>
<pre>最近的工作表明，在大量未标记文本语料库上训练大容量自回归语言模型（GPT，GPT-2，GPT-3）以生成文本是非常成功的。尽管取得了很好的结果，但自回归模型面临着越来越严重的训练不稳定性问题。我们对GPT-2模型（117M和1.5B参数）的研究表明，较大的模型大小、序列长度、批量大小和学习率将导致较低的训练稳定性和增加分歧风险。为了避免发散和获得更好的泛化性能，必须使用较小的批量和学习率进行训练，这会导致训练效率降低和训练时间延长。为了克服这种稳定性-效率困境，我们提出了一种基于课程学习的方法，这有助于提高自回归模型的训练前收敛速度。更重要的是，我们发现课程学习作为一种正则化方法，具有梯度方差缩减效应，能够训练具有更大批量和学习率的自回归模型，而不存在训练不稳定性，进一步提高了训练速度。我们的评估表明，课程学习使GPT-2模型的培训批量增加了8倍，学习率提高了4倍，而基线方法则难以解决培训分歧。为了在预培训期间实现相同的验证困惑目标，课程学习将所需的代币数量和挂钟时间分别减少了61%和49%。为了在预培训结束时获得相同或更好的零拍WikiText-103/LAMBADA评估结果，课程学习将所需的代币数量和挂钟时间分别减少54%和70%。</pre></li>
<li><a href="https://arxiv.org/abs/2012.08789">Focusing More on Conflicts with Mis-Predictions Helps Language Pre-Training</a>
<pre>在这项工作中，我们建议通过训练前的错误预测来提高语言训练前方法的有效性。忽略输入句子中与错误预测语义冲突的单词可能是在预训练时产生错误预测的原因。因此，我们假设训练前的错误预测可以作为模型病态焦点的检测器。如果我们训练模型更多地关注与错误预测的冲突，而较少关注输入句子中的其余单词，那么错误预测可以更容易地得到纠正，整个模型也可以得到更好的训练。为此，我们将介绍较少关注错误预测上下文（McMisP）。在McMisP中，我们记录单词之间的共现信息，以无监督的方式检测预测错误的冲突单词。然后，当发生错误预测时，McMisP使用这些信息指导注意模块。具体地说，变压器中的几个注意模块经过优化，以便更多地关注输入句子中很少与错误预测同时出现的单词，反之亦然。结果表明，McMisP显著加速了BERT和ELECTRA，提高了它们在下游任务中的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2001.07676">Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference</a> (EACL2021) [<a href="https://github.com/timoschick/pet">github</a>]
<pre>一些NLP任务可以完全无监督的方式解决，方法是提供一个预训练的语言模型，其中包含自然语言中的“任务描述”（例如Radford等人，2019）。虽然这种方法的表现不如有监督的方法，但我们在这项工作中表明，这两种思想可以结合起来：我们引入了模式利用训练（PET），这是一种半监督的训练过程，将输入示例重新组织为完形填空式短语，以帮助语言模型理解给定的任务。然后使用这些短语为大量未标记的示例指定软标签。最后，对生成的训练集执行标准监督训练。对于一些任务和语言，PET在低资源环境下的性能大大优于监督训练和强半监督方法。</pre></li>
<li><a href="https://arxiv.org/abs/2009.07118">It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners</a> (NAACL2021) [<a href="https://github.com/timoschick/pet">github</a>]
<pre>当扩展到数千亿个参数时，GPT-3（Brown等人，2020）等预训练语言模型实现了显著的少镜头性能。然而，训练和应用这样大的模型需要大量的计算，这导致了巨大的碳足迹，使得研究人员和实践者很难使用它们。我们表明，与GPT-3类似的性能可以通过更“绿色”的语言模型获得，因为它们的参数计数要小几个数量级。这是通过将文本输入转换成包含任务描述的完形填空问题，并结合基于梯度的优化来实现的；利用未标记的数据提供了进一步的改进。我们通过小型语言模型确定了成功理解自然语言所需的关键因素。</pre></li>
<li><a href="https://arxiv.org/abs/2012.15723">Making Pre-trained Language Models Better Few-shot Learners</a> (ACL2021) [<a href="https://github.com/princeton-nlp/LM-BFF">github</a>]
<pre>最近的GPT-3模型（Brown et al.，2020）仅通过利用自然语言提示和一些任务演示作为输入上下文，就实现了显著的少镜头性能。受他们发现的启发，我们在更实际的场景中研究了少数镜头学习，在这种场景中，我们使用更小的语言模型，微调在计算上是有效的。我们介绍了LM-BFF——更好的语言模型微调——这是一套简单且互补的技术，用于在少量带注释的示例上微调语言模型。我们的方法包括：（1）基于提示的微调，以及一种用于自动生成提示的新管道；（2）一种改进的策略，用于动态地、有选择地将演示合并到每个上下文中。最后，我们提出了一个系统的评估，用于分析一系列NLP任务的少数镜头性能，包括分类和回归。我们的实验表明，在这种低资源环境下，我们的方法组合起来显著优于标准微调过程，实现了高达30%的绝对改善，所有任务的平均改善率为11%。我们的方法对任务资源和领域专业知识进行了最少的假设，因此构成了一种用于少量镜头学习的强大的任务不可知方法。</pre></li>
<li><a href="https://arxiv.org/abs/2104.08835">CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP</a>
<pre>人类可以利用在学习之前的任务时获得的知识，通过很少的例子有效地学习新的语言任务。在本文中，我们探讨了这种跨任务泛化能力是否可以获得，以及如何获得，并进一步应用于在不同NLP任务中构建更好的少镜头学习者。我们介绍了CrossFit，一个用于研究跨任务泛化能力的问题设置，它标准化了可见/不可见的任务划分、不同学习阶段的数据访问以及评估协议。为了在CrossFit中实例化不同的可见/不可见任务分区并促进深入分析，我们介绍了NLP少数镜头健身房，它是一个由160个不同的少数镜头NLP任务组成的存储库，这些任务是从开放存取NLP数据集创建的，并转换为统一的文本到文本格式。我们的分析表明，通过使用一组可见任务的上游学习阶段，可以提高对不可见任务的少量快照学习能力。我们还观察到，上游学习任务的选择会显著影响不可见任务的少数镜头表现，要求进一步分析任务相似性和可转移性。</pre></li>
<li><a href="https://arxiv.org/abs/2104.08808">Lifelong Learning of Few-shot Learners across NLP Tasks</a>
<pre>随着时间的推移不断扩展知识并利用知识快速概括到新任务的能力是人类语言智能的一个关键特征。然而，追求对新任务快速概括的现有模型（例如，很少有镜头学习方法）大多在固定数据集上进行单镜头训练，无法动态扩展其知识；而连续学习算法并不是专门为快速泛化而设计的。我们提出了一种新的学习设置，即少数镜头学习者的持续学习（CLIF），以解决统一设置中两种学习设置的挑战。CLIF假设模型从顺序到达的不同NLP任务序列中学习，积累知识以改进对新任务的泛化，同时保持先前学习的任务的性能。我们研究了在连续学习环境中泛化能力是如何受到影响的，评估了许多连续学习算法，并提出了一种新的正则化适配器生成方法。我们发现，灾难性遗忘对泛化能力的影响程度小于对所见任务的影响程度；而连续学习算法仍然可以给泛化能力带来相当大的好处。</pre></li>
<li><a href="https://arxiv.org/abs/2004.10964">Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks</a> (ACL2020)
<pre>在各种来源的文本上预先训练的语言模型是当今NLP的基础。鉴于这些广泛覆盖模型的成功，我们研究了根据目标任务领域定制预训练模型是否仍然有用。我们提出了一项跨四个领域（生物医学和计算机科学出版物、新闻和评论）和八项分类任务的研究，表明在高资源和低资源环境下，领域预培训的第二阶段（领域自适应预培训）会带来性能提升。此外，即使在域自适应预训练之后，适应任务的未标记数据（任务自适应预训练）也可以提高性能。最后，我们表明，适应使用简单数据选择策略扩充的任务语料库是一种有效的选择，尤其是在领域自适应预训练资源可能不可用的情况下。总的来说，我们一致发现多阶段自适应预训练在任务绩效方面有很大的提高。</pre></li>
<li><a href="https://arxiv.org/abs/2010.00784">An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training</a> [<a href="https://github.com/aws-health-ai/multi_domain_lm">github</a>]
<pre>预训练大型语言模型已经成为自然语言处理社区的标准。这类模型是在一般数据（如图书语料库和英文维基百科）上预先训练的，并且经常在同一领域的任务上进行微调。然而，为了在领域外任务（如临床命名实体识别和关系提取）上实现最先进的性能，需要额外的领域内预培训。在实践中，分阶段的多域预训练在使用通用基准（如GLUE）进行评估时，表现为灾难性遗忘（CF）。在本文中，我们对缓解CF的已知方法进行了实证研究。我们发现，弹性权重整合提供了最好的总体得分，在七项一般任务中，绩效仅下降0.33%，同时在生物医学任务中保持竞争力。此外，我们还探索了基于梯度和潜在聚类的数据选择技术，以提高使用弹性权重合并和经验重放方法时的覆盖率。</pre></li>
<li><a href="https://arxiv.org/abs/2006.08671">To Pretrain or Not to Pretrain: Examining the Benefits of Pretraining on Resource Rich Tasks</a> (ACL2020)
<pre>使用蒙面语言模型（MLM）目标的变体对NLP模型进行预训练，最近在许多任务上取得了显著的改进。本文考察了作为下游任务中使用的训练样本数量函数的预训练模型的好处。在几个文本分类任务中，我们发现，随着训练示例的数量增加到数百万，基于finetunning-BERT的模型和训练vanilla-LSTM之间的精度差距从无到有地缩小到1%以内。我们的研究结果表明，基于传销的模型可能会达到一个递减的回报点，因为监督数据的大小显著增加。</pre></li>
<li><a href="https://arxiv.org/abs/2006.05987">Revisiting Few-sample BERT Fine-tuning</a>
<pre>本文是一项微调伯特上下文表示的研究，重点是在少数样本场景中观察到的常见不稳定性。我们确定了导致这种不稳定性的几个因素：通常使用带有偏差梯度估计的非标准优化方法；BERT网络的重要部分对下游任务的适用性有限；以及使用预先确定的、少量训练迭代的流行实践。我们对这些因素的影响进行了实证测试，并确定了解决过程中常见的不稳定性的替代做法。鉴于这些观察结果，我们再次访问最近提出的方法，以改进使用BERT的少数样本微调，并重新评估其有效性。一般来说，我们观察到这些方法的影响随着我们改进的过程而显著降低。</pre></li>
<li><a href="https://arxiv.org/abs/2002.03079">Blank Language Models</a>
<pre>我们提出了空白语言模型（BLM），一种通过动态创建和填充空白来生成序列的模型。空格控制序列的哪一部分要展开，使BLM成为各种文本编辑和重写任务的理想选择。模型可以从单个空白文本开始，也可以从指定位置的空白部分开始。它迭代地确定要在空格中放置哪个单词以及是否插入新空格，并在没有空格需要填充时停止生成。BLM可以使用边际数据可能性的下限进行有效训练。在填补缺失文本片段的任务中，BLM在准确性和流利性方面明显优于所有其他基线。对风格转换和受损古文本恢复的实验表明，该框架具有广泛的应用潜力。</pre></li>
<li><a href="https://arxiv.org/abs/2005.05339">Enabling Language Models to Fill in the Blanks</a> (ACL2020)
<pre>我们提出了一种简单的文本填充方法，即预测文档中任何位置的文本缺失跨度。虽然填充可以实现丰富的功能，特别是对于写作辅助工具，但更多的注意力集中在语言建模上——一种填充的特例，其中文本在文档末尾被预测。在本文中，我们的目标是将语言模型（LMs）的功能扩展到更一般的填充任务。为此，我们在包含人工屏蔽文本和被屏蔽文本的串联的序列上训练（或微调）现成的LMs。我们证明了这种方法，我们称之为通过语言建模填充，可以使LMs在三个不同的领域有效地填充整个句子：短篇小说、科学摘要和歌词。此外，我们还表明，人类很难将我们的方法填充的句子识别为短篇小说领域中机器生成的句子。</pre></li>
<li><a href="http://proceedings.mlr.press/v97/gong19a.html">Efficient Training of BERT by Progressively Stacking</a> (ICML2019) [<a href="https://github.com/gonglinyuan/StackingBERT">github</a>]</li>
<li><a href="https://arxiv.org/abs/1907.11692">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a> [<a href="https://github.com/pytorch/fairseq/tree/master/examples/roberta">github</a>]
<pre>语言模型预训练带来了显著的性能提升，但仔细比较不同方法是一项挑战。训练在计算上是昂贵的，通常在不同大小的私有数据集上进行，并且，正如我们将要展示的，超参数的选择对最终结果有重大影响。我们提出了一项伯特预训练的复制研究（Devlin等人，2019年），该研究仔细测量了许多关键超参数和训练数据大小的影响。我们发现，BERT的训练明显不足，可以匹配或超过发布后的每个模型的性能。我们最好的车型在胶水、比赛和阵容方面都达到了最先进的水平。这些结果突出了以前被忽视的设计选择的重要性，并对最近报告的改进来源提出了疑问。我们发布了我们的模型和代码。</pre></li>
<li><a href="https://arxiv.org/abs/2010.01694">On Losses for Modern Language Models</a> (EMNLP2020) [<a href="https://github.com/StephAO/olfmlm">github</a>]
<pre>伯特通过两项任务的预训练，在不同的NLU基准上取得了许多最新成果：蒙面语言建模（MLM）和下一句预测（NSP），后者受到了高度批评。在本文中，我们1）阐明NSP对BERT预训练的影响，2）探索14项可能的辅助预训练任务，其中7项是现代语言模型中的新任务，3）研究将多个任务纳入预训练的不同方式。我们发现，NSP由于其上下文分裂和浅层语义信号，不利于训练。我们还确定了六项辅助训练前任务——句子排序、相邻句子预测、TF预测、TF-IDF预测、FastSent变体和Quick Thinks变体——它们的表现优于纯粹的传销基线。最后，我们证明了在多任务预训练框架中使用多个任务比使用任何单个辅助任务提供更好的结果。使用这些方法，我们在使用不到四分之一的训练标记的GLUE基准测试上优于BERT Base。</pre></li>
<li><a href="https://arxiv.org/abs/1909.11942">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</a> (ICLR2020)
<pre>在对自然语言表示进行预训练时，增加模型大小通常会提高下游任务的性能。然而，由于GPU/TPU内存限制和更长的训练时间，在某些情况下，进一步增加模型变得更加困难。为了解决这些问题，我们提出了两种参数缩减技术来降低内存消耗和提高训练速度。综合的经验证据表明，我们提出的方法导致模型的规模比原始的BERT好得多。我们还使用了一种自我监督损失模型，该模型侧重于句子间连贯性的建模，并表明它对多句子输入的下游任务具有一致的帮助。因此，我们的最佳模型在GLUE、RACE和\squad基准测试上建立了新的最先进的结果，同时与BERT相比，它的参数更少。代码和预训练模型可在https://github.com/google-research/ALBERT.</pre></li>
<li><a href="https://openreview.net/forum?id=xpFFI_NtgpW">Rethinking Embedding Coupling in Pre-trained Language Models</a> (ICLR2021)</li>
<li><a href="https://openreview.net/forum?id=r1xMH1BtvB">ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</a> (ICLR2020) [<a href="https://github.com/google-research/electra">github</a>] [<a href="https://ai.googleblog.com/2020/03/more-efficient-nlp-model-pre-training.html">blog</a>]</li>
<li><a href="https://arxiv.org/abs/2106.00139">Training ELECTRA Augmented with Multi-word Selection</a> (ACL2021 Findings)
<pre>经过预先训练的文本编码器，如BERT及其变体，最近在许多NLP任务中取得了最先进的性能。这些预训练方法虽然有效，但通常需要大量计算资源。为了加速预训练，ELECTRA训练了一个鉴别器，该鉴别器预测每个输入令牌是否被生成器替换。然而，这个新任务作为一个二元分类，语义信息较少。在这项研究中，我们提出了一种新的文本编码器预训练方法，改进了基于多任务学习的ELECTRA。具体来说，我们训练鉴别器同时检测替换的令牌并从候选集中选择原始令牌。我们进一步开发了两种技术来有效地结合所有训练前任务：（1）使用基于注意的网络来处理特定任务的头部；（2）共享生成器和鉴别器的底层。在GLUE和SQuAD数据集上的大量实验证明了我们提出的方法的有效性和效率。</pre></li>
<li><a href="https://arxiv.org/abs/2106.13715">Learning to Sample Replacements for ELECTRA Pre-Training</a> (ACL2021 Findings)
<pre>ELECTRA预先训练一个鉴别器来检测替换的令牌，替换的令牌从一个经过蒙面语言建模训练的生成器中采样。尽管表现令人信服，ELECTRA仍面临以下两个问题。首先，鉴别器和发生器之间没有直接反馈回路，这使得替换采样效率低下。第二，生成器的预测往往过于自信，并伴随着训练，使得替换偏向于正确的标记。在本文中，我们提出了两种方法来改进ELECTRA预训练的替换抽样。具体地说，我们使用硬度预测机制来增加采样，以便生成器可以鼓励鉴别器学习它未获取的内容。我们还证明了有效抽样减少了鉴别器的训练方差。此外，我们建议为发生器使用焦点损耗，以减轻作为替换的正确令牌的过采样。实验结果表明，我们的方法提高了ELECTRA对各种下游任务的预训练。</pre></li>
<li><a href="https://aclanthology.org/2021.naacl-main.409/">SCRIPT: Self-Critic PreTraining of Transformers</a> (NAACL2021)</li>
<li><a href="https://www.aclweb.org/anthology/2020.emnlp-main.20/">Pre-Training Transformers as Energy-Based Cloze Models</a> (EMNLP2020) [<a href="https://github.com/google-research/electra">github</a>]</li>
<li><a href="https://arxiv.org/abs/2006.05744">MC-BERT: Efficient Language Pre-Training via a Meta Controller</a>
<pre>预先训练的上下文表示（例如，伯特）已经成为实现许多NLP任务的最先进结果的基础。然而，大规模的预训练在计算上是昂贵的。ELECTRA是加速预训练的早期尝试，它训练了一个判别模型，预测每个输入令牌是否被生成器替换。我们的研究表明，ELECTRA的成功主要是因为它降低了预训练任务的复杂性：二元分类（替换标记检测）比生成任务（掩蔽语言建模）学习效率更高。然而，这样一个简化的任务在语义上的信息较少。为了获得更好的效率和效果，我们提出了一种新的元学习框架MC-BERT。训练前任务是一个带有拒绝选项的多项选择完形填空测试，其中元控制器网络提供训练输入和候选人。通过GLUE自然语言理解基准测试的结果表明，我们提出的方法是高效的：在相同的计算预算下，它在GLUE语义任务上的性能优于基线。</pre></li>
<li><a href="https://openreview.net/forum?id=BygzbyHFvB">FreeLB: Enhanced Adversarial Training for Language Understanding</a> (ICLR2020)</li>
<li><a href="https://arxiv.org/abs/1906.01604">KERMIT: Generative Insertion-Based Modeling for Sequences</a>
<pre>我们介绍了KERMIT，一种简单的基于插入的序列和序列对生成建模方法。KERMIT使用单个神经网络对联合分布及其分解（即边缘和条件）进行建模，与之前的许多工作不同，它不依赖于数据分布的预先指定因子分解。在培训期间，您可以输入KERMIT配对数据$（x，y）$以学习联合分布$p（x，y）$，并可以选择混合未配对数据$x$或$y$以细化边缘$p（x）$或$p（y）$。在推断过程中，我们可以在两个方向上访问条件$p（x\mid y）$和$p（y\mid x）$。我们也可以从联合分布或边际分布中取样。该模型支持串行完全自回归解码和并行部分自回归解码，后者表现为经验对数运行时间。我们通过机器翻译、表征学习和零镜头完形填空问答的实验证明，我们的统一方法能够在广泛的任务范围内匹配或超过专用最先进系统的性能，而无需特定于问题的体系结构调整。</pre></li>
<li><a href="https://arxiv.org/abs/2004.03794">CALM: Continuous Adaptive Learning for Language Modeling</a>
<pre>训练大型语言表示模型已经成为自然语言处理领域的标准。这允许对任意数量的特定任务进行微调，但是，这些大型高容量模型可以继续对特定于域的未标记数据进行训练，从而使初始化对于受监督的任务更加健壮。我们证明，在实践中，这些预先训练的模型在对一般领域（如GLUE）的任务进行评估时，表现出灾难性遗忘的形式。在这项工作中，我们为语言建模提出了冷静、持续的自适应学习：呈现跨多个领域保留知识的模型的技术。通过这些方法，我们能够减少由任务特定模型引入的监督任务之间的性能差距，我们在生物医学和临床领域使用持续学习设置进行演示。</pre></li>
<li><a href="https://arxiv.org/abs/2004.14996">SegaBERT: Pre-training of Segment-aware BERT for Language Understanding</a>
<pre>转换器对于序列建模非常强大。几乎所有最先进的语言模型和预先训练的语言模型都基于Transformer体系结构。但是，它仅使用令牌位置索引来区分顺序令牌。我们假设，更丰富的位置信息可以从转换器生成更好的上下文表示。为了验证这一点，我们提出了一种段感知转换器（Segatron），将原始标记位置编码替换为段落、句子和标记的组合位置编码。我们首先将段感知机制引入Transformer XL，它是一种流行的基于Transformer的语言模型，具有内存扩展和相对位置编码。我们发现，我们的方法可以进一步改进Transformer XL基本模型和大型模型，在WikiText-103数据集上实现17.1困惑。我们进一步研究了Segatron的训练前掩蔽语言建模任务。实验结果表明，使用Segatron（SegaBERT）预训练的BERT在各种自然语言处理任务上都优于使用vanilla Transformer的BERT，在零炮句子表征学习上也优于RoBERTa。</pre></li>
<li><a href="https://arxiv.org/abs/1710.04334">DisSent: Sentence Representation Learning from Explicit Discourse Relations</a> (ACL2019)
<pre>学习句子的有效表征是自然语言理解的核心任务之一。现有的模型要么训练大量的文本，要么需要昂贵的、人工管理的句子关系数据集。我们表明，通过依赖分析和基于规则的准则，我们可以通过利用显式话语关系来管理高质量的句子关系任务。我们的研究表明，我们策划的数据集为学习句子意义的向量表示提供了一个极好的信号，表示只有当两个句子的意义结合在一起时才能确定的关系。我们证明，自动管理的语料库允许双向LSTM句子编码器产生高质量的句子嵌入，并且可以作为较大模型（如BERT）的监督微调数据集。我们的固定句嵌入在包括SentEval在内的各种迁移任务中都取得了很好的效果，我们在Penn话语树库的内隐关系预测任务中也取得了最新的成果。</pre></li>
<li><a href="https://arxiv.org/abs/2005.10389">Pretraining with Contrastive Sentence Objectives Improves Discourse Performance of Language Models</a> (ACL2020)
<pre>最近的文本无监督表征学习模型采用了许多技术来改善语境词表征，但很少关注语篇层面的表征。我们提出了CONPONO，一个用于训练前语言模型的句间目标，它模拟了语篇连贯和句子之间的距离。在给定锚定句的情况下，我们的模型使用一个抽样的softmax目标训练预测文本k句，其中候选句子由相邻句子和从语料库中随机抽样的句子组成。在话语表征基准DiscoEval上，我们的模型在7项任务中比之前的最新水平提高了13%，平均绝对值提高了4%。我们的模型与BERT-Base的大小相同，但优于更大的BERT-Large模型和其他更新的包含语篇的方法。我们还表明，即使对于没有明确评估语篇的任务：文本蕴涵（RTE）、常识推理（COPA）和阅读理解（ReCoRD），CONPONO也能产生2%-6%的绝对增益。</pre></li>
<li><a href="https://arxiv.org/abs/2010.06351">CAPT: Contrastive Pre-Training for Learning Denoised Sequence Representations</a>
<pre></pre></li>
<li><a href="https://arxiv.org/abs/2010.16249">SLM: Learning a Discourse Language Representation with Sentence Unshuffling</a> (EMNLP2020)
<pre>我们引入了句子级语言建模，这是一个新的预训练目标，用于以完全自我监督的方式学习语篇语言表征。最近NLP中的预训练方法侧重于学习底层或顶层语言表示：一个极端是从语言模型目标派生的上下文化单词表示，另一个极端是通过对两个给定文本段的顺序分类学习的全序列表示。然而，这些模型并没有被直接鼓励去捕捉自然语言中存在的中等大小结构的表示，例如句子以及它们之间的关系。为此，我们提出了一种新的方法，通过洗牌输入句子序列和训练层次变换模型来重建原始顺序，从而鼓励学习上下文化的句子级表示。通过对GLUE、SQuAD和DiscoEval等下游任务的实验，我们表明我们模型的这一特性大大提高了原始BERT的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2107.09852">CausalBERT: Injecting Causal Knowledge Into Pre-trained Models with Minimal Supervision</a>
<pre>最近的工作表明，成功地将预先训练好的模型（如BERT）结合起来，以改进NLP系统。然而，现有的预先训练的模型缺乏因果知识，这使得今天的NLP系统无法像人类一样思考。在本文中，我们研究了将因果知识注入预训练模型的问题。有两个基本问题：1）如何从非结构化文本中收集各种因果对粒度；2） 如何有效地将因果知识注入预先训练的模型。为了解决这些问题，我们从以前的研究中扩展了因果关系的概念，并在各种数据集上进行实验以评估其有效性。此外，我们采用了一种基于正则化的方法，在注入因果知识的同时，用一个额外的正则化项来保存已经学习的知识。在7个数据集（包括4个因果对分类任务、2个因果QA任务和1个因果推理任务）上进行的大量实验表明，因果分析捕获了丰富的因果知识，并优于所有预先训练的基于模型的最新方法，实现了一个新的因果推理基准。</pre></li>
<li><a href="https://arxiv.org/abs/1908.04577">StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding</a> (ICLR2020)
<pre>最近，预训练的语言模型BERT（及其经过稳健优化的版本RoBERTa）在自然语言理解（NLU）中引起了广泛关注，并在各种自然语言理解任务（如情感分类、自然语言推理、语义-文本相似性和问答）中实现了最先进的准确性。受Elman[8]线性化探索工作的启发，我们通过将语言结构纳入预训练，将BERT扩展到一个新模型StructBERT。具体来说，我们使用两个辅助任务对StructBERT进行预训练，以充分利用单词和句子的顺序，这分别在单词和句子级别利用语言结构。因此，新模型适用于下游任务所需的不同语言理解水平。StructBERT和结构性预训练在各种下游任务上提供了令人惊讶的良好实证结果，包括将胶水基准的最新水平提高到89.0（优于所有已发布的模型），F1队v1.1问题回答的分数达到93.0，SNLI的准确度达到91.7。</pre></li>
<li><a href="https://arxiv.org/abs/2105.10956">Structural Pre-training for Dialogue Comprehension</a> (ACL2021)
<pre>预训练语言模型（PrLMs）由于其从自我监督的预训练中学习通用语言表示的强大能力而表现出优异的性能。然而，即使在强大的PrLMs的帮助下，有效地从对话文本中获取任务相关知识仍然是一个挑战，因为对话文本通过说话人感知话语之间的相关性而丰富。在这项工作中，我们提出蜘蛛，结构预先训练的对话读者，捕捉对话独有的功能。为了模拟类似对话的特征，除了最初的LM目标外，我们还提出了两个训练目标：1）话语顺序恢复，它预测对话语境中置换话语的顺序；2） 句子主干正则化，对模型进行正则化，以提高总结的主谓宾三元组的事实正确性。在广泛使用的对话基准上的实验结果验证了新引入的自我监督任务的有效性。</pre></li>
<li><a href="https://arxiv.org/abs/2009.07408">Retrofitting Structure-aware Transformer Language Model for End Tasks</a> (EMNLP2020)
<pre>我们认为，改进结构感知的基于变压器的语言模型，以促进结束任务，提出利用句法距离来编码短语选区和依赖性连接到语言模型。采用中间层结构学习策略进行结构整合，在多任务学习模式下通过主语义任务训练完成结构整合。实验结果表明，改进后的结构感知转换语言模型在获得准确句法短语的同时，提高了复杂度。通过执行结构感知微调，我们的模型实现了语义和句法相关任务的显著改进。</pre></li>
<li><a href="https://arxiv.org/abs/2012.14116">Syntax-Enhanced Pre-trained Model</a>
<pre>我们研究了利用文本的句法结构来增强预训练模型（如BERT和RoBERTa）的问题。现有的方法要么在预训练阶段，要么在微调阶段利用文本的语法，因此这两个阶段之间存在差异。这样一个问题将导致需要有人工注释的语法信息，这将现有方法的应用限制在更广泛的场景中。为了解决这个问题，我们提出了一个模型，该模型在预训练和微调阶段都利用了文本的语法。我们的模型基于Transformer，它具有语法感知的注意层，考虑文本的依赖树。我们进一步介绍了一个新的预训练任务，即预测依赖树中标记之间的语法距离。我们在三个下游任务上评估了该模型，包括关系分类、实体类型和问题回答。结果表明，我们的模型在六个公共基准数据集上实现了最先进的性能。我们有两个主要发现。首先，我们证明了注入自动生成的文本语法可以改进预先训练好的模型。其次，与相邻标记之间的局部头部关系相比，标记之间的全局语法距离带来了更大的性能增益。</pre></li>
<li><a href="https://arxiv.org/abs/1911.06156">Syntax-Infused Transformer and BERT models for Machine Translation and Natural Language Understanding</a>
<pre>在一些NLP任务中，基于注意的模型比传统算法有显著的改进。例如，Transformer是一个说明性示例，它根据输入到编码器的令牌与序列中所有令牌的关系生成其抽象表示。最近的研究表明，尽管这些模型能够通过查看示例来学习句法特征，但将这些信息显式地反馈给深度学习模型可以显著提高其性能。在复杂模型（如Transformer）的有限训练数据设置中，利用诸如词性（POS）之类的句法信息可能特别有益。我们表明，当在完整的WMT 14英译德翻译数据集上进行训练时，具有多种功能的语法注入转换器实现了0.7 BLEU的改进，当在数据集的一小部分上进行训练时，最大提高了1.99 BLEU点。此外，我们发现，将语法合并到BERT微调中，在GLUE基准的许多下游任务上都优于基线。</pre></li>
<li><a href="https://arxiv.org/abs/2008.09084">Do Syntax Trees Help Pre-trained Transformers Extract Information?</a>
<pre>最近的许多工作表明，合并依赖树中的语法信息可以改进特定于任务的转换器模型。然而，将依赖树信息合并到预先训练的转换器模型（例如，BERT）中的效果仍然不清楚，特别是考虑到最近强调这些模型如何隐式编码语法的研究。在这项工作中，我们系统地研究了将依赖树合并到预先训练好的变换器中对三个具有代表性的信息提取任务的效用：语义角色标记（SRL）、命名实体识别和关系提取。我们提出并研究了两种不同的合并依赖结构的策略：后期融合方法（将图形神经网络应用于转换器的输出）和联合融合方法（将语法结构注入转换器注意层）。这些策略代表了先前的工作，但我们引入了额外的模型设计元素，这些元素对于获得更好的性能是必要的。我们的实证分析表明，这些注入语法的转换器在SRL和关系提取任务上获得了最先进的结果。然而，我们的分析也揭示了这些模型的一个关键缺点：我们发现，它们的性能提高在很大程度上取决于人类注释依赖解析的可用性，这就提出了关于语法增强转换器在实际应用中的可行性的重要问题。</pre></li>
<li><a href="https://arxiv.org/abs/1908.05646">SenseBERT: Driving Some Sense into BERT</a>
<pre>从大型未标记语料库中学习的能力使得神经语言模型能够推进自然语言理解的前沿。然而，现有的自我监督技术是在词的形式层面上运作的，作为底层语义内容的替代物。本文提出了一种直接在词义层面采用弱监督的方法。我们的模型名为SenseBERT，经过预先训练，不仅可以预测隐藏的单词，还可以预测它们的WordNet超级意义。因此，我们获得了一个词汇语义层次的语言模型，而不使用人工注释。SenseBERT显著提高了词汇理解，正如我们在SemEval词义消歧实验中所证明的那样，通过在语境任务中实现单词的最新结果，SenseBERT取得了显著的词汇理解。</pre></li>
<li><a href="https://arxiv.org/abs/1909.02209">Semantics-aware BERT for Language Understanding</a> (AAAI2020)
<pre>语言表征的最新研究仔细地将语境化特征融入到语言模型训练中，这使得一系列的成功尤其是在各种机器阅读理解和自然语言推理任务中。然而，现有的语言表示模型（包括ELMo、GPT和BERT）仅利用简单的上下文敏感特性，如字符或单词嵌入。他们很少考虑结合结构化语义信息，它可以为语言表达提供丰富的语义。为了促进对自然语言的理解，我们建议将预先训练好的语义角色标记中的显式上下文语义结合起来，并引入一种改进的语言表示模型，语义感知的BERT（SemBERT），该模型能够通过BERT主干显式地吸收上下文语义。SemBERT以轻松微调的方式保持其BERT前体的方便可用性，而无需进行实质性的任务特定修改。与BERT相比，语义感知的BERT在概念上同样简单，但功能更强大。它在十项阅读理解和语言推理任务中取得了新的水平或显著提高了结果。</pre></li>
<li><a href="https://arxiv.org/abs/2010.12532">GiBERT: Introducing Linguistic Knowledge into BERT through a Lightweight Gated Injection Method</a>
<pre>大型预先训练的语言模型（如BERT）是许多NLP任务最近改进的驱动力。然而，BERT只接受过预测缺失单词的训练，不管是在面具后面还是在下一句话中，除了通过无监督的预训练获得的信息外，他对词汇、句法或语义信息一无所知。我们提出了一种新的方法，将语言知识以单词嵌入的形式显式地注入到预训练的BERT的任何层中。我们在注入依赖性嵌入和反拟合嵌入时对多个语义相似性数据集的性能改进表明，这些信息是有益的，并且目前从原始模型中缺失。我们的定性分析表明，反向拟合嵌入注入对涉及同义词对的情况特别有帮助。</pre></li>
<li><a href="https://arxiv.org/abs/1909.07606">K-BERT: Enabling Language Representation with Knowledge Graph</a>
<pre>预先训练的语言表示模型，如BERT，从大规模语料库中获取一般的语言表示，但缺乏特定领域的知识。在阅读领域文本时，专家们利用相关知识进行推理。为了使机器能够实现这一功能，我们提出了一种基于知识的语言表示模型（K-BERT）和知识图（KGs），其中三元组作为领域知识注入到句子中。然而，过多的知识整合可能会使句子偏离正确的意思，这就是知识噪音（knowledgenoise，KN）问题。为了克服KN，K-BERT引入了软位置和可见矩阵来限制知识的影响。K-BERT能够从预先训练的BERT中加载模型参数，因此，通过配备KG，K-BERT可以轻松地将领域知识注入到模型中，而无需自己进行预训练。我们的调查显示在12个NLP任务中有很好的结果。特别是在特定领域的任务（包括金融、法律和医学）中，K-BERT显著优于BERT，这表明K-BERT是解决需要专家的知识驱动问题的最佳选择。</pre></li>
<li><a href="https://arxiv.org/abs/1909.04164">Knowledge Enhanced Contextual Word Representations</a> (EMNLP2019)
<pre>上下文单词表示法通常是在非结构化、未标记的文本上训练的，它不包含任何与真实世界实体相关的明确基础，并且通常无法记住关于这些实体的事实。我们提出了一种将多个知识库（KBs）嵌入到大规模模型中的通用方法，从而用结构化的、人工管理的知识增强其表示。对于每个KB，我们首先使用一个集成的实体链接器来检索相关的实体嵌入，然后通过一种词到实体注意的形式更新上下文单词表示。与以前的方法不同，实体链接器和自监督语言建模目标是在多任务设置中端到端联合训练的，该设置将少量实体链接监督与大量原始文本相结合。在将WordNet和Wikipedia的一个子集集成到BERT中后，知识增强型BERT（KnowBert）显示出更好的困惑度、在探测任务中测量的事实回忆能力以及在关系提取、实体类型和词义消歧方面的下游性能。KnowBert的运行时与BERT的运行时相当，并且可以扩展到大KBs。</pre></li>
<li><a href="https://arxiv.org/abs/2007.00655">Knowledge-Aware Language Model Pretraining</a>
<pre>预先训练的语言模型能掌握多少知识？最近的研究发现，经过预训练的变形金刚擅长于建模语义，但尚不清楚它们在多大程度上掌握了人类知识，或者如何确保它们掌握了人类知识。在本文中，我们在不改变转换器结构、插入显式知识层或添加语义信息的外部存储的情况下，将知识意识纳入语言模型预训练中。相反，我们只是在预训练中用实体扩展标记器向转换器的输入发送实体存在的信号；在输出端，执行额外的实体预测任务。我们的实验表明，仅通过在预训练中添加这些实体信号，变压器参数中就包含了更多的知识：我们观察到语言建模的准确性、LAMA知识探测任务中的事实正确性、，我们还表明，我们的知识感知语言模型（KALM）可以作为GPT-2模型的替代品，在不进行任务相关培训的情况下显著改善下游任务，如零炮问答。</pre></li>
<li><a href="https://arxiv.org/abs/2002.01808">K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters</a>
<pre>我们研究将知识注入大型预训练模型（如BERT和RoBERTa）的问题。现有方法通常在注入知识时更新预训练模型的原始参数。然而，当多种知识被注入时，历史注入的知识就会被冲走。为了解决这个问题，我们提出了K-Adapter，这是一个框架，它保留了预先训练好的模型的原始参数，并支持多功能知识注入模型的开发。以RoBERTa为主干模型，K-Adapter为每种注入的知识都提供了一个神经适配器，就像连接到RoBERTa的插件一样。不同适配器之间没有信息流，因此可以以分布式方式有效地训练多个适配器。作为一个案例研究，我们在这项工作中注入了两种知识，包括（1）从Wikipedia和Wikidata上自动对齐的文本三元组获得的事实知识和（2）通过依赖解析获得的语言知识。在三个知识驱动任务（包括关系分类、实体类型和问答）上的结果表明，每个适配器都提高了性能，并且两个适配器的组合带来了进一步的改进。进一步的分析表明，K-Adapter比RoBERTa掌握了更多的知识。</pre></li>
<li><a href="https://arxiv.org/abs/2010.00796">JAKET: Joint Pre-training of Knowledge Graph and Language Understanding</a>
<pre>知识图（KG）包含关于世界知识、实体和关系的丰富信息。因此，它们可以成为现有预训练语言模型的重要补充。然而，如何将KG中的信息有效地集成到语言建模中仍然是一个挑战。理解知识图需要相关的上下文。我们提出了一个新的联合预训练框架JAKET，对知识图和语言进行建模。知识模块和语言模块提供相互帮助的基本信息：知识模块为文本中的实体生成嵌入，而语言模块为图形中的实体和关系生成上下文感知的初始嵌入。我们的设计使预先训练的模型能够很容易地适应新领域中看不见的知识图。在几个具有知识意识的NLP任务上的实验结果表明，我们提出的框架通过有效地利用语言理解中的知识实现了优异的性能。</pre></li>
<li><a href="https://arxiv.org/abs/1911.03681">E-BERT: Efficient-Yet-Effective Entity Embeddings for BERT</a> (EMNLP2020)
<pre>我们提出了一种将实体的事实知识注入预训练的BERT模型的新方法（Devlin等人，2019）：我们将Wikipedia2Vec实体向量（Yamada等人，2016）与BERT的原生词条向量空间对齐，并将对齐的实体向量当作词条向量使用。由此产生的实体增强版BERT（称为E-BERT）在精神上与ERNIE（Zhang等人，2019）和KnowBert（Peters等人，2019）类似，但不需要对BERT编码器进行昂贵的进一步预培训。我们在无监督问答（QA）、监督关系分类（RC）和实体链接（EL）三个方面对E-BERT进行了评估。在所有三项任务中，E-BERT都优于BERT和其他基线。我们还定量地表明，原始的伯特模型过度依赖实体名称的表面形式（例如，猜测某人的名字听起来像意大利语），而e-伯特缓解了这个问题。</pre></li>
<li><a href="https://arxiv.org/abs/1911.06136">KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation</a>
<pre>预先训练的语言表示模型（PLM）不能很好地从文本中获取事实知识。相比之下，知识嵌入（KE）方法可以通过信息实体嵌入有效地表示知识图（KG）中的关系事实，但传统的KE模型不能充分利用丰富的文本信息。在本文中，我们提出了一个统一的知识嵌入和预训练语言表示模型（KEPLER），该模型不仅可以更好地将事实知识集成到PLM中，而且可以生成具有强PLM的有效文本增强KE。在开普勒中，我们使用PLM作为嵌入对文本实体描述进行编码，然后联合优化KE和语言建模目标。实验结果表明，开普勒在各种NLP任务上都取得了最新的性能，并且在KG链路预测方面也可以作为一个归纳的KE模型。此外，为了对开普勒进行预训练和评估，我们构建了Wikidata5M，这是一个具有对齐实体描述的大规模KG数据集，并在此基础上对最先进的KE方法进行了基准测试。它将作为一个新的KE基准，促进对大型KG、归纳式KE和带文本KG的研究。源代码可以从https://github.com/THU-KEG/KEPLER.</pre></li>
<li><a href="https://arxiv.org/abs/2004.07202">Entities as Experts: Sparse Memory Access with Entity Supervision</a> (EMNLP2020)
<pre>我们关注在语言模型的学习参数中获取关于实体的声明性知识的问题。我们引入了一个新模型——实体作为专家（Entities as Experts，EAE）——它可以访问文本中提到的实体的不同记忆。与以前将实体知识集成到序列模型中的工作不同，EAE的实体表示直接从文本中学习。我们表明，EAE的学习表示法获取了足够的知识，能够回答诸如“罗杰·德尔加多、安东尼·安利、埃里克·罗伯茨扮演了哪一个恶棍博士？”等琐碎问题，优于参数为10倍的编码器-发电机-变压器模型。根据LAMA知识探索，EAE包含的事实知识比同等规模的BERT更多，以及以前整合实体知识外部来源的方法。由于EAE将参数与特定实体相关联，因此在推理时只需要访问其参数的一小部分，我们证明了实体的正确标识和表示对EAE的性能至关重要。</pre></li>
<li><a href="https://www.aclweb.org/anthology/2020.emnlp-main.722/">Exploiting Structured Knowledge in Text via Graph-Guided Representation Learning</a> (EMNLP2020)</li>
<li><a href="https://arxiv.org/abs/2004.12006">Contextualized Representations Using Textual Encyclopedic Knowledge</a>
<pre>我们提出了一种通过将输入文本与动态检索的文本百科全书背景知识结合起来表示输入文本的方法。我们将我们的方法应用到阅读理解任务中，将问题和段落以及它们提到的实体的背景句子编码在一起。我们表明，从文本中整合背景知识对于注重事实推理的任务是有效的，并允许直接重用强大的预训练伯特式编码器。此外，通过对背景增强输入文本中单词的自监督掩蔽语言模型目标进行适当的预训练，可以进一步提高知识集成。在TriviaQA上，我们的方法与不动态集成背景知识的可比RoBERTa模型相比，F1提高了1.6到3.1。在MRQA这一大量不同的QA数据集上，我们看到了领域内的一致收益，以及BioASQ（2.1到4.2 F1）、TextbookQA（1.6到2.0 F1）和DuoRC（1.1到2.0 F1）领域外的巨大改进。</pre></li>
<li><a href="https://arxiv.org/abs/2010.00309">CoLAKE: Contextualized Language and Knowledge Embedding</a> (COLING2020)
<pre>随着将事实知识融入到诸如伯特的预训练语言模型中的新兴分支，大多数现有模型考虑浅、静态和单独预训练的实体嵌入，这限制了这些模型的性能增益。很少有作品在注入知识时探索深层情境化知识表示的潜力。在本文中，我们提出了语境化语言和知识嵌入（CoLAKE），它与扩展的MLM目标共同学习语言和知识的语境化表示。CoLAKE不是只注入实体嵌入，而是从大规模知识库中提取实体的知识上下文。为了处理知识上下文和语言上下文的异构性，我们将它们集成到一个统一的数据结构中，即单词知识图（WK-graph）。CoLAKE使用改进的Transformer编码器对大规模WK图进行预训练。我们在知识驱动任务、知识探索任务和语言理解任务上进行实验。实验结果表明，CoLAKE在大多数任务上都优于以前的同类。此外，CoLAKE在我们称为单词知识图完成的合成任务中取得了令人惊讶的高性能，这显示了同时上下文化语言和知识表示的优越性。</pre></li>
<li><a href="https://arxiv.org/abs/2104.08145">KI-BERT: Infusing Knowledge Context for Better Language and Domain Understanding</a>
<pre>由最先进的基于转换器的语言模型（transformer-based language models，TLM，如BERT、GPT、T5等）学习的情境化实体表示，利用注意机制从训练数据语料库中学习数据上下文。但是，这些模型不使用知识上下文。知识上下文可以理解为知识图中实体及其与相邻实体关系的语义。我们提出了一种新颖有效的技术，在微调过程中，将概念和模糊实体的多个知识图中的知识上下文注入TLM。它将知识图嵌入到齐次向量空间中，为实体引入新的标记类型，对齐实体位置ID，以及选择性注意机制。我们将BERT作为基线模型，并通过注入来自ConceptNet和WordNet的知识上下文来实现“知识注入的BERT”，这在GLUE benchmark的八个不同子任务上显著优于BERT和其他最近的知识感知BERT变体，如ERNIE、SenseBERT和BERT_CS。KI-BERT基本模型甚至在特定领域任务（如QQP、QNLI和MNLI的SciTail和学术子集）方面显著优于BERT大型模型。</pre></li>
<li><a href="https://arxiv.org/abs/2104.10649">K-XLNet: A General Method for Combining Explicit Knowledge with Language Model Pretraining</a>
<pre>尽管诸如Bert和XLNet等预先训练的语言模型在许多自然语言处理任务中迅速提高了技术水平，但它们仅依赖于语料库中单词之间的表面信息来隐含语义。直觉上，背景知识会影响理解的效果。受这一常识的启发，我们专注于通过利用显性知识改进模型预训练。与最近通过知识掩蔽策略优化预训练模型的研究不同，我们提出了一种简单而通用的方法，将显性知识与预训练相结合。具体地说，我们首先从知识图（KG）中匹配知识事实，然后直接向transformer添加一个知识禁令层，而不改变其架构。本研究旨在发现显性知识对培训的直接影响。我们针对不同的下游任务在不同的数据集上进行实验。实验结果表明，仅通过向transformer添加外部知识，就可以提高许多NLP任务的学习性能。</pre></li>
<li><a href="https://arxiv.org/abs/2101.12294">Combining pre-trained language models and structured knowledge</a>
<pre>近年来，基于transformer的语言模型在各种NLP基准测试中取得了最先进的性能。这些模型能够从非结构化文本中提取大部分具有一定语义的分布式信息，但将结构化信息（如知识图）集成到这些模型中已被证明是一项挑战。我们研究了将结构化知识集成到当前语言模型中的各种方法，并确定了利用结构化和非结构化信息源的挑战和可能的机会。从我们的调查中，我们发现仍然有机会利用基于适配器的注入，并且有可能进一步将各种探索的方法组合到一个系统中。</pre></li>
<li><a href="https://arxiv.org/abs/2010.08210">Coarse-to-Fine Pre-training for Named Entity Recognition</a> (EMNLP2020)
<pre>最近，命名实体识别在诸如BERT等预训练方法的帮助下取得了巨大的进步。然而，当前的预培训技术侧重于建立语言建模目标以学习一般表示，而忽略了命名实体相关知识。为此，我们提出了一个特定的预训练框架，将自动挖掘的实体知识从粗到精转化为预训练模型。具体地说，我们首先通过一个完整的跨度识别任务对模型进行预热，方法是使用维基百科锚对其进行训练，该锚可以被视为一般类型的实体。然后利用基于地名索引的远程监控策略训练模型，提取粗粒度的类型特征。最后，我们设计了一个自我监督的辅助任务，通过聚类挖掘细粒度的姓名识别知识。对三个公共NER数据集的实证研究表明，我们的框架在几个预先训练的基线上实现了显著改进，在三个基准上建立了新的最先进性能。此外，我们还展示了我们的框架在不使用人工标记的培训数据的情况下获得了有希望的结果，证明了它在标记较少和资源较少的情况下的有效性</pre></li>
<li><a href="https://arxiv.org/abs/2011.05431">E.T.: Entity-Transformers. Coreference augmented Neural Language Model for richer mention representations via Entity-Transformer blocks</a> (COLING2020 WS)
<pre>在过去的十年中，神经语言建模领域发生了巨大的变化，通过使用变压器结构开发了新的模型。然而，由于内存限制和计算复杂性的增加，即使这些模型也难以对长序列建模。训练数据上的共指注释可以提供远远超出此类语言模型建模限制的上下文。在本文中，我们对神经语言模型中使用的Transformer块结构进行了扩展，特别是在GPT2中，以便在训练期间合并实体注释。我们的模型GPT2E将GPT2的Transformer层架构扩展到实体Transformer，实体Transformer是一种设计用于处理共引用信息的架构。为此，我们实现了实体提及的更丰富表示，而培训成本很低。我们展示了GPT2和GPT2E在CoNLL 2012和LAMBADA数据集上的复杂度方面的比较模型性能，以及实体表示的关键差异及其在下游任务（如命名实体识别）中的影响。此外，我们的方法可以被大多数基于Transformer的语言模型采用。</pre></li>
<li><a href="https://arxiv.org/abs/2002.08909">REALM: Retrieval-Augmented Language Model Pre-Training</a> (ICML2020) [<a href="https://github.com/google-research/language/tree/master/language/realm">github</a>]
<pre>语言模型预训练已被证明能够获取惊人数量的世界知识，这对于NLP任务（如问答）至关重要。然而，这些知识隐含在神经网络的参数中，需要更大的网络来覆盖更多的事实。为了以更加模块化和可解释的方式获取知识，我们使用潜在知识检索器对语言模型预训练进行了增强，该检索器允许模型从大型语料库（如维基百科）中检索和处理文档，这些语料库在预训练、微调和推理过程中使用。我们首次展示了如何以无监督的方式预先训练这样的知识检索器，使用蒙面语言建模作为学习信号，并通过考虑数百万文档的检索步骤进行反向传播。我们通过对具有挑战性的开放领域问答（openqa）任务进行微调，证明了检索增强语言模型预训练（REALM）的有效性。我们在三个流行的开放式QA基准上对比了显式和隐式知识存储的最新模型，发现我们在提供可解释性和模块化等定性优势的同时，显著优于所有以前的方法（绝对准确率为4-16%）。</pre></li>
<li><a href="https://arxiv.org/abs/2104.08710">Simple and Efficient ways to Improve REALM</a>
<pre>密集检索已被证明是有效的检索相关文件的开放领域质量保证，超过流行的稀疏检索方法，如BM25。REALM（Guu et al.，2020）是一个端到端的密集检索系统，它依赖于基于传销的预培训来提高多个数据集的下游QA效率。我们研究了各种QA任务的领域微调，并探索了各种超参数和监督选择的限制。我们发现，当培训、监督和推理设置中的微调和简单改进可以显著提高QA结果并超过发布后的其他模型的性能时，REALM的培训明显不足。我们最好的模型REALM++，包含了所有最好的工作结果，并在没有任何模型设计更改的情况下，在基线（约5.5%绝对精度）上实现了显著的QA精度改进。此外，REALM++与大型开放域QA模型的性能相匹配，这些模型的参数多了3倍，可以证明设置的效率。</pre></li>
<li><a href="https://arxiv.org/abs/2005.11401">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a> (NeurIPS2020)
<pre>大型预先训练的语言模型已被证明在其参数中存储事实知识，并在对下游NLP任务进行微调时实现最先进的结果。然而，它们访问和精确操作知识的能力仍然有限，因此在知识密集型任务中，它们的性能落后于特定于任务的体系结构。此外，为他们的决策提供出处和更新他们的世界知识仍然是一个开放的研究问题。具有显式非参数记忆可微访问机制的预训练模型可以克服这一问题，但迄今为止仅针对提取下游任务进行了研究。我们探索了检索增强生成（RAG）的通用微调方法——将预先训练的参数和非参数记忆结合起来用于语言生成的模型。我们介绍了RAG模型，其中参数记忆是预先训练的seq2seq模型，非参数记忆是Wikipedia的稠密向量索引，通过预先训练的神经检索器访问。我们比较了两种RAG公式，一种是在整个生成序列的相同检索通道上设置条件，另一种是每个标记可以使用不同的通道。我们在广泛的知识密集型NLP任务上对我们的模型进行了微调和评估，并在三个开放域QA任务上设置了最新技术，优于参数化seq2seq模型和特定于任务的检索和提取架构。对于语言生成任务，我们发现RAG模型生成的语言比最先进的参数化seq2seq基线更具体、多样和真实。</pre></li>
<li><a href="https://arxiv.org/abs/2106.11517">Fine-tune the Entire RAG Architecture (including DPR retriever) for Question-Answering</a>
<pre>在本文中，我们将演示如何以端到端的方式微调整个检索增强生成（RAG）体系结构。我们强调了实现这一目标需要解决的主要工程挑战。我们还比较了端到端RAG体系结构在回答问题方面的性能如何优于原始RAG体系结构。我们在HuggingFace Transformers库中公开了我们的实现。</pre></li>
<li><a href="https://arxiv.org/abs/2105.06597">Joint Retrieval and Generation Training for Grounded Text Generation</a>
<pre>GPT-3等大规模预培训的最新进展允许从给定提示生成看似高质量的文本。然而，这类生成系统通常会遇到幻觉事实的问题，并且其固有设计不是为了包含有用的外部信息。扎根生成模型似乎提供了补救措施，但它们的培训通常依赖于很少可用的并行数据，其中为上下文提供了相应的信息相关文档。我们提出了一个框架，通过在语言模型信号上联合训练接地生成器和文档检索器来缓解这种数据约束。该模型学习如何奖励在生成过程中具有最高效用的文档检索，并使用混合专家（MoE）集成将其仔细组合，生成后续文本。我们证明，生成器和检索器都可以利用这种联合训练，协同工作，在散文和对话生成中生成更多信息和相关的文本。</pre></li>
<li><a href="https://arxiv.org/abs/2104.07567">Retrieval Augmentation Reduces Hallucination in Conversation</a>
<pre>尽管显示出越来越人性化的对话能力，但最先进的对话模式往往存在事实错误和知识幻觉（Roller等人，2020年）。在这项工作中，我们探索了在循环架构中使用神经检索——最近证明在开放领域QA中是有效的（Lewis等人，2020b；Izacard和Grave，2020）——进行知识基础对话，这项任务可能更具挑战性，因为它需要基于复杂的多回合对话上下文进行查询，并生成连贯的对话响应。我们研究具有多个组件的各种类型的体系结构-检索器、RANKER和编码器-解码器-目标是在保持会话能力的同时最大化知识能力。我们证明了我们的最佳模型在两个基于知识的会话任务中获得了最先进的性能。这些模型展示了开放领域的对话能力，有效地推广到不在培训数据范围内的场景，并且，如人类评估所验证的，大大减少了最先进聊天机器人中众所周知的知识幻觉问题。</pre></li>
<li><a href="https://arxiv.org/abs/2007.01528">On-The-Fly Information Retrieval Augmentation for Language Models</a>
<pre>在这里，我们尝试使用信息检索作为预先训练的语言模型的补充。信息检索中使用的文本语料库可以看作是一种随时间增长的情景记忆形式。通过使用信息检索增强GPT2.0，我们在Gigaword语料库上实现了相对减少15%的困惑度，而无需任何再训练。我们还验证了事件共同参考任务上的IR增强。</pre></li>
<li><a href="https://arxiv.org/abs/2009.06857">Current Limitations of Language Models: What You Need is Retrieval</a>
<pre>我们分类并重新检查了一些当前的方法，以改进语言模型的性能计算权衡，包括（1）非因果模型（如掩蔽语言模型），（2）有效注意的批次长度扩展，（3）重复性，（4）条件计算和（5）检索。我们确定了（1）-（4）遭受的一些限制。例如，（1）由于需要一个特定的微调数据集，目前正在努力实现开放式文本生成，输出受到输入的松散约束，并执行GPT-2/3等一般文本任务。（2） 和（3）不改进第一个$\sim 10^3$令牌的预测。放大模型大小（例如，使用（4）进行有效缩放）仍然会导致某些任务的性能缩放效果不佳。我们认为（5）将解决许多这些限制，并且它可以（a）减少监督的数量，（b）有效地扩展整个训练数据集和当前样本的整个过去的上下文。我们推测如何修改MARGE以执行无监督因果建模，从而实现（b）与联合训练的寻回犬。</pre></li>
<li><a href="https://arxiv.org/abs/2112.04426">Improving language models by retrieving from trillions of tokens</a> [<a href="https://deepmind.com/research/publications/2021/improving-language-models-by-retrieving-from-trillions-of-tokens">blog</a>] [<a href="http://jalammar.github.io/illustrated-retrieval-transformer/">blog</a>]
<pre>我们通过基于与前面标记的局部相似性，对从大型语料库检索到的文档块进行条件化处理，来增强自回归语言模型。凭借价值2万亿美元的令牌数据库，我们的检索增强型Transformer（RETRO）获得了与GPT-3和Jurassic-1相当的性能，尽管使用的参数少了25$\倍。经过微调后，复古性能转化为下游知识密集型任务，如问答。RETRO结合了一个冻结的伯特检索器、一个可微编码器和一个分块交叉注意机制，根据比训练期间通常消耗的数据多一个数量级的数据来预测令牌。我们通常从头开始训练RETRO，但也可以通过检索快速改装预先训练过的变压器，并且仍能获得良好的性能。我们的工作为通过前所未有的规模的外显记忆改进语言模型开辟了新途径。</pre></li>
<li><a href="https://arxiv.org/abs/2008.01466">Taking Notes on the Fly Helps BERT Pre-training</a>
<pre>如何使无监督的语言预训练更高效、资源更少，是自然语言处理的一个重要研究方向。在本文中，我们致力于通过提供更好的数据利用率来提高语言预训练方法的效率。众所周知，在语言数据语料库中，词的分布是重尾分布。大部分单词只出现很少几次，稀有单词的嵌入通常没有得到很好的优化。我们认为，这种嵌入携带的语义信号不足，这可能会导致数据利用效率低下，并减缓整个模型的预训练。为了缓解这一问题，我们建议动态记录（TNF），它在预训练期间动态记录罕见单词，以帮助模型在下次出现时理解它们。具体地说，TNF维护一个注释字典，当稀有词出现在句子中时，它会将稀有词的上下文信息保存为注释。当同一个稀有词在训练过程中再次出现时，可以利用事先保存的注释信息来增强当前句子的语义。通过这样做，TNF提供了更好的数据利用率，因为跨句子信息被用来覆盖句子中罕见的单词所造成的语义不足。我们在BERT和ELECTRA上实现TNF以检查其效率和有效性。实验结果表明，在达到相同性能时，TNF的训练时间比其主干预训练模型少60\%$。当使用相同的迭代次数进行训练时，TNF在大多数下游任务和平均GLUE分数上都优于其主干方法。源代码附在补充材料中。</pre></li>
<li><a href="https://arxiv.org/abs/2006.15020">Pre-training via Paraphrasing</a>
<pre>我们介绍了MARGE，一个预先训练好的序列到序列模型，具有无监督的多语言多文档释义目标。MARGE提供了一种替代主流蒙面语言建模范式的方法，在这种范式中，我们通过检索一组相关文本（多种语言）并对其进行条件化处理来自我监督目标文本的重建，以最大限度地提高生成原始文本的可能性。我们表明，只要给定一个随机初始化，就可以共同学习进行检索和重建。目标噪音捕获了释义、翻译、多文档摘要和信息检索等方面，允许在多个任务上实现强大的零拍性能。例如，在没有额外的特定任务培训的情况下，我们在文档翻译方面的BLEU分数高达35.8。我们进一步表明，微调在许多语言中的一系列辨别性和生成性任务中都有很好的表现，使MARGE成为迄今为止最普遍适用的预训练方法。</pre></li>
<li><a href="https://arxiv.org/abs/2005.05635">SKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis</a> (ACL2020)
<pre>最近，情绪分析在训练前方法的帮助下取得了显著进展。然而，尽管传统的情感分析方法广泛使用情感词和方面情感对等情感知识，但它们在预训练过程中却被忽视。在本文中，我们引入情感知识增强预训练（SKEP），以便为多个情感分析任务学习统一的情感表示。SKEP借助于自动挖掘的知识，进行情感掩蔽，构造三个情感知识预测目标，从而将词、极性和体层的情感信息嵌入到预先训练好的情感表示中。特别地，将方面情感对的预测转化为多标签分类，旨在捕获一对词之间的依赖关系。对三种情绪任务的实验表明，SKEP显著优于强训练前基线，并在大多数测试数据集上获得了最新的结果。我们在https://github.com/baidu/Senta.</pre></li>
<li><a href="https://arxiv.org/abs/2011.02610">Improving Event Duration Prediction via Time-aware Pre-training</a> (EMNLP2020 Findings)
<pre>NLP中的端到端模型很少编码关于时间长度的外部世界知识。我们介绍了两种有效的持续时间预测模型，它们通过阅读与时间相关的新闻句子（时间感知预训练）结合外部知识。具体而言，一个模型预测持续时间值的范围/单位（R-pred）；另一个预测了确切的持续时间值E-pred。我们最好的模型——E-pred大大优于以前的工作，并且比R-pred更准确地捕获持续时间信息。我们还证明了我们的模型能够在无监督的情况下预测持续时间，优于基线。</pre></li>
<li><a href="https://arxiv.org/abs/2009.13199">Knowledge-Aware Procedural Text Understanding with Multi-Stage Training</a>
<pre>程序文本描述了逐步自然过程（如光合作用）中的动态状态变化。在这项工作中，我们将重点放在程序性文本理解的任务上，其目的是理解这些文档，并在过程中跟踪实体的状态和位置。尽管最近的方法取得了实质性进展，但其结果远远落后于人的表现。两个挑战，即常识推理的困难和数据不足，仍然没有得到解决，这需要纳入外部知识库。以前关于外部知识注入的工作通常依赖于嘈杂的web挖掘工具和启发式规则，适用场景有限。在本文中，我们提出了一种新的知识感知程序文本理解（KOALA）模型，该模型有效地利用了任务中多种形式的外部知识。具体来说，我们从ConceptNet检索信息性知识三元组，并在跟踪实体的同时执行知识感知推理。此外，我们采用了一个多阶段的训练模式，在对最终模型进行进一步微调之前，对从维基百科收集的未标记数据进行微调。在两个程序文本数据集ProPara和Recipes上的实验结果验证了所提出方法的有效性，其中我们的模型与各种基线相比达到了最先进的性能。</pre></li>
<li><a href="https://arxiv.org/abs/1905.01969">Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring</a> (ICLR2020)
<pre>深度预培训双向变压器的使用在许多应用中取得了显著进展（Devlin等人，2018年）。对于在序列之间进行成对比较、将给定输入与相应标签匹配的任务，有两种方法是常见的：交叉编码器对序列对进行完全自我注意，双编码器对序列对进行单独编码。前者通常性能更好，但实际使用起来太慢。在这项工作中，我们开发了一种新的转换器架构，即Poly编码器，它学习全局而非令牌级别的自我关注特性。我们对这三种方法进行了详细的比较，包括哪些预培训和微调策略最有效。我们展示了我们的模型在三个现有任务上实现了最先进的结果；多边形编码器比交叉编码器更快，比双编码器更准确；通过对类似于下游任务的大型数据集进行预训练，可以获得最佳结果。</pre></li>
<li><a href="https://arxiv.org/abs/2006.15595">Rethinking Positional Encoding in Language Pre-training</a>
<pre>在这项工作中，我们研究了语言预训练中使用的位置编码方法（例如，BERT），并确定了现有公式中的几个问题。首先，我们证明了在绝对位置编码中，对位置嵌入和单词嵌入的加法运算在两种异构信息资源之间带来了混合相关性。它可能会在注意中带来不必要的随机性，并进一步限制模型的表达能力。第二，考虑到符号在下游任务中的特殊作用（整个句子的表示），我们质疑将符号的位置与其他单词一样对待是否是合理的设计。基于上述分析，我们提出了一种新的位置编码方法，称为\textbf{T}变压器，具有\textbf{U}位\textbf{P}位\textbf{E}编码（TUPE）。在自我注意模块中，TUPE使用不同的参数化分别计算单词上下文相关性和位置相关性，然后将它们相加。该设计消除了异构嵌入中的混合和噪声相关性，并通过使用不同的投影矩阵提供了更高的表达能力。此外，TUPE将\texttt{[CLS]}符号从其他位置解开，从而更容易从所有位置捕获信息。大量实验和胶基准烧蚀研究证明了该方法的有效性。代码和型号发布于https://github.com/guolinke/TUPE.</pre></li>
<li><a href="https://arxiv.org/abs/2009.13658">Improve Transformer Models with Better Relative Position Embeddings</a> (EMNLP2020 Findings)
<pre>Transformer架构依赖于显式位置编码，以保留词序的概念。在本文中，我们认为现有的工作没有充分利用位置信息。例如，正弦波嵌入的初始方案是固定的，不可学习。在本文中，我们首先回顾了绝对位置嵌入和现有的相对位置嵌入方法。然后，我们提出了新的技术，鼓励在自我注意机制中增加查询、关键和相对位置嵌入之间的交互。我们最有希望的方法是绝对位置嵌入的推广，与以前的位置嵌入方法相比，改进了SQuAD1.1的结果。此外，我们还讨论了位置嵌入是否能够足够鲁棒地处理长序列的归纳性质。我们的经验证明，我们的相对位置嵌入方法是合理的推广和鲁棒性从归纳的角度。最后，我们表明，我们提出的方法可以作为一种近似的替代方法，在计算量较小的情况下提高大型模型的精度。</pre></li>
<li><a href="https://arxiv.org/abs/2104.09864">RoFormer: Enhanced Transformer with Rotary Position Embedding</a>
<pre>transformer体系结构中的位置编码为序列中不同位置的元素之间的依赖关系建模提供了监督。我们研究了在基于转换器的语言模型中对位置信息进行编码的各种方法，并提出了一种新的实现方法——旋转位置嵌入（RoPE）。提出的RoPE利用旋转矩阵对绝对位置信息进行编码，并自然地将显式的相对位置依赖性融入到自我注意公式中。值得注意的是，RoPE具有一些有价值的特性，例如扩展到任意序列长度的灵活性、随着相对距离的增加而衰减的标记间依赖性，以及用相对位置编码装备线性自我注意的能力。因此，带有旋转位置嵌入的增强型变压器，或RoFormer，在长文本任务中实现了优异的性能。我们发布了理论分析和一些关于中国数据的初步实验结果。正在进行的英语基准测试将很快更新。</pre></li>
<li><a href="https://arxiv.org/abs/2102.11090">Position Information in Transformers: An Overview</a>
<pre>在最近的自然语言处理研究中，变形金刚可以说是主要的工作马。根据定义，转换器对于输入的重新排序是不变的。然而，语言本质上是连续的，语序对话语的语义和句法至关重要。在这篇文章中，我们提供了一个概述和现有方法的理论比较，将位置信息纳入变压器模型。本次调查的目的是：（1）展示变压器中的位置信息是一个充满活力和广泛的研究领域；（2） 通过提供重要模型维度不同方法的统一符号和系统化，使读者能够比较现有方法；（3） 指示在选择位置编码时应考虑应用程序的哪些特征；（4） 为未来的研究提供刺激。</pre></li>
<li><a href="https://arxiv.org/abs/2009.05959">BoostingBERT:Integrating Multi-Class Boosting into BERT for NLP Tasks</a>
<pre>作为一种预训练的变压器模型，BERT（变压器的双向编码器表示）在多个NLP任务中取得了突破性的性能。另一方面，Boosting是一种流行的集成学习技术，它结合了许多基本分类器，在许多机器学习任务中表现出更好的泛化性能。一些研究表明，BERT的集成可以进一步提高应用性能。然而，目前的集成方法主要集中在装袋或堆叠上，在探索提升方面还没有太多的努力。在这项工作中，我们提出了一种新的Boosting-BERT模型，将多类Boosting集成到BERT中。我们提出的模型使用预训练的转换器作为基础分类器，选择更难的训练集进行微调，并在NLP任务中获得预训练语言知识和增强集成的好处。我们在GLUE数据集和3个流行的中文NLU基准上对所提出的模型进行了评估。实验结果表明，我们提出的模型在所有数据集上都显著优于BERT，并在许多NLP任务中证明了它的有效性。BoostingBERT用RoBERTa作为基分类器取代了BERT基，在几个NLP任务中获得了最新的结果。我们还使用“教师-学生”框架中的知识提取来减少BoostingBERT的计算开销和模型存储，同时保持其实际应用的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2004.13947">BURT: BERT-inspired Universal Representation from Twin Structure</a>
<pre>预先训练的语境化语言模型（如BERT）在广泛的下游自然语言处理（NLP）任务中表现出了极大的有效性。然而，模型提供的有效表示针对序列中的每个标记，而不是每个序列，微调步骤涉及同时输入两个序列，导致具有不同粒度的各种序列的不满意表示。特别是，在这些模型中，句子级表征作为完整的训练上下文，在较低级别的语言单位（短语和单词）上表现较差。在这项工作中，我们提出了BURT（受伯特启发的孪生结构通用表示法），它能够使用大量自然语言推理和复述数据，以多个训练目标，为任何粒度的输入序列（即单词、短语和句子）生成通用的、固定大小的表示法。我们提出的BURT采用暹罗网络，分别从自然语言推理数据集中学习句子级表示，从释义数据集中学习单词/短语级表示。我们评估了不同粒度的文本相似性任务，包括STS任务、SemEval2013任务5（a）和一些常用的单词相似性任务，其中BURT在句子级数据集上显著优于其他表示模型，并在单词/短语级表示方面取得了显著改进。</pre></li>
<li><a href="https://arxiv.org/abs/1910.07973">Universal Text Representation from BERT: An Empirical Study</a>
<pre>我们对通用文本表示的分层伯特激活进行了系统研究，以了解它们捕获的语言信息以及它们在不同任务中的可传递性。句子层次的嵌入在SentEval的下游任务和探测任务中根据两种最先进的模型进行评估，而在学习排序问题设置下，在四个问答（QA）数据集上评估段落层次的嵌入。预训练的BERT模型嵌入在语义相似性和句子表面信息探测任务中表现不佳。对自然语言推理数据进行微调可以极大地提高嵌入的质量。组合来自不同层的嵌入可以进一步提高性能。在通道水平上，BERT嵌入在factoid QA数据集上显著优于BM25基线，但在非factoid数据集上的表现不如BM25。对于所有QA数据集，基于嵌入的方法和域内微调BERT（我们报告了两个数据集的最新研究结果）之间存在差距，这表明问答对之间的深度交互对于这些困难任务至关重要。</pre></li>
<li><a href="https://arxiv.org/abs/1909.03405">Symmetric Regularization based BERT for Pair-wise Semantic Reasoning</a> (SIGIR2020)
<pre>句子对的语义推理能力对于许多自然语言理解任务至关重要，例如自然语言推理和机器阅读理解。最近这些任务的一个显著改进来自于BERT。据报道，BERT中的下一句预测（NSP）学习两个句子之间的上下文关系，对于句子对输入的下游问题具有重要意义。尽管NSP有效，但我们认为NSP仍然缺乏区分隐含关联和浅层关联的基本信号。为了弥补这一缺陷，我们建议将NSP任务扩展为三类分类任务，其中包括前一句预测（PSP）类别。PSP的参与促使模型关注信息语义来确定句子顺序，从而提高语义理解能力。这种简单的修改产生了对香草的显著改善。为了进一步纳入文档级信息，NSP和PSP的范围被扩展到更广的范围，即NSP和PSP还包括紧密但不连续的句子，其噪声通过标签平滑技术得到缓解。定性和定量实验结果都证明了该方法的有效性。我们的方法不断提高NLI和MRC基准的性能，包括具有挑战性的HANS数据集，这表明文档级任务仍然有希望用于预培训。</pre></li>
<li><a href="https://arxiv.org/abs/2004.12297">Beyond 512 Tokens: Siamese Multi-depth Transformer-based Hierarchical Encoder for Document Matching</a>
<pre>许多自然语言处理和信息检索问题可以形式化为语义匹配任务。该领域的现有工作主要集中在短文本之间的匹配（如问答），或短文本和长文本之间的匹配（如即席检索）。长格式文档间的语义匹配在新闻推荐、相关文章推荐、文档聚类等领域有着重要的应用，但目前对长格式文档间语义匹配的研究相对较少，需要更多的研究工作。近年来，基于自我注意的模型，如Transformers和BERT，在文本匹配任务中取得了最先进的性能。然而，由于自我注意相对于输入文本长度的二次计算复杂性，这些模型仍然局限于短文本，如几个句子或一个段落。在本文中，我们通过提出用于长格式文档匹配的基于暹罗多深度转换器的分层（SMITH）编码器来解决这个问题。我们的模型包含一些创新，以适应较长文本输入的自我注意模型。为了更好地捕获文档中的句子级语义关系，除了BERT使用的蒙蔽词语言建模任务外，我们还使用一个新的蒙蔽句块语言建模任务对模型进行预训练。我们在多个长格式文档匹配基准数据集上的实验结果表明，我们提出的SMITH模型优于以前的最新模型，包括分层注意、基于多深度注意的分层递归神经网络和BERT。与基于BERT的基线相比，我们的模型能够将最大输入文本长度从512增加到2048。我们将开源一个基于Wikipedia的基准数据集、代码和一个预先训练的检查点，以加速未来对长格式文档匹配的研究。</pre></li>
<li><a href="https://arxiv.org/abs/2106.01040">Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling</a> (ACL2021)
<pre>Transformer对于文本建模非常重要。然而，由于输入文本长度的二次复杂性，它在处理长文档时有困难。为了解决这个问题，我们提出了一种分层交互转换器（Hi-Transformer），用于高效的长文档建模。Hi Transformer以分层方式对文档建模，即首先学习句子表示，然后学习文档表示。它可以有效地降低复杂度，同时在每个句子的建模中捕获全局文档上下文。更具体地说，我们首先使用一个句子转换器来学习每个句子的表示。然后，我们使用文档转换器从这些句子表示中建模全局文档上下文。接下来，我们使用另一个句子转换器来增强使用全局文档上下文的句子建模。最后，我们使用分层池方法来实现文档嵌入。在三个基准数据集上的大量实验验证了Hi Transformer在长文档建模中的效率和有效性。</pre></li>
<li><a href="https://arxiv.org/abs/1909.00931">Transfer Fine-Tuning: A BERT Case Study</a> (EMNLP2019)
<pre>语义等价性评估是指通过二元判断（即释义识别）或分级（即语义文本相似性度量）来评估句子对中语义等价性的任务。它构成了自然语言理解研究的一系列关键任务。最近，BERT在句子表征学习方面取得了突破性进展（Devlin等人，2019年），这种学习方法可广泛应用于各种NLP任务。虽然BERT的性能通过增加其模型大小而得到改善，但所需的计算能力是实际应用无法采用该技术的障碍。在此，我们建议将短语释义关系注入到BERT中，以生成合适的语义等价性评估表示，而不是增加模型的大小。在标准自然语言理解任务上的实验证实，我们的方法在保持模型大小的同时有效地改进了较小的BERT模型。生成的模型在语义等价性评估任务上表现出比更大的BERT模型更好的性能。此外，它在训练数据集有限的任务上获得了更大的性能增益，这是迁移学习所需要的特性。</pre></li>
<li><a href="https://arxiv.org/abs/1909.12440">Improving Pre-Trained Multilingual Models with Vocabulary Expansion</a> (CoNLL2019)
<pre>最近，预训练语言模型在广泛的自然语言处理任务中取得了显著的成功。然而，在多语言环境中，在每种语言的大规模语料库上预先训练深层语言模型是非常耗费资源的。一种替代方案是在数百种语言的大规模语料库上预先训练一个强大的多语言深层语言模型，而不是完全独立地预先训练单语语言模型。然而，在这样的模型中，每种语言的词汇量相对较小，尤其是对于低资源语言。这一限制不可避免地阻碍了这些多语言模型在序列标记等任务上的性能，其中深入的标记级或句子级理解至关重要。在本文中，受先前设计用于单语环境的方法的启发，我们研究了两种基于预先训练的多语言模型的方法（即联合映射和混合映射），用于解决各种任务的词汇表外（OOV）问题，包括词性标注、命名实体识别、，机器翻译质量评估和机器阅读理解。实验结果表明，使用混合映射更具前景。据我们所知，这是首次尝试在多语言环境中解决和讨论OOV问题的工作。</pre></li>
<li><a href="https://arxiv.org/abs/1910.07181">BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance</a> (ACL2020)
<pre>预训练的深层语言模型在NLP中带来了巨大的性能提升。尽管取得了成功，Schick和Sch\“utze（2020年）最近的研究表明，这些模型很难理解稀有词。对于静态词嵌入，这个问题已经通过单独学习稀有词的表示来解决。在这项工作中，我们将这个想法转移到预训练语言模型中：我们引入BERTRAM，这是一个基于BERT的强大体系结构，能够推断出高质量的语言适合作为深层语言模型输入表示的稀有词的可嵌入性。这是通过使词的表面形式和上下文在深层体系结构中相互交互来实现的。由于稀有和中频表示的改进，将BERTRAM集成到BERT中可大大提高性能一个罕见单词探测任务和三个下游任务中的单词。</pre></li>
<li><a href="https://arxiv.org/abs/2005.06537">A Mixture of h−1 Heads is Better than h Heads</a> (ACL2020)
<pre>多头专注神经结构在各种自然语言处理任务上取得了最新成果。有证据表明，它们被过度参数化了；注意力集中可以在没有显著性能损失的情况下进行修剪。在这项工作中，我们取而代之的是“重新分配”它们——模型学习在不同的输入上激活不同的头部。借鉴多头注意与专家混合的关系，提出了注意专家混合模型（MAE）。MAE使用块坐标下降算法进行训练，该算法在更新（1）专家职责和（2）专家参数之间交替进行。机器翻译和语言建模实验表明，MAE在这两项任务上都优于强基线。特别是，在WMT14英语到德语翻译数据集上，MAE比“transformer base”提高了0.8 BLEU，参数数量相当。我们的分析表明，我们的模型学习将不同的专家专门化到不同的输入。</pre></li>
<li><a href="https://arxiv.org/abs/1910.03176">SesameBERT: Attention for Anywhere</a>
<pre>使用预先训练好的模型进行微调在许多语言任务中取得了优异的效果。在这项研究中，我们重点研究了这样一种自我注意网络模型，即BERT，它在不同语言理解基准的层次叠加方面表现良好。然而，在许多下游任务中，层之间的信息被用于微调的BERT忽略。此外，尽管自我注意网络以其捕捉全局依赖性的能力而闻名，但在强调局部环境的重要性方面仍有改进的余地。鉴于这些优点和缺点，本文提出了一种广义微调方法SesameBERT，该方法（1）通过压缩和激励来提取所有层之间的全局信息，（2）通过高斯模糊捕获相邻上下文来丰富局部信息。此外，我们在HANS数据集中证明了我们的方法的有效性，该数据集用于确定模型是否采用了浅层启发式而不是学习潜在的泛化。实验表明，SesameBERT在GLUE基准测试和HANS评估集方面优于BERT。</pre></li>
<li><a href="https://arxiv.org/abs/2006.16362">Multi-Head Attention: Collaborate Instead of Concatenate</a>
<pre>注意层广泛应用于自然语言处理（NLP），并开始影响计算机视觉体系结构。训练非常大的变压器模型可以在这两个领域取得显著的改进，但一旦训练，这些网络就会表现出过度参数化的症状。例如，众所周知，许多注意力头可以在不影响准确性的情况下被剪掉。这项工作的目的是加强目前对多个头部如何相互作用的理解。基于注意头学习冗余关键/查询投射的观察结果，我们提出了一个协作的多头注意层，使注意头能够学习共享投射。我们的方案减少了注意层中的参数数量，并且可以作为任何变压器架构中的替代品。我们的实验证实，共享键/查询维度可用于语言理解、机器翻译和视觉。我们还表明，可以将预先训练的多头注意层重新参数化为我们的协作注意层。协作式多头注意将密钥和查询投影的大小减少了4，以获得相同的准确性和速度。我们的代码是公开的。</pre></li>
<li><a href="https://arxiv.org/abs/2006.03654">DeBERTa: Decoding-enhanced BERT with Disentangled Attention</a> [<a href="https://github.com/microsoft/DeBERTa">github</a>]
<pre>预训练神经语言模型的最新进展显著提高了许多自然语言处理（NLP）任务的性能。在本文中，我们提出了一种新的模型体系结构DeBERTa（解码增强型BERT与分散注意），它使用两种新技术改进了BERT和RoBERTa模型。第一种是解纠缠注意机制，其中每个单词分别使用两个向量表示，这两个向量分别编码其内容和位置，单词之间的注意权重分别使用其内容和相对位置上的解纠缠矩阵计算。第二，使用增强的掩码解码器在解码层中合并绝对位置，以在模型预训练中预测掩码令牌。此外，采用一种新的虚拟对抗训练方法进行微调，以提高模型的泛化能力。我们发现，这些技术显著提高了模型预训练的效率以及自然语言理解（NLU）和自然语言生成（NLG）下游任务的性能。与RoBERTa Large相比，基于一半训练数据训练的DeBERTa模型在广泛的NLP任务中表现始终更好，MNLI提高了+0.9%（90.2%对91.1%），球队v2.0提高了+2.3%（88.4%对90.7%），比赛提高了+3.6%（83.2%对86.8%）。值得注意的是，我们通过训练一个更大的版本来扩展DeBERTa，该版本由48个具有15亿个参数的变换层组成。在宏观平均得分方面（89.9分对89.8分），显著的性能提升使单德贝塔模型在SuperGLUE基准（Wang et al.，2019a）上首次超过了人类性能，而整体德贝塔模型在2021年1月6日的SuperGLUE排行榜上名列前茅，以相当大的幅度超过人类基线（90.3比89.8）。</pre></li>
<li><a href="https://arxiv.org/abs/1911.01940">Deepening Hidden Representations from Pre-trained Language Models</a>
<pre>基于变压器的预训练语言模型已被证明是学习语境化语言表达的有效方法。然而，当前的方法仅在微调下游任务时利用编码器最后一层的输出。我们认为，仅获取单层的输出限制了预训练表示的能力。因此，我们通过将隐藏表示融合到显式隐藏表示提取器（HIRE）中来深化模型学习的表示，该提取器自动吸收关于最后一层输出的互补表示。利用RoBERTa作为主干编码器，我们提出的对预先训练的模型的改进在多个自然语言理解任务中表现出有效性，并帮助我们的模型在GLUE基准上与最先进的模型竞争。</pre></li>
<li><a href="https://arxiv.org/abs/2010.12562">On the Transformer Growth for Progressive BERT Training</a>
<pre>由于大规模语言模型预训练的成本过高，已经做出了相当大的努力来逐步训练BERT——从一个较低但成本较低的模型开始，逐步扩展模型以增加计算复杂性。我们的目标是促进对变压器增长的理解，并发现指导渐进式培训的原则。首先，我们发现，与网络架构搜索类似，转换器的增长也有利于复合扩展。具体而言，虽然现有方法仅在单个维度上进行网络增长，但我们观察到，使用复合增长算子并平衡多个维度（例如，模型的深度、宽度和输入长度）是有益的。此外，我们还通过受控比较探索了各个维度的替代增长算子，为算子选择提供了实际指导。根据我们的分析，所提出的方法将基本模型和大型模型的BERT预训练速度分别提高了73.6%和82.2%，同时实现了相当的性能</pre></li>
<li><a href="https://arxiv.org/abs/2004.03808">Improving BERT with Self-Supervised Attention</a>
<pre>应用大型预训练NLP模型（如BERT）的最流行范例之一是在较小的数据集上对其进行微调。然而，一个挑战仍然存在，因为微调后的模型往往过于适合较小的数据集。这种现象的一个症状是，句子中不相关的词，即使它们对人类来说是显而易见的，也会严重降低这些经过微调的伯特模型的性能。在本文中，我们提出了一种新的技术，称为自我监督注意（SSA），以帮助促进这一推广挑战。具体来说，SSA通过“探测”上一次迭代中经过微调的模型，自动迭代生成弱的标记级注意标签。我们研究了将SSA集成到BERT中的两种不同方法，并提出了一种混合方法来结合它们的优点。根据经验，在各种公共数据集上，我们使用SSA增强的BERT模型说明了显著的性能改进。</pre></li>
<li><a href="https://arxiv.org/abs/2010.02399">Guiding Attention for Self-Supervised Learning with Transformers</a> (EMNLP2020 Findings)
<pre>在本文中，我们提出了一种简单有效的技术，允许有效的自监督学习与双向变压器。最近的研究表明，训练模型中的自我注意模式包含了大多数非语言规律，这是我们研究的动机。我们提出了一个计算效率很高的辅助损失函数来引导注意头符合这种模式。我们的方法与实际的预训练目标无关，与基线相比，模型收敛速度更快，下游任务性能更好，在低资源环境下实现最先进的结果。令人惊讶的是，我们还发现注意头的语言特性并不一定与语言建模性能相关。</pre></li>
<li><a href="https://arxiv.org/abs/2004.05323">Improving Disfluency Detection by Self-Training a Self-Attentive Model</a>
<pre>目前，使用上下文化单词嵌入（如ELMo或BERT）的自我关注神经句法分析器在语音转录本的联合句法分析和不流利检测方面产生了最先进的结果。由于上下文化的单词嵌入是在大量未标记数据上预先训练的，因此使用额外的未标记数据来训练神经模型似乎是多余的。然而，我们证明了自训练（一种用于合并未标记数据的半监督技术）为自关注解析器在不流畅检测方面提供了一种新的技术，证明了自训练提供了与预先训练的上下文化单词表示正交的好处。我们还表明，置乱自训练解析器为不流畅检测提供了进一步的增益。</pre></li>
<li><a href="https://arxiv.org/abs/2010.02194">Self-training Improves Pre-training for Natural Language Understanding</a> [<a href="https://github.com/facebookresearch/SentAugment">github</a>]
<pre>无监督的预培训在自然语言理解方面取得了很大的进步。在本文中，我们研究了自我训练作为另一种通过半监督学习利用未标记数据的方法。为了获得特定任务的附加数据，我们引入了SentAugment，这是一种数据增强方法，它从标记数据计算特定于任务的查询嵌入，以从从web上抓取的数十亿个未标记句子库中检索句子。与以前的半监督方法不同，我们的方法不需要域内未标记的数据，因此更普遍适用。实验表明，在各种任务中，自我训练与强大的罗伯塔基线是相辅相成的。我们的增强方法实现了可扩展和有效的自我训练，在标准文本分类基准上提高了2.6%。最后，我们还展示了在知识提炼和少量学习方面的强大收益。</pre></li>
<li><a href="https://arxiv.org/abs/2005.12766">CERT: Contrastive Self-supervised Learning for Language Understanding</a>
<pre>诸如BERT、GPT等预训练语言模型在语言理解中表现出了极大的有效性。现有预训练方法中的辅助预测任务大多是在标记上定义的，因此可能无法很好地捕捉句子级语义。为了解决这个问题，我们提出了CERT：来自Transformers的对比自监督编码器表示法，它在句子层面上使用对比自监督学习预训练语言表示模型。CERT使用回译创建原始句子的增广。然后，通过预测两个增广句是否来自同一个句子，对预先训练好的语言编码器（如BERT）进行微调。CERT使用简单，可以灵活地插入任何预调试NLP管道。我们在GLUE基准测试中对11项自然语言理解任务的CERT进行了评估，其中CERT在7项任务上优于BERT，在2项任务上达到与BERT相同的性能，在2项任务上的性能不如BERT。在11项任务的平均得分上，CERT优于BERT。有关数据和代码，请访问https://github.com/UCSD-AI4H/CERT</pre></li>
<li><a href="https://arxiv.org/abs/2108.02340">Robust Transfer Learning with Pretrained Language Models through Adapters</a> (ACL2021)
<pre>在大多数NLP任务中，使用基于变换器的大容量预训练语言模型（如BERT）进行迁移学习已成为一种主要方法。简单地对下游任务上的大型语言模型进行微调，或者将其与特定任务的预训练相结合，通常是不可靠的。特别是，随着随机种子的变化或预训练和/或微调迭代次数的变化，性能会有很大的变化，微调模型容易受到对抗性攻击。我们提出了一种简单而有效的基于适配器的方法来缓解这些问题。具体地说，我们在预训练模型的每一层中插入小瓶颈层（即适配器），然后固定预训练层并在下游任务数据上训练适配器层，使用（1）特定于任务的无监督预训练，然后（2）特定于任务的有监督训练（例如，分类、序列标记）。我们的实验表明，这样的训练方案提高了向各种下游任务转移学习的稳定性和对抗鲁棒性。</pre></li>
<li><a href="https://arxiv.org/abs/2010.12854">ReadOnce Transformers: Reusable Representations of Text for Transformers</a> (ACL2021)
<pre>我们介绍ReadOnce Transformers，一种将基于transformer的模型转换为能够构建信息捕获、任务独立和文本压缩表示的模型的方法。结果表示可在不同的示例和任务之间重用，因此需要在多个示例或任务之间共享一个文档，\emph{read once}。这将加快模型的培训和评估。此外，我们将标准文本到文本转换器模型扩展为表示+文本到文本模型，并对多个下游任务进行评估：多跳QA、抽象QA和长文档摘要。与标准文本到文本模型相比，我们的一次性计算表示法的速度提高了2-5倍，而压缩还允许现有语言模型处理更长的文档，而无需设计新的预训练模型。</pre></li>
<li><a href="https://arxiv.org/abs/2106.11740">LV-BERT: Exploiting Layer Variety for BERT</a> (ACL2021 Findings) [<a href="https://github.com/yuweihao/LV-BERT">github</a>]
<pre>现代预先训练的语言模型大多建立在主干上，以交错顺序堆叠自我注意和前馈层。在本文中，除了这种刻板的层模式之外，我们还从层类型集和层顺序两个方面利用层的多样性来改进预先训练的模型。具体来说，除了原始的自我注意和前馈层外，我们还将卷积引入层类型集中，实验发现这对预训练模型是有益的。此外，除了原来的交错顺序，我们还探索了更多的层顺序，以发现更强大的体系结构。然而，引入的层种类导致了超过数十亿个候选的大型体系结构空间，而从头开始训练单个候选模型已经需要巨大的计算成本，这使得通过直接训练大量候选模型来搜索这样的空间是不可承受的。为了解决这个问题，我们首先预先训练一个超网，从中可以继承所有候选模型的权重，然后采用一种基于预训练精度的进化算法来寻找最优结构。大量实验表明，该方法得到的LV-BERT模型在各种下游任务上都优于BERT及其变体。例如，LV BERT small在胶水测试集上达到79.8，比强基线ELECTRA small高1.8。</pre></li>
<li><a href="https://arxiv.org/abs/2010.03881">Large Product Key Memory for Pretrained Language Models</a> (EMNLP2020 Findings)
<pre>Lample等人（2019）提出的产品密钥存储器（PKM）能够通过有效地增加模型容量来提高预测精度，而计算开销很小。然而，他们的实证应用仅限于因果语言建模。受预训练语言模型（PLM）最近取得的成功的推动，我们研究了如何将大型PKM整合到PLM中，以便对广泛的下游NLP任务进行微调。我们定义了一个新的内存使用度量，使用该度量进行仔细观察发现，在PKM增强模型的训练期间，大多数内存插槽仍然过时。为了通过解决这个问题来训练更好的PLM，我们提出了简单但有效的解决方案：（1）从无记忆预训练的模型权重初始化；（2）通过添加而不是替换前馈网络来增强PKM。我们验证了它们对于PKM增强PLM的预训练、提高内存利用率和下游性能至关重要。代码和预训练重量可在https://github.com/clovaai/pkm-transformers.</pre></li>
<li><a href="https://arxiv.org/abs/2012.15070">Enhancing Pre-trained Language Model with Lexical Simplification</a>
<pre>对于人类读者和预先训练的语言模型（PrLMs），词汇多样性在理解给定句子的潜在语义时可能会导致混乱和不准确。通过用简单的替代词替换复杂的词，词汇简化（LS）是一种公认的减少词汇多样性，从而提高句子可理解性的方法。在本文中，我们利用LS，提出了一种新的方法，可以有效地提高PrLMs在文本分类中的性能。将基于规则的简化过程应用于给定的句子。鼓励PRLM使用简化版本的辅助输入预测给定句子的真实标签。使用强PrLMs（BERT和ELECTRA）作为基线，我们的方法可以进一步提高在各种文本分类任务中的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2010.15778">Contextual BERT: Conditioning the Language Model Using a Global State</a> (COLING2020 WS)
<pre>伯特是一个流行的语言模型，其主要的预训练任务是填空，即根据剩余的单词预测出一个被掩蔽的单词。然而，在某些应用程序中，具有额外的上下文可以帮助模型做出正确的预测，例如，通过考虑域或编写时间。这促使我们通过在固定大小的上下文中添加一个全局状态来改进BERT体系结构。我们提出了两种新颖的方法，并将其应用到一个行业用例中，在该用例中，我们根据特定的客户完成了带有缺失物品的时装套装。与文献中其他方法的实验比较表明，我们的方法显著提高了个性化。</pre></li>
<li><a href="https://arxiv.org/abs/1911.03437">SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization</a> (ACL2020)
<pre>迁移学习从根本上改变了自然语言处理（NLP）研究的格局。许多现有的最先进的模型首先在大型文本语料库上进行预训练，然后在下游任务上进行微调。然而，由于下游任务的数据资源有限，且预训练模型的容量非常大，主动微调往往会导致自适应模型过度拟合下游任务的数据，并忘记预训练模型的知识。为了以更具原则性的方式解决上述问题，我们提出了一种新的计算框架，用于对预先训练的语言模型进行鲁棒和高效的微调。具体而言，我们提出的框架包含两个重要组成部分：1。平滑诱导正则化，有效管理模型的容量；2.Bregman近邻点优化是一类信赖域方法，可以防止知识遗忘。我们的实验表明，我们提出的方法在多个NLP基准上达到了最先进的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2109.05687">Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning</a> (EMNLP2021) [<a href="https://github.com/RunxinXu/ChildTuning">github</a>]
<pre>最近的预训练语言模型从数百万个参数扩展到数十亿个参数。因此，在各种下游任务中，需要使用有限的训练语料对一个非常大的预训练模型进行微调。在本文中，我们提出了一种简单而有效的微调技术，即子调优，它通过在向后过程中策略性地屏蔽非子网络的梯度来更新大型预训练模型的参数子集（称为子网络）。在GLUE benchmark中对各种下游任务的实验表明，在四种不同的预训练模型中，子调优始终比香草微调的平均分高1.5~8.6分，比先前的微调技术高0.6~1.3分。此外，领域转移和任务转移的实证结果表明，子调优可以获得更大幅度的泛化性能。</pre></li>
<li><a href="https://arxiv.org/abs/2105.08050">Pay Attention to MLPs</a>
<pre>变压器已成为深度学习中最重要的架构创新之一，并在过去几年中实现了许多突破。在这里，我们提出了一个简单的网络体系结构，gMLP，基于带选通的MLP，并表明它可以在关键的语言和视觉应用程序中执行和转换。我们的比较表明，自我注意对于视觉转换器来说并不重要，因为gMLP可以达到同样的精确度。对于BERT，我们的模型在训练前的复杂度上与Transformers实现了对等，并且在一些下游NLP任务上更好。在gMLP性能较差的微调任务中，使gMLP模型大得多可以缩小与变压器的差距。总的来说，我们的实验表明，随着数据和计算量的增加，gMLP可以扩展和转换。</pre></li>
<li><a href="https://arxiv.org/abs/2105.03322">Are Pre-trained Convolutions Better than Pre-trained Transformers?</a> (ACL2021)
<pre>在预先训练语言模型的时代，变形金刚实际上是模型架构的选择。虽然最近的研究显示了完全卷积（CNN）架构的前景，但尚未使用预训练微调范式对其进行探索。在语言模型的上下文中，当预先训练时，卷积模型是否与转换器竞争？本文调查了这个研究问题，并提出了一些有趣的发现。通过对8个数据集/任务的大量实验，我们发现基于CNN的预训练模型具有竞争力，在某些情况下优于变压器模型，尽管有一些警告。总的来说，本文中概述的研究结果表明，将预培训和体系结构进步混为一谈是错误的，应该单独考虑这两种进步。我们相信，我们的研究为替代架构中的健康乐观铺平了道路。</pre></li>
<li><a href="https://arxiv.org/abs/2012.11995">Pre-Training a Language Model Without Human Language</a>
<pre>在本文中，我们研究了预训练数据的内在性质如何影响微调的下游性能。为此，我们在几个具有特定特征的语料库上预先训练不同的基于转换器的蒙面语言模型，并在GLUE基准上微调这些语言模型。我们发现，在非结构化数据上预先训练的模型比在下游任务上直接从头训练的模型要好。我们的结果还表明，对结构化数据的预训练并不总是能够使模型获得可以转移到自然语言下游任务的能力。令我们大吃一惊的是，我们发现，对某些非人类语言数据的预训练使GLUE的表现接近于对另一种非英语语言的预训练。</pre></li>
</ul>
<h2 id="tokenization">Tokenization</h2>
<ul>
<li><a href="https://arxiv.org/abs/2101.09012">Training Multilingual Pre-trained Language Model with Byte-level Subwords</a>
<pre>在本文中，我们提出了一个迁移学习系统来对多语言文本数据进行技术领域识别。我们提交了两次运行，一次使用transformer模型BERT，另一次使用XLM ROBERTa和CNN模型进行文本分类。这些模型允许我们为ICON 2020共享任务TechDOfication:Technical domain Identification识别给定句子的域。对于给定的TechDOfication数据集，我们的系统在子任务1d、1g方面排名最佳。</pre></li>
<li><a href="https://arxiv.org/abs/2004.03720">Byte Pair Encoding is Suboptimal for Language Model Pretraining</a> (EMNLP2020 Findings)
<pre>预训练变换器语言模型（LMs）在自然语言处理中的成功导致了广泛的预训练设置。特别是，这些模型采用了多种子词标记化方法，最显著的是字节对编码（BPE）（Sennrich等人，2016；Gage，1994）、词条法（Schuster和Nakajima，2012）和单格语言建模（Kudo，2018）来分割文本。然而，据我们所知，文献中没有直接评估标记化对语言模型预训练的影响。我们分析了BPE和unigram LM标记化之间的差异，发现后一种方法可以恢复与形态学更紧密对齐的子词单元，并避免了因BPE贪婪的构造过程而产生的问题。然后，我们比较使用这些标记化预训练的相同转换器屏蔽语言模型的微调任务性能。在下游任务和两种语言（英语和日语）中，我们发现unigram LM标记化方法匹配或优于BPE。我们希望未来的预训练LMS的开发人员会考虑在更普遍的BPE上采用UNIGRAM LM方法。</pre></li>
<li><a href="https://arxiv.org/abs/2103.06874">CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation</a> (TACL2022) [<a href="https://github.com/google-research/language/tree/master/language/canine">github</a>]
<pre>流水线NLP系统在很大程度上已经被端到端的神经建模所取代，但几乎所有常用的模型仍然需要显式的标记化步骤。尽管最近基于数据派生的子词词典的标记化方法比手工设计的标记化器不那么脆弱，但这些技术并不同样适用于所有语言，并且使用任何固定的词汇表可能会限制模型的适应能力。在本文中，我们介绍了CANINE，一种直接对字符序列进行操作的神经编码器，无需显式标记或词汇，以及一种直接对字符进行操作或选择性地使用子词作为软归纳偏差的预训练策略。为了有效地使用更细粒度的输入，CANINE将减少输入序列长度的下采样与对上下文进行编码的深层转换器堆栈相结合。尽管模型参数减少了28%，但犬科动物在TyDi QA（一种具有挑战性的多语言基准测试）上的表现优于可比mBERT模型2.8 F1。</pre></li>
<li><a href="https://arxiv.org/abs/2105.13626">ByT5: Towards a token-free future with pre-trained byte-to-byte models</a> (TACL2022) [<a href="https://github.com/google-research/byt5">github</a>]
<pre>最广泛使用的预先训练的语言模型操作对应于单词或子单词单元的标记序列。将文本编码为标记序列需要标记器，该标记器通常作为独立于模型的工件创建。相反，直接在原始文本（字节或字符）上操作的无令牌模型有许多好处：它们可以用任何语言即时处理文本，它们对噪音的鲁棒性更强，并且通过删除复杂且容易出错的文本预处理管道将技术债务降至最低。由于字节或字符序列比令牌序列长，过去关于无令牌模型的工作经常引入新的模型体系结构，用于分摊直接在原始文本上操作的成本。在本文中，我们展示了一个标准的转换器体系结构，只需对处理字节序列进行最小的修改即可使用。我们仔细描述了参数计数、训练失败和推理速度方面的权衡，并表明字节级模型与令牌级模型具有竞争力。我们还证明了字节级模型对噪声的鲁棒性显著提高，并且在对拼写和发音敏感的任务上表现更好。作为我们贡献的一部分，我们发布了一组新的基于T5体系结构的预训练字节级转换器模型，以及我们实验中使用的所有代码和数据。</pre></li>
<li><a href="https://arxiv.org/abs/2103.08490">Multi-view Subword Regularization</a> (NAACL2021)
<pre>多语言预训练表示通常依赖于子词切分算法来创建共享的多语言词汇表。然而，标准的启发式算法往往导致次优分割，特别是对于数据量有限的语言。在本文中，我们采取了两个主要步骤来缓解这个问题。首先，我们从经验上证明，在对预先训练的多语言表征进行微调的过程中，应用现有的子词正则化方法（Kudo，2018；Provilkov et al.，2020）可以提高跨语言迁移的有效性。其次，为了充分利用不同的可能输入分割，我们提出了多视图子词正则化（MVR），这是一种加强使用标准分割和概率分割标记的输入预测之间一致性的方法。XTREME多语言基准测试（Hu等人，2020年）的结果表明，MVR比使用标准分割算法带来了高达2.5个点的一致改进。</pre></li>
<li><a href="https://arxiv.org/abs/2106.06125">Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation</a> (ACL2021)
<pre>pretrain-finetune范式的一个众所周知的局限性在于其由“一刀切”词汇表造成的不灵活。这可能会削弱将预训练模型应用于自然语言生成（NLG）任务时的效果，特别是对于存在显著差异的上游和下游任务之间的子词分布。为了解决这个问题，我们扩展了vanilla-pretrain-finetune管道，增加了一个嵌入传输步骤。具体地说，引入了一个即插即用嵌入生成器，根据其形态相似的预训练嵌入生成任意输入标记的表示。因此，也可以有效地初始化下游任务中不匹配令牌的嵌入。我们在pretrain finetune模式下对各种NLG任务进行了实验。实验结果和广泛的分析表明，所提出的策略为我们提供了自由转移词汇的机会，从而使下游NLG模型更高效、性能更好。</pre></li>
<li><a href="https://arxiv.org/abs/2010.02534">An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks</a> (AACL-IJCNLP2020)
<pre>通常，标记化是大多数文本处理工作的第一步。由于令牌作为嵌入文本上下文信息的原子单元，如何定义令牌对模型的性能起着决定性的作用。尽管字节对编码（BPE）由于其简单性和通用性被认为是事实上的标准令牌化方法，目前仍不清楚BPE是否在所有语言和任务中都工作得最好。在本文中，我们测试了几种标记化策略，以回答我们的主要研究问题，即“韩国NLP任务的最佳标记化策略是什么？”实验结果表明，形态学分割和BPE相结合的混合方法在韩语到/从英语机器翻译和自然语言理解任务（如KorNLI、KorSTS、NSMC和PAWS-X）中效果最好。作为一个例外，KorQuAD是韩国班的扩展，BPE细分结果是最有效的。</pre></li>
<li><a href="https://arxiv.org/abs/2008.11869">AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization</a>
<pre>在自然语言理解（NLU）的许多任务中，诸如BERT等预训练语言模型表现出了显著的性能。模型中的标记通常是细粒度的，因为对于像英语这样的语言，它们是单词或子单词，对于像汉语这样的语言，它们是字符。例如，在英语中，存在形成自然词汇单位的多词表达式，因此粗粒度标记化的使用似乎也是合理的。事实上，细粒度和粗粒度标记化在学习预先训练的语言模型方面都有优点和缺点。在本文中，我们在细粒度和粗粒度标记化的基础上提出了一种新的预训练语言模型AMBERT（Multi-grainedbert）。对于英语，AMBERT将单词序列（细粒度标记）和短语序列（粗粒度标记）作为标记化后的输入，使用一个编码器处理单词序列，另一个编码器处理短语序列，利用两个编码器之间的共享参数，最后创建一系列词语的语境化表示和一系列短语的语境化表示。实验已经在中文和英文的基准数据集上进行，包括线索、胶水、队伍和种族。结果表明，AMBERT在所有情况下都优于BERT，特别是对于中国人来说，改进是显著的。我们还开发了一种提高AMBERT推理效率的方法，该方法在与BERT相同的计算量下仍然比BERT具有更好的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2108.00801">LICHEE: Improving Language Model Pre-training with Multi-grained Tokenization</a> (ACL2021 Findings)
<pre>基于大型语料库的语言模型预训练在构建丰富的语境表征方面取得了巨大成功，并在各种自然语言理解（NLU）任务中取得了显著的成绩。尽管取得了成功，但大多数当前的预训练语言模型（如BERT）都是基于单粒度标记化进行训练的，通常使用细粒度字符或子单词，这使得它们很难学习粗粒度单词和短语的精确含义。在本文中，我们提出了一种简单而有效的预训练方法LICHEE来有效地融合输入文本的多粒度信息。我们的方法可以应用于各种预先训练的语言模型，提高它们的表示能力。在CLUE和SuperGLUE上进行的大量实验表明，我们的方法在几乎不增加额外推理成本的情况下，对各种中文和英文NLU任务实现了全面改进，并且我们的最佳集成模型在CLUE基准测试中达到了最先进的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2104.07204">Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models</a> (NAACL2021)
<pre>中国预先训练的语言模型通常将文本处理为一系列字符，而忽略了更粗的粒度，例如单词。在这项工作中，我们提出了一种新的汉语预训练范式——格子BERT，它明确地结合了单词表示和字符，从而可以以多粒度的方式对句子进行建模。具体来说，我们从一个句子中的字符和单词构造一个格图，并将所有这些文本单元输入到转换器中。我们设计了一种晶格位置注意机制来利用自注意层中的晶格结构。我们进一步提出了一个掩蔽段预测任务，以推动模型从晶格中固有的丰富但冗余的信息中学习，同时避免学习意外的技巧。在11个中文自然语言理解任务上的实验表明，在12层设置下，我们的模型平均提高了1.5%，在线索基准上实现了基础尺寸模型的新水平。进一步的分析表明，latticebert可以利用晶格结构，其改进来自于对冗余信息和多粒度表示的探索。我们的代码将在https://github.com/alibaba/pretrained-language-models/LatticeBERT.</pre></li>
<li><a href="https://arxiv.org/abs/2011.01513">CharBERT: Character-aware Pre-trained Language Model</a> (COLING2020) [<a href="https://github.com/wtma/CharBERT">github</a>]
<pre>大多数预先训练的语言模型（PLM）在子词级别上使用字节对编码（BPE）或其变体构造词表示，通过这种编码，几乎可以避免OOV（词汇量外）词。然而，这些方法将一个单词分割成子单词单元，使得表示不完整和脆弱。在本文中，我们提出了一个字符感知的预训练语言模型，名为CharBERT，它改进了以前的方法（如BERT、RoBERTa）来解决这些问题。我们首先从序列字符表示中构造每个标记的上下文单词嵌入，然后通过一个新的异构交互模块融合字符表示和子单词表示。我们还提出了一个新的训练前任务NLM（Noised LM），用于无监督的字符表示学习。我们在原始数据集和对抗性拼写错误测试集上评估了我们在问答、序列标记和文本分类任务上的方法。实验结果表明，该方法能显著提高PLMs的性能和鲁棒性。预训练模型、评估集和代码可在https://github.com/wtma/CharBERT</pre></li>
<li><a href="https://arxiv.org/abs/2010.10392">CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters</a> (COLING2020)
<pre>由于BERT带来的引人注目的改进，许多最近的表示模型采用Transformer架构作为其主要构建块，因此继承了文字标记化系统，尽管它与Transformer的概念没有本质联系。虽然该系统被认为能够在字符的灵活性和完整单词的效率之间实现良好的平衡，但使用来自一般领域的预定义词条词汇并不总是合适的，尤其是在为专门领域（例如，医学领域）构建模型时。此外，采用词条标记化将焦点从词级转移到子词级，这使得模型在概念上更复杂，在实践中可能更不方便。基于这些原因，我们提出了CharacterBERT，这是BERT的一个新变体，它完全放弃了词条系统，而是使用字符CNN模块通过查询字符来表示整个单词。我们表明，这种新模型提高了BERT在各种医学领域任务上的性能，同时产生了健壮的、单词级的和开放的词汇表示。</pre></li>
<li><a href="https://arxiv.org/abs/2106.12672">Charformer: Fast Character Transformers via Gradient-based Subword Tokenization</a> [<a href="https://github.com/google-research/google-research/tree/master/charformer">github</a>]
<pre>自然语言处理中最先进的模型依赖于独立的刚性子词标记化算法，这限制了它们的泛化能力和对新设置的适应能力。在本文中，我们提出了一个新的模型归纳偏见学习子字标记端到端作为模型的一部分。为此，我们引入了一个基于软梯度的子字标记化模块（GBST），该模块以数据驱动的方式自动从字符中学习潜在子字表示。具体地说，GBST枚举候选子词块并学习使用块评分网络以位置方式对它们进行评分。我们还介绍了Charformer，这是一种深度转换器模型，它集成了GBST并在字节级别上运行。通过对英文胶水、多语种和嘈杂文本数据集的大量实验，我们表明，字符分析器优于一系列竞争的字节级基线，而通常执行PAR和有时优于基于子词的模型。此外，Charformer速度很快，将字节级和子字级转换器的速度提高了28%-100%，同时保持了具有竞争力的质量。我们相信这项工作为完全端到端培训的高性能无令牌模型铺平了道路。</pre></li>
<li><a href="https://arxiv.org/abs/2012.15524">Fast WordPiece Tokenization</a> (EMNLP2021)
<pre>标记化是几乎所有NLP任务的基本预处理步骤。在本文中，我们提出了用于BERT的词条标记化的有效算法，从单个单词标记化到一般文本（例如句子）标记化。当标记单个单词时，WordPiece使用最长匹配优先策略，称为最大匹配。到目前为止，最著名的算法是O（n^2）（其中n是输入长度）或O（nm）（其中m是最大词汇表令牌长度）。我们提出了一种新的算法，其标记化复杂度严格为O（n）。我们的方法受到了Aho-Corasick算法的启发。我们在词汇表构建的trie之上引入了额外的链接，允许在trie匹配无法继续时进行智能转换。对于一般文本，我们进一步提出了一种算法，该算法将预标记化（将文本拆分为单词）和我们的线性时间字条方法结合到一个单一过程中。实验结果表明，对于一般的文本标记化，我们的方法平均比HuggingFace标记化器快8.2倍，比TensorFlow文本快5.1倍。</pre></li>
</ul>
<h2 id="prompt">Prompt</h2>
<ul>
<li><a href="https://arxiv.org/abs/2107.13586">Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</a>
<pre>本文以自然语言处理的一种新范式，即我们称之为“基于提示的学习”来审视和组织研究工作。与传统的有监督学习不同，传统的有监督学习训练模型接收输入x并将输出y预测为P（y | x），基于提示的学习是基于语言模型的，语言模型直接模拟文本的概率。为了使用这些模型执行预测任务，使用模板将原始输入x修改为具有一些未填充槽的文本字符串提示x'，然后使用语言模型概率地填充未填充信息以获得最终字符串x，从中可以导出最终输出y。该框架功能强大且具有吸引力，原因有很多：它允许语言模型在大量原始文本上进行预训练，并且通过定义新的提示函数，该模型能够执行少量甚至零次学习，以适应具有少量或无标记数据的新场景。在本文中，我们介绍了这一有前途的范例的基础，描述了一组统一的数学符号，这些符号可以涵盖广泛的现有工作，并沿几个维度组织现有工作，例如选择预先训练的模型、提示和调整策略。为了让感兴趣的初学者更容易进入该领域，我们不仅对现有作品和基于提示的概念的高度结构化类型进行了系统的回顾，而且还发布了其他资源，例如网站http://pretrain.nlpedia.ai/ 包括不断更新的调查和纸质清单。</pre></li>
<li><a href="https://arxiv.org/abs/2010.15980">AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts</a> (EMNLP2020) [<a href="https://github.com/ucinlp/autoprompt">github</a>]
<pre>训练前语言模型的显著成功促使人们研究这些模型在训练前学习了哪些知识。将任务重新编排为填空问题（例如完形填空测试）是衡量此类知识的自然方法，但是，其使用受到编写适当提示所需的手动工作和猜测的限制。为了解决这个问题，我们开发了AutoPrompt，这是一种基于梯度引导搜索的自动方法，用于为各种任务创建提示。使用AutoS提示，我们表明掩蔽语言模型（MLMS）具有固有的能力来执行情感分析和自然语言推理而不需要额外的参数或精细化，有时达到与最近的最先进的监督模型相一致的性能。我们还表明，与在LAMA基准上手动创建的提示相比，我们的提示可以从MLM中获得更准确的事实知识，并且MLM可以比有监督的关系提取模型更有效地用作关系提取器。这些结果表明，自动生成的提示是现有探测方法的一种可行的无参数替代方法，并且随着预训练LMs变得更加复杂和有能力，有可能替代微调。</pre></li>
<li><a href="https://arxiv.org/abs/2102.09690">Calibrate Before Use: Improving Few-Shot Performance of Language Models</a>
<pre>当提供包含一些训练示例的自然语言提示时，GPT-3可以执行许多任务。我们发现，这种类型的少数镜头学习可能是不稳定的：提示格式、训练示例的选择，甚至训练示例的顺序都可能导致准确性从近乎偶然到近乎先进的变化。我们证明，这种不稳定性源于语言模型对预测某些答案的偏见，例如，那些放在提示末尾附近或在训练前数据中常见的答案。为了缓解这种情况，我们首先通过在给定训练提示和无内容测试输入（如“N/a”）的情况下询问模型的预测来估计模型对每个答案的偏差。然后，我们拟合校准参数，使该输入的预测在所有答案中保持一致。在一组不同的任务中，此上下文校准程序大大提高了GPT-3和GPT-2的平均精度（高达30.0%绝对精度），并减少了不同提示选择之间的差异。</pre></li>
<li><a href="https://arxiv.org/abs/2102.07350">Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm</a>
<pre>将大型生成性语言模型映射到监督任务的主流方法可能无法充分探测模型的新功能。使用GPT-3作为一个案例研究，我们表明0-shot提示可以显著优于少数shot提示。我们建议，在这些情况下，少数镜头示例的功能最好描述为定位已学习的任务，而不是元学习。这种分析促使人们重新思考提示在控制和评估强大的语言模型中的作用。在这项工作中，我们讨论了提示编程的方法，强调了从自然语言的角度考虑提示的有用性。我们探讨了利用叙事和文化锚定的能力来编码细微差别的意图的技术，以及鼓励在做出裁决之前将问题分解为组件的技术。受这一更全面的提示编程理论的启发，我们还引入了元提示的思想，它为模型播种种子，为一系列任务生成自己的自然语言提示。最后，我们将讨论如何将这些与语言模型交互的更通用方法整合到现有和未来的基准测试和实际应用中。</pre></li>
<li><a href="https://arxiv.org/abs/2103.10385">GPT Understands, Too</a> [<a href="https://github.com/THUDM/P-tuning">github</a>]
<pre>虽然传统微调的GPT在自然语言理解（NLU）方面无法取得很好的效果，但我们通过一种新的方法P-调优（采用可训练的连续提示嵌入）证明了GPT在NLU任务上优于或可与类似大小的BERT相媲美。在知识探索（LAMA）基准测试中，最佳GPT恢复率为64\%(P@1)在测试期间，无需提供任何额外的文本即可获得世界知识，这大大提高了20%以上的先前最佳成绩。在SuperGlue基准上，GPT在监督学习中取得了与类似规模的BERT相当甚至更好的性能。重要的是，我们发现P-调整还提高了BERTs在少镜头和监督设置下的性能，同时大大减少了对即时工程的需求。因此，P-tuning在少镜头的SuperGlue基准测试中优于最先进的方法。</pre></li>
<li><a href="https://arxiv.org/abs/2103.08493">How Many Data Points is a Prompt Worth?</a> (NAACL2021) [<a href="https://huggingface.co/blog/how_many_data_points/">website</a>]
<pre>当微调预训练模型进行分类时，研究人员要么使用通用模型头，要么使用特定于任务的提示进行预测。提示的支持者认为，提示提供了一种注入特定于任务的指导的方法，这在低数据状态下是有益的。我们的目标是通过在公平的环境中对提示进行严格测试来量化这一好处：在许多任务和数据量相同的条件下比较提示和基于头部的微调。通过控制许多优势来源，我们发现激励确实提供了好处，并且这种好处可以在每个任务中量化。结果表明，在整个分类任务中，提示通常相当于平均100个数据点。</pre></li>
<li><a href="https://arxiv.org/abs/2104.06599">Learning How to Ask: Querying LMs with Mixtures of Soft Prompts</a> (NAACL2021)
<pre>自然语言提示最近被用于引导预训练的语言模型执行其他人工智能任务，使用填充空白范式（Petroni等人，2019年）或少数镜头外推范式（Brown等人，2020年）。例如，语言模型保留训练语料库中的事实知识，这些知识可以通过要求他们在句子提示中“填空”来提取。但是，这个提示来自哪里？我们探索了通过梯度下降学习提示的想法——或者是从以前的工作中微调提示，或者是从随机初始化开始。我们的提示由“软词”组成，即不一定是语言模型中嵌入的词类型的连续向量。此外，对于每个任务，我们优化了提示的混合，学习哪些提示最有效，以及如何集成它们。在多个英语LMs和任务中，我们的方法大大优于以前的方法，表明语言模型中隐含的事实知识以前被低估了。此外，获取这些知识的成本很低：随机初始化几乎和知情初始化一样好。</pre></li>
<li><a href="https://arxiv.org/abs/2104.04670">Meta-tuning Language Models to Answer Prompts Better</a>
<pre>大型预训练语言模型（LMs），如GPT-3，已经获得了执行零射击学习的惊人能力。例如，为了在没有任何训练示例的情况下对情绪进行分类，我们可以用评论和标签描述“用户喜欢这部电影吗？”来“提示”LM，并询问下一个单词是“是”还是“否”。然而，下一个单词预测训练目标仍然与目标零射击学习目标不一致。为了解决这个弱点，我们提出了元调优，它通过在一组数据集上微调预先训练的语言模型，直接优化零炮学习目标。我们专注于分类任务，并通过聚合43个现有数据集和以问答（QA）格式注释441个标签描述来构建元数据集。当对看不见的任务进行评估时，元调优模型的性能优于相同大小的QA模型和先前基于自然语言推理的SOTA零镜头学习系统。此外，将参数计数从220M增加到770M，AUC-ROC得分提高了6.3%，我们预测，更大的模型将表现更好。因此，在开箱即用的语言模型上衡量零射击学习成绩可能低估了它们的真正潜力，而社区范围内聚合数据集并统一其格式的努力可以帮助构建更好地回答提示的模型。</pre></li>
<li><a href="https://arxiv.org/abs/2104.08786">Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity</a>
<pre>当仅使用少量训练样本进行预训练时，与完全监督微调的大型预训练语言模型相比，GPT-3等大型预训练语言模型显示出了具有竞争力的结果。我们证明了样本提供的顺序可能是接近最先进水平和随机猜测性能之间的差异：本质上，一些排列是“奇妙的”，而另一些则不是。我们详细分析了这一现象，确定：它存在于不同的模型大小（即使是最大的当前模型），它与特定的样本子集无关，并且一个模型的给定良好排列不能转移到另一个模型。虽然可以使用开发集来确定执行哪些排列，但这将与少数镜头设置不同，因为它需要额外的注释数据。相反，我们使用语言模型的生成性来构建一个人工开发集，并基于该集合中候选排列的熵统计信息来识别性能提示。我们的方法改进了GPT族模型，在11个不同的文本分类任务中平均提高了13%。</pre></li>
<li><a href="https://arxiv.org/abs/2104.08691">The Power of Scale for Parameter-Efficient Prompt Tuning</a> (EMNLP2021)
<pre>在这项工作中，我们探索了“提示调优”，这是一种简单而有效的学习“软提示”的机制，用于调节冻结的语言模型以执行特定的下游任务。与GPT-3使用的离散文本提示不同，软提示是通过反向传播学习的，可以调整为包含来自任意数量标记示例的信号。我们的端到端学习方法大大优于GPT-3的“少量”学习。更值得注意的是，通过使用T5对模型大小进行烧蚀，我们表明快速调整与缩放更具竞争力：当模型超过数十亿个参数时，我们的方法“缩小了差距”，并与模型调整的强大性能相匹配（所有模型权重都进行了调整）。这一发现尤其重要，因为大型模型的共享和服务成本很高，而将一个冻结的模型重新用于多个下游任务的能力可以减轻这一负担。我们的方法可以被看作是Li和Liang（2021）最近提出的“前缀调整”的简化，我们提供了与此和其他类似方法的比较。最后，我们表明，与完全模型调整相比，使用软提示调整冻结模型在域转移鲁棒性方面具有优势。</pre></li>
<li><a href="https://arxiv.org/abs/2106.13353">Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models</a>
<pre>使用训练示例和任务描述来提示语言模型（LMs）被视为最近在少数镜头学习中取得成功的关键。在这项工作中，我们表明，在少数镜头设置中微调LMs可以大大减少对即时工程的需要。事实上，可以使用空提示，即既不包含特定于任务的模板也不包含培训示例的提示，并在大量任务中实现手动调整提示的竞争准确性。虽然微调LMs确实为每个下游任务引入了新的参数，但我们表明，这种内存开销可以大大减少：仅微调偏差项可以实现与标准微调相当或更好的精度，同时只更新0.1%的参数。总之，我们建议微调LMs进行少量镜头学习，因为它更精确，对不同提示更鲁棒，并且可以使其与使用冻结LMs的效率几乎相同。</pre></li>
<li><a href="https://arxiv.org/abs/2109.04332">PPT: Pre-trained Prompt Tuning for Few-shot Learning</a>
<pre>预训练语言模型（PLM）提示通过弥合预训练任务和各种下游任务之间的差距，显示出显著的性能。在这些方法中，prompt tuning冻结PLM，只调整软提示，为使大规模PLM适应下游任务提供了一种高效的解决方案。然而，即时调优尚未得到充分探索。在我们的试点实验中，我们发现，当下游数据足够时，快速调谐的性能与传统的全模型微调相当，而在少数镜头学习设置下，快速调谐的性能要差得多，这可能会阻碍快速调谐在实践中的应用。我们将这种低性能归因于初始化软提示的方式。因此，在这项工作中，我们建议通过在预训练阶段添加软提示来预训练提示，以获得更好的初始化效果。我们将这个经过预先训练的提示调优框架命名为“PPT”。为了保证PPT的通用性，我们将相似的分类任务制定成统一的任务形式，并为该统一任务预训练软提示。大量的实验表明，在全数据和少量镜头设置下，为下游任务调整预先训练的提示可以达到甚至优于全模型微调。我们的方法在实际应用中是有效的。</pre></li>
<li><a href="https://arxiv.org/abs/2105.11447">True Few-Shot Learning with Language Models</a>
<pre>预训练语言模型（LMs）在许多任务中表现良好，即使是从几个例子中学习，但之前的工作使用了许多突出的例子来调整学习的各个方面，如超参数、训练目标和自然语言模板（“提示”）。在这里，我们评估了LMs在无法获得此类示例时的少数镜头能力，我们称之为真正的少数镜头学习。我们测试了两个模型选择标准，交叉验证和最小描述长度，用于在真少数镜头设置中选择LM提示和超参数。平均而言，两者的表现都略好于随机选择，也大大低于基于所举例子的选择。此外，选择标准通常更倾向于性能比随机选择的模型差得多的模型。即使考虑到我们在选择过程中模型真实性能的不确定性，以及改变计算量和用于选择的示例数量，我们也会发现类似的结果。总的来说，我们的研究结果表明，鉴于少数镜头模型选择的困难，先前的工作明显高估了LMs的真实少数镜头能力。</pre></li>
<li><a href="https://arxiv.org/abs/2012.09543">Few-shot Sequence Learning with Transformers</a> (NeurIPS2020 WS)
<pre>很少有shot算法是为了学习新任务而提供的，只有少数几个训练示例。在这项工作中，我们研究了在数据点是令牌序列的情况下的少量镜头学习，并提出了一种基于变压器的有效学习算法。在最简单的设置中，我们将令牌附加到表示要执行的特定任务的输入序列中，并显示，在给出几个标记示例的情况下，可以动态优化该令牌的嵌入。我们的方法不需要对模型体系结构进行复杂的更改，例如适配器层，也不需要计算二阶导数，正如目前在元学习和少数快照学习文献中流行的那样。我们在各种任务上演示了我们的方法，并分析了几种模型变体和基线方法的泛化特性。特别是，我们证明了组合任务描述符可以提高性能。实验表明，我们的方法至少和其他方法一样有效，同时计算效率更高。</pre></li>
<li><a href="https://arxiv.org/abs/2105.11259">PTR: Prompt Tuning with Rules for Text Classification</a>
<pre>经过微调的预训练语言模型（PLM）在几乎所有NLP任务上都取得了令人敬畏的性能。通过使用额外的提示来微调PLMs，我们可以进一步激发PLMs中分布的丰富知识，以便更好地为下游任务服务。在情感分类和自然语言推理等少数类别分类任务中，即时调优取得了令人满意的结果。然而，手动设计大量的语言提示既麻烦又容易出错。对于那些自动生成的提示，在非少镜头场景中验证它们的有效性也是昂贵和耗时的。因此，及时调整以解决许多类分类任务仍然是一个挑战。为此，我们提出了针对多类文本分类的规则提示调优（PTR），并应用逻辑规则构造具有多个子提示的提示。通过这种方式，PTR能够将每个类的先验知识编码到即时调优中。我们在关系分类这一典型且复杂的多类分类任务上进行了实验，结果表明，PTR可以显著且持续地优于现有的最新基线。这表明PTR是一种很有前途的方法，可以利用人类的先验知识和PLMs来完成那些复杂的分类任务。</pre></li>
<li><a href="https://arxiv.org/abs/2108.02035">Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification</a>
<pre>使用特定于任务的提示调整预先训练的语言模型（PLM）是一种很有前途的文本分类方法。特别是，以前的研究表明，与带有额外分类器的通用微调方法相比，快速调优在低数据场景中具有显著的优势。提示调优的核心思想是将文本片段（即模板）插入到输入中，并将分类问题转化为蒙面语言建模问题，其中关键步骤是在标签空间和标签词空间之间构建投影（即描述器）。描述者通常是手工制作或通过梯度下降进行搜索，这可能缺乏覆盖范围，并给结果带来相当大的偏差和高度差异。在这项工作中，我们将重点放在将外部知识合并到言语化器中，形成知识丰富的提示调优（KPT），以改进和稳定提示调优。具体地说，我们使用外部知识库（KBs）扩展描述词的标签词空间，并在使用扩展的标签词空间进行预测之前使用PLM本身细化扩展的标签词空间。在零镜头和少量镜头文本分类任务上的大量实验证明了知识提示调整的有效性。</pre></li>
<li><a href="https://arxiv.org/abs/2109.03630">Discrete and Soft Prompting for Multilingual Models</a> (EMNLP2021)
<pre>在英语学习中，离散和软提示在预训练语言模型（PLM）的少镜头学习中表现得很好。在本文中，我们证明了离散和软提示在多语种情况下的表现优于微调：跨语种迁移和多语种自然语言推理的语言训练。例如，在48个英语培训示例中，FineTunning在跨语言迁移方面获得了33.74%的准确率，几乎没有超过大多数基线（33.33%）。相比之下，离散和软激励的表现优于微调，分别达到36.43%和38.79%。我们还用英语以外的多种语言的训练数据演示了良好的提示性能。</pre></li>
<li><a href="https://arxiv.org/abs/2109.07830">Reframing Instructional Prompts to GPTk’s Language</a>
<pre>模型设计者如何将任务说明转化为语言模型的有效提示？通过对GPT3的大量实证分析，我们观察到了成功教学提示的重要特征，并为模型设计者提出了几种重构技术来创建此类提示。例如，一个复杂的任务可以分解为多个简单的任务。我们对6个不同类别（问题生成、分类等）的12个NLP任务进行了实验。我们的结果表明，与现有的少数镜头基线相比，重构将少数镜头学习性能提高了14\%，同时降低了样本复杂性。在大型语言模型（如GPT3）上，性能提升尤为重要，因为在GPT3中，无法对大型数据集进行模型调优或提示。此外，我们观察到，此类收益不仅限于GPT3；在不同的模型体系结构中，重新格式化的任务仍然优于原始指令，这突出了这些指南的跨模型通用性。我们希望这些经验驱动的技术将为将来更有效地促进LMs铺平道路。</pre></li>
<li><a href="https://arxiv.org/abs/2106.13884">Multimodal Few-Shot Learning with Frozen Language Models</a>
<pre>当在足够大的范围内进行训练时，自回归语言模型表现出显著的能力，即在仅用几个例子提示后学习新的语言任务。在这里，我们提出了一种简单而有效的方法，将这种少镜头的学习能力转移到多模式环境（视觉和语言）。使用对齐的图像和字幕数据，我们训练一个视觉编码器，将每个图像表示为一系列连续的嵌入，这样一个预先训练过的、用这个前缀提示的冻结语言模型就会生成适当的字幕。由此产生的系统是一个多模式的少数镜头学习者，当以示例为条件时，具有学习各种新任务的惊人能力，表现为多个交错图像和文本嵌入序列。我们证明，通过在各种已建立和新的基准上测量单个模型，它可以快速学习新对象和新视觉类别的单词，仅使用少数示例进行视觉问答，并利用外部知识。</pre></li>
<li><a href="https://arxiv.org/abs/2107.07170">FLEX: Unifying Evaluation for Few-Shot NLP</a>
<pre>很少有镜头NLP研究是高度活跃的，但在不相交的研究线程中进行，评估套件缺乏具有挑战性但现实的测试设置，并且未能采用仔细的实验设计。因此，社区不知道哪种技术表现最好，甚至不知道它们是否优于简单的基线。我们为一个理想的少镜头NLP基准制定了desiderata，并展示了FLEX，第一个基准，公共排行榜，以及为少镜头NLP技术提供统一、全面测量的框架。FLEX整合并引入了新的少镜头评估最佳实践，包括四种传输设置的测量、零镜头评估的文本标签，以及优化统计准确性的基准设计原则性方法，同时使研究人员无需大量计算资源即可获得评估成本。此外，我们还提出了UniFew，这是一个简单但强大的基于提示的少镜头学习模型，它统一了预训练和微调提示格式，避免了最近基于提示的方法在使下游任务格式适应语言模型预训练目标方面的复杂机制。我们证明，尽管简单，但UniFew取得的结果与流行的元学习和基于提示的方法具有竞争力。</pre></li>
<li><a href="https://arxiv.org/abs/2109.01247">Do Prompt-Based Models Really Understand the Meaning of their Prompts?</a>
<pre>最近，大量的论文表明，在使用各种基于提示的模型进行少数镜头学习方面取得了非凡的进展。这样的成功给人的印象是，提示可以帮助模型更快地学习，就像人类在使用自然语言表达的任务指令时学习更快一样。在这项研究中，我们实验了30多个为自然语言推理（NLI）手工编写的提示。我们发现，模型学习的速度与许多有意无关甚至病态误导的提示一样快，就像它们学习有指导意义的“好”提示一样。此外，我们发现模型性能更依赖于LM目标词的选择（即将LM词汇预测转换为类标签的“言语化器”），而不是提示本身的文本。总之，我们发现几乎没有证据表明现有的基于提示的模型真正理解其给定提示的含义。</pre></li>
</ul>
<h1 id="sentence-embedding">Sentence embedding</h1>
<ul>
<li><a href="https://arxiv.org/abs/1811.01088">Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks</a>
<pre>具有语言建模和相关无监督任务的预训练句子编码器最近被证明对于语言理解任务非常有效。通过补充语言模型风格的预训练和对数据丰富的监督任务（如自然语言推理）的进一步训练，我们在GLUE基准上获得了额外的性能改进。通过对BERT进行补充培训（Devlin等人，2018年），我们获得了81.8分——最新水平（截至2019年2月24日），比BERT提高了1.4分。我们还观察到，在这种设置下，随机重启的方差减小。当应用于ELMo（Peters等人，2018a）和Radford等人（2018）的模型时，我们的方法产生了类似的改进。此外，补充训练的好处在数据受限的情况下尤其明显，正如我们在人工限制训练数据的实验中所显示的那样。</pre></li>
<li><a href="https://arxiv.org/abs/1908.10084">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</a> (EMNLP2019)
<pre>BERT（Devlin et al.，2018）和RoBERTa（Liu et al.，2019）在句子对回归任务（如语义-文本相似性（STS））上建立了新的最先进的性能。然而，这需要将两个句子都输入到网络中，这会造成巨大的计算开销：在10000个句子的集合中找到最相似的句子对需要使用BERT进行大约5000万次推理计算（~65小时）。BERT的构造使得它不适用于语义相似性搜索以及聚类等无监督任务。在本出版物中，我们介绍了句子BERT（SBERT），这是对预训练BERT网络的一种修改，它使用连体和三重网络结构来推导语义上有意义的句子嵌入，可以使用余弦相似性进行比较。这减少了寻找最相似配对的工作量，从使用BERT/RoBERTa的65小时减少到使用SBERT的约5秒，同时保持了BERT的准确性。我们评估了SBERT和SRoBERTa在普通STS任务和迁移学习任务中的表现，其表现优于其他最先进的句子嵌入方法。</pre></li>
<li><a href="https://arxiv.org/abs/1810.00438">Parameter-free Sentence Embedding via Orthogonal Basis</a> (EMNLP2019)
<pre>我们提出了一种简单而健壮的非参数化句子表示方法。受几何理论中Gram-Schmidt过程的启发，我们构建了一个由单词及其周围上下文组成的子空间的正交基。我们从两个方面对句子中单词的语义进行建模。一个是它与上下文词已经跨越的词向量子空间的关联性。另一个是单词的新语义，它将作为垂直于现有子空间的新基向量引入。基于这个动机，我们开发了一种基于正交基的创新方法，将预先训练好的单词嵌入到句子表示中。这种方法需要零参数以及高效的推理性能。我们评估了11个下游NLP任务的方法。与非参数化方案相比，我们的模型表现出优越的性能，并且与依赖大量标记数据或延长训练时间的其他方法相比具有竞争力。</pre></li>
<li><a href="https://arxiv.org/abs/2002.06652">SBERT-WK: A Sentence Embedding Method By Dissecting BERT-based Word Models</a>
<pre>句子嵌入是自然语言处理（NLP）中的一个重要研究课题，因为它可以将知识传递给下游任务。同时，一种被称为BERT的上下文化单词表示法在很多NLP任务中都达到了最先进的性能。然而，从基于BERT的词模型生成高质量的句子表示是一个开放的问题。先前的研究表明，不同层次的BERT具有不同的语言特性。这使我们能够跨层融合信息，以找到更好的句子表示。在这项工作中，我们研究了深层语境化模型中单词表示的分层模式。然后，通过对单词表示所跨越的空间进行几何分析，对基于BERT的单词模型进行剖析，提出了一种新的句子嵌入方法。它被称为SBERT-WK方法。SBERT-WK无需进一步培训。我们评估了SBERT-WK的语义-文本相似度和下游监督任务。此外，本文还提出了十个句子层次的探测任务，用于详细的语言分析。实验表明，SBERT-WK实现了最先进的性能。我们的代码是公开的。</pre></li>
<li><a href="https://arxiv.org/abs/2011.05864">On the Sentence Embeddings from Pre-trained Language Models</a> (EMNLP2020)
<pre>像BERT这样预先训练好的上下文表示在自然语言处理中取得了巨大的成功。然而，人们发现，来自预先训练的语言模型的句子嵌入在没有微调的情况下很难捕捉到句子的语义。在本文中，我们认为在伯特嵌入中的语义信息没有被充分利用。我们首先从理论上揭示了蒙面语言模型预训练目标与语义相似性任务之间的理论联系，然后对BERT语句嵌入进行了实证分析。我们发现，BERT总是导致句子语义空间的非光滑各向异性，这损害了它的语义相似性。为了解决这个问题，我们建议通过规范化使用无监督目标学习的流，将各向异性句子嵌入分布转化为平滑的各向同性高斯分布。实验结果表明，我们提出的BERT-flow方法在各种语义-文本相似性任务上比最先进的句子嵌入方法获得了显著的性能提升。该守则可于https://github.com/bohanli/BERT-flow.</pre></li>
<li><a href="https://openreview.net/forum?id=Ov_sMNau-PF">Semantic Re-tuning with Contrastive Tension</a> (ICLR2021)</li>
<li><a href="https://arxiv.org/abs/2006.03659">DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations</a> (ACL2021)
<pre>句子嵌入是许多自然语言处理（NLP）系统的重要组成部分。与单词嵌入一样，句子嵌入通常在大型文本语料库上学习，然后转移到各种下游任务，如聚类和检索。与单词嵌入不同，学习句子嵌入的最高性能解决方案需要标记数据，从而将其用途限制在标记数据丰富的语言和领域。在本文中，我们提出了DeCLUTR：无监督文本表征的深层对比学习。受深度度量学习（DML）最新进展的启发，我们精心设计了一个用于学习通用句子嵌入的自监督目标，该目标不需要标记的训练数据。当用于扩展基于转换器的语言模型的预训练时，我们的方法缩小了通用句子编码器的无监督和有监督预训练之间的性能差距。重要的是，我们的实验表明，学习嵌入的质量与可训练参数的数量和未标记训练数据的数量成正比。我们的代码和预训练模型是公开的，可以很容易地适应新的领域或用于嵌入看不见的文本。</pre></li>
<li><a href="https://arxiv.org/abs/2105.11741">ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer</a> (ACL2021)
<pre>学习高质量的句子表征有利于广泛的自然语言处理任务。尽管基于BERT的预训练语言模型在许多下游任务上都取得了很高的性能，但是本地派生的句子表示被证明是折叠的，因此在语义-文本相似性（STS）任务上的性能较差。在本文中，我们提出了一个用于自我监督句子表征迁移的对比框架ConSERT，该框架采用对比学习以无监督和有效的方式对BERT进行微调。通过使用未标记文本，ConSERT解决了BERT派生语句表示的崩溃问题，使其更适用于下游任务。在STS数据集上的实验表明，ConSERT比之前的最新技术水平相对提高了8%，甚至可以与受监督的SBERT-NLI相媲美。当进一步纳入NLI监管时，我们在STS任务上实现了新的最先进的表现。此外，ConSERT仅在1000个可用样本的情况下获得了可比结果，显示了其在数据稀缺场景中的稳健性。</pre></li>
<li>[CLEAR: Contrastive Learning for Sentence Representation]](https://arxiv.org/abs/2012.15466)
<pre>经过预先训练的语言模型已经证明了它们在捕获隐式语言特征方面的独特能力。然而，大多数预训练方法侧重于单词级训练目标，而很少研究句子级目标。在本文中，我们提出了句子表征对比学习（CLEAR），它采用多种句子级增强策略来学习噪声不变的句子表征。这些增强包括单词和跨度的删除、重新排序和替换。此外，我们还通过大量实验探讨了对比学习有效性的关键原因。我们观察到，在训练前不同的句子强化会导致不同下游任务的不同表现改善。我们的方法在SentEval和GLUE基准上都优于现有的多种方法。</pre></li>
<li><a href="https://arxiv.org/abs/2104.08821">SimCSE: Simple Contrastive Learning of Sentence Embeddings</a> (EMNLP2021) [<a href="https://github.com/princeton-nlp/simcse">github</a>]
<pre>本文介绍了SimCSE，一个简单的对比学习框架，它极大地促进了最先进的句子嵌入。我们首先描述了一种无监督的方法，该方法采用输入句子并在对比目标中预测自身，仅使用标准差作为噪声。这个简单的方法令人惊讶地很好地完成，与以前的监督同行一致。我们发现，辍学充当了最小的数据扩充，删除它会导致表示崩溃。然后，我们提出了一种有监督的方法，该方法将自然语言推理数据集中的注释对纳入我们的对比学习框架，使用“蕴涵”对作为肯定词，“矛盾”对作为硬否定词。我们在标准语义文本相似性（STS）任务中评估SimCSE，我们使用BERT-base的无监督和监督模型分别实现了76.3%和81.6%的Spearman相关性，与之前的最佳结果相比，分别提高了4.2%和2.2%。我们还从理论和经验上证明，对比学习目标将预训练嵌入的各向异性空间正则化，使其更加均匀，并且在有监督信号可用时，它可以更好地对齐正对。</pre></li>
<li><a href="https://arxiv.org/abs/2104.08027">Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders</a> (EMNLP2021) [<a href="https://github.com/cambridgeltl/mirror-bert">github</a>]
<pre>近年来，预训练蒙面语言模型（MLM）彻底改变了NLP。然而，以前的工作表明，如果没有对NLI、句子相似性或使用带注释的任务数据对任务进行进一步的特定任务微调，现成的MLM就不能有效地作为通用词汇或句子编码器。在这项工作中，我们证明了即使没有任何额外的数据和监督，MLM也有可能成为有效的通用词汇和句子编码器。我们提出了一种非常简单、快速和有效的对比学习技术，称为镜像BERT，它可以在20-30秒内将MLM（如BERT和RoBERTa）转换为此类编码器，而无需任何额外的外部知识。Mirror BERT依赖于完全相同或稍微修改的字符串对作为正向（即同义）微调示例，并旨在在身份微调期间最大化它们的相似性。我们报告了在词汇级和句子级任务中，在不同领域和不同语言中，使用镜像BERT比现成的MLM获得了巨大的收益。值得注意的是，在标准句子语义相似性（STS）任务中，我们的自监督镜像伯特模型甚至与先前工作中的任务调整句子伯特模型的性能相匹配。最后，我们深入研究了MLM的内部工作机制，并提出了一些证据，说明为什么这种简单的方法可以产生有效的通用词汇和句子编码器。</pre></li>
<li><a href="https://arxiv.org/abs/2104.06979">TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning</a> (EMNLP2021 Findings)
<pre>学习句子嵌入通常需要大量的标记数据。然而，对于大多数任务和域，标记数据很少可用，而且创建它的成本很高。在这项工作中，我们提出了一种基于预训练变压器和顺序去噪自动编码器（TSDAE）的最新无监督方法，其性能比以前的方法高出6.4个百分点。它可以实现93.1%的域内监督方法的性能。此外，我们还证明了TSDAE是一种用于句子嵌入的强领域适应和预训练方法，显著优于其他方法，如掩蔽语言模型。以往研究的一个关键缺点是评估范围狭窄：大多数工作主要评估单一任务的语义文本相似性（STS），这不需要任何领域知识。目前尚不清楚这些方法是否适用于其他领域和任务。我们填补了这一空白，并对来自异构域的四种不同数据集上的TSDAE和其他最新方法进行了评估。</pre></li>
<li><a href="https://arxiv.org/abs/2109.13059">Trans-Encoder: Unsupervised sentence-pair modelling through self- and mutual-distillations</a> [<a href="https://github.com/amzn/trans-encoder">github</a>]
<pre>在NLP中，大量任务涉及两个序列之间的成对比较（例如句子相似性和释义识别）。主要有两种公式用于句子对任务：双编码器和交叉编码器。双编码器产生固定维的句子表示，并且计算效率高，但是，它们通常不如交叉编码器。交叉编码器可以利用它们的注意力头来利用句子间的交互来获得更好的性能，但它们需要任务微调，并且计算成本更高。在本文中，我们提出了一个称为Trans-Encoder的完全无监督的句子表示模型，该模型将两种学习范式结合到一个迭代联合框架中，以同时学习增强的双编码器和交叉编码器。具体地说，在预训练语言模型（PLM）的基础上，我们首先将其转换为无监督的bi编码器，然后在bi编码器和交叉编码器任务公式之间交替。在每次交替中，一个任务公式将产生伪标签，作为另一个任务公式的学习信号。然后，我们提出了一种扩展，在多个并行PLM上执行这种自蒸馏方法，并使用它们的伪标签的平均值进行相互蒸馏。据我们所知，Trans-Encoder创建了第一个完全无监督的交叉编码器，也是一个用于句子相似性的最先进的无监督双编码器。Trans-encoder的双编码器和交叉编码器公式在句子相似性基准上都比最近提出的最先进的无监督句子编码器（如Mirror BERT和SimCSE）高出5%。</pre></li>
<li><a href="https://arxiv.org/abs/2103.15316">Whitening Sentence Representations for Better Semantics and Faster Retrieval</a> [<a href="https://github.com/bojone/BERT-whitening">github</a>]
<pre>诸如BERT等预训练模型在许多自然语言处理任务中取得了巨大的成功。然而，如何通过这些预训练模型获得更好的句子表示仍然值得探索。以往的研究表明，各向异性问题是基于BERT的句子表示的一个关键瓶颈，它阻碍了模型充分利用底层语义特征。因此，一些增强句子分布各向同性的尝试，如基于流的模型，已被应用于句子表征，并取得了一些改进。在本文中，我们发现传统机器学习中的白化操作同样可以增强句子表示的各向同性并获得竞争性结果。此外，白化技术还能够降低句子表征的维数。实验结果表明，该算法不仅具有良好的性能，而且大大降低了存储成本，加快了模型检索速度。</pre></li>
<li><a href="https://arxiv.org/abs/2010.08240">Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks</a> (NAACL2021)
<pre>两两句子评分有两种方法：交叉编码器，对输入对执行完全关注；双编码器，将每个输入独立映射到密集向量空间。虽然交叉编码器通常可以获得更高的性能，但对于许多实际用例来说，它们的速度太慢了。另一方面，Bi编码器需要大量的训练数据和针对目标任务的微调，以实现具有竞争力的性能。我们提出了一种简单而有效的数据增强策略，称为增强SBERT，其中我们使用交叉编码器来标记一组更大的输入对，以增强bi编码器的训练数据。我们表明，在这个过程中，选择句子对是非常重要的，也是该方法成功的关键。我们在多个任务（域内）以及域适应任务上评估我们的方法。与原始bi编码器性能相比，增强SBERT在域内和域自适应任务上分别提高了6个点和37个点。</pre></li>
<li><a href="https://arxiv.org/abs/2104.06719">Sentence Embeddings by Ensemble Distillation</a>
<pre>本文为语义-文本相似度（STS）提供了一种新的研究方法。我们比较和组合了一些最近提出的STS句子嵌入方法，并提出了一种新的简单的集成知识提取方案，该方案改进了以前的方法。我们的实验表明，训练学习多个集成学生的平均嵌入空间的模型比所有其他个体模型具有更高的鲁棒性。利用我们的蒸馏方法与以前的方法相结合，我们显著改进了SOTA无监督STS，并且通过对以前的方法进行适当的超参数调整，我们改进了有监督SOTA分数。</pre></li>
<li><a href="https://arxiv.org/abs/2108.08877">Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models</a>
<pre>我们提供了文本到文本转换器（T5）句子嵌入的首次探索。句子嵌入在语言处理任务中非常有用。虽然T5在语言任务中取得了令人印象深刻的性能，如序列到序列映射问题，但如何从编码器-解码器模型生成句子嵌入尚不清楚。我们研究了三种提取T5语句嵌入的方法：两种仅使用T5编码器，一种使用完整的T5编码器-解码器模型。我们的纯编码器模型在传输任务和语义文本相似性（STS）方面都优于基于BERT的句子嵌入。我们的编解码方法在STS上实现了进一步的改进。我们发现，将T5从数百万个参数扩展到数十亿个参数可以持续改进下游任务。最后，我们介绍了一种两阶段对比学习方法，该方法使用句子嵌入实现了STS的一种新技术，优于句子BERT和SimCSE。</pre></li>
<li><a href="https://arxiv.org/abs/2104.08675">Dual-View Distilled BERT for Sentence Embedding</a> (SIGIR2021)
<pre>最近，BERT通过词级跨句注意实现了句子匹配的重大进展。然而，当使用暹罗BERT网络推导两个句子嵌入时，性能显著下降，这在捕获全局语义方面有不足，因为两个句子之间没有单词级注意。在这篇文章中，我们提出了一个双视图的提取BERT~（DvBERT）语句匹配与句子嵌入。我们的方法从两个不同的角度处理句子对，即暹罗视角和交互视角。暹罗视图是我们生成句子嵌入的主干。交互观将跨句交互整合为多个教师，以提高句子嵌入的表征能力。在六个STS任务上的实验表明，我们的方法明显优于现有的句子嵌入方法。</pre></li>
<li><a href="https://arxiv.org/abs/2105.04339">DefSent: Sentence Embeddings using Definition Sentences</a> (ACL2021)
<pre>使用自然语言推理（NLI）数据集的句子嵌入方法已成功应用于各种任务。然而，由于严重依赖大型NLI数据集，这些方法仅适用于有限的语言。在本文中，我们提出了DefSent，一种使用单词词典中定义句的句子嵌入方法，该方法在无监督语义文本相似性（STS）任务中的性能相当，在句子评价任务中的性能略优于传统方法。由于字典可用于多种语言，因此DefSent比使用NLI数据集而不构建额外数据集的方法更广泛地适用。我们证明，与使用大型NLI数据集的方法相比，DefSent在无监督语义文本相似性（STS）任务上的性能相当，在SentEval任务上的性能稍好。我们的代码在https://github.com/hpprc/defsent .</pre></li>
<li><a href="https://arxiv.org/abs/2104.15114">Paraphrastic Representations at Scale</a> [<a href="https://github.com/jwieting/paraphrastic-representations-at-scale">github</a>]
<pre>我们提出了一个系统，允许用户在各种语言中训练自己最先进的复述句子表达。我们还为英语、阿拉伯语、德语、法语、西班牙语、俄语、土耳其语和汉语发布经过培训的模型。我们在大量数据上训练这些模型，与最初提出单语语义相似性、跨语言语义相似性和双文本挖掘任务的方法相比，取得了显著的性能改进。此外，由此产生的模型超过了所有先前关于无监督语义-文本相似度的工作，显著优于基于伯特的模型，如句子伯特（Reimers和Gurevych，2019）。此外，我们的模型比以前的工作快了几个数量级，并且可以在CPU上使用，推理速度几乎没有差异（甚至在使用更多CPU内核时比GPU的速度更快），这使得这些模型对于没有访问GPU的用户或用于嵌入式设备的用户来说是一个有吸引力的选择。最后，我们为训练复述句模型的代码库添加了显著增加的功能，从而简化了它们用于推理和使用并行数据训练任何所需语言的使用。我们还包括自动下载和预处理训练数据的代码。</pre></li>
<li><a href="https://arxiv.org/abs/2012.12624">Learning Dense Representations of Phrases at Scale</a> (ACL2021) [<a href="https://github.com/princeton-nlp/DensePhrases">github</a>]
<pre>开放域问答可以重新表述为短语检索问题，而无需在推理过程中按需处理文档（Seo等人，2019年）。然而，当前的短语检索模型严重依赖于稀疏表示，仍然不如检索器-读取器方法。在这项工作中，我们首次表明，我们可以单独学习短语的密集表示，从而在开放域QA中获得更强的性能。我们提出了一种从阅读理解任务的监督中学习短语表征的有效方法，以及新颖的否定抽样方法。我们还提出了一种查询端微调策略，可以支持迁移学习，减少训练和推理之间的差异。在五个流行的开放域QA数据集上，我们的DensePhrases模型比以前的短语检索模型提高了15%-25%的绝对准确性，并与最先进的检索器-阅读器模型的性能相匹配。我们的模型很容易并行化，因为它是纯密集表示，并且在CPU上每秒处理超过10个问题。最后，我们直接将我们的预索引密集短语表示用于两个时隙填充任务，展示了利用密集短语作为下游任务密集知识库的前景。</pre></li>
<li><a href="https://arxiv.org/abs/2109.06304">Phrase-BERT: Improved Phrase Embeddings from BERT with an Application to Corpus Exploration</a> (EMNLP2021)
<pre>从BERT派生的短语表示通常不表现出复杂的短语组成性，因为该模型依赖于词汇相似性来确定语义关联性。在本文中，我们提出了一个对比微调目标，使伯特能够产生更强大的短语嵌入。我们的方法（短语BERT）依赖于使用释义生成模型自动生成的各种短语释义数据集，以及从Books3语料库中挖掘的大规模上下文短语数据集。短语BERT在各种短语级相似性任务中的表现优于基线，同时也表明向量空间中最近邻之间的词汇多样性增加。最后，作为一个案例研究，我们表明短语BERT嵌入可以很容易地与一个简单的自动编码器集成，以建立一个基于短语的神经主题模型，该模型通过在嵌入空间中执行最近邻搜索将主题解释为词和短语的混合。众包评估表明，这种基于短语的主题模型比基线单词和短语级主题模型产生的主题更加连贯和有意义，进一步验证了短语模型的效用。</pre></li>
</ul>
<h1 id="transformer-variants">Transformer variants</h1>
<ul>
<li><a href="https://arxiv.org/abs/2009.06732">Efficient Transformers: A Survey</a>
<pre>变压器模型体系结构由于其在语言、视觉和强化学习等一系列领域的有效性，最近引起了极大的兴趣。例如，在自然语言处理领域，变形金刚已经成为现代深度学习堆栈中不可或缺的一部分。最近，提出了一系列令人眼花缭乱的“X-former”模型——Reformer、Linformer、Performer、Longformer等等——这些模型改进了原始的Transformer体系结构，其中许多都围绕计算和内存效率进行了改进。为了帮助热心的研究人员驾驭这场风波，本文描述了大量和深思熟虑的最近效率型“X-former”模型，提供了跨多个领域的现有工作和模型的有组织和全面的概述。</pre></li>
<li><a href="https://arxiv.org/abs/1905.07799">Adaptive Attention Span in Transformers</a> (ACL2019)
<pre>我们提出了一种新的自我注意机制，可以学习其最佳注意广度。这允许我们显著扩展Transformer中使用的最大上下文大小，同时保持对内存占用和计算时间的控制。我们在字符级语言建模任务中展示了我们的方法的有效性，通过使用8k字符的最大上下文，我们在text8和enwiki8上实现了最先进的性能。</pre></li>
<li><a href="https://arxiv.org/abs/1901.02860">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a> (ACL2019) [<a href="https://github.com/kimiyoung/transformer-xl">github</a>]
<pre>变形金刚具有学习长期依赖性的潜力，但在语言建模环境中受到固定长度上下文的限制。我们提出了一种新的神经结构Transformer XL，它能够在不破坏时间一致性的情况下，实现超过固定长度的学习依赖性。它由一个段级递归机制和一个新的位置编码方案组成。我们的方法不仅能够捕获长期依赖，而且还解决了上下文碎片问题。因此，Transformer XL学习的依赖性比RNN长80%，比vanilla Transformers长450%，在短序列和长序列上都实现了更好的性能，并且在评估期间比vanilla Transformers快1800多倍。值得注意的是，我们将bpc/Implexity的最新结果在enwiki8上提高到0.99，在text8上提高到1.08，在WikiText-103上提高到18.3，在十亿字上提高到21.8，在Penn Treebank上提高到54.5（无微调）。当仅在WikiText-103上接受培训时，Transformer XL能够生成具有数千个标记的合理连贯、新颖的文本文章。我们的代码、预训练模型和超参数在Tensorflow和PyTorch中都可用。</pre></li>
<li><a href="https://arxiv.org/abs/1904.10509">Generating Long Sequences with Sparse Transformers</a>
<pre>变压器是强大的序列模型，但需要的时间和内存随序列长度二次增长。在本文中，我们引入了注意矩阵的稀疏分解，它将注意矩阵分解为$O（n\sqrt{n}）$。我们还介绍了a）结构和初始化的变化以训练更深层次的网络，b）重新计算注意矩阵以节省内存，以及c）用于训练的快速注意核。我们称这些变化为稀疏变压器的网络，并表明它们可以使用数百层对数万时间步长的序列进行建模。我们使用相同的体系结构对原始字节中的图像、音频和文本进行建模，为Enwik8、CIFAR-10和ImageNet-64的密度建模设定了新的技术水平。我们生成了无条件的样本，证明了全球一致性和巨大的多样性，并表明在原则上可以对长度为一百万或更多的模型序列使用自我注意。</pre></li>
<li><a href="https://arxiv.org/abs/2007.03356">Do Transformers Need Deep Long-Range Memory</a> (ACL2020)
<pre>深度注意模型促进了跨多个领域的序列数据建模。特别是在语言建模方面，Transformer XL——一种使用过去激活的远程内存进行扩充的转换器——已被证明在各种经过充分研究的基准测试中是最先进的。Transformer XL在网络的每一层都集成了一个远程内存，这使得它的状态比RNN的前身大数千倍。然而，目前尚不清楚这是否必要。我们进行了一系列的干预，以表明在长程记忆减少6倍的情况下可以获得类似的表现，并且通过限制网络较低层的注意力范围可以获得更好的表现。</pre></li>
<li><a href="https://arxiv.org/abs/2010.06925">DA-Transformer: Distance-aware Transformer</a> (NAACL2021)
<pre>Transformer通过组合各种高级模型（如BERT和GPT），在NLP领域取得了巨大成功。然而，Transformer及其现有变体在捕获令牌距离方面可能不是最优的，因为这些方法使用的位置或距离嵌入通常无法保持真实距离的精确信息，这可能不利于建模上下文的顺序和关系。在本文中，我们提出了DA Transformer，这是一种能够利用真实距离的距离感知转换器。我们建议结合令牌之间的实际距离来重新缩放原始自我注意权重，该权重由注意查询和密钥之间的相关性计算。具体地说，在不同的自我注意头部中，每对标记之间的相对距离由不同的可学习参数加权，这些参数控制这些头部对长期或短期信息的不同偏好。由于原始加权真实距离可能不是调整自我注意权重的最佳方法，因此我们提出了一个可学习的sigmoid函数，将其映射到具有适当范围的重新缩放系数中。我们首先通过ReLU函数剪裁原始的自我注意权重，以保持非负性并引入稀疏性，然后将其与重新缩放的系数相乘，从而将真实距离信息编码到自我注意中。在五个基准数据集上的大量实验表明，DA Transformer可以有效地提高许多任务的性能，并且优于vanilla Transformer及其几个变体。</pre></li>
<li><a href="https://arxiv.org/abs/1909.00015">Adaptively Sparse Transformers</a> (EMNLP2019)
<pre>注意机制在NLP中已变得无处不在。最近的体系结构，尤其是Transformer，通过分层、多头关注学习强大的上下文感知单词表示。多头学习不同类型的单词关系。然而，使用标准softmax注意，所有注意头都是密集的，为所有上下文单词分配了非零权重。在这项工作中，我们引入了自适应稀疏变换，其中注意头具有灵活的、上下文相关的稀疏模式。这种稀疏性是通过将softmax替换为$\alpha$-entmax来实现的：softmax是一种可微的泛化，它允许低得分单词精确地获得零权重。此外，我们还推导了一种自动学习$\alpha$参数的方法，该参数控制$\alpha$-entmax的形状和稀疏性，从而允许注意头在集中或分散行为之间进行选择。与机器翻译数据集上的softmax转换器相比，我们的自适应稀疏转换器提高了可解释性和头部多样性。我们的方法的定量和定性分析结果包括，不同层次的头部学习不同的稀疏性偏好，并且在注意力分布上比softmax变形金刚更加多样化。此外，在不牺牲准确性的情况下，注意力头部的稀疏性有助于揭示不同的头部特化。</pre></li>
<li><a href="https://arxiv.org/abs/1911.05507">Compressive Transformers for Long-Range Sequence Modelling</a>
<pre>我们提出了压缩变压器，一个专注的序列模型，压缩过去的记忆，用于远程序列学习。我们发现Compression Transformer在WikiText-103和Enwik8基准测试中获得了最先进的语言建模结果，分别达到17.1 ppl和0.97 bpc。我们还发现，它可以有效地模拟高频语音，并且可以作为RL的记忆机制，在一个对象匹配任务中进行了演示。为了促进远程序列学习领域的发展，我们提出了一个新的开放词汇语言建模基准，该基准源于书籍PG-19。</pre></li>
<li><a href="https://arxiv.org/abs/1901.11117">The Evolved Transformer</a> (ICML2019)
<pre>最近的工作突出了Transformer体系结构在序列任务上的优势，同时，神经体系结构搜索（NAS）已经开始优于人类设计的模型。我们的目标是应用NAS来寻找变压器的更好替代品。我们首先根据前馈序列模型的最新进展构建了一个大的搜索空间，然后通过在初始种群中植入Transformer来运行热启动的进化架构搜索。为了直接搜索计算昂贵的WMT 2014英德翻译任务，我们开发了渐进式动态障碍方法，该方法允许我们动态地将更多资源分配给更有希望的候选模型。在我们的实验中发现的体系结构——进化的Transformer——在四项成熟的语言任务上表现出比Transformer一致的改进：WMT 2014英语德语、WMT 2014英语法语、WMT 2014英语捷克语和LM1B。在一个大的模型大小，进化变压器建立了一个新的国家最先进的BLEU分数为29.8的WMT'14英语德语；在更小的尺寸下，它达到了与原始“大”变压器相同的质量，参数减少了37.6%，并且在7M参数的移动友好型模型尺寸下，它的性能优于变压器0.7BLEU。</pre></li>
<li><a href="https://arxiv.org/abs/2001.04451">Reformer: The Efficient Transformer</a> (ICLR2020) [<a href="https://github.com/google/trax/tree/master/trax/models/reformer">github</a>]
<pre>大型变压器模型通常在许多任务上实现最先进的结果，但培训这些模型的成本可能过高，特别是在长序列上。我们介绍了两种提高变压器效率的技术。首先，我们将点积注意力替换为使用位置敏感哈希的点积注意力，将其复杂性从O（$L^2$）更改为O（$L\log L$），其中$L$是序列的长度。此外，我们使用可逆剩余层而不是标准剩余层，这允许在训练过程中只存储一次激活，而不是$N$次，其中$N$是层数。所得到的模型，重整器，与变压器模型一致，同时在存储器上效率更高，并且在长序列上快得多。</pre></li>
<li><a href="https://arxiv.org/abs/2002.10101">GRET: Global Representation Enhanced Transformer</a> (AAAI2020)
<pre>Transformer基于编码器-解码器框架，在几个自然语言生成任务上实现了最先进的性能。编码器将输入句子中的单词映射成一系列隐藏状态，然后将这些隐藏状态反馈给解码器以生成输出句子。这些隐藏状态通常对应于输入单词，并侧重于捕获本地信息。然而，很少对全局（句子级）信息进行探索，这为提高生成质量留下了空间。在本文中，我们提出了一种新的全局表示增强变压器（GRET）来显式地建模变压器网络中的全局表示。具体而言，在所提出的模型中，从编码器生成全局表示的外部状态。然后在解码过程中将全局表示融合到解码器中，以提高生成质量。我们在两个文本生成任务中进行了实验：机器翻译和文本摘要。在四个WMT机器翻译任务和LCSTS文本摘要任务上的实验结果表明了该方法对自然语言生成的有效性。</pre></li>
<li><a href="https://arxiv.org/abs/2006.03274">GMAT: Global Memory Augmentation for Transformers</a>
<pre>基于转换器的模型由于其大容量、固有的并行性和高性能，在自然语言处理中已变得无处不在。Transformer块的上下文化组件是$\textit{pairwise dot product}$注意，它对长度为$L$的序列有很大的$\Omega（L^2）$内存需求，限制了它处理长文档的能力。最近，人们对这一问题产生了极大的兴趣，提出了多种近似方法来减少使用稀疏注意矩阵的二次记忆需求。在这项工作中，我们建议使用长度为$M$（$\ll L$）的密集的基于注意的$\textit{global memory}$来扩充稀疏转换器块，它提供每个位置的整个输入序列的聚合全局视图。我们的扩展具有可管理的$O（M\cdot（L+M））$内存开销，并且可以与以前的稀疏解决方案无缝集成。此外，全局内存还可以用于序列压缩，方法是仅使用内存表示来表示长输入序列。我们的经验表明，我们的方法在一系列任务上取得了实质性的改进，包括（a）需要全局推理的合成任务，（b）掩蔽语言建模，以及（c）阅读理解。</pre></li>
<li><a href="https://arxiv.org/abs/2006.11527">Memory Transformer</a>
<pre>基于Transformer的模型在许多自然语言处理任务中取得了最先进的成果。自我注意体系结构允许transformer将序列中所有元素的信息组合到上下文感知表示中。但是，有关上下文的信息主要存储在相同的元素表示中。这可能会限制处理与整个序列相关的属性的难度。添加可训练内存以选择性地存储序列的局部和全局表示是改进变压器模型的一个有希望的方向。记忆增强神经网络（MANN）扩展了传统的神经结构，具有通用的表示记忆。MANN已经证明了学习简单算法（如复制或反向）的能力，并且可以通过反向传播在不同任务（从问答到语言建模）上成功地进行训练，其性能优于具有类似复杂性的RNN和LSTM。在这项工作中，我们提出并研究了转换器基线的一些扩展（1）通过添加内存令牌来存储非本地表示，（2）为全局信息创建内存瓶颈，（3）使用专用层控制内存更新。我们评估了这些记忆增强变压器，并证明记忆的存在与机器翻译和语言建模任务的模型性能正相关。使用内存标记对预先训练的蒙面语言模型进行扩充，对于GLUE基准测试中的任务，结果喜忧参半。记忆中注意力模式的可视化表明，它提高了模型处理全局上下文的能力。</pre></li>
<li><a href="https://arxiv.org/abs/2002.06170">Transformer on a Diet</a> [<a href="https://github.com/cgraywang/transformer-on-diet">github</a>]
<pre>由于Transformer能够以高效的方式捕获序列信息，因此得到了广泛的应用。然而，最近的开发，如BERT和GPT-2，只提供了注重有效性的重型体系结构。在本文中，我们探讨了三种精心设计的光变压器结构，以确定变压器是否能够以较少的计算产生具有竞争力的结果。在语言模型基准数据集上的实验结果表明，这种折衷是有希望的，光变压器最多可以减少70%的参数，同时与标准变压器相比，获得了竞争性的困惑。源代码是公开的。</pre></li>
<li><a href="https://arxiv.org/abs/1906.09777">A Tensorized Transformer for Language Modeling</a> (NeurIPS2019)
<pre>神经模型的最新发展通过自我注意机制将编码器和解码器连接起来。特别是，完全基于自我注意的Transformer在自然语言处理（NLP）任务方面取得了突破。然而，作为Transformer关键组件的多头注意机制将模型的有效部署限制在资源有限的环境中。本文基于张量分解和参数共享的思想，提出了一种新的基于块项张量分解的自我注意模型（即多重线性注意）。我们测试和验证提出的注意方法在三个语言建模任务（即，PTB，WiKeTeXT-103和十亿）和神经机器翻译任务（即，WMT-2016英语德语）。与Transformer、Transformer XL和Transformer with tensor train decomposition等多种语言建模方法相比，多重线性注意不仅可以大大压缩模型参数，而且可以获得性能改进。</pre></li>
<li><a href="https://arxiv.org/abs/1911.12385">DeFINE: DEep Factorized INput Token Embeddings for Neural Sequence Modeling</a> (ICLR2020) [<a href="https://github.com/sacmehta/delight">github</a>]
<pre>对于具有大型词汇表的序列模型，大多数网络参数位于输入和输出层。在这项工作中，我们描述了一种新的方法DeFINE，用于有效地学习深层令牌表示。我们的体系结构使用具有新颖跳跃连接的分层结构，允许使用低维输入和输出层，减少总参数和训练时间，同时提供与现有方法类似或更好的性能。定义可以很容易地合并到新的或现有的序列模型中。与包括自适应输入表示在内的最新方法相比，该技术的复杂度降低了6%到20%。在WikiText-103上，DeFINE将Transformer XL的总参数减少一半，对性能的影响最小。在Penn Treebank上，DeFINE将AWD-LSTM提高了4个点，参数减少了17%，实现了与具有较少参数的最先进方法相当的性能。对于机器翻译，DeFINE将Transformer模型的效率提高了约1.4倍，同时提供了类似的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2008.00623">DeLighT: Very Deep and Light-weight Transformer</a> [<a href="https://github.com/sacmehta/delight">github</a>]
<pre>我们介绍了一种深度和重量轻的变压器，喜悦，提供类似或更好的性能比标准变压器为基础的模型，大大减少了参数。DeLighT更有效地分配参数：（1）在每个变压器块内使用DeLighT变换（深度和轻型变换），以及（2）在块间使用分块缩放，这允许在输入附近使用浅和窄的DeLighT块，在输出附近使用宽和深的DeLighT块。总的来说，喜悦网络比标准变压器模型深2.5到4倍，但参数和操作更少。在基准机器翻译和语言建模任务上的实验表明，DeLighT匹配或提高了基线转换器的性能，平均减少了2到3倍的参数。我们的源代码位于：\url{https://github.com/sacmehta/delight}</pre></li>
<li><a href="https://arxiv.org/abs/2004.11886">Lite Transformer with Long-Short Range Attention</a> [<a href="https://github.com/mit-han-lab/lite-transformer">github</a>] (ICLR2020)
<pre>Transformer在自然语言处理（如机器翻译、问答）中已变得无处不在；然而，它需要大量的计算来实现高性能，这使得它不适合于受到硬件资源和电池的严格限制的移动应用。在本文中，我们提出了一种高效的移动NLP体系结构Lite Transformer，以便于在边缘设备上部署移动NLP应用程序。关键原语是长-短距离注意（LSRA），其中一组头部专门进行局部上下文建模（通过卷积），而另一组则专门进行长距离关系建模（通过注意）。这种专业化在三个成熟的语言任务上带来了比vanilla transformer一致的改进：机器翻译、抽象摘要和语言建模。在资源有限的情况下（500M/100M MAC），Lite Transformer在WMT'14英法版上的性能分别比Transformer高1.2/1.7 BLEU。Lite Transformer将变压器基础模型的计算量减少了2.5倍，BLEU分数降低了0.3。结合剪枝和量化，我们进一步将Lite Transformer的模型大小压缩了18.2x。对于语言建模，Lite Transformer的复杂度比500米左右的Transformer低1.8。值得注意的是，Lite Transformer在移动NLP设置上比基于AutoML的Evolutiond Transformer高出0.5个BLEU，而无需花费超过250 GPU年的昂贵架构搜索。守则已于https://github.com/mit-han-lab/lite-transformer.</pre></li>
<li><a href="https://openreview.net/forum?id=B1gjs6EtDr">Efficient Content-Based Sparse Attention with Routing Transformers</a></li>
<li><a href="https://arxiv.org/abs/1911.04070">BP-Transformer: Modelling Long-Range Context via Binary Partitioning</a>
<pre>Transformer模型在许多自然语言处理任务中获得了广泛的成功。然而，自我注意的二次复杂性限制了它在长文本中的应用。在本文中，通过二进制划分（BP）在多尺度跨度上采用从细到粗的注意机制，我们提出了BP转换器（简称BPT）。BPT产生$O（k\cdot n\log（n/k））$连接，其中$k$是控制注意力密度的超参数。BPT在计算复杂度和模型容量之间有很好的平衡。在文本分类、机器翻译和语言建模方面的一系列实验表明，与以往的自我注意模型相比，BPT对长文本具有更好的性能。我们的代码、hyperparameters和CUDA内核可在PyTorch中获得。</pre></li>
<li><a href="https://arxiv.org/abs/2004.05150">Longformer: The Long-Document Transformer</a> [<a href="https://github.com/allenai/longformer">github</a>]
<pre>基于变压器的模型无法处理长序列，因为它们的自我注意操作随序列长度二次缩放。为了解决这一限制，我们引入了Longformer，它具有一种注意机制，该机制随序列长度线性扩展，使得处理数千个或更长令牌的文档变得容易。朗弗雷特的注意机制是标准自我注意的一个替代品，它将局部窗口注意与任务驱动的全局注意相结合。在之前关于长序列转换器的工作之后，我们在字符级语言建模上评估了Longformer，并在text8和enwik8上实现了最先进的结果。与大多数以前的工作相比，我们还对Longformer进行了预训练，并在各种下游任务上对其进行了微调。我们经过培训的Longformer在长文档任务方面始终优于RoBERTa，并在WikiHop和TriviaQA上取得了最新的成果。最后，我们介绍了Longformer编码器-解码器（LED），它是一种支持长文档生成序列到序列任务的Longformer变体，并在arXiv摘要数据集上演示了它的有效性。</pre></li>
<li><a href="https://arxiv.org/abs/2007.14062">Big Bird: Transformers for Longer Sequences</a>
<pre>基于变压器的模型，如BERT，已经成为NLP最成功的深度学习模型之一。不幸的是，他们的一个核心限制是，由于他们的完全注意机制，他们对序列长度的二次依赖性（主要是在记忆方面）。为了解决这个问题，我们提出了BigBird，一种稀疏注意机制，将这种二次依赖性降低为线性。我们证明了BigBird是序列函数的通用逼近器，并且是图灵完备的，从而保持了二次全注意模型的这些性质。一路上，我们的理论分析揭示了拥有$O（1）$全局令牌（如CLS）的一些好处，这些令牌作为稀疏注意机制的一部分关注整个序列。建议的稀疏注意可以处理长度为以前使用类似硬件可能处理长度的8倍的序列。由于能够处理更长的上下文，BigBird极大地提高了各种NLP任务（如问答和摘要）的性能。我们还提出了基因组数据的新应用。</pre></li>
<li><a href="https://arxiv.org/abs/2012.07436">Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting</a> (AAAI2021)
<pre>许多实际应用需要预测长序列时间序列，例如电力消耗规划。长序列时间序列预测（LSTF）对模型的预测能力提出了更高的要求，即能够有效地捕捉输出和输入之间精确的长期依赖耦合。最近的研究表明，变压器具有提高预测能力的潜力。然而，Transformer存在一些严重问题，使其无法直接应用于LSTF，包括二次时间复杂度、高内存使用率以及编码器-解码器体系结构的固有限制。为了解决这些问题，我们为LSTF设计了一个高效的基于转换器的模型，名为Informer，它具有三个显著的特点：（i）一个$ProbSparse$自我注意机制，它在时间复杂度和内存使用方面达到$O（L\log L）$，并且在序列依赖性对齐方面具有相当的性能。（ii）自注意提取通过将级联层输入减半来突出控制注意，并有效地处理超长输入序列。（iii）生成式解码器虽然概念上简单，但在一次正向操作而不是一步一步地预测长时间序列，这大大提高了长序列预测的推理速度。在四个大规模数据集上的大量实验表明，Informer的性能明显优于现有的方法，并为LSTF问题提供了一种新的解决方案。</pre></li>
<li><a href="https://arxiv.org/abs/2102.03902">Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention</a> (AAAI2021) [<a href="https://github.com/mlpen/Nystromformer">github</a>]
<pre>Transformers已经成为一个强大的工具，用于广泛的自然语言处理任务。驱动Transformers令人印象深刻的性能的一个关键组件是自我注意机制，它编码其他令牌对每个特定令牌的影响或依赖。虽然有益，但自我关注输入序列长度的二次复杂性限制了其应用于更长的序列——社区正在积极研究这一主题。为了解决这一局限性，我们提出了Nystr\“{o}mformer——一种具有良好可扩展性的序列长度函数模型。我们的想法是基于调整Nystr\“{o}m方法，使其以$o（n）$复杂度近似标准自我注意。Nystr\“{o}mformer的可伸缩性使应用程序能够使用数千个令牌执行更长的序列。我们在GLUE基准上对多个下游任务进行评估，并使用标准序列长度对IMDB进行审查，发现我们的Nystr\“{o}mformer的性能相当，或者在少数情况下甚至稍好一些，而不是标准的自我关注。在远程竞技场（LRA）基准测试中的长序列任务中，Nystr\“{o}mformer相对于其他有效的自我注意方法表现良好。我们的代码可在https://github.com/mlpen/Nystromformer.</pre></li>
<li><a href="https://arxiv.org/abs/1911.03864">Improving Transformer Models by Reordering their Sublayers</a> (ACL2020)
<pre>多层变压器网络由交错的自注意子层和前馈子层组成。以不同的模式排列子层是否会导致更好的性能？我们生成随机排序的变压器，并使用语言建模目标对其进行训练。我们观察到，其中一些模型能够实现比交错基线更好的性能，并且那些成功的变体往往在底部有更多的自我关注，在顶部有更多的前馈子层。我们提出了一种新的转换器模式，即三明治转换器，并表明它在不消耗参数、内存或训练时间的情况下，改善了多单词级和字符级语言建模基准的复杂性。然而，正如我们在机器翻译模型上所展示的那样，三明治重新排序模式并不能保证每个任务的性能提高。相反，我们建议需要进一步探索任务特定的子层重新排序，以释放额外的收益。</pre></li>
<li><a href="https://arxiv.org/abs/2004.08178">Highway Transformer: Self-Gating Enhanced Self-Attentive Networks</a>
<pre>自我注意机制在各种顺序学习任务中取得了惊人的最新进展（SOTA），通过关注不同位置的所有全球环境，站在多头点积注意上。通过一条伪信息高速公路，我们引入了一个门控组件自依单元（SDU），它结合了LSTM样式的门控单元，以补充单个表示的多维潜在空间中的内部语义重要性。辅助的基于内容的SDU门允许通过跳过连接的调制潜在嵌入的信息流，从而通过梯度下降算法获得明显的收敛速度。我们可以揭示门机制在基于上下文的变压器模块中的作用，假设SDU门，特别是浅层SDU门，可以在优化过程中更快地向次优点移动。</pre></li>
<li><a href="https://arxiv.org/abs/2103.13597">Mask Attention Networks: Rethinking and Strengthen Transformer</a> (NAACL2021)
<pre>Transformer是一种基于注意的神经网络，它由两个子层组成，即自注意网络（SAN）和前馈网络（FFN）。现有的研究试图分别增强这两个子层，以提高Transformer的文本表示能力。在本文中，我们对SAN和FFN作为掩码注意网络（MAN）提出了一种新的理解，并证明它们是具有静态掩码矩阵的MAN的两种特殊情况。然而，它们的静态掩码矩阵限制了文本表示学习中的局部性建模能力。因此，我们引入了一个新的层称为动态掩码注意网络（DMAN），该层具有可学习的掩码矩阵，能够自适应地建模局部性。为了结合DMAN、SAN和FFN的优点，我们提出了一种顺序分层结构来组合这三种类型的层。对各种任务的广泛实验，包括神经机器翻译和文本摘要表明，我们的模型优于原始变压器。</pre></li>
<li><a href="https://arxiv.org/abs/2005.00743">Synthesizer: Rethinking Self-Attention in Transformer Models</a>
<pre>众所周知，dot产品自我关注是最先进变压器型号的核心和不可或缺的部分。但这真的需要吗？本文研究了基于点积的自我注意机制对变压器模型性能的真正重要性和贡献。通过大量实验，我们发现：（1）随机对齐矩阵令人惊讶地表现出相当的竞争力；（2）从令牌（查询键）交互中学习注意权重是有用的，但毕竟不是那么重要。为此，我们提出\textsc{Synthesizer}，这是一种学习合成注意权重的模型，无需令牌交互。在我们的实验中，我们首先展示了简单合成器在一系列任务（包括机器翻译、语言建模、文本生成和GLUE/SuperGLUE基准测试）中，与vanilla Transformer模型相比，能够获得极具竞争力的性能。当用点积注意力合成时，我们发现合成器始终优于变压器。此外，我们还对合成器与动态卷积进行了额外的比较，结果表明，简单的随机合成器不仅比动态卷积快60\%$，而且相对提高了3.5\%$的复杂度。最后，我们展示了简单因子合成器在仅编码任务上的性能优于LINFORMER。</pre></li>
<li><a href="https://arxiv.org/abs/2010.04245">Query-Key Normalization for Transformers</a> (EMNLP2020 Findings)
<pre>低资源语言翻译是一项具有挑战性但具有社会价值的自然语言处理任务。在最近的工作的基础上，我们提出了QKNorm，这是一种规范化技术，它修改了注意机制，使softmax函数在不牺牲表现力的情况下不易出现任意饱和。具体地说，我们在每个查询和键矩阵的头维度上应用$\ell_2$规范化，然后将它们相乘，然后按可学习的参数放大，而不是除以嵌入维度的平方根。我们从TED Talks语料库和IWSLT'15的5对低资源翻译对中显示出比最先进的双语基准平均0.928 BLEU的改进。</pre></li>
<li><a href="https://arxiv.org/abs/2009.14794">Rethinking Attention with Performers</a> (ICLR2021)
<pre>我们介绍了表演者、变换器架构，它可以以可证明的精度估计规则（softmax）满秩注意变换器，但只使用线性（相对于二次）空间和时间复杂性，而不依赖任何先验，如稀疏性或低rankness。为了逼近softmax注意核，表演者使用了一种新的通过正正交随机特征的快速注意方法（FAVOR+），这可能是可伸缩核方法的独立兴趣。除softmax外，FAVOR+还可用于有效模拟可内核化的注意机制。这种表现力对于第一次在常规变形金刚无法完成的大规模任务中准确比较softmax与其他内核以及研究最佳注意内核至关重要。表演者是完全兼容常规变换器的线性结构，具有强大的理论保证：注意矩阵的无偏或近似无偏估计、一致收敛和低估计方差。我们在从像素预测到文本模型到蛋白质序列建模的一系列丰富任务中测试了执行者。我们展示了与其他有效稀疏和密集注意方法的竞争结果，展示了表演者利用的新型注意学习范式的有效性。</pre></li>
<li><a href="https://arxiv.org/abs/2005.02008">Dynamically Adjusting Transformer Batch Size by Monitoring Gradient Direction Change</a>
<pre>超参数的选择影响神经模型的性能。虽然之前的许多研究（Sutskever等人，2013年；Duchi等人，2011年；Kingma和Ba，2015年）侧重于加速收敛和降低学习速度的影响，但相对较少的论文集中于批量大小的影响。在本文中，我们分析了批量大小的增加对梯度方向的影响，并提出用梯度角度的变化来评估梯度的稳定性。根据我们的观察，在累积小批量时，梯度方向的角度变化首先趋于稳定（即逐渐减小），然后开始波动。我们建议通过累积小批量的梯度并在梯度方向开始波动时执行优化步骤来自动和动态地确定批量大小。为了提高我们的方法对大型模型的效率，我们提出了一种抽样方法来选择对批量大小敏感的参数梯度。我们的方法在培训期间动态地确定适当和有效的批量大小。在我们对WMT 14英语到德语和英语到法语任务的实验中，我们的方法将固定25k批量的变压器分别提高+0.73和+0.82 BLEU。</pre></li>
<li><a href="https://arxiv.org/abs/2005.14187">HAT: Hardware-Aware Transformers for Efficient Natural Language Processing</a> (ACL2020) [<a href="https://github.com/mit-han-lab/hardware-aware-transformers">github</a>]
<pre>转换器在自然语言处理（NLP）任务中无处不在，但由于计算量大，很难在硬件上部署。为了在资源受限的硬件平台上实现低延迟推断，我们建议使用神经架构搜索设计硬件感知转换器（HAT）。我们首先用$\textit{arbitral encoder decoder attention}$和$\textit{heterogeneous layers}$构建一个大的设计空间。然后，我们训练一个$\textit{SuperTransformer}$，它覆盖设计空间中的所有候选对象，并通过权重共享有效地生成许多$\textit{SubTransformer}$。最后，我们使用硬件延迟约束执行演化搜索，以找到专用于在目标硬件上快速运行的$\textit{SubTransformer}$。对四个机器翻译任务的大量实验表明，HAT可以发现不同硬件（CPU、GPU、物联网设备）的有效模型。在Raspberry Pi-4上运行WMT'14翻译任务时，HAT可以实现$\textbf{3}\times$加速，比基线转换器小$\textbf{3.7}\times$$\textbf{2.7}\times$加速比，$\textbf{3.6}\times$比演进的Transformer更小，搜索成本减少$\textbf{12041}\times$，并且没有性能损失。HAT代码是https://github.com/mit-han-lab/hardware-aware-transformers.git</pre></li>
<li><a href="https://arxiv.org/abs/2006.04768">Linformer: Self-Attention with Linear Complexity</a>
<pre>大型transformer模型在许多自然语言处理应用中取得了非凡的成功。然而，训练和部署这些模型对于长序列来说可能代价高昂，因为变压器的标准自我注意机制在序列长度方面使用$O（n^2）$时间和空间。在本文中，我们证明了自我注意机制可以用低秩矩阵来近似。我们进一步利用这一发现提出了一种新的自我注意机制，该机制在时间和空间上将整体自我注意复杂性从$O（n^2）$降低到$O（n）$。得到的线性变压器，\TrimtT{LeNeMeX }，与标准变压器模型相匹配，同时具有更大的存储和具有时效性的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2109.03939">What’s Hidden in a One-layer Randomly Weighted Transformer?</a> (EMNLP2021)
<pre>我们证明，隐藏在一层随机加权神经网络中的子网络可以在机器翻译任务中实现令人印象深刻的性能，而无需修改权重初始化。为了寻找单层随机加权神经网络的子网络，我们对同一权重矩阵应用不同的二元掩码来生成不同的层。隐藏在一层随机加权变压器中，我们发现在IWSLT14/WMT14上可以实现29.45/17.29 BLEU的子网络。使用固定的预训练嵌入层，先前发现的子网络比经过训练的基于IWSLT14/WMT14的小型变压器的性能小，但可以达到98%/92%（34.14/25.24 BLEU）。此外，我们还演示了在此设置中更大和更深变压器的有效性，以及不同初始化方法的影响。我们在上发布了源代码https://github.com/sIncerass/one_layer_lottery_ticket.</pre></li>
<li><a href="https://arxiv.org/abs/2006.16236">Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention</a>
<pre>变压器在若干任务中取得了显著的性能，但由于其二次复杂性，相对于输入的长度，它们在很长的序列中速度非常慢。为了解决这个限制，我们将自我关注表示为内核特征映射的线性点积，并利用矩阵积的关联性属性将复杂性从$\mathcal{O}\left（N^2\right）$降低到$\mathcal{O}\left（N\right）$，其中，$N$是序列长度。我们证明了这个公式允许一个迭代实现，极大地加速了自回归变换器，并揭示了它们与递归神经网络的关系。我们的线性变压器实现了与普通变压器类似的性能，并且它们在很长序列的自回归预测上快了4000x。</pre></li>
<li><a href="https://arxiv.org/abs/2004.08249">Understanding the Difficulty of Training Transformers</a> (EMNLP2020)
<pre>变压器在许多NLP任务中被证明是有效的。然而，他们的培训需要在设计尖端优化器和仔细学习速率调度器方面付出大量努力（例如，传统SGD无法有效培训变压器）。我们的目标是从实证和理论的角度理解$\textit{变压器培训的复杂性}$。我们的分析表明，不平衡的梯度不是训练不稳定的根本原因。相反，我们确定了一种放大效应，它会对训练产生重大影响——对于多层变压器模型中的每一层，对其剩余分支的严重依赖会使训练不稳定，因为它会放大小参数扰动（例如，参数更新），并导致模型输出中的显著扰动。然而，我们观察到，轻微的依赖性限制了模型的潜力，并导致训练较差的模型。受我们分析的启发，我们建议使用Admin（$\textbf{Ad}$aptive$\textbf{m}$odel$\textbf{in}$initialization）来稳定早期阶段的训练，并在后期充分发挥其潜力。大量实验表明，Admin更稳定，收敛速度更快，性能更好。具体实施发布于：https://github.com/LiyuanLucasLiu/Transforemr-Clinic.</pre></li>
<li><a href="https://arxiv.org/abs/2009.08034">Towards Fully 8-bit Integer Inference for the Transformer Model</a> (IJCAI2020)
<pre>8位整数推理作为减少深度神经网络延迟和存储的一个有前途的方向，近年来取得了很大的进展。另一方面，以前的系统在复杂模型中的某些函数（例如，Transformer中的Softmax）仍然依赖32位浮点，并且大量使用量化和反量化。在这项工作中，我们表明，在对转换器结构进行了原则性修改（称为Integer Transformer）后，可以导出（几乎）完全8位整数推断算法，即规模传播。必要时采用去量化，使网络更加高效。我们在WMT16 En<->Ro、WMT14 En<->De和En->Fr翻译任务以及WikiText-103语言建模任务上的实验表明，全8位转换器系统实现了与浮点基线相当的性能，但需要的内存占用量减少了近4倍。</pre></li>
<li><a href="https://arxiv.org/abs/2009.07453">Extremely Low Bit Transformer Quantization for On-Device Neural Machine Translation</a>
<pre>由于推理过程中的大量计算负载和内存开销，特别是当目标设备的计算资源（如移动设备或边缘设备）有限时，广泛使用的Transformer架构的部署具有挑战性。量化是解决这些挑战的有效技术。我们的分析表明，对于给定数量的量化比特，每个转换器块以不同的方式对翻译质量和推理计算作出贡献。此外，即使在嵌入块中，每个单词的贡献也大不相同。相应地，我们提出了一种混合精度量化策略，以极低的比特数（例如，低于3比特）表示变压器权重。例如，对于嵌入块中的每个字，我们根据统计特性分配不同的量化位。我们的量化变压器模型比基线模型小11.8$\倍，BLEU小于-0.5。我们实现了8.3$\倍的运行时内存占用减少和3.5$\倍的加速（Galaxy N10+），因此我们提出的压缩策略能够有效地实现设备上的NMT。</pre></li>
<li><a href="https://arxiv.org/abs/2011.04006">Long Range Arena: A Benchmark for Efficient Transformers</a>
<pre>变压器不能很好地扩展到长序列长度，这主要是因为二次自我关注的复杂性。近几个月来，人们提出了一系列高效、快速的变压器来解决这一问题，通常都声称其模型质量优于普通变压器模型或与之相当。到目前为止，对于如何评估这类模型，还没有形成共识。此外，在广泛的任务和数据集上不一致的基准测试使得在许多模型中难以评估相对模型质量。本文提出了一个系统和统一的基准，LRA，特别侧重于评估长上下文场景下的模型质量。我们的基准是一套任务，由从$1K$到$16K$的标记序列组成，包含广泛的数据类型和模式，如文本、自然、合成图像和数学表达式，需要相似性、结构和视觉空间推理。我们在新提出的基准测试套件上系统地评估了十种成熟的远程变压器模型（改革器、LINFORMER、线性变压器、伸角变压器、表演者、合成器、稀疏变压器和Longformer）。LRA为更好地理解此类高效变压器模型铺平了道路，促进了这方面的更多研究，并提出了新的挑战性任务。我们的基准代码将在https://github.com/google-research/long-range-arena.</pre></li>
</ul>
<h1 id="probe">Probe</h1>
<ul>
<li><a href="https://aclweb.org/anthology/papers/N/N19/N19-1419/">A Structural Probe for Finding Syntax in Word Representations</a> (NAACL2019)</li>
<li><a href="https://arxiv.org/abs/2006.00995">When Bert Forgets How To POS: Amnesic Probing of Linguistic Properties and MLM Predictions</a>
<pre>越来越多的工作利用探测来研究神经模型（通常被认为是黑匣子）的工作。最近，围绕探测范式的局限性出现了一场持续的辩论。在这项工作中，我们指出了无法从探测结果中推断行为结论的问题，并提供了一种替代方法，重点关注信息的使用方式，而不是信息的编码方式。我们的方法，健忘症探测，遵循一种直觉，即一个属性对于给定任务的效用可以通过测量从表征中移除它的因果干预的影响来评估。有了这个新的分析工具，我们可以提出以前不可能提出的问题，例如，词性信息对单词预测是否重要？我们对伯特进行了一系列分析，以回答这些类型的问题。我们的研究结果表明，传统的探测性能与任务重要性无关，我们呼吁加强对从探测结果中得出行为或因果结论的说法的审查。</pre></li>
<li><a href="https://arxiv.org/abs/2005.04511">Finding Universal Grammatical Relations in Multilingual BERT</a> (ACL2020)
<pre>最近的研究发现，多语言伯特（mBERT）是一种基于变换器的多语言掩蔽语言模型，能够进行零次跨语言迁移，这表明其表示的某些方面是跨语言共享的。为了更好地理解这种重叠，我们将最近在神经网络内部表示中寻找句法树的工作扩展到多语言环境。我们证明了mBERT表示的子空间可以恢复除英语以外的其他语言的句法树距离，并且这些子空间在不同语言之间近似共享。基于这些结果，我们提出了一种无监督的分析方法，该方法提供了mBERT学习句法依赖标签表示的证据，其形式为聚类，这在很大程度上符合通用依赖分类法。这一证据表明，即使没有明确的监督，多语言蒙面语言模型也能学习某些语言共性。</pre></li>
<li><a href="https://arxiv.org/abs/2011.02070">Probing Multilingual BERT for Genetic and Typological Signals</a> (COLING2020)
<pre>我们在多语言BERT（mBERT）中探测100种语言的系统发育和地理语言信号层，并基于mBERT表示计算语言距离。我们1）利用语言距离推断和评估语言树，发现它们在四叉树距离方面与参考家谱接近，2）进行距离矩阵回归分析，发现语言距离可以最好地用系统发育因素来解释，最坏的是结构因素。3）提出了一种新的测量历时意义稳定性的方法（基于跨语言表征变异性），该方法与基于语言学方法的已发表排名表显著相关。我们的研究结果有助于跨语言文本表征的类型解释性这一新兴领域。</pre></li>
<li><a href="https://arxiv.org/abs/1903.08855">Linguistic Knowledge and Transferability of Contextual Representations</a> (NAACL2019) [<a href="https://github.com/nelson-liu/contextual-repr-analysis">github</a>]
<pre>来自大规模神经语言模型的上下文词表示在一组不同的NLP任务中是成功的，这表明它们编码了语言的有用和可转移的特征。为了阐明他们所获取的语言知识，我们研究了最近几个经过预训练的语境化者（ELMo的变体、OpenAI transformer语言模型和BERT）通过一系列17种不同的探测任务所产生的表征。我们发现，在许多情况下，在冻结的上下文表示之上训练的线性模型与最先进的任务特定模型具有竞争性，但在需要细粒度语言知识（例如，联合识别）的任务上失败。为了研究语境词表征的可转移性，我们量化了语境化者中各个层面的可转移性差异，特别是递归神经网络（RNN）和变换器之间的差异。例如，RNN的更高层更具有任务特定性，而transformer层并没有表现出相同的单调趋势。此外，为了更好地理解语境词表征的可转移性，我们将语言模型预训练与11项有监督的预训练任务进行了比较。对于任何给定的任务，当预训练数据集固定时，对密切相关的任务进行预训练比语言模型预训练（平均而言更好）产生更好的性能。然而，语言模型对更多的数据进行预训练会得到最好的结果。</pre></li>
<li><a href="https://arxiv.org/abs/1904.11544">Probing What Different NLP Tasks Teach Machines about Function Word Comprehension</a> (*SEM2019)
<pre>我们介绍了一组九个挑战性任务，测试对虚词的理解。这些任务是通过对现有数据集中的句子进行结构变异来创建的，目的是理解特定类型的虚词（例如，介词、wh词）。使用这些探测任务，我们探索了句子编码者的各种预训练目标（例如，语言建模、CCG超级标记和自然语言推理（NLI））对学习表征的影响。我们的研究结果表明，语言建模的预训练在我们的探索任务中平均表现最好，支持其广泛用于预训练最先进的NLP模型，CCG supertagging和NLI预训练表现相当。总的来说，没有训练前目标占主导地位，我们的虚词探测任务突出了训练前目标之间的几个直观差异，例如NLI有助于理解否定。</pre></li>
<li><a href="https://arxiv.org/abs/1905.05950">BERT Rediscovers the Classical NLP Pipeline</a> (ACL2019)
<pre>预先训练的文本编码器已经在许多NLP任务上迅速提升了最新水平。我们关注一个这样的模型，BERT，目的是量化语言信息在网络中的捕获位置。我们发现，该模型以可解释和本地化的方式表示传统NLP管道的步骤，并且负责每个步骤的区域按预期顺序出现：词性标记、解析、NER、语义角色，然后是共同引用。定性分析表明，该模型可以而且经常会动态调整该管道，在消除高层表示信息歧义的基础上修改下层决策。</pre></li>
<li><a href="https://arxiv.org/abs/2106.14282">A Closer Look at How Fine-tuning Changes BERT</a> (ACL2022)
<pre>考虑到当今NLP中普遍存在预先培训的情境化表达，人们已经做出了很多努力来理解它们包含的信息，以及为什么它们似乎普遍成功。使用这些表示的最常见方法是为最终任务对其进行微调。然而，微调如何改变底层嵌入空间的研究较少。在这项工作中，我们研究了英国伯特家族，并使用两种探测技术来分析微调是如何改变空间的。我们假设微调通过增加与不同标签相关的示例之间的距离来影响分类性能。我们通过精心设计的五个不同NLP任务的实验来证实这一假设。通过这些实验，我们还发现了一个例外，即“微调总是提高性能”。最后，通过比较微调前后的表示，我们发现微调不会对表示进行任意更改；相反，它会根据下游任务调整表示，同时在很大程度上保留数据点的原始空间结构。</pre></li>
<li><a href="https://arxiv.org/abs/2104.06400">Mediators in Determining what Processing BERT Performs First</a> (NAACL2021)
<pre>利用激活模式探测神经模型执行下游任务的能力，通常用于定位网络的哪些部分专门执行哪些任务。然而，在这种比较中，很少有工作涉及潜在的中介因素。作为一个测试用例中介因子，我们考虑预测的上下文长度，即最小限度地执行处理所需的跨度的长度。我们发现，不控制上下文长度可能会导致关于网络定位模式的矛盾结论，这取决于探测数据集的分布。事实上，当我们用七个任务探测BERT时，我们发现在探测数据集中操纵上下文长度的分布时，它们之间有可能得到196个不同的排名。最后，我们介绍了未来进行此类比较的最佳实践。</pre></li>
<li><a href="https://arxiv.org/abs/1907.07355">Probing Neural Network Comprehension of Natural Language Arguments</a> (ACL2019)
<pre>我们惊讶地发现，伯特在辩论推理理解任务中77%的最高表现仅比未经训练的人类平均基线低三个点。然而，我们表明，这一结果完全可以通过利用数据集中的虚假统计线索来解释。我们分析了这些线索的性质，并证明了一系列模型都利用了它们。该分析提供了一个对抗性数据集的构建，所有模型都在该数据集上实现随机精度。我们的对抗性数据集提供了对论点理解的更可靠的评估，应该作为未来工作的标准。</pre></li>
<li><a href="https://arxiv.org/abs/1910.01157">Cracking the Contextual Commonsense Code: Understanding Commonsense Reasoning Aptitude of Deep Contextual Representations</a> (EMNLP2019 WS)
<pre>经过预先训练的深层语境表征已经在各种常识NLP任务上提升了最新水平，但我们对这些模型的能力缺乏具体的理解。因此，我们调查和挑战伯特的常识表达能力的几个方面。首先，我们探讨了BERT对各种对象属性的分类能力，证明了BERT在其嵌入空间中对各种常识特征进行编码的能力很强，但在许多领域仍然存在不足。接下来，我们表明，通过使用与缺陷属性相关的额外数据来增加BERT的预训练数据，我们能够在使用最少数据量的情况下提高下游常识推理任务的性能。最后，我们开发了一种微调知识图嵌入的方法，并展示了显式知识图的持续重要性。</pre></li>
<li><a href="https://arxiv.org/abs/1911.05758">What do you mean, BERT? Assessing BERT as a Distributional Semantics Model</a>
<pre>语境化词语嵌入，即语境中词语的向量表示，自然被视为先前非文本分布语义模型的扩展。在这项工作中，我们重点研究了BERT，这是一种产生语境化嵌入并在若干语义任务中创造了最新水平的深层神经网络，并研究了其嵌入空间的语义一致性。虽然呈现出连贯性的趋势，但伯特并没有完全达到对语义向量空间的自然期望。特别是，我们发现单词出现的句子位置虽然没有意义关联，但在单词嵌入上留下了明显的痕迹，扰乱了相似关系。</pre></li>
<li><a href="https://arxiv.org/abs/1909.00111">Quantity doesn’t buy quality syntax with neural language models</a> (EMNLP2019)
<pre>递归神经网络平均可以很好地预测即将出现的单词；然而，在句法复杂的语境中，他们往往会意外地赋予不合语法的单词很高的概率。我们调查通过增加网络规模和训练语料库在多大程度上可以缓解这些缺点。我们发现，增加网络规模所带来的收益在一定程度上是最小的。同样，扩大训练语料库会产生递减的回报；我们估计，为了使模型与人的表现相匹配，训练语料库需要非常大。通过与GPT和BERT（基于变换器的基于数十亿单词的训练模型）的比较，可以发现这些模型在某些结构中的性能甚至比我们的LSTM差。我们的结果为更高效的数据架构提供了依据。</pre></li>
<li><a href="https://openreview.net/forum?id=H1xPR3NtPB">Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction</a> (ICLR2020)</li>
<li><a href="https://arxiv.org/abs/2104.05882">Discourse Probing of Pretrained Language Models</a> (NAACL2021)
<pre>已有的关于预训练语言模型（LMs）的研究主要集中在句子层面的句法任务上。在本文中，我们引入文档级话语探测来评估预训练LMs捕捉文档级关系的能力。我们对7个预训练的LMs、4种语言和7个语篇探测任务进行了实验，发现BART总体上是捕获语篇的最佳模型——但仅在其编码器中，与基线模型相比，BERT的表现出奇地好。在不同的模型中，哪一层最能捕获话语信息存在着巨大的差异，模型之间也存在着巨大的差异。</pre></li>
<li><a href="https://arxiv.org/abs/1912.13283">oLMpics – On what Language Model Pre-training Captures</a>
<pre>最近，预训练语言模型（LMs）的成功激发了人们对其语言能力的广泛兴趣。然而，了解LM表示是否对符号推理任务有用的努力是有限和分散的。在这项工作中，我们提出了八个推理任务，这些任务在概念上需要诸如比较、连接和组合之类的操作。一个基本的挑战是理解LM在任务上的性能是应该归因于预先训练的表示还是任务数据的微调过程。为了解决这个问题，我们提出了一个评估协议，该协议包括零炮评估（无微调），以及将微调LM的学习曲线与多个控件的学习曲线进行比较，从而描绘出LM能力的丰富画面。我们的主要发现是：（a）不同的LMs表现出质的不同推理能力，例如，RoBERTa成功地完成了BERT完全失败的推理任务；（b） LMs不以抽象的方式进行推理，并且依赖于上下文，例如，虽然RoBERTa可以比较年龄，但它只能在年龄在人类年龄的典型范围内时进行比较；（c） 在一半的推理任务中，所有模型都完全失败。我们的发现和基础设施可以帮助未来设计新的数据集、模型和培训前目标函数。</pre></li>
<li><a href="https://arxiv.org/abs/2004.14096">Do Neural Language Models Show Preferences for Syntactic Formalisms?</a> (ACL2020)
<pre>关于深层神经语言模型可解释性的最新研究表明，自然语言语法的许多属性都编码在它们的表征空间中。然而，这类研究往往局限于单一语言和单一语言形式主义。在这项研究中，我们的目的是调查语言模型所捕获的句法结构的外表在多大程度上依附于表层句法或深层句法分析风格，以及这些模式在不同语言中是否一致。我们对在13种不同语言上训练的BERT和ELMo模型应用了一个提取有向依赖树的探测，探测了两种不同的语法注释样式：通用依赖（UD），优先处理深层语法关系，以及表面语法通用依赖（SUD），重点关注表面结构。我们发现，这两种模型都表现出对UD的偏好，而不是SUD——在不同语言和层次上存在有趣的变化——并且这种偏好的强度与树形状的差异相关。</pre></li>
<li><a href="https://aclanthology.org/2022.acl-long.316/">Probing for Predicate Argument Structures in Pretrained Language Models</a> (ACL2022)</li>
<li><a href="https://arxiv.org/abs/2004.14786">Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT</a> (ACL2020)
<pre>通过引入一小部分附加参数，探针可以学习使用特征表示（例如，上下文嵌入）以有监督的方式解决特定的语言任务（例如，依赖项解析）。这种探测任务的有效性被视为预先训练的模型编码语言知识的证据。然而，这种评估语言模型的方法由于探针本身所学知识量的不确定性而受到影响。作为对这些工作的补充，我们提出了一种无参数探测技术，用于分析预先训练的语言模型（例如，BERT）。我们的方法不需要探测任务的直接监督，也不需要在探测过程中引入额外的参数。我们在BERT上的实验表明，使用我们的方法从BERT中恢复的语法树明显优于语言上未提供信息的基线。我们进一步将经验诱导的依赖结构输入到下游情绪分类任务中，发现其改进与人类设计的依赖模式兼容，甚至优于人类设计的依赖模式。</pre></li>
<li><a href="https://arxiv.org/abs/2005.00628">Intermediate-Task Transfer Learning with Pretrained Models for Natural Language Understanding: When and Why Does It Work?</a> (ACL2020)
<pre>虽然像BERT这样的预训练模型在自然语言理解任务中表现出了巨大的优势，但在对目标任务进行微调之前，可以通过在数据丰富的中间任务中进一步训练模型来提高其性能。然而，对于中间任务训练在什么时候以及为什么对给定的目标任务有益，人们仍然知之甚少。为了研究这一点，我们对预训练的RoBERTa模型进行了大规模研究，其中包括110个中间目标任务组合。我们进一步评估了所有经过培训的模型，其中包括25项探测任务，旨在揭示驱动转移的特定技能。我们观察到，需要高级推理和推理能力的中级任务往往效果最好。我们还观察到，目标任务绩效与更高层次的能力（如共指消解）密切相关。然而，我们没有观察到探测和目标任务性能之间更细微的相关性，这突出了需要在广泛的探测基准上开展进一步的工作。我们还观察到有证据表明，在训练前遗忘所学知识可能会限制我们的分析，这突出了在这些环境下进一步研究迁移学习方法的必要性。</pre></li>
<li><a href="https://arxiv.org/abs/2005.04315">Probing Linguistic Systematicity</a> (ACL2020)
<pre>近年来，人们对深层自然语言理解模型是否具有系统性的问题产生了极大的兴趣；泛化，使像单词这样的单位对它们出现的句子的意义做出一致的贡献。越来越多的证据表明，神经模型通常是非系统地推广的。我们从语言学的角度研究了系统性的概念，定义了一组探针和一组度量系统行为的指标。我们还确定了网络体系结构可以进行非系统概括的方式，并讨论了这种概括形式可能不令人满意的原因。作为一个案例研究，我们在自然语言推理（NLI）环境中进行了一系列实验，证明了一些NLU系统尽管不是系统性的，但总体性能还是很高。</pre></li>
<li><a href="https://arxiv.org/abs/2004.14999">A Matter of Framing: The Impact of Linguistic Formalism on Probing Results</a>
<pre>像BERT（Delvin等人，2019年）这样经过深入训练的情境化编码器在一系列下游任务中表现出卓越的性能。最近的一项探索性研究调查了这些模型在训练前所隐含的语言知识。虽然大多数探究工作是在任务层面上进行的，但语言任务很少是统一的，可以用各种形式来表示。因此，任何以语言学为基础的探究性研究都不可避免地要遵循用于注释底层数据的形式主义。形式主义的选择会影响探测结果吗？为了调查，我们在角色语义学中进行了深入的跨形式主义层探索性研究。我们发现，根据形式主义，BERT对语义角色和原型角色信息的编码在语言学上有意义的差异，并证明层探测可以检测相同语言形式主义实现之间的细微差异。我们的研究结果表明，语言形式主义与常用的跨任务和跨语言实验环境一样，是探究研究的一个重要维度。</pre></li>
<li><a href="https://www.aclweb.org/anthology/2020.repl4nlp-1.20/">A Cross-Task Analysis of Text Span Representations</a> (ACL2020 WS)</li>
<li><a href="https://arxiv.org/abs/2011.04946">When Do You Need Billions of Words of Pretraining Data?</a> [<a href="https://github.com/nyu-mll/pretraining-learning-curves">github</a>]
<pre>NLP目前主要由诸如RoBERTa这样的通用预训练语言模型控制，这些模型通过对数十亿个单词的预训练，在NLU任务中取得了优异的成绩。但Transformer LMs从大规模预培训中学到了哪些他们无法从较少数据中学到的确切知识或技能？我们采用四种探测方法——分类器探测、信息论探测、无监督的相对可接受性判断和NLU任务微调——并使用MiniBERTas绘制学习曲线，跟踪这些不同语言能力指标相对于预训练数据量的增长，一组罗伯塔模特预先训练了1M、10M、100M和1B单词。我们发现LMs只需要大约10万或100万个单词就可以学习能够可靠编码我们测试的大多数语法和语义特征的表示。为了获得足够的常识知识和掌握典型下游NLU任务所需的其他技能，需要大量的数据。结果表明，虽然编码语言特征的能力几乎肯定是语言理解所必需的，但在大型预训练模型中，其他形式的知识可能是最近语言理解改进的主要驱动力。</pre></li>
<li><a href="https://arxiv.org/abs/2011.12073">Picking BERT’s Brain: Probing for Linguistic Dependencies in Contextualized Embeddings Using Representational Similarity Analysis</a>
<pre>顾名思义，语言的语境化表达通常是由它们编码语境的能力所驱动的。这些表示法捕捉到了上下文的哪些方面？我们介绍了一种方法来解决这个问题，使用代表性相似性分析（RSA）。作为案例研究，我们调查了动词嵌入编码动词主语的程度，代词嵌入编码代词的先行词，完整的句子表示编码句子的中心词（由依赖性分析确定）。在所有情况下，我们都表明，BERT的语境化嵌入反映了所研究的语言依赖性，并且BERT对这些依赖性的编码程度大于对不太突出的语言控制的编码程度。这些结果证明了我们的方法能够在关于语境的哪些方面被编码在语言表达中的假设之间做出判断。</pre></li>
<li><a href="https://arxiv.org/abs/1909.01066">Language Models as Knowledge Bases?</a> (EMNLP2019) [<a href="https://github.com/facebookresearch/LAMA">github</a>]
<pre>在大型文本语料库上预训练语言模型的最新进展导致了对下游NLP任务的大量改进。在学习语言知识的同时，这些模型还可以存储训练数据中存在的关系知识，并且可以回答“填空”完形填空语句结构的查询。与结构化知识库相比，语言模型有许多优点：它们不需要模式工程，允许实践者查询一个开放的关系类，易于扩展到更多数据，并且不需要人工监督培训。我们对大量最先进的预训练语言模型中已经存在的关系知识（无需微调）进行了深入分析。我们发现（i）在没有微调的情况下，BERT包含与传统NLP方法竞争的关系知识，传统NLP方法具有一定的oracle知识访问权限，（ii）BERT在开放领域问题回答方面也表现出色，并且（iii）在有监督的基线下通过标准的语言模型预训练方法，某些类型的事实知识比其他类型的更容易学习。这些模型在不进行任何微调的情况下回忆事实知识的能力惊人地强大，证明了它们作为无监督的开放领域QA系统的潜力。重现我们分析的代码可在https://github.com/facebookresearch/LAMA.</pre></li>
<li><a href="https://arxiv.org/abs/1911.03681">BERT is Not a Knowledge Base (Yet): Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA</a>
<pre>我们提出了一种将实体的事实知识注入预训练的BERT模型的新方法（Devlin等人，2019）：我们将Wikipedia2Vec实体向量（Yamada等人，2016）与BERT的原生词条向量空间对齐，并将对齐的实体向量当作词条向量使用。由此产生的实体增强版BERT（称为E-BERT）在精神上与ERNIE（Zhang等人，2019）和KnowBert（Peters等人，2019）类似，但不需要对BERT编码器进行昂贵的进一步预培训。我们在无监督问答（QA）、监督关系分类（RC）和实体链接（EL）三个方面对E-BERT进行了评估。在所有三项任务中，E-BERT都优于BERT和其他基线。我们还定量地表明，原始的伯特模型过度依赖实体名称的表面形式（例如，猜测某人的名字听起来像意大利语），而e-伯特缓解了这个问题。</pre></li>
<li><a href="https://arxiv.org/abs/2002.08910">How Much Knowledge Can You Pack Into the Parameters of a Language Model?</a> (EMNLP2020)
<pre>最近有人观察到，在非结构化文本上训练的神经语言模型可以使用自然语言查询隐式地存储和检索知识。在这篇短文中，我们通过微调预先训练的模型来回答问题，而无需接触任何外部环境或知识，从而衡量这种方法的实际效用。我们表明，这种方法随着模型的大小而扩展，并且在回答问题时能够与从外部知识源显式检索答案的开放域系统进行竞争。为了促进再现性和未来的工作，我们在https://goo.gle/t5-cbqa.</pre></li>
<li><a href="https://arxiv.org/abs/2008.09036">Language Models as Knowledge Bases: On Entity Representations, Storage Capacity, and Paraphrased Queries</a> (EACL2021)
<pre>预先训练的语言模型被认为是结构化知识库的一种可能的替代或补充。然而，到目前为止，这种新兴的LM-as-KB范式只在非常有限的设置中被考虑，它只允许处理21k个实体，这些实体的单个令牌名称可以在通用的LM词汇表中找到。此外，这种范式的主要好处，即使用各种自然语言释义查询知识库，到目前为止还没有得到充分的探索。在这里，我们制定了将LMs视为KBs的两个基本要求：（i）存储涉及大量实体的大量事实的能力和（ii）查询存储事实的能力。我们探讨了三种实体表示法，这些实体表示法允许LMs表示数百万个实体，并对LMs中世界知识的释义查询进行了详细的案例研究，从而提供了语言模型确实可以作为知识库的概念证明。</pre></li>
<li><a href="https://arxiv.org/abs/2104.05240">Factual Probing Is [MASK]: Learning vs. Learning to Recall</a> (NAACL2021) [<a href="https://github.com/princeton-nlp/OptiPrompt">github</a>]
<pre>Petroni等人（2019年）证明，通过将世界事实表示为完形填空式提示，可以从预先训练的语言模型中检索世界事实，并将模型的预测精度解释为其编码的事实信息量的下限。随后的工作试图通过搜索更好的提示，使用一组不相交的事实作为训练数据来收紧估计。在这项工作中，我们做出了两个互补的贡献，以更好地理解这些事实探测技术。首先，我们提出OptiPrompt，这是一种在连续嵌入空间中直接优化的新方法。我们发现，这种简单的方法能够预测LAMA基准中另外6.4%的事实。其次，我们提出了一个更重要的问题：我们真的能将这些探测结果解释为下限吗？这些快速搜索方法是否也可以从训练数据中学习？我们发现，有些令人惊讶的是，这些方法使用的训练数据包含潜在事实分布的某些规律性，所有现有的提示方法，包括我们的方法，都能够利用它们进行更好的事实预测。我们进行了一组控制实验，将“学习”与“学习回忆”分开，提供了不同提示可以揭示预训练语言模型的更详细的图片。</pre></li>
<li><a href="https://arxiv.org/abs/2104.08696">Knowledge Neurons in Pretrained Transformers</a>
<pre>大规模的预训练语言模型令人惊讶地擅长回忆训练语料库中呈现的事实知识。本文通过引入知识神经元的概念，探讨了内隐知识是如何存储在预训练变压器中的。在给定一个关系事实的情况下，我们提出了一种知识归因方法来识别表达该事实的神经元。我们提出，这些知识神经元的激活与其相应事实的表达高度相关。此外，即使没有微调，我们也可以利用知识神经元显式编辑（如更新和删除）预训练变压器的特定事实知识。</pre></li>
<li><a href="https://www.aclweb.org/anthology/2021.naacl-main.401/">DirectProbe: Studying Representations without Classifiers</a> (NAACL2021)</li>
<li><a href="https://arxiv.org/abs/2109.07848">The Language Model Understood the Prompt was Ambiguous: Probing Syntactic Uncertainty Through Generation</a> (EMNLP2021 WS)
<pre>当一个句子的开头与多种句法分析兼容时，会出现暂时的句法歧义。我们检验了在处理暂时模糊的输入时，神经语言模型（LMs）在何种程度上表现出这种分析的不确定性，以及这种不确定性是如何通过消除歧义的线索来调节的。我们通过从中生成来探测LM的期望：我们使用随机解码来导出一组句子完成，并根据完成过程中解析的分布来估计LM分配给每个解释的概率。与基于评分的有针对性的句法评估方法不同，这种技术使研究人员能够探索未事先假设的补全。我们运用这种方法，利用人类句子处理实验的材料，研究了两种LMs（GPT2和LSTM）在三种类型的暂时歧义上的行为。我们发现LMs可以同时跟踪多个分析；不确定性的程度因结构和上下文而异。作为对消除歧义线索的回应，LMs通常选择正确的解释，但偶尔出现的错误指出了潜在的改进领域。</pre></li>
<li><a href="https://arxiv.org/abs/2010.06189">X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models</a> (EMNLP2020)
<pre>事实证明，语言模型（LMs）通过完成完形填空式填空题（如“Punta Cana位于____________________________________，关于LMs的事实表征能力的研究几乎都是在英语中进行的。为了评估LMs在不同语言中的事实知识检索，我们为23种不同类型的语言创建了一个完形填空式的多语言基准测试。为了正确处理语言变化，我们将探测方法从单词实体扩展到多词实体，并开发了几种解码算法来生成多标记预测。大量的实验结果提供了关于当前最先进的LMs在具有更多或更少可用资源的语言中执行此任务的情况。我们进一步提出了一种基于代码切换的方法来提高多语言LMs的知识获取能力，并在几种基准语言上验证了其有效性。基准数据和代码已在https://x-factr.github.io.</pre></li>
<li><a href="https://arxiv.org/abs/2104.03869">Probing BERT in Hyperbolic Spaces</a> (ICLR2021)
<pre>最近，人们提出了各种各样的探测任务来发现在语境化单词嵌入中学习到的语言特性。其中许多工作隐含地假设这些嵌入位于某些度量空间中，通常是欧几里德空间。这项工作考虑了一系列几何上特殊的空间，即双曲空间，这些空间对层次结构表现出更好的归纳偏见，并且可能更好地揭示在语境化表示中编码的语言层次。我们引入一个庞加莱探针，一个结构探针，将这些嵌入投影到具有明确定义的层次结构的庞加莱子空间。我们关注两个探索目标：（a）依赖树，其中层次结构定义为头部依赖结构；（b） 词汇情感，其中层级被定义为词汇的极性（积极性和消极性）。我们认为，探测的一个关键要求是它对语言结构存在的敏感性。我们将我们的探索应用于BERT，一个典型的情境化嵌入模型。在句法子空间中，我们的探测比欧几里德探测更好地恢复树结构，揭示了BERT语法的几何不一定是欧几里德的可能性。在情感子空间中，我们揭示了积极情感和消极情感的两种可能的元嵌入，并展示了词汇控制的语境化如何改变嵌入的几何本地化。我们通过大量的实验和可视化展示了我们的庞加莱探针的发现。我们的结果可以在https://github.com/FranxYao/PoincareProbe.</pre></li>
<li><a href="https://arxiv.org/abs/2104.07885">Probing Across Time: What Does RoBERTa Know and When?</a>
<pre>在大型语料库上训练的语言模型已被证明对NLP有用。作为固定的人工制品，它们已成为深入研究的对象，许多研究人员“探索”了它们获得和易于证明的语言抽象、事实和常识知识以及推理能力的程度。在这一系列工作的基础上，我们考虑了一个新的问题：语言类型的知识，学习，当（前）培训时，他们获得的知识类型？我们使用RoBERTa作为案例研究，绘制了跨迭代的探测性能图。我们的发现包括：跨领域快速、稳定、稳健地获取语言知识。事实和常识更慢，对领域更敏感。一般来说，推理能力不是稳定获得的。随着新的数据集、训练前协议和探针的出现，我们相信跨时间分析的探针可以帮助研究人员理解这些模型所经历的复杂、混合的学习，并指导我们更快地实现必要学习的更有效方法。</pre></li>
<li><a href="https://arxiv.org/abs/1909.07940">Do NLP Models Know Numbers? Probing Numeracy in Embeddings</a> (EMNLP2019)
<pre>理解和处理数字（算术）的能力对于许多复杂的推理任务至关重要。目前，大多数NLP模型以与其他令牌相同的方式处理文本中的数字——它们将数字作为分布式向量嵌入。这是否足以证明算术性？我们首先研究DROP数据集上最先进的问答模型的数值推理能力。我们发现该模型在需要数值推理的问题上表现出色，也就是说，它已经捕捉到了计算能力。为了了解这种能力是如何产生的，我们探索了合成列表最大值、数字解码和加法任务上的令牌嵌入方法（例如，BERT、手套）。在标准嵌入中自然会出现令人惊讶的计算能力。例如，GloVe和word2vec可以精确地编码最大为1000的数字的大小。此外，字符级的嵌入更加精确——埃尔莫在所有预先训练的方法中都能捕捉到最好的运算能力——但使用子单词单位的伯特则不那么精确。</pre></li>
<li><a href="https://arxiv.org/abs/2005.00683">Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models</a> [<a href="https://github.com/INK-USC/NumerSense">github</a>] [<a href="https://inklab.usc.edu/NumerSense/">website</a>]
<pre>最近的研究表明，预训练语言模型（PTLM），如伯特，具有一定的常识和事实知识。他们认为，通过预测掩蔽词，将PTLM用作“神经知识库”是有希望的。令人惊讶的是，我们发现这可能不适用于数字常识知识（例如，一只鸟通常有两条腿）。在本文中，我们研究是否以及在多大程度上我们可以从PTLMs中归纳出数字常识知识，以及这个过程的稳健性。为了研究这一点，我们引入了一个新的探测任务，该任务包含一个诊断数据集NumerSense，其中包含13.6k掩蔽词预测探测（10.5k用于微调，3.1k用于测试）。我们的分析表明：（1）在任何微调之前，BERT及其强变异体RoBERTa在诊断数据集上表现不佳；（2） 远程监控的微调带来了一些改进；（3） 与人的绩效相比，最佳监督模型的绩效仍然很差（准确率分别为54.06%和96.3%）。</pre></li>
<li><a href="https://www.aclweb.org/anthology/2020.acl-main.698/">Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly</a> (ACL2020)</li>
<li><a href="https://arxiv.org/abs/2105.07452">How is BERT surprised? Layerwise detection of linguistic anomalies</a> (ACL2021)
<pre>Transformer语言模型在检测单词在上下文中是否异常方面表现出了卓越的能力，但似然度分数没有提供异常原因的信息。在这项工作中，我们使用高斯模型在三种语言模型（BERT、RoBERTa和XLNet）的中间层进行密度估计，并在BLiMP（语法判断基准）上评估我们的方法。在较低的层中，Superrisal与较低的标记频率高度相关，但在较高的层中，这种相关性减弱。接下来，我们从心理语言学研究中收集形态句法、语义和常识异常的数据集；我们发现，表现最好的模型RoBERTa在形态句法异常比语义异常的早期层次表现出惊奇，而常识异常在任何中间层都不表现出惊奇。这些结果表明，语言模型采用不同的机制来检测不同类型的语言异常。</pre></li>
<li><a href="https://arxiv.org/abs/2104.01477">Exploring the Role of BERT Token Representations to Explain Sentence Probing Results</a>
<pre>人们已经进行了一些研究来揭示伯特捕捉到的语言特征。这通常是通过在从不同层次的BERT中获得的表示上训练诊断分类器来实现的。随后的分类精度被解释为模型编码相应语言属性的能力。尽管提供了见解，但这些研究忽略了代币表征的潜在作用。在本文中，我们对BERT的表示空间进行了更深入的分析，以寻找能够解释这些探测结果背后原因的不同且有意义的子空间。基于一组探测任务并借助归因方法，我们表明，BERT倾向于在特定的标记表示中编码有意义的知识（在标准分类设置中通常被忽略），从而允许模型检测句法和语义异常，区分语法数字和时态子空间。</pre></li>
<li><a href="https://arxiv.org/abs/1912.13337">What Does My QA Model Know? Devising Controlled Probes using Expert Knowledge</a>
<pre>众所周知，开放领域问答（QA）涉及一些基础知识和推理挑战，但模型在接受基准任务培训时是否真正学习了这些知识？为了研究这一点，我们引入了几个新的挑战性任务，以探索最先进的QA模型是否具有关于单词定义和一般分类推理的一般知识，这两种知识对于更复杂的推理形式来说都是基本的，并且在基准数据集中广泛存在。作为昂贵的众包的替代方案，我们引入了一种从各种类型的专家知识（如知识图和词汇分类法）自动构建数据集的方法，允许系统地控制产生的探测，并进行更全面的评估。我们发现自动构造的探测容易受到注释工件的攻击，我们对此进行了仔细的控制。我们的评估证实，基于transformer的QA模型已经倾向于识别某些类型的结构词汇知识。然而，它也揭示了一个更微妙的画面：它们的性能大幅下降，甚至在基本分类层次结构中的啤酒花数量略有增加，或者随着更具挑战性的干扰物候选答案的引入。此外，即使这些模型在标准实例级评估中获得成功，但在语义连接探测集群级评估时（例如，关于概念的所有Isa问题），它们仍有很大的改进空间。</pre></li>
<li><a href="https://arxiv.org/abs/2006.01346">A Pairwise Probe for Understanding BERT Fine-Tuning on Machine Reading Comprehension</a>
<pre>预先训练的模型为许多NLP任务带来了显著的改进，并得到了广泛的分析。但对于微调对特定任务的影响知之甚少。直觉上，人们可能同意，预先训练的模型已经学习了单词的语义表示（例如，同义词彼此更接近），而微调进一步提高了其需要更复杂推理（例如，共指解析、实体边界检测等）的能力。然而，如何分析和定量地验证这些论点是一项具有挑战性的任务，很少有人关注这一主题。在本文中，受大多数探测任务涉及识别匹配的短语对（例如，共指需要匹配实体和代词）的观察的启发，我们提出了一种成对探测来理解机器阅读理解（MRC）任务中的伯特微调。具体来说，我们确定了MRC中的五种现象。根据成对探测任务，我们比较了预训练和微调的BERT的每一层隐藏表示的性能。所提出的成对探测缓解了不准确模型训练的干扰问题，并进行了稳健的定量比较。我们的实验分析得出了非常有信心的结论：（1）微调对基本信息、低级信息和一般语义任务的影响很小。（2） 对于下游任务所需的特定能力，微调的BERT优于预先训练的BERT，并且在第五层之后，这种差距是明显的。</pre></li>
<li><a href="https://arxiv.org/abs/2005.00782">Can BERT Reason? Logically Equivalent Probes for Evaluating the Inference Capabilities of Language Models</a>
<pre>预先训练的语言模型（PTLM）在常识推理基准上取得了令人印象深刻的性能，但它们利用常识进行稳健推理的能力仍存在争议，这对于与人类的有效沟通至关重要。在追求流畅的人工智能通信的过程中，我们提出了一个新的挑战，RICA：基于常识公理的鲁棒推理能力，该能力在文本干扰的情况下评估鲁棒常识推理。为了为这一挑战生成数据，我们使用常识知识库开发了一个系统化和可扩展的程序，并在两个不同的评估环境中探索PTLM。对我们生成的具有超过10k条语句的探测集进行的大量实验表明，PTLM在零炮设置上的性能并不比随机猜测好，受统计偏差的严重影响，并且对扰动攻击不具有鲁棒性。我们还发现，对类似陈述的微调带来的收益有限，因为PTLM仍然无法推广到看不见的推论。我们新的大规模基准暴露了PTLM和人类语言理解之间的巨大差距，并为PTLM展示常识提供了新的挑战。</pre></li>
<li><a href="https://arxiv.org/abs/2010.13912">Probing Task-Oriented Dialogue Representation from Language Models</a> (EMNLP2020)
<pre>本文研究了预先训练的语言模型，以找出哪种模型在本质上对面向任务的对话任务具有最丰富的信息。我们从有监督的分类器探测和无监督的互信息探测两个方面来研究这个问题。我们在一个固定的预训练语言模型上以有监督的方式微调前馈层作为分类器探针，该模型带有带注释的标签。同时，我们提出了一种无监督的互信息探针来评估真实聚类和表示聚类之间的相互依赖性。本实证研究的目的是1）调查探究技术，特别是从无监督的互信息方面，2）为对话研究社区提供预培训语言模式选择指南，3）发现对话应用的预培训因素，这可能是成功的关键。</pre></li>
<li><a href="https://arxiv.org/abs/2104.09400">Probing for Bridging Inference in Transformer Language Models</a>
<pre>我们探讨预先训练的变压器语言模型桥接推理。我们首先调查了BERT中的个体注意头，发现与低层和中层相比，高层的注意头更关注桥接关系，而且，很少有特定的注意头始终专注于桥接。更重要的是，我们认为语言模型作为一个整体，在我们的第二种方法中，桥接回指消解被制定为掩蔽表征预测任务（完形测试）。我们的公式在没有任何微调的情况下产生了乐观的结果，这表明预先训练的语言模型实质上捕获了桥接推理。我们进一步的研究表明，回指先行词和提供给语言模型的语境之间的距离在推理中起着重要作用。</pre></li>
<li><a href="https://arxiv.org/abs/2010.04098">BERTering RAMS: What and How Much does BERT Already Know About Event Arguments? – A Study on the RAMS Dataset</a> (EMNLP2020 WS)
<pre>使用（Clark et al.，2019）的基于注意力地图的探测框架，我们观察到，在RAMS数据集（Ebner et al.，2020）上，BERT的注意力头在没有任何训练或领域微调的情况下，具有适度但远高于偶然性的发现事件论点的能力，从位置的17.77%低到工件的51.61%高不等。接下来，我们发现这些头部的线性组合（估计约占可用总事件参数检测监督的11%）可以提高某些角色的性能-最高的两个是受害者（准确率68.29%）和工件（准确率58.82%）。此外，我们还研究了我们的方法对跨句事件参数的处理效果。我们提出了一个程序来分离句子间论据检测的“最佳头部”和句子内论据检测的“最佳头部”。与联合估计的对等词相比，这样估计的词首具有更好的跨句表现，尽管只是在不现实的假设下，即我们已经知道该论点存在于另一句话中。最后，我们试图找出我们的数字在多大程度上来源于黄金参数和角色之间基于词汇频率的关联。我们提出了NONCE，这是一种通过用随机生成的“NONCE”单词替换gold参数来创建对抗性测试示例的方案。我们发现，学习到的线性组合对NONCE具有鲁棒性，尽管单个最佳头部可能更敏感。</pre></li>
<li><a href="https://arxiv.org/abs/2011.04134">CxGBERT: BERT meets Construction Grammar</a> (COLING2020) [<a href="https://github.com/H-TayyarMadabushi/CxGBERT-BERT-meets-Construction-Grammar">github</a>]
<pre>尽管词汇语义元素无疑捕获了大量的语言信息，但有人认为它们并不能捕获文本中包含的所有信息。这一假设是建构主义语言观的核心。建构主义语言观认为，语言由结构、形式和功能或意义的习得配对组成，这些结构或形式和功能或意义要么频繁出现，要么其意义无法从其组成部分中预测。BERT的训练目标使其能够获得大量的词汇语义信息，虽然BERTology表明BERT抓住了某些重要的语言维度，但还没有研究探索BERT可能获得结构信息的程度。在这项工作中，我们设计了几个探针并进行了广泛的实验来回答这个问题。我们的研究结果让我们得出结论，伯特确实能够获得大量的信息，语言学家通常称之为结构信息。这一观察的影响可能是深远的，因为它提供了深入学习方法从文本中学习到什么的见解，同时也表明结构中包含的信息在词汇语义中被冗余编码。</pre></li>
<li><a href="https://arxiv.org/abs/2105.04949">BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies?</a> (ACL2021)
<pre>类比在人类常识推理中起着核心作用。识别类比的能力，如“眼睛是看的，耳朵是听的”，有时被称为类比比例，塑造我们如何构造知识和理解语言。然而，令人惊讶的是，在语言模型时代，识别这种类比的任务还没有得到太多的关注。在本文中，我们使用从教育环境中获得的基准以及更常用的数据集，分析了基于transformer的语言模型在这项无监督任务中的能力。我们发现，现成的语言模型可以在一定程度上识别类比，但难以处理抽象和复杂的关系，并且结果对模型结构和超参数高度敏感。总的来说，GPT-2和RoBERTa获得了最好的结果，而使用BERT的配置无法优于单词嵌入模型。我们的结果为未来的工作提出了重要的问题，即预先训练的语言模型如何以及在多大程度上获取抽象语义关系的知识。</pre></li>
</ul>
<h1 id="inside-bert">Inside BERT</h1>
<ul>
<li><a href="https://hal.inria.fr/hal-02131630/document">What does BERT learn about the structure of language?</a> (ACL2019)</li>
<li><a href="https://arxiv.org/abs/1905.09418">Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned</a> (ACL2019) [<a href="https://github.com/lena-voita/the-story-of-heads">github</a>]
<pre>多头自我关注是变压器的一个重要组成部分，是神经机器翻译的最新体系结构。在这项工作中，我们评估了编码器中个体注意头对模型整体性能的贡献，并分析了它们所扮演的角色。我们发现，最重要、最自信的头脑扮演着始终如一的角色，而且往往在语言上可以解释。当使用基于随机门和L0惩罚可微松弛的方法修剪头部时，我们观察到专门化的头部最后被修剪。我们新颖的修剪方法在不严重影响性能的情况下去除了绝大多数头部。例如，在英俄WMT数据集上，删减48个编码器头中的38个只会导致0.15 BLEU的下降。</pre></li>
<li><a href="https://arxiv.org/abs/2106.09650">Multi-head or Single-head? An Empirical Comparison for Transformer Training</a>
<pre>多头注意在变压器模型最近的成功中起着至关重要的作用，这导致在各种应用中，与传统注意相比，性能得到了一致的改进。人们普遍认为，这种有效性源于共同参与多个职位的能力。在本文中，我们首先证明了联合注意多个位置并不是多头注意的一个独特特征，因为多层单头注意也注意多个位置并且更有效。然后，我们认为多头注意的主要优点是训练的稳定性，因为当参加相同数量的位置时，多头注意的层数比单头注意的层数少。例如，24层16头变压器（BERT large）和384层单头变压器具有相同的总注意头数量和大致相同的模型尺寸，而多头变压器则明显较浅。同时，我们表明，随着深度学习的最新进展，我们可以成功地稳定384层变压器的培训。由于培训难度不再是一个瓶颈，因此，在不调整超参数的情况下，单头变压器可实现持续的性能改进。</pre></li>
<li><a href="https://arxiv.org/abs/1906.01698">Open Sesame: Getting Inside BERT’s Linguistic Knowledge</a> (ACL2019 WS)
<pre>BERT如何以及在多大程度上编码语法敏感的层次信息或位置敏感的线性信息？最近的研究表明，像伯特这样的语境表征在需要对语言结构敏感的任务中表现良好。我们在此提出两项研究，旨在更好地理解伯特表示的本质。第一个重点是使用诊断量词识别结构定义的元素，第二个重点是通过对自我注意向量的定量评估，探索BERT对主谓一致性和回指先行词依赖性的表征。在这两种情况下，我们发现BERT在其较低的层上很好地编码了关于单词标记的位置信息，但在较高的层上转换为面向层次的编码。然后，我们得出结论，伯特的表述确实模拟了层次结构的语言相关方面，尽管它们似乎没有表现出人类在处理反身回指时对层次结构的敏锐敏感性。</pre></li>
<li><a href="https://arxiv.org/abs/1906.04284">Analyzing the Structure of Attention in a Transformer Language Model</a> (ACL2019 WS)
<pre>Transformer是一种完全基于注意力的网络替代品，它已经在一系列NLP任务中实现了最先进的结果。在本文中，我们分析了一个转换语言模型GPT-2小预训练模型中的注意结构。我们对单个实例的注意进行可视化，并在大型语料库上分析注意和语法之间的交互作用。我们发现，关注目标的不同部分的语音在不同的层深度模型中，注意对齐与依赖关系最强的中间层。我们还发现，模型的最深层捕捉了最遥远的关系。最后，我们提取了典型的句子，这些句子揭示了特定注意头所针对的高度特定的模式。</pre></li>
<li><a href="https://arxiv.org/abs/1906.04341">What Does BERT Look At? An Analysis of BERT’s Attention</a> (ACL2019 WS)
<pre>大型预训练神经网络（如BERT）最近在NLP方面取得了巨大成功，促使越来越多的研究机构调查他们能够从未标记数据中学习语言的哪些方面。最近的分析主要集中在模型输出（例如，语言模型超越）或内部向量表示（例如，探测分类器）。作为对这些工作的补充，我们提出了分析预训练模型注意机制的方法，并将其应用于BERT。伯特的注意头表现出诸如注意定界符标记、特定位置偏移或广泛注意整个句子等模式，同一层的注意头通常表现出类似的行为。我们进一步表明，某些注意头与句法和共指的语言学概念很好地对应。例如，我们发现大脑能够非常准确地处理动词的直接宾语、名词的限定词、介词的宾语以及相关的提及。最后，我们提出了一个基于注意的探测分类器，并用它进一步证明了大量的句法信息被捕获在伯特的注意中。</pre></li>
<li><a href="https://arxiv.org/abs/1911.12246">Do Attention Heads in BERT Track Syntactic Dependencies?</a>
<pre>我们研究了在预训练的变形金刚语言模型（如BERT和RoBERTa）中，个体注意头隐式捕捉句法依赖关系的程度。我们采用两种方法——取最大注意权重和计算最大生成树——从每个层/头的注意权重中提取隐式依赖关系，并将其与基本真理通用依赖（UD）树进行比较。我们发现，对于某些UD关系类型，存在能够比解析英语文本上的基线更好地恢复依赖类型的头部，这表明一些自我注意头部充当句法结构的代理。我们还对两个数据集——面向语法的COA和面向语义的MNLI——进行了微调分析，以研究微调是否会影响他们的自我注意模式，但我们没有观察到使用我们的方法提取的总体依赖关系存在实质性差异。我们的结果表明，这些模型有一些跟踪个别依赖类型的专家注意头，但没有一个通才注意头能够比普通基线更好地执行整体解析，而且，直接分析注意力权重可能不会揭示出伯特式模型所能学到的很多语法知识。</pre></li>
<li><a href="https://arxiv.org/abs/1906.01539">Blackbox meets blackbox: Representational Similarity and Stability Analysis of Neural Language Models and Brains</a> (ACL2019 WS)
<pre>在本文中，我们定义并应用了表征稳定性分析（ReStA），一种分析神经语言模型的直观方法。ReStA是认知神经科学中流行的表征相似性分析（RSA）的变体。RSA可以用于比较模型、模型组件和人脑中的表示，而ReStA则可以比较同一模型的实例，同时系统地改变单个模型参数。使用ReStA，我们研究了四个最近成功的神经语言模型，并评估了它们的内部表示对先前上下文量的敏感程度。使用RSA，我们对这些模型的第一和第二（或更高）层中的表征空间彼此之间以及与人脑中的激活模式之间的相似性进行了系统研究。我们的研究结果揭示了语言模型之间惊人的巨大差异，并揭示了这些模型中深层语言处理（整合多个句子的信息）的发生位置。ReStA和RSA在模型和大脑上的结合使我们能够开始解决一个重要问题，即我们希望在功能磁共振脑成像数据中观察到什么样的语言过程。特别是，我们的结果表明，Wehbe et al.（2014）的故事阅读数据包含浅层语言处理的信号，但没有证据表明更有趣的深层语言处理。</pre></li>
<li><a href="https://arxiv.org/abs/1906.11511">Inducing Syntactic Trees from BERT Representations</a> (ACL2019 WS)
<pre>我们使用BERT的英语模型，探索句子中一个单词的删除如何改变其他单词的表示。我们的假设是，删除一个可还原词（如形容词）不会影响其他词的表示，而会删除一个主要动词，这会使句子不合语法，并对语言模型造成“高度惊讶”。我们估计单个单词和较长连续短语（单词n-gram）的可约性，研究它们的语法相关属性，然后使用它们来诱导完全依赖树。</pre></li>
<li><a href="https://arxiv.org/abs/1906.05714">A Multiscale Visualization of Attention in the Transformer Model</a> (ACL2019 Demo)
<pre>Transformer是一种序列模型，它放弃了传统的循环架构，转而采用完全基于注意力的方法。除了提高性能外，使用注意力的一个优点是，它还可以通过显示模型如何为不同的输入元素分配权重来帮助解释模型。然而，变压器模型中的多层、多头注意机制可能难以解读。为了使模型更易于访问，我们引入了一个开源工具，它可以在多个尺度上可视化注意，每个尺度都提供了关于注意机制的独特视角。我们在BERT和OpenAI GPT-2上演示了该工具，并给出了三个示例用例：检测模型偏差、定位相关注意头以及将神经元与模型行为联系起来。</pre></li>
<li><a href="https://arxiv.org/abs/1906.02715">Visualizing and Measuring the Geometry of BERT</a>
<pre>Transformer架构为自然语言处理带来了巨大的希望。考虑到一个单一的预训练模型可以被微调以在许多不同的任务上表现良好，这些网络似乎可以提取出普遍有用的语言特征。一个自然的问题是这些网络如何在内部表示这些信息。本文描述了一个特别有效的模型，伯特的定性和定量研究。在高层次上，语言特征似乎表现在单独的语义和句法子空间中。我们发现了词义的精细几何表示的证据。我们还提供了注意矩阵和单个单词嵌入中句法表征的经验描述，以及解释这些表征几何结构的数学论证。</pre></li>
<li><a href="https://arxiv.org/abs/1909.00512">How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings</a> (EMNLP2019)
<pre>用上下文化的单词表示代替静态单词嵌入在许多NLP任务中取得了显著的改进。然而，像ELMo和BERT这样的模型所产生的语境化表示究竟有多上下文？每个单词是否有无限多个特定于上下文的表示，或者单词本质上被指定为有限数量的词义表示之一？首先，我们发现所有单词的语境化表示在语境化模型的任何层中都不是各向同性的。虽然同一个词在不同语境中的表现形式仍然比两个不同词的表现形式具有更大的余弦相似性，但这种自相似性在上层要低得多。这表明上下文化模型的上层产生了更多上下文特定的表示，就像LSTM的上层产生了更多任务特定的表示一样。在ELMo、BERT和GPT-2的所有层中，平均而言，一个单词的上下文化表示中只有不到5%的方差可以通过该单词的静态嵌入来解释，这为上下文化表示的成功提供了一些理由。</pre></li>
<li><a href="https://arxiv.org/abs/1905.10650">Are Sixteen Heads Really Better than One?</a> (NeurIPS2019)
<pre>注意力是一种强大且普遍存在的机制，它允许神经模型在进行预测时通过加权平均值来关注特定的显著信息。特别是，多头注意力是许多最新的NLP模型背后的驱动力，例如基于变压器的MT模型和BERT。这些模型并行应用多个注意机制，每个注意“头”潜在地集中在输入的不同部分，这使得能够表达简单加权平均之外的复杂功能。在本文中，我们做出了一个令人惊讶的观察，即即使模型已经使用多个头部进行了训练，但在实践中，大部分注意力头部都可以在测试时移除，而不会显著影响性能。事实上，有些层甚至可以简化为一个头部。我们进一步研究了用于修剪模型的贪婪算法，以及从中获得的潜在速度、内存效率和准确性改进。最后，我们分析了模型中哪些部分更依赖于有多个头部的结果，并提供了前兆证据，证明训练动力学在多个头部注意所提供的收益中发挥了作用。</pre></li>
<li><a href="https://arxiv.org/abs/1908.04211">On the Validity of Self-Attention as Explanation in Transformer Models</a>
<pre>在本文中，我们深入研究了Transformer架构的两个核心组件：自我关注和上下文嵌入。特别地，我们研究了注意权重和标记嵌入的可识别性，以及上下文到隐藏标记的聚合。我们发现，对于长度超过注意头维度的序列，注意权重是不可识别的。我们建议将有效注意作为一种补充工具，用于改进基于注意的解释性解释。此外，我们还表明，输入标记在很大程度上保留了它们在整个模型中的身份。我们还发现证据表明，身份信息主要编码在嵌入的角度，并随着深度的增加而逐渐减少。最后，我们通过一种新的基于梯度属性的量化方法证明了在生成上下文嵌入时输入信息的强烈混合。总的来说，我们证明了自我注意分布是不可直接解释的，并且提供了更好地理解和进一步研究变压器模型的工具。</pre></li>
<li><a href="https://arxiv.org/abs/1908.05620">Visualizing and Understanding the Effectiveness of BERT</a> (EMNLP2019)
<pre>语言模型预训练，如BERT，在许多NLP任务中取得了显著的效果。然而，尚不清楚为什么预训练-然后微调范式可以提高不同任务的性能和泛化能力。在本文中，我们建议在特定数据集上可视化损失景观和微调BERT的优化轨迹。首先，我们发现，与从头开始的培训相比，预培训在下游任务中达到了一个良好的初始点，这导致了更广泛的优化和更容易的优化。我们还证明了微调过程对过度拟合的鲁棒性，即使对于下游任务，BERT是高度过度参数化的。第二，可视化结果表明，由于训练损失面和泛化误差面之间的一致性以及平坦、宽的最优解，微调BERT更易于泛化。第三，在微调过程中，BERT的较低层更具不变性，这表明靠近输入的层可以学习更多可转换的语言表示。</pre></li>
<li><a href="https://arxiv.org/abs/1909.11218">Attention Interpretability Across NLP Tasks</a>
<pre>神经网络模型中的注意层提供了对模型预测背后的推理的洞察，这些推理通常被批评为不透明。最近，关于注意力权重的可解释性出现了看似矛盾的观点（Jain&Wallace，2019；Vig&Belinkov，2019）。在这种混乱中，需要更系统地理解注意机制。在这项工作中，我们试图通过给出一个全面的解释来填补这一空白，该解释证明了这两种观察结果的合理性（即，注意何时可解释，何时不可解释）。通过对不同NLP任务的一系列实验，我们验证了我们的观察结果，并通过手动评估强化了我们关于注意力可解释性的主张。</pre></li>
<li><a href="https://arxiv.org/abs/1908.08593">Revealing the Dark Secrets of BERT</a> (EMNLP2019)
<pre>基于BERT的体系结构目前在许多NLP任务上提供了最先进的性能，但对其成功的确切机制知之甚少。在目前的工作中，我们着重于对自我注意的解释，这是BERT的基本组成部分之一。使用胶水任务的子集和一组手工制作的感兴趣的特征，我们提出了该方法，并对单个BERT头部编码的信息进行了定性和定量分析。我们的研究结果表明，有一组有限的注意力模式在不同的大脑中重复，这表明整体模型过度参数化。虽然不同的大脑始终使用相同的注意力模式，但它们对不同任务的表现有不同的影响。我们表明，在某些大脑中手动禁用注意比常规微调的BERT模型有更好的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2004.04010">Analyzing Redundancy in Pretrained Transformer Models</a> (EMNLP2020)
<pre>基于Transformer的深层NLP模型使用数亿个参数进行训练，限制了它们在计算受限环境中的适用性。在本文中，我们通过定义冗余的概念来研究这些限制的原因，我们将冗余分为两类：一般冗余和任务特定冗余。我们剖析了两种流行的预训练模型，BERT和XLNet，研究它们在表示层和更细粒度的神经元层上表现出多少冗余。我们的分析揭示了有趣的见解，例如：i）网络中85%的神经元是冗余的，ii）当优化下游任务时，至少92%的神经元可以被移除。基于我们的分析，我们提出了一个有效的基于特征的迁移学习过程，在使用最多10%的原始神经元的同时，保持了97%的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2004.06499">What’s so special about BERT’s layers? A closer look at the NLP pipeline in monolingual and multilingual models</a>
<pre>深入了解BERT的内部工作过程表明，它的层类似于经典的NLP管道，越来越复杂的任务集中在后面的层中。为了研究这些结果在多大程度上也适用于英语以外的语言，我们探索了一个基于荷兰BERT的模型和荷兰NLP任务的多语言BERT模型。此外，通过对词性标注的深入分析，我们发现，在给定的任务中，信息分布在网络的不同部分，管道可能不像看上去那么整洁。每一层都有不同的专业知识，因此将不同层的信息结合起来可能更有用，而不是根据最佳的总体性能选择单个层。</pre></li>
<li><a href="https://arxiv.org/abs/2004.10102">Attention Module is Not Only a Weight: Analyzing Transformers with Vector Norms</a> (ACL2020 SRW)
<pre>注意力是Transformers的关键组成部分，Transformers最近在自然语言处理方面取得了相当大的成功。因此，人们对注意力进行了广泛的研究，以调查变形金刚的各种语言能力，重点是分析注意力权重与特定语言现象之间的相似性。本文表明，注意权重只是决定注意输出的两个因素之一，并提出了一种基于范数的分析方法，该方法结合了第二个因素，即转换输入向量的范数。我们对伯特和基于变压器的神经机器翻译系统的基于规范的分析的结果包括：（i）与先前的研究相反，伯特对特殊令牌的关注不够，并且（ii）可以从变压器的注意机制中提取合理的词对齐。这些发现提供了变压器内部工作的见解。</pre></li>
<li><a href="https://arxiv.org/abs/2109.07152">Incorporating Residual and Normalization Layers into Analysis of Masked Language Models</a> (EMNLP2021)
<pre>Transformer体系结构在自然语言处理领域已经变得无处不在。为了解释基于变压器的模型，对它们的注意模式进行了广泛的分析。然而，变压器的结构不仅仅是由多个头部组成的；其他部件也有助于变压器的进步性能。在本研究中，我们将变压器的分析范围从单纯的注意模式扩展到整个注意块，即多头注意、剩余连接和层规范化。我们对基于转换器的掩蔽语言模型的分析表明，通过注意执行的令牌到令牌的交互对中间表示的影响比之前假设的要小。这些结果为现有报告提供了新的直观解释；例如，放弃学习到的注意模式往往不会对表现产生负面影响。我们的实验代码是公开的。</pre></li>
<li><a href="https://arxiv.org/abs/2005.00928">Quantifying Attention Flow in Transformers</a>
<pre>在Transformer模型中，“自我注意”将有注意嵌入的信息结合到下一层的焦点嵌入表示中。因此，在转换器的各个层中，来自不同令牌的信息变得越来越混合。这使得注意力权重在解释时不可靠。在本文中，我们考虑的问题，通过自我关注量化这一信息流。当我们使用注意权重作为输入标记的相对相关性时，我们提出了两种方法来近似给定注意权重的输入标记的注意，即注意卷展和注意流。我们发现，这些方法在信息流方面提供了互补的观点，并且与原始注意相比，两者都与使用消融方法和输入梯度获得的输入标记的重要性得分具有更高的相关性。</pre></li>
<li><a href="https://arxiv.org/abs/2004.05916">Telling BERT’s full story: from Local Attention to Global Aggregation</a> (EACL2021)
<pre>我们深入研究了transformer体系结构中自我注意头的行为。鉴于最近的工作不鼓励使用注意分布来解释模型的行为，我们表明注意分布仍然可以提供对注意头的局部行为的洞察。通过这种方式，我们提出了注意所揭示的局部模式和参照输入的全局模式之间的区别，并从这两个角度分析了伯特。我们使用梯度属性来分析注意头的输出如何依赖于输入标记，有效地扩展了基于局部注意的分析，以解释整个转换层中的信息混合。我们发现，注意和归因分布之间存在着显著的差异，这是由模型内部的语境混合造成的。我们量化了这一差异，并观察到有趣的是，尽管存在混合，但在所有层中仍存在一些模式。</pre></li>
<li><a href="https://arxiv.org/abs/2011.00943">How Far Does BERT Look At:Distance-based Clustering and Analysis of BERT′s Attention</a>
<pre>最近关于多头注意机制的研究，特别是在诸如BERT等预训练模型中的研究，已经向我们展示了分析该机制各个方面的启发和线索。由于大多数研究集中于探索任务或隐藏状态，以前的工作已经通过启发式分析方法发现了注意头行为的一些原始模式，但是针对注意模式的更系统的分析仍然是原始的。在这项工作中，我们通过在一组建议的特征之上进行无监督聚类，将注意力热图清晰地聚类成显著不同的模式，这与之前的观察结果相一致。通过分析研究，进一步研究了它们的相应功能。此外，我们提出的特征可以用来解释和校准变压器模型中的不同注意头。</pre></li>
<li><a href="https://arxiv.org/abs/2108.08375">Contributions of Transformer Attention Heads in Multi- and Cross-lingual Tasks</a> (ACL2021)
<pre>本文研究了注意头在基于变压器的模型中的相对重要性，以帮助它们在跨语言和多语言任务中的解释能力。先前的研究发现，在每项单语言自然语言处理（NLP）任务中，只有少数注意头是重要的，修剪其余的注意头可以使模型的性能相当或得到改进。然而，在跨语言和多语言任务中，修剪注意头的影响尚不清楚。通过大量的实验，我们发现：（1）在基于多语言转换的模型中剪除大量的注意头通常对其在跨语言和多语言任务中的表现有积极的影响；（2）可以使用梯度对要剪除的注意头进行排序，并通过一些试验进行识别。我们的实验集中在序列标记任务上，可能适用于其他跨语言和多语言任务。为了全面性，我们研究了两个预先训练的多语言模型，即多语言BERT（mBERT）和XLM-R，它们分别涉及9种语言的三个任务。我们还讨论了我们的发现的有效性及其对真正资源稀缺的语言和其他任务设置的可扩展性。</pre></li>
<li><a href="https://arxiv.org/abs/2010.04903">What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding</a> (EMNLP2020)
<pre>近年来，预培训的变压器主导了NLP基准测试任务的大部分。许多经过预培训的变形金刚不断涌现，大多数都专注于设计不同的预培训目标或自我关注的变体。在自我注意机制中嵌入位置信息也是变形金刚中不可或缺的因素，但这一点经常被随意讨论。因此，本文对主流预培训变压器的位置嵌入进行了实证研究，主要关注两个问题：1）位置嵌入真的了解位置的含义吗？2） 这些不同的学习位置嵌入如何影响NLP任务的转换器？本文通过对大多数标志性NLP任务的特征级分析和实证实验，为预先训练好的位置嵌入提供了新的视角。我们相信，我们的实验结果可以指导未来的工作，在给定应用程序属性的情况下，为特定任务选择合适的位置编码函数。</pre></li>
<li><a href="https://arxiv.org/abs/1909.02597">Investigating BERT’s Knowledge of Language: Five Analysis Methods with NPIs</a> (EMNLP2019)
<pre>尽管最先进的句子表示模型可以执行需要大量语法知识的任务，但如何最好地评估他们的语法知识仍是一个悬而未决的问题。我们探索了五种实验方法，这五种方法的灵感来源于先前评估预训练句子表征模型的工作。我们使用一个单一的语言现象，英语中的负极性项目（NPI）许可，作为我们实验的案例研究。NPI与“any”一样，只有当它们出现在否定等许可环境中时才符合语法（“Sue没有任何猫”与“Sue有任何猫”）。由于NPI许可环境的多样性，这一现象具有挑战性。我们引入了一个人工生成的数据集，用于处理NPI授权实验的关键特性。我们发现，伯特对这些特征有着重要的认识，但其成功与否在不同的实验方法中差异很大。我们得出结论，在给定的领域中，有多种方法可以揭示模型语法知识的所有相关方面。</pre></li>
<li><a href="https://arxiv.org/abs/2006.10413">Are Pretrained Language Models Symbolic Reasoners Over Knowledge?</a> (CoNLL2020)
<pre>预训练语言模型（PLM）如何从训练集中学习事实知识？我们研究了两个最重要的机制：推理和记忆。先前的工作试图量化PLM学习到的事实数量，但我们使用合成数据提出了第一项研究，调查培训中出现的事实与PLM学习到的事实之间的因果关系。对于推理，我们表明，PLM似乎学会了正确应用一些符号推理规则，但与其他规则（包括两跳推理）存在冲突。进一步的分析表明，即使是学习推理规则的应用也存在缺陷。对于记忆，我们确定图式一致性（事实系统地由其他事实支持）和频率是其成功的关键因素。</pre></li>
<li><a href="https://arxiv.org/abs/2011.03803">Rethinking the Value of Transformer Components</a> (COLING2020)
<pre>Transformer成为了最先进的转换模型，但还没有很好地研究每个中间组件对模型性能的贡献，这对设计最佳体系结构提出了重大挑战。在这项工作中，我们通过从不同角度评估培训变压器模型中单个组件（子层）的影响来弥合这一差距。跨语言对、训练策略和模型能力的实验结果表明，某些成分始终比其他成分更重要。我们还报告了一些有趣的发现，这些发现可能有助于人类更好地分析、理解和改进变压器模型。基于这些观察，我们进一步提出了一种新的训练策略，通过区分训练中不重要的部分来提高翻译性能。</pre></li>
<li><a href="https://arxiv.org/abs/2012.14913">Transformer Feed-Forward Layers Are Key-Value Memories</a>
<pre>前馈层占变压器模型参数的三分之二，但它们在网络中的作用仍有待探索。我们展示了基于转换器的语言模型中的前馈层作为键值记忆运行，其中每个键值与训练示例中的文本模式相关，并且每个值在输出词汇表上诱导分布。我们的实验表明，学习到的模式是人类可以解释的，较低的层倾向于捕捉浅层模式，而较高的层学习更多的语义模式。这些值通过诱导输出分布来补充键的输入模式，输出分布将概率质量集中在每个模式之后可能立即出现的标记上，尤其是在上层。最后，我们证明了前馈层的输出是其存储器的组成，随后通过剩余连接在整个模型层中进行细化，以产生最终的输出分布。</pre></li>
<li><a href="https://arxiv.org/abs/2004.14975">Investigating Transferability in Pretrained Language Models</a>
<pre>语言模式训练前如何帮助迁移学习？我们考虑一个简单的消融技术来确定每个预训练层对传输任务性能的影响。这种方法，即部分重新初始化，包括用随机权重替换预训练模型的不同层，然后在转移任务上微调整个模型，并观察性能的变化。这项技术表明，在BERT中，对下游胶水任务具有高探测性能的层对于这些任务的高精度既不必要也不充分。此外，对层使用预训练参数的好处随着数据集大小的微调而显著不同：在数据丰富时提供巨大性能改进的参数在数据稀缺的环境中可能提供微不足道的好处。这些结果揭示了迁移学习过程的复杂性，突出了对冻结模型或单个数据样本进行操作的方法的局限性。</pre></li>
<li><a href="https://arxiv.org/abs/2004.14448">What Happens To BERT Embeddings During Fine-tuning?</a>
<pre>虽然最近有很多工作研究语言信息如何编码在预先训练的句子表示中，但对于这些模型在适应于解决下游任务时如何变化的了解相对较少。使用一套分析技术（探测分类器、表征相似性分析和模型烧蚀），我们研究微调如何影响伯特模型的表征。我们发现，虽然微调必然会产生重大变化，但它不会导致语言现象的灾难性遗忘。相反，我们发现微调主要影响BERT的顶层，但在不同的任务中有显著的变化。特别是，依赖解析重新配置了大部分模型，而SQuAD和MNLI似乎涉及更浅的处理。最后，我们还发现，微调对域外句子的表示有较弱的影响，这表明模型泛化还有改进的余地。</pre></li>
<li><a href="https://arxiv.org/abs/2010.02695">Analyzing Individual Neurons in Pre-trained Language Models</a> (EMNLP2020)
<pre>虽然已经进行了大量分析，以证明在深层NLP模型中学习到的表征所捕获的语言知识，但很少关注单个神经元。我们使用预测形态学、语法和语义的核心语言任务，在预先训练的语言模型上进行神经元级分析，有这样的问题：i）预训练模型中的单个神经元是否捕捉到语言信息？ii）网络的哪些部分可以更多地了解某些语言现象？iii）信息的分布或集中程度如何？和iv）不同的体系结构在学习这些属性方面有什么不同？我们发现了预测语言任务的小神经元亚群，与预测语法的高水平任务相比，低水平任务（如形态学）局限于较少的神经元。我们的研究还揭示了有趣的跨架构比较。例如，我们发现XLNet中的神经元在预测属性时比BERT和其他神经元更局部化和不相交，因为它们更分散和耦合。</pre></li>
<li><a href="https://arxiv.org/abs/2004.14129">How fine can fine-tuning be? Learning efficient language models</a> (AISTATS2020)
<pre>随着网络规模的不断扩大，语言理解任务的最新表现已经实现；目前的记录保持者拥有数十亿个参数。给定在大规模未标记文本语料库上预先训练的语言模型，学习任务只需要非常轻的监督微调：微调步骤的数量通常比总参数计数低五个数量级。这是否意味着微调只会在参数空间中引入与预训练模型的微小差异？如果是这样的话，可以避免为每个任务存储和计算整个模型吗？在这项工作中，我们以变压器的双向编码器表示（BERT）为例来解决这些问题。正如预期的那样，我们发现微调后的模型在参数空间上与预先训练的模型非常接近，其接近程度因层而异。我们表明，仅微调最关键的层就足够了。此外，我们发现，在预训练模型的稀疏版本集中，有许多令人惊讶的好解。因此，只需将预先训练参数的特定层中的一定数量的条目设置为零，就可以实现大型语言模型的微调，从而节省特定任务的参数存储和计算成本。</pre></li>
<li><a href="https://arxiv.org/abs/1909.01380">The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives</a> (EMNLP2019)
<pre>我们试图了解在不同的学习目标下，深层神经网络中各个标记的表示和学习特征空间的结构是如何在层之间演化的。我们将重点放在变压器上进行分析，因为它们在各种任务中都被证明是有效的，包括机器翻译（MT）、标准从左到右语言模型（LM）和屏蔽语言建模（MLM）。以前的工作使用黑盒探测任务来显示变压器学习到的表示因目标的不同而显著不同。在这项工作中，我们使用典型相关分析和互信息估计器来研究信息如何在变压器层之间流动，以及这个过程如何依赖于学习目标的选择。例如，当您从底层到顶层时，从左到右的语言模型中关于过去的信息会消失，关于未来的预测也会形成。相比之下，对于传销，表示最初获取关于令牌周围的上下文的信息，部分忘记令牌标识并产生更广义的令牌表示。然后在顶级传销层重新创建令牌标识。</pre></li>
<li><a href="https://arxiv.org/abs/2002.12327">A Primer in BERTology: What we know about how BERT works</a> (TACL2020)
<pre>基于Transformer的模型推动了NLP许多领域的最新发展，但我们对其成功背后的原因的理解仍然有限。本文是对流行的伯特模型的150多项研究的第一次调查。我们回顾了有关BERT如何工作、它学习什么样的信息以及如何表示这些信息、对其训练目标和体系结构的常见修改、过参数化问题和压缩方法的知识现状。然后，我们概述了未来研究的方向。</pre></li>
<li><a href="https://arxiv.org/abs/2010.02480">Pretrained Language Model Embryology: The Birth of ALBERT</a> (EMNLP2020) [<a href="https://github.com/d223302/albert-embryology">github</a>]
<pre>虽然人们已经对预训练语言模型（LMs）的行为进行了彻底的研究，但很少对预训练期间发生的事情进行研究。因此，我们研究了从一组随机初始化的参数到全能语言模型的发展过程，我们称之为预训练语言模型的胚胎学。我们的研究结果表明，ALBERT在训练前以不同的学习速度学习重建和预测不同词性（POS）的标记。我们还发现，语言知识和世界知识通常不会随着训练前的进行而提高，下游任务的绩效也不会提高。这些发现表明，在预训练过程中，对预训练模型的了解会有所不同，并且有更多的预训练步骤并不一定能为模型提供更全面的知识。我们将提供源代码和预训练模型，以在https://github.com/d223302/albert-embryology.</pre></li>
<li><a href="https://arxiv.org/abs/2104.05824">Evaluating Saliency Methods for Neural Language Models</a> (NAACL2021)
<pre>显著性方法被广泛用于解释神经网络预测，但显著性方法的不同变体甚至在同一模型对同一预测的解释上常常存在分歧。在这些情况下，我们如何确定这些解释何时值得在分析中使用？为了解决这个问题，我们对NLP模型的一个基本类别：神经语言模型的显著性方法进行了全面和定量的评估。我们从两个角度评估预测解释的质量，每个角度都代表了这些解释的一个可取属性：合理性和忠实性。我们的评估是在四个不同的数据集上进行的，这四个数据集是根据现有的人类对句法和语义协议的注释构建的，包括句子层面和文档层面。通过我们的评估，我们确定了显著性方法产生低质量解释的各种方式。我们建议，未来将这些方法应用于神经语言模型的工作应该在得出见解之前仔细验证它们的解释。</pre></li>
<li><a href="https://arxiv.org/abs/2009.05021">Investigating Gender Bias in BERT</a>
<pre>上下文语言模型（CLM）将NLP基准推向了一个新的高度。在文本分类等下游任务中利用CLM提供的单词嵌入已经成为一种新的规范。然而，除非得到解决，否则CLM容易在数据集中学习到内在的性别偏见。因此，下游NLP模型的预测可能因性别词的不同而显著不同，例如将“他”替换为“她”，甚至性别中性词。在本文中，我们将重点分析一种流行的CLM，即BERT。我们分析了在与情绪和情绪强度预测相关的五个下游任务中，它导致的性别偏见。对于每项任务，我们利用BERT的单词嵌入训练一个简单的回归器。然后，我们使用公平评估语料库评估回归者的性别偏见。理想情况下，从具体的设计来看，模型应该摒弃输入中的性别信息特征。然而，研究结果表明，该系统的预测对性别特定的单词和短语有很大的依赖性。我们声称，这种偏见可以通过从单词嵌入中去除性别特征来减少。因此，对于BERT中的每一层，我们确定主要编码性别信息的方向。在词语嵌入的语义空间中，由这些方向形成的空间称为性别子空间。我们提出了一种算法，可以找到细粒度的性别方向，即每个层有一个主方向。这避免了在多个维度中实现性别子空间的需要，并防止遗漏其他重要信息。实验表明，去除这些方向上的嵌入成分在减少下游任务中的BERT诱导偏差方面取得了巨大成功。</pre></li>
<li><a href="https://arxiv.org/abs/2010.06032">Measuring and Reducing Gendered Correlations in Pre-trained Models</a> [<a href="https://ai.googleblog.com/2020/10/measuring-gendered-correlations-in-pre.html">website</a>]
<pre>预先训练的模型已经彻底改变了自然语言理解。然而，研究人员发现，他们可以对许多应用程序中不需要的工件进行编码，例如与一种性别相关的职业多于与另一种性别相关的职业。我们探讨了这种性别相关性，作为如何在预先训练的模型中解决意外相关性的案例研究。我们定义了度量，并揭示了具有相似精度的模型可以以非常不同的速率编码相关性。我们展示了如何使用通用技术降低测量相关性，并强调了不同策略的权衡。根据这些结果，我们提出了训练稳健模型的建议：（1）仔细评估意外相关性，（2）注意看似无害的配置差异，（3）关注一般缓解措施。</pre></li>
<li><a href="https://arxiv.org/abs/2010.14534">Unmasking Contextual Stereotypes: Measuring and Mitigating BERT’s Gender Bias</a> (COLING2020 WS)
<pre>在自然语言处理系统中，语境化的词语嵌入已经取代了标准的嵌入，成为人们选择的代表性知识源。由于以前在标准单词嵌入中发现了各种各样的偏见，因此评估替换词中编码的偏见也至关重要。以BERT（Devlin等人，2018年）为重点，我们通过研究英语和德语中表示性别的目标词与职业名称之间的关联来衡量性别偏见，并将研究结果与现实世界的劳动力统计数据进行比较。在应用反事实数据替代（CDS）后，我们通过微调GAP语料库上的BERT（Webster et al.，2018）来缓解偏差（Maudslay et al.，2019）。我们表明，我们的测量偏差的方法适用于英语等语言，但不适用于具有丰富词法和性别标记的语言，如德语。我们的研究结果强调了跨语言研究偏见和缓解技术的重要性，特别是考虑到目前对大规模、多语言语言模型的重视。</pre></li>
<li><a href="https://arxiv.org/abs/2101.09688">Stereotype and Skew: Quantifying Gender Bias in Pre-trained and Fine-tuned Language Models</a> (EACL2021)
<pre>本文提出了两个直观的量度，歪斜和刻板印象，量化和分析了在处理WinoBias代词解析任务时上下文语言模型中存在的性别偏见。我们发现，在开箱即用的模型中，性别刻板印象与性别倾斜大致呈负相关，这表明这两种形式的偏见之间存在权衡。我们研究了两种方法来缓解偏差。第一种方法是一种在线方法，它可以有效地以牺牲刻板印象为代价消除歪斜。第二个是受以前关于ELMo的工作的启发，涉及到使用增强的性别平衡数据集对BERT进行微调。我们表明，相对于未经整理的微调对应项，这减少了歪斜和刻板印象。然而，我们发现现有的性别偏见基准并没有完全探究职业偏见，因为代词消解可能会被性别偏见其他表现形式的相互关联所混淆。我们的代码可在线获取，网址为https://github.com/12kleingordon34/NLP_masters_project.</pre></li>
<li><a href="https://arxiv.org/abs/2010.00133">CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models</a> (EMNLP2020)
<pre>预训练语言模型，特别是蒙面语言模型（MLM）在许多NLP任务中都取得了成功。然而，有充分的证据表明，他们使用的文化偏见毫无疑问存在于他们接受培训的语料库中，含蓄地通过有偏见的表述造成伤害。为了测量美国受保护的人口群体在语言模型中的某些形式的社会偏见，我们引入了众包刻板印象对基准（CrowS Pairs）。CrowS Pairs有1508个例子，涵盖了涉及九种偏见的刻板印象，如种族、宗教和年龄。在CrowS Pairs中，一个模型由两句话组成：一句是更多的刻板印象，另一句是更少的刻板印象。数据侧重于对历史弱势群体的刻板印象，并将其与优势群体进行对比。我们发现，我们评估的所有三种广泛使用的传销方式都基本上倾向于在乌鸦对中的每个类别中表达刻板印象的句子。随着构建较少偏差模型的工作进展，该数据集可以用作评估进展的基准。</pre></li>
<li><a href="https://arxiv.org/abs/2104.07496">Unmasking the Mask – Evaluating Social Biases in Masked Language Models</a>
<pre>蒙面语言模型（MLM）作为文本编码器在许多下游NLP任务中表现出了优异的性能。不幸的是，传销还显示出令人担忧的社会偏见水平。我们表明，由于以下原因，先前提出的用于量化MLM中社会偏见的评估指标存在问题：（1）在某些MLM中，蒙面代币本身的预测精度往往较低，这就对使用（伪）模型的评估指标的可靠性提出了问题预测标记的可能性，以及（2）未考虑掩码的预测精度与下游NLP任务中的性能之间的相关性，（3）训练数据中的高频词被更频繁地屏蔽，由于测试用例中的这种选择偏差而引入噪声。为了克服上述不流畅性，我们提出了全无掩码似然（All UNMAKED LIQUALITY，AUL），这是一种偏差评估度量，在给定无掩码输入的MLM嵌入的情况下，预测测试用例中的所有标记。我们发现，AUL可以准确地检测MLM中不同类型的偏差。我们还提出了具有注意权重的AUL（AULA）来根据标记在句子中的重要性来评估标记。然而，与AUL和AULA不同，先前提出的MLM偏差评估措施系统地高估了测量的偏差，并且受到上下文中未屏蔽标记的严重影响。</pre></li>
<li><a href="https://arxiv.org/abs/2010.02686">BERT Knows Punta Cana is not just beautiful, it’s gorgeous: Ranking Scalar Adjectives with Contextualised Representations</a> (EMNLP2020)
<pre>形容词pretty、beautiful和flanger描述它们修饰的名词的积极性质，但强度不同。这些差异对于自然语言理解和推理非常重要。我们提出了一种新的基于BERT的标量形容词强度检测方法。我们通过直接从上下文化表示中导出的向量对强度进行建模，并表明它们可以成功地对标量形容词进行排序。我们对我们的模型进行了内在的、基于金标准数据集的和基于间接问答任务的评估。我们的结果表明，BERT编码了关于标量形容词语义的丰富知识，并且能够提供比静态嵌入和以前的模型更好的质量强度排名，并且可以访问专用资源。</pre></li>
<li><a href="https://arxiv.org/abs/2010.07711">Does Chinese BERT Encode Word Structure?</a> (COLING2020) [<a href="https://github.com/ylwangy/BERT_zh_Analysis">github</a>]
<pre>语境化表示法在广泛的NLP任务中提供了显著改进的结果。很多工作都致力于分析代表性模型（如BERT）捕获的特征。现有的研究发现，句法、语义和词义知识都编码在BERT中。然而，很少有人研究基于字符的语言（如汉语）的词汇特征。我们使用注意权重分布统计和探测任务两种方法研究了汉语BERT，发现（1）BERT捕获了单词信息；（2）词级特征主要集中在中间表示层；（3） 下游任务在BERT中对单词特征的使用不同，词性标注和组块最依赖单词特征，自然语言推理最不依赖这些特征。</pre></li>
<li><a href="https://arxiv.org/abs/1909.04925">How Does BERT Answer Questions? A Layer-Wise Analysis of Transformer Representations</a> (CIKM2019)
<pre>来自Transformers（BERT）的双向编码器表示在各种自然语言处理任务中达到最先进的结果。然而，对其内部功能的了解仍然不够，令人不满意。为了更好地理解BERT和其他基于变压器的模型，我们对BERT的隐藏状态进行了分层分析。与以前的研究不同，我们认为隐藏状态包含同样有价值的信息，而以前的研究主要是通过注意权重来解释变压器模型。具体而言，我们的分析侧重于对问答任务（QA）进行微调的模型，作为复杂下游任务的一个示例。我们检查QA模型如何转换令牌向量以找到正确答案。为此，我们应用了一组通用的和特定于QA的探测任务，这些任务揭示了存储在每个表示层中的信息。我们对隐藏状态可视化的定性分析为伯特的推理过程提供了更多的见解。我们的结果表明，BERT中的转换经历了与传统管道任务相关的阶段。因此，系统可以隐式地将特定于任务的信息合并到其令牌表示中。此外，我们的分析表明，微调对模型的语义能力几乎没有影响，甚至可以在早期层的向量表示中识别预测错误。</pre></li>
<li><a href="https://arxiv.org/abs/1910.06431">Whatcha lookin’ at? DeepLIFTing BERT’s Attention in Question Answering</a>
<pre>最近，通过对大量任务数据进行预训练和微调的神经网络在处理具有挑战性的NLP任务方面取得了巨大成功。在本文中，我们研究了一个这样的模型，用于问答的伯特模型，目的是分析为什么它能够获得比其他模型更好的结果。我们在模型预测上运行DeepLIFT并测试结果，以监控输入的注意力值的变化。我们还根据输入段落的类型和模型试图回答的问题，对结果进行聚类分析，以分析与人类推理类似的任何可能模式。</pre></li>
<li><a href="https://arxiv.org/abs/1910.12391">What does BERT Learn from Multiple-Choice Reading Comprehension Datasets?</a>
<pre>多项选择阅读理解（MCRC）要求模型阅读文章和问题，并在给定选项中选择正确答案。最近最先进的模型在多个MCRC数据集上取得了令人印象深刻的性能。然而，这种性能可能并不反映模型的真正语言理解和推理能力。在这项工作中，我们采用两种方法来研究BERT从MCRC数据集中学到的东西：1）一种不可读的数据攻击，在这种攻击中，我们添加关键字来混淆BERT，导致显著的性能下降；2）一种不负责任的数据训练，在这种训练中，我们对部分或混合输入进行训练。在不负责任的数据训练下，BERT取得了出人意料的高性能。基于我们对5个关键MCRC数据集（RACE、MCTest、MCScript、MCScript2.0、DREAM）的实验，我们观察到1）微调的BERT主要学习关键词如何导致正确预测，而不是学习语义理解和推理；（2）伯特不需要正确的句法信息来完成任务；3） 这些数据集中存在工件，因此即使没有完整的上下文，也可以解决这些工件。</pre></li>
<li><a href="https://arxiv.org/abs/2004.03490">What do Models Learn from Question Answering Datasets?</a>
<pre>虽然模型在流行的问答（QA）数据集（如SQuAD）上已经达到了超人的性能，但它们在问答本身的任务上还没有超越人类。在本文中，我们通过评估五个数据集中基于BERT的模型，研究模型是否在从QA数据集中学习阅读理解。我们评估模型对域外示例的可推广性、对缺失或错误数据的响应以及处理问题变化的能力。我们发现，没有一个数据集对我们的所有实验都具有鲁棒性，并指出了数据集和评估方法中的缺陷。根据我们的分析，我们提出了构建未来QA数据集的建议，这些数据集可以更好地评估通过阅读理解回答问题的任务。我们还发布了将QA数据集转换为共享格式的代码，以便于在https://github.com/amazon-research/qa-dataset-converter.</pre></li>
<li><a href="https://arxiv.org/abs/2010.08983">Towards Interpreting BERT for Reading Comprehension Based QA</a> (EMNLP2020)
<pre>BERT及其变体在各种NLP任务中取得了最先进的性能。从那时起，各种各样的工作被提出来分析BERT中捕获的语言信息。然而，目前的工作并没有提供一个关于伯特如何能够在基于阅读理解的问答任务上达到接近人类水平的表现的洞察。在这项工作中，我们试图为RCQA解释BERT。由于BERT层没有预定义的角色，我们使用集成的渐变来定义层的角色或功能。根据定义的角色，我们对所有层进行初步分析。我们观察到，最初的层关注于查询通道交互，而后面的层更关注于上下文理解和增强答案预测。特别是对于量词问题（多少/多少），我们注意到BERT在后面的几层中重点关注容易混淆的单词（即文章中的其他数字量），但仍然能够正确预测答案。微调和分析脚本将在https://github.com/iitmnlp/BERT-Analysis-RCQA .</pre></li>
<li><a href="https://arxiv.org/abs/2009.08257">Compositional and Lexical Semantics in RoBERTa, BERT and DistilBERT: A Case Study on CoQA</a> (EMNLP2020)
<pre>许多自然语言处理任务都受益于从语境化单词嵌入中转移知识，但是关于转移的知识类型的描述是不完整的。本文研究了在会话问答（CoQA）任务中，由语言模型描述的语言现象的类型。我们通过系统错误分析——基本算法（计算短语）、合成语义（否定和语义角色标记）和词汇语义（惊奇和反义词），确定了微调后的RoBERTa、BERT和DistilBERT模型存在的问题。当通过多任务学习使用相关的语言知识进行增强时，模型的性能会提高。增强型车型的组合在F1总成绩上提高了2.2到2.7分，在最难的问题类上提高了42.1分。结果表明，罗伯塔、伯特和迪斯蒂尔伯特在表达作文和词汇信息的能力上存在差异。</pre></li>
<li><a href="https://www.aclweb.org/anthology/2020.acl-main.429/">How does BERT’s attention change when you fine-tune? An analysis methodology and a case study in negation scope</a> (ACL2020)</li>
<li><a href="https://arxiv.org/abs/2003.07892">Calibration of Pre-trained Transformers</a>
<pre>经过预训练的转换器现在在自然语言处理中无处不在，但尽管它们具有高端任务性能，但从经验上看，对于它们是否经过校准知之甚少。具体地说，这些模型的后验概率是否提供了一个精确的经验度量，来衡量模型在给定示例中的正确性？在这项工作中，我们关注伯特和罗伯塔，并分析他们在三个任务中的校准：自然语言推理、释义检测和常识推理。对于每个任务，我们考虑域和挑战性的域外设置，其中模型面临更多的例子，他们应该是不确定的。我们发现：（1）当使用开箱即用时，预训练模型在域内进行校准，与基线相比，其域外校准误差可低3.5倍；（2） 温度标度在进一步降低域内校准误差方面是有效的，而使用标签平滑故意增加经验不确定度有助于在域外校准后验误差。</pre></li>
<li><a href="https://arxiv.org/abs/2005.00561">When BERT Plays the Lottery, All Tickets Are Winning</a> (EMNLP2020)
<pre>基于变压器的大型模型被证明可以简化为较少数量的自我关注头和层。我们从彩票假说的角度考虑这一现象，使用结构和数量修剪。对于微调的BERT，我们表明：（a）可以找到实现与完整模型相当的性能的子网络，以及（b）从模型其余部分取样的类似大小的子网络性能较差。引人注目的是，通过结构化剪枝，即使是最差的子网络也仍然具有高度的可训练性，这表明大多数预先训练的伯特权重都是潜在有用的。我们还研究了“好”的子网络，看看它们的成功是否可以归因于优秀的语言知识，但发现它们不稳定，并且不能用有意义的自我注意模式来解释。</pre></li>
<li><a href="https://arxiv.org/abs/2007.12223">The Lottery Ticket Hypothesis for Pre-trained BERT Networks</a>
<pre>在自然语言处理（natural language processing，NLP）中，像BERT这样的大量预训练模型已经成为一系列下游任务训练的标准起点，在深度学习的其他领域也出现了类似的趋势。同时，关于彩票假设的研究表明，NLP和计算机视觉的模型包含更小的匹配子网络，这些子网络能够孤立地进行完全精确的训练并转移到其他任务。在这项工作中，我们结合这些观察来评估在预先训练的伯特模型中是否存在这样的可训练、可转移的子网络。对于一系列下游任务，我们确实发现匹配子网络的稀疏度为40%到90%。我们发现这些子网络是在（预先训练的）初始化阶段出现的，这与之前的NLP研究有所不同，在NLP研究中，这些子网络是在经过一定训练后才出现的。蒙面语言建模任务（用于预训练模型的相同任务）上发现的子网络普遍传输；那些在其他任务中发现的，如果有的话，会以有限的方式转移。随着大规模的预培训逐渐成为深度学习的中心范式，我们的研究结果表明，主要的彩票观察结果仍然与此相关。代码可在https://github.com/VITA-Group/BERT-Tickets.</pre></li>
<li><a href="https://arxiv.org/abs/2106.08367">What Context Features Can Transformer Language Models Use?</a> (ACL2021)
<pre>基于Transformer的语言模型受益于对数百到数千个先前标记的上下文进行条件化。这些上下文的哪些方面有助于准确的模型预测？我们描述了一系列实验，这些实验通过选择性地去除在英语维基百科上训练的transformer语言模型中的词汇和结构信息来测量可用信息。在中长期语境中，我们发现一些极具破坏性的语境操作——包括改变句子中的词序和删除除名词以外的所有单词——删除了不到15%的可用信息。我们的结果表明，长上下文，而不是其详细的句法和命题内容，对于电流互感器语言模型的低复杂度是重要的。</pre></li>
<li><a href="https://arxiv.org/abs/1910.05276">exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformers Models</a> [<a href="https://github.com/bhoov/exbert">github</a>]
<pre>大型语言模型可以生成强大的上下文表示，从而改进许多NLP任务。由于这些模型通常由一系列学习到的自我注意机制引导，并且可能包含不希望出现的诱导偏差，因此能够探索注意学到了什么至关重要。虽然对这些模型的静态分析会带来有针对性的见解，但交互式工具更具动态性，可以帮助人们更好地获得模型内部推理过程的直觉。我们介绍了exBERT，一种以流行的BERT语言模型命名的交互式工具，它通过将人类指定的输入与大型注释数据集中的类似上下文进行匹配，从而深入了解上下文表示的含义。通过聚合匹配相似上下文的注释，exBERT可以直观地解释每个注意头都学到了什么。</pre></li>
<li><a href="https://arxiv.org/abs/2008.05122">The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models</a> [<a href="https://github.com/pair-code/lit">github</a>]
<pre>我们介绍了语言可解释性工具（LIT），一个用于可视化和理解NLP模型的开源平台。我们关注关于模型行为的核心问题：为什么我的模型做出这样的预测？什么时候表现不佳？在输入的受控变化下会发生什么？LIT将本地解释、聚合分析和反事实生成集成到一个基于浏览器的简化界面中，以实现快速探索和错误分析。我们包括一系列不同工作流程的案例研究，包括探索情绪分析的反事实，测量共指系统中的性别偏见，以及探索文本生成中的局部行为。LIT支持广泛的模型——包括分类、seq2seq和结构化预测——并通过声明性、框架无关的API进行高度扩展。LIT正在积极开发中，代码和完整文档可在https://github.com/pair-code/lit.</pre></li>
<li><a href="https://www.aclweb.org/anthology/2020.acl-main.469/">What Does BERT with Vision Look At?</a> (ACL2020)</li>
<li><a href="">Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models</a> (ECCV2020)</li>
<li><a href="https://arxiv.org/pdf/2102.00529.pdf">Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers</a> (TACL2021)</li>
<li><a href="https://arxiv.org/pdf/2109.07301.pdf">What Vision-Language Models ‘See’ when they See Scenes</a></li>
</ul>
<h1 id="multi-lingual">Multi-lingual</h1>
<ul>
<li><a href="https://arxiv.org/abs/2107.00676">A Primer on Pretrained Multilingual Language Models</a>
<pre>多语言语言模型（MLLMs），如mBERT、XLM、XLM-R、\textit{等）已经成为一种可行的选择，可以将预训练的力量引入到大量语言中。鉴于他们在零射击转移学习方面取得的成功，在以下方面出现了大量工作：（i）构建涵盖大量语言的更大的MLLMs；（ii）创建涵盖更广泛任务和语言的详尽基准，以评估MLLMs（iii）分析单语MLLMs的性能，零射击跨语言和双语任务（iv）理解MLLMs学习的通用语言模式（如果有）和（v）增强MLLMs（通常）有限的能力，以提高其在可见或甚至不可见语言上的表现。在这项调查中，我们回顾了现有文献，涵盖了与MLLMs相关的上述广泛研究领域。根据我们的调查，我们提出了一些未来研究的方向。</pre></li>
<li><a href="https://arxiv.org/abs/1812.11760">Multilingual Constituency Parsing with Self-Attention and Pre-Training</a> (ACL2019)
<pre>我们表明，选区分析得益于跨多种语言和一系列预训练条件的无监督预训练。我们首先比较了无预培训、快速文本、ELMo和BERT对英语的好处，发现BERT优于ELMo，这在很大程度上是由于模型容量的增加，而ELMo反过来又优于非上下文快速文本嵌入。我们还发现，在所有11种测试语言中，预培训都是有益的；然而，大型模型（超过1亿个参数）使得为每种语言训练单独的模型在计算上非常昂贵。为了解决这个缺点，我们展示了联合多语言预训练和微调允许在最终模型中的十种语言之间共享除少量参数以外的所有参数。与微调每种语言的一个模型相比，模型大小减少了10倍，只会导致总体相对误差增加3.2%。我们进一步探讨了联合微调的思想，并表明它为低资源语言提供了一种从其他语言的较大数据集中获益的方法。最后，我们展示了11种语言的最新结果，包括英语（95.8 F1）和汉语（91.8 F1）。</pre></li>
<li><a href="https://arxiv.org/abs/1901.07291">Cross-lingual Language Model Pretraining</a> (NeurIPS2019) [<a href="https://github.com/facebookresearch/XLM">github</a>]
<pre>最近的研究证明了生成性预训练对英语自然语言理解的有效性。在这项工作中，我们将这种方法扩展到多种语言，并展示了跨语言预训练的有效性。我们提出了两种学习跨语言语言模型（XLM）的方法：一种是仅依赖单语数据的无监督学习方法，另一种是利用平行数据的有监督学习方法，该方法具有新的跨语言模型目标。我们在跨语言分类、无监督和有监督机器翻译方面取得了最新的成果。在XNLI上，我们的方法以4.9%的绝对精度提升了最先进的水平。在无监督机器翻译方面，我们在WMT'16德语英语中获得了34.3 BLEU，比以前的技术水平提高了9 BLEU以上。在监督机器翻译方面，我们在WMT'16罗马尼亚语英语中获得了38.5 BLEU的最新水平，比以前的最佳方法高出4 BLEU以上。我们的代码和预训练模型将公开提供。</pre></li>
<li><a href="https://arxiv.org/abs/2106.16138">XLM-E: Cross-lingual Language Model Pre-training via ELECTRA</a>
<pre>在本文中，我们将ELECTRA风格的任务引入到跨语言模型预训练中。具体来说，我们提出了两个预训练任务，即多语言替换标记检测和翻译替换标记检测。此外，我们在多语言语料库和平行语料库上对模型XLM-E进行了预训练。我们的模型在各种跨语言理解任务上都优于基线模型，并且计算量小得多。此外，分析表明，XLM-E具有更好的跨语言迁移能力。</pre></li>
<li><a href="https://arxiv.org/abs/2109.12573">XLM-K: Improving Cross-Lingual Language Model Pre-Training with Multilingual Knowledge</a>
<pre>使用单语和双语纯文本语料库进行跨语言预培训取得了巨大成功。然而，现有的预训练模型忽略了多语言知识，多语言知识是语言不可知的，但包含丰富的跨语言结构对齐。在这篇文章中，我们提出了XLM-K，一个在预训练中融入多种语言知识的跨语言语言模型。XLM-K将现有的多语言预训练扩展为两个知识任务，即掩蔽实体预测任务和对象蕴涵任务。我们在MLQA、NER和XNLI上评估XLM-K。实验结果清楚地表明，与现有的多语言语言模型相比，有了显著的改进。MLQA和NER的结果显示了XLM-K在知识相关任务中的优越性。XNLI的成功表明，XLM-K具有更好的跨语言迁移能力。此外，我们提供了详细的探索性分析，以确认在我们的训练前方案中获得的所需知识。</pre></li>
<li><a href="https://arxiv.org/abs/1904.02099">75 Languages, 1 Model: Parsing Universal Dependencies Universally</a> (EMNLP2019) [<a href="https://github.com/hyperparticle/udify">github</a>]
<pre>我们提出了UDify，一个多语言多任务模型，能够准确预测75种语言的所有124个通用依赖树库的通用词性、形态特征、引理和依赖树。通过利用在104种语言上预训练的多语言BERT自我注意模型，我们发现在所有数据集上对其进行微调，并为每个UD任务使用简单的softmax分类器，可以产生最先进的UPO、UFEAT、引理、UAS和LAS分数，而不需要任何重复或特定于语言的组件。我们对UDify进行了多语言学习评估，结果表明，低资源语言从跨语言注释中获益最大。我们还评估了零镜头学习，结果表明，多语言培训提供了强大的UD预测，即使对于UDify和BERT都没有接受过培训的语言也是如此。UDify的代码可在https://github.com/hyperparticle/udify.</pre></li>
<li><a href="https://arxiv.org/abs/1910.05479">Zero-shot Dependency Parsing with Pre-trained Multilingual Sentence Representations</a> (EMNLP2019 WS)
<pre>我们研究了在大规模多语言语料库（multilingual BERT）上训练的现成深层双向句子表示是否能够开发无监督的通用依赖解析器。这种方法只利用多种语言中的单语语料库，并且不需要任何翻译数据，因此适用于低资源语言。在我们的实验中，我们在使用单一系统的情况下，在共享任务的六种真正低资源语言的所有方面都优于最佳CoNLL 2018语言特定系统。然而，我们还发现：（i）当改变训练语言时，解析精度仍然存在显著差异；（ii）在某些目标语言中，零炮传输在所有测试条件下都失败，这引起了对整个方法“普遍性”的关注。</pre></li>
<li><a href="https://arxiv.org/abs/2009.14124">Parsing with Multilingual BERT, a Small Corpus, and a Small Treebank</a> (EMNLP2020 Findings)
<pre>经过预训练的多语言上下文表示已经显示出巨大的成功，但由于其预训练数据的限制，它们的好处并不平等地适用于所有语言变体。这对这些模型不熟悉的语言变体提出了挑战，这些模型的标记和未标记数据太有限，无法有效地训练单语模型。我们建议使用额外的特定于语言的预训练和词汇扩充，以使多语言模型适应低资源环境。通过对四种不同的低资源语言变体的依赖性分析作为案例研究，我们证明了这些方法显著提高了基线性能，尤其是在资源最低的情况下，并且证明了这些模型的预训练数据与目标语言变体之间关系的重要性。</pre></li>
<li><a href="https://arxiv.org/abs/1904.09077">Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT</a> (EMNLP2019)
<pre>预先训练的情境表征模型（Peters等人，2018；Devlin等人，2018）推动了许多NLP任务的最新进展。BERT的最新版本（Devlin，2018）包括一个同时对104种语言进行预训练的模型，该模型在自然语言推理任务中具有令人印象深刻的零镜头跨语言迁移性能。本文探讨了mBERT（Multilanguage）作为一种零镜头语言迁移模型在5项NLP任务中更广泛的跨语言潜力，这些任务涉及来自不同语系的39种语言：NLI、文档分类、NER、词性标记和依存分析。我们比较了mBERT和最佳的零射跨语言迁移方法，发现mBERT在每项任务上都具有竞争力。此外，我们还研究了以这种方式利用mBERT的最有效策略，确定mBERT在多大程度上概括了语言特有的特征，并测量了影响跨语言迁移的因素。</pre></li>
<li><a href="https://arxiv.org/abs/1906.01502">How multilingual is Multilingual BERT?</a> (ACL2019)
<pre>在本文中，我们发现Devlin et al.（2018）发布的多语言BERT（M-BERT）作为从104种语言的单语语料库中预先训练的单一语言模型，在零镜头跨语言模型迁移方面出人意料地出色，其中，使用一种语言中特定于任务的注释来微调模型，以便使用另一种语言进行评估。为了理解其中的原因，我们进行了大量的探索性实验，结果表明，即使对不同脚本中的语言，迁移也是可能的，在类型相似的语言之间，迁移效果最好，单语语料库可以训练代码转换模型，并且模型可以找到翻译对。从这些结果中，我们可以得出结论，M-BERT确实创建了多语言表示，但这些表示显示出影响某些语言对的系统缺陷。</pre></li>
<li><a href="https://arxiv.org/abs/1911.03310">How Language-Neutral is Multilingual BERT?</a>
<pre>多语言BERT（mBERT）提供了104种语言的句子表示，这对许多多语言任务都很有用。以前的工作在形态学和句法任务中使用零镜头迁移学习来探索mBERT的跨语言性。我们转而关注mBERT的语义属性。我们证明了mBERT表示可以分为语言特定组件和语言中性组件，语言中性组件在建模语义方面具有足够的通用性，可以实现高精度的单词对齐和句子检索，但对于更困难的机器翻译质量评估任务来说还不够好。我们的工作提出了一些有趣的挑战，必须解决这些挑战才能构建更好的语言中立表示，特别是对于需要语义语言转换的任务。</pre></li>
<li><a href="https://arxiv.org/abs/2106.02124">How to Adapt Your Pretrained Multilingual Model to 1600 Languages</a> (ACL2021)
<pre>预训练多语言模型（PMM）通过跨语言迁移实现零镜头学习，在预训练期间表现最好。虽然有一些方法可以提高看不见的语言的性能，但它们几乎完全是使用世界上一小部分语言的原始文本进行评估的。在本文中，我们使用一个可用于1600多种语言的资源：新约，来评估现有方法使PMM适应新语言的性能。这具有挑战性，原因有两个：（1）语料库规模小，（2）领域狭窄。尽管所有方法的性能都有所下降，但令人惊讶的是，与XLM-R相比，在所有语言中，词性标注的准确率高达17.69\%$，NER的平均准确率高达6.29$。另一个意外的发现是，最简单的持续预训练方法的性能最好。最后，我们进行了一个案例研究，以理清域和大小的影响，并阐明微调源语言的影响。</pre></li>
<li><a href="https://arxiv.org/abs/2010.05609">Load What You Need: Smaller Versions of Multilingual BERT</a> (EMNLP2020) [<a href="https://github.com/Geotrend-research/smaller-transformers">github</a>]
<pre>预训练的基于Transformer的模型在各种自然语言处理数据集上实现了最先进的结果。然而，这些模型的大小通常是它们在实际生产应用程序中部署的一个缺点。对于多语言模型，大多数参数位于嵌入层。因此，减少词汇量应该对参数总数产生重要影响。在本文中，我们建议根据目标语料库生成处理较少语言的较小模型。我们在XNLI数据集上对多语言BERT的较小版本进行了评估，但我们认为该方法可能适用于其他多语言变压器。获得的结果证实，我们可以生成更小的模型，保持可比结果，同时减少多达45%的参数总数。我们将我们的模型与DistilmBERT（一种多语言BERT的蒸馏版本）进行了比较，结果表明，与语言简化不同，蒸馏导致XNLI数据集的总体准确性下降了1.7%到6%。所提供的模型和代码是公开的。</pre></li>
<li><a href="https://arxiv.org/abs/1910.03806">Is Multilingual BERT Fluent in Language Generation?</a>
<pre>多语言BERT模型在104种语言上进行训练，旨在作为通用语言模型和句子编码工具。我们探讨了该模型在多个任务中对几种语言的表现：一个诊断分类探测特定语法属性的嵌入，一个完形填空任务测试语言建模能力以填补句子中的空白，以及自然语言生成任务，测试生成符合给定上下文的连贯文本的能力。我们发现，目前可用的多语言BERT模型明显不如单语模型，并且在许多情况下不能替代训练有素的单语模型。我们发现，英语和德语模型在生成时表现良好，而多语言模型尤其缺乏北欧语言。</pre></li>
<li><a href="https://arxiv.org/abs/2106.01597">ZmBART: An Unsupervised Cross-lingual Transfer Framework for Language Generation</a> (ACL2021 Findings)
<pre>尽管自然语言处理研究最近取得了一些进展，但自然语言生成中的跨语言迁移研究相对较少。在这项工作中，我们将对自然语言生成（NLG）的监督从高资源语言（HRL）转移到多个低资源语言（LRL）。我们考虑了四个NLG任务（文本摘要、问题生成、新闻标题生成和分词生成）和三种句法上不同的语言，即英语、印地语和日语。我们提出了一个无监督的跨语言语言生成框架（称为ZmBART），该框架不使用任何并行或伪并行/反向翻译数据。在这个框架中，我们进一步使用三种语言的单语数据，通过辅助任务对mBART序列到序列去噪自动编码器模型进行预训练。辅助任务的目标函数接近目标任务，丰富了mBART的多语言潜在表示，为目标任务提供了良好的初始化。然后，使用特定任务的有监督英语数据对该模型进行微调，并在零镜头设置下使用低资源语言直接评估该模型。为了克服灾难性遗忘和伪相关问题，我们分别采用了冻结模型组件和数据论证方法。这种简单的建模方法给了我们很好的结果。我们尝试了少量的镜头训练（1000个有监督的数据点），这进一步提高了模型的性能。我们进行了几次消融和跨语言转移性分析，以证明ZmBART的稳健性。</pre></li>
<li><a href="https://www.aclweb.org/anthology/D19-1252/">Unicoder: A Universal Language Encoder by Pre-training with Multiple Cross-lingual Tasks</a> (EMNLP2019)</li>
<li><a href="https://www.aclweb.org/anthology/D19-6106/">BERT is Not an Interlingua and the Bias of Tokenization</a> (EMNLP2019 WS)</li>
<li><a href="https://openreview.net/forum?id=HJeT3yrtDr">Cross-Lingual Ability of Multilingual BERT: An Empirical Study</a> (ICLR2020)</li>
<li><a href="https://arxiv.org/abs/2002.03518">Multilingual Alignment of Contextual Word Representations</a> (ICLR2020)
<pre>我们提出了评估和加强上下文嵌入对齐的程序，并表明它们在分析和改进多语言BERT方面是有用的。特别是，在我们提出的对齐程序之后，与基础模型相比，BERT在XNLI上表现出显著改善的零炮性能，显著匹配保加利亚语和希腊语的伪全监督翻译训练模型。此外，为了测量对齐程度，我们引入了一种上下文版本的单词检索，并表明它与下游零镜头转移有很好的相关性。使用这个词检索任务，我们还分析了BERT，发现它存在系统性缺陷，例如，使用不同脚本编写的开放类词类和词对对齐较差，这些缺陷通过对齐过程得到纠正。这些结果支持上下文对齐作为理解大型多语言预训练模型的有用概念。</pre></li>
<li><a href="https://arxiv.org/abs/1911.01464">Emerging Cross-lingual Structure in Pretrained Language Models</a> (ACL2020)
<pre>我们研究了多语言掩蔽语言建模的问题，即对来自多种语言的连接文本的单个模型的训练，并详细研究了影响这些模型对跨语言迁移如此有效的几个因素。我们发现，与之前的假设相反，即使单语语料库中没有共享词汇，而且文本来自非常不同的领域，迁移也是可能的。唯一的要求是在多语言编码器的顶层有一些共享参数。为了更好地理解这一结果，我们还表明，来自不同语言的独立训练模型的表示可以非常有效地在事后对齐，这有力地表明，与非上下文单词嵌入一样，在学习的嵌入空间中存在普遍的潜在对称性。对于多语言蒙面语言建模，这些对称性似乎会在联合训练过程中自动发现并对齐。</pre></li>
<li><a href="https://arxiv.org/abs/1910.11856">On the Cross-lingual Transferability of Monolingual Representations</a>
<pre>最先进的无监督多语言模型（例如，多语言BERT）已被证明可以在零炮跨语言环境下进行推广。这种泛化能力归因于使用共享的子词词汇和跨多种语言的联合训练，从而产生深刻的多语言抽象。我们通过设计一种替代方法来评估这一假设，该方法将单语模型转移到词汇层面的新语言中。更具体地说，我们首先在一种语言上训练一个基于转换器的掩蔽语言模型，然后通过学习具有相同掩蔽语言建模目标的新嵌入矩阵将其转换为一种新语言，冻结所有其他层的参数。这种方法不依赖于共享词汇或联合培训。然而，我们表明，在标准的跨语言分类基准和一个新的跨语言问答数据集（XQuAD）上，它与多语言BERT具有竞争力。我们的结果与关于多语言模型泛化能力基础的共同信念相矛盾，并表明深层单语模型学习一些跨语言泛化的抽象。我们还发布了XQuAD作为一个更全面的跨语言基准，它包括240个段落和1190个问题-答案对，由专业翻译人员从SQuAD v1.1翻译成十种语言。</pre></li>
<li><a href="https://arxiv.org/abs/1911.02116">Unsupervised Cross-lingual Representation Learning at Scale</a> (ACL2020)
<pre>这篇论文表明，大规模的多语种语言模型预训练可以显著提高跨语种迁移任务的成绩。我们在100种语言上训练一个基于转换器的屏蔽语言模型，使用超过2 TB的过滤公共爬网数据。我们的模型被称为XLM-R，在多种跨语言基准测试中显著优于多语言BERT（mBERT），包括XNLI的平均准确率+14.6%，MLQA的平均F1分数+13%，NER的平均F1分数+2.4%。XLM-R在低资源语言上表现尤其出色，与以前的XLM模型相比，斯瓦希里语的XNLI准确率提高了15.7%，乌尔都语的准确率提高了11.4%。我们还对实现这些收益所需的关键因素进行了详细的实证分析，包括（1）正迁移和容量稀释，以及（2）高资源和低资源语言在规模上的性能之间的权衡。最后，我们首次展示了在不牺牲每种语言性能的情况下进行多语言建模的可能性；XLM-R在GLUE和XNLI基准上与强大的单语模型非常有竞争力。我们将公开我们的代码、数据和模型。</pre></li>
<li><a href="https://arxiv.org/abs/2009.05166">FILTER: An Enhanced Fusion Method for Cross-lingual Language Understanding</a>
<pre>大规模跨语言模型（LM）如mBERT、Unicoder和XLM在跨语言表征学习方面取得了巨大成功。然而，当应用于零触发跨语言迁移任务时，大多数现有方法仅使用单一语言输入进行LM微调，而没有利用不同语言之间的固有跨语言对齐，这对于多语言任务来说是必不可少的。在本文中，我们提出了滤波器，一种增强的融合方法，将跨语言数据作为XLM微调的输入。具体地说，FILTER首先在浅层中独立地对源语言中的文本输入及其在目标语言中的翻译进行编码，然后在中间层中执行跨语言融合以提取多语言知识，最后执行进一步的特定于语言的编码。在推理过程中，该模型根据目标语言中的文本输入及其在源语言中的翻译进行预测。对于分类等简单任务，目标语言中的翻译文本与源语言共享相同的标签。但是，对于更复杂的任务，例如问答、NER和词性标记，这种共享标签变得不那么准确，甚至不可用。为了解决这个问题，我们进一步提出了一个额外的KL发散自我教学损失模型训练，基于自动生成软伪标签翻译文本在目标语言。大量实验表明，过滤器在两个具有挑战性的多语言多任务基准测试XTREME和XGLUE上达到了新的水平。</pre></li>
<li><a href="https://arxiv.org/abs/2009.14304">Cross-lingual Alignment Methods for Multilingual BERT: A Comparative Study</a> (EMNLP2020 Findings)
<pre>当对下游任务进行微调时，多语言BERT（mBERT）已显示出合理的零镜头跨语言传输能力。由于mBERT没有经过明确的跨语言监督的预先训练，因此可以通过将mBERT与跨语言信号对齐来进一步提高迁移性能。先前的工作提出了几种方法来对齐上下文化嵌入。在本文中，我们分析了不同形式的跨语言监督和不同的对齐方法如何影响mBERT在零炮设置下的传输能力。具体来说，我们比较了并行语料库与基于词典的监控以及旋转与基于微调的对齐方法。我们评估了八种语言的不同对齐方法在两个任务上的性能：名称实体识别和语义槽填充。此外，我们还提出了一种新的归一化方法，该方法持续改进了基于旋转的对齐的性能，包括对远距离和类型不同的语言显著提高了3%。重要的是，我们确定了对齐方法对任务类型的偏见以及与迁移语言的接近程度。我们还发现，平行语料库的监督通常优于词典对齐。</pre></li>
<li><a href="https://arxiv.org/abs/1911.01464">Emerging Cross-lingual Structure in Pretrained Language Models</a>
<pre>我们研究了多语言掩蔽语言建模的问题，即对来自多种语言的连接文本的单个模型的训练，并详细研究了影响这些模型对跨语言迁移如此有效的几个因素。我们发现，与之前的假设相反，即使单语语料库中没有共享词汇，而且文本来自非常不同的领域，迁移也是可能的。唯一的要求是在多语言编码器的顶层有一些共享参数。为了更好地理解这一结果，我们还表明，来自不同语言的独立训练模型的表示可以非常有效地在事后对齐，这有力地表明，与非上下文单词嵌入一样，在学习的嵌入空间中存在普遍的潜在对称性。对于多语言蒙面语言建模，这些对称性似乎会在联合训练过程中自动发现并对齐。</pre></li>
<li><a href="https://arxiv.org/abs/1911.03913">Can Monolingual Pretrained Models Help Cross-Lingual Classification?</a>
<pre>多语言预训练语言模型（如多语言BERT）在跨语言迁移方面取得了令人印象深刻的成果。然而，由于模型容量不变，多语种预培训通常落后于单语竞争对手。在这项工作中，我们提出了两种改进零镜头跨语言分类的方法，将知识从单语预训练模型转移到多语模型。在两个跨语言分类基准上的实验结果表明，我们的方法优于香草多语言微调。</pre></li>
<li><a href="https://arxiv.org/abs/2004.09205">A Study of Cross-Lingual Ability and Language-specific Information in Multilingual BERT</a>
<pre>最近，多语种的BERT在跨语言迁移任务中表现得非常出色，优于静态的非语境化单词嵌入。在这项工作中，我们提供了一个深入的实验研究，以补充现有文献的跨语言能力。我们比较了相同数据下非语境化和语境化表征模式的跨语言能力。我们发现数据大小和上下文窗口大小是影响可转移性的关键因素。我们还观察了多语言文本中特定于语言的信息。通过操纵潜在表示，我们可以控制多语种BERT的输出语言，实现无监督的标记翻译。我们进一步表明，基于这一观察，有一种计算成本低廉但有效的方法可以提高多语种语言学习者的跨语言能力。</pre></li>
<li><a href="https://www.aclweb.org/anthology/K19-1020/">Fully Unsupervised Crosslingual Semantic Textual Similarity Metric Based on BERT for Identifying Parallel Data</a> (CoNLL2019)</li>
<li><a href="https://arxiv.org/abs/2003.02912">What the [MASK]? Making Sense of Language-Specific BERT Models</a>
<pre>近年来，自然语言处理（NLP）在许多领域取得了令人瞩目的进展，这得益于新颖的、经过训练的上下文表示模型的出现。特别是，Devlin等人（2019年）提出了一种称为BERT（来自变压器的双向编码器表示）的模型，该模型使研究人员能够通过微调数据集和任务的表示，在众多NLP任务中获得最先进的性能，无需开发和培训高度特定的体系结构。作者还发布了多语言BERT（multilingual BERT，mBERT），这是一个在104种语言的语料库上训练的模型，可以作为通用语言模型。该模型在跨语言自然推理任务中取得了令人印象深刻的结果。在BERT模型潜力的驱动下，NLP社区已经开始调查并生成大量的BERT模型，这些模型在特定的语言上进行训练，并在特定的数据域和任务上进行测试。这使我们能够通过将mBERT与这些更具体的模型的性能进行比较来评估其作为通用语言模型的真正潜力。本文介绍了特定语言的BERT模型的最新进展，提供了不同维度（即体系结构、数据域和任务）的总体情况。我们的目标是提供语言特定（语言特定）BERT模型和mBERT之间的共性和差异的直接概述。我们还提供了一个互动和不断更新的网站，可用于探索我们收集的信息，网址：https://bertlang.unibocconi.it.</pre></li>
<li><a href="https://arxiv.org/abs/2003.11080">XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization</a> (ICML2020)
<pre>机器学习模型在自然语言处理中应用的最新进展是由评估各种任务模型的基准驱动的。然而，这些覆盖范围广泛的基准大多局限于英语，尽管人们对多语言模式越来越感兴趣，但仍然缺少一个能够对各种语言和任务的这种方法进行全面评估的基准。为此，我们介绍了多语言编码器的跨语言迁移评估XTREME benchmark，这是一个多任务基准，用于评估跨40种语言和9个任务的多语言表示的跨语言泛化能力。我们证明，尽管英语测试模型在许多任务上达到了人类的表现，但跨语言迁移模型的表现仍有相当大的差距，特别是在句法和句子检索任务上。结果也广泛分布在不同的语言中。我们发布该基准旨在鼓励跨语言学习方法的研究，这种方法可以跨多种不同且具有代表性的语言和任务传递语言知识。</pre></li>
<li><a href="https://arxiv.org/abs/2104.07412">XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation</a> (EMNLP2021)
<pre>过去一年，机器学习在多语言自然语言处理能力方面取得了显著进步。例如，最新技术将XTREME多语言基准的最新性能提高了13个百分点以上。虽然与人的绩效水平仍有相当大的差距，但在某些任务中，改进比在其他任务中更容易实现。本文分析了跨语言迁移学习的现状，总结了一些经验教训。为了促进有意义的进展，我们将XTREME扩展到XTREME-R，它由一组改进的十种自然语言理解任务组成，包括挑战性的语言不可知检索任务，涵盖50种类型多样的语言。此外，我们通过交互式公共排行榜提供了大规模多语言诊断套件（MultiCheckList）和细粒度多数据集评估功能，以更好地理解此类模型。XTREME-R的排行榜和代码将在https://sites.research.google/xtreme 和https://github.com/google-research/xtreme 分别地</pre></li>
<li><a href="https://arxiv.org/abs/2004.01401">XGLUE: A New Benchmark Dataset for Cross-lingual Pre-training, Understanding and Generation</a>
<pre>在本文中，我们介绍了XGLUE，这是一个新的基准数据集，可用于使用多语言和双语语料库训练大规模跨语言预训练模型，并评估其在不同跨语言任务集上的性能。与GLUE（Wang et al.，2019）相比，XGLUE具有两个主要优势：（1）它提供了11个多样化的任务，涵盖自然语言理解和生成场景；（2） 对于每个任务，它都提供多种语言的标记数据。我们扩展了最近的跨语言预训练模型Unicoder（Huang et al.，2019），以涵盖理解和生成任务，该模型在XGLUE上作为一个强大的基线进行评估。我们还评估了多语言BERT、XLM和XLM-R的基本版本（12层），以进行比较。</pre></li>
<li><a href="https://arxiv.org/abs/2004.03032">A Systematic Analysis of Morphological Content in BERT Models for Multiple Languages</a>
<pre>这项工作描述了实验，探索隐藏的表示形式的几个伯特风格模型的形态内容。其目的是检验以形态特征和特征值形式存在的离散语言结构在多大程度上呈现在五种欧洲语言的预训练语言模型的向量表示和注意分布中。本文包含的实验表明：（i）变换器结构将其嵌入空间划分为与形态特征值高度相关的凸面子区域；（ii）变换器嵌入的上下文性质允许模型在许多情况下（但并非所有情况下）区分模糊的形态形式，以及（iii）非常具体的注意头/层组合似乎磨练了主谓一致性。</pre></li>
<li><a href="https://arxiv.org/abs/2004.13640">Extending Multilingual BERT to Low-Resource Languages</a>
<pre>多语BERT（M-BERT）在有监督和零次跨语言迁移学习中都取得了巨大的成功。然而，这一成功只集中在维基百科培训的前104种语言上。在本文中，我们提出了一种简单而有效的方法来扩展M-BERT（E-BERT），从而使它能够对任何新的语言都有好处，并且表明我们的方法也有利于已经存在于M-BERT中的语言。我们在27种语言上进行了大量的命名实体识别（NER）实验，其中只有16种是M-BERT语言，并且在已经使用M-BERT语言的语言上，F1平均增长约6%，在新语言上，F1平均增长23%。</pre></li>
<li><a href="https://arxiv.org/abs/2004.13947">Learning Better Universal Representations from Pre-trained Contextualized Language Models</a>
<pre>预先训练的语境化语言模型（如BERT）在广泛的下游自然语言处理（NLP）任务中表现出了极大的有效性。然而，模型提供的有效表示针对序列中的每个标记，而不是每个序列，微调步骤涉及同时输入两个序列，导致具有不同粒度的各种序列的不满意表示。特别是，在这些模型中，句子级表征作为完整的训练上下文，在较低级别的语言单位（短语和单词）上表现较差。在这项工作中，我们提出了BURT（受伯特启发的孪生结构通用表示法），它能够使用大量自然语言推理和复述数据，以多个训练目标，为任何粒度的输入序列（即单词、短语和句子）生成通用的、固定大小的表示法。我们提出的BURT采用暹罗网络，分别从自然语言推理数据集中学习句子级表示，从释义数据集中学习单词/短语级表示。我们评估了不同粒度的文本相似性任务，包括STS任务、SemEval2013任务5（a）和一些常用的单词相似性任务，其中BURT在句子级数据集上显著优于其他表示模型，并在单词/短语级表示方面取得了显著改进。</pre></li>
<li><a href="https://arxiv.org/abs/2004.14620">Universal Dependencies according to BERT: both more specific and more general</a>
<pre>这项工作的重点是通过从自我注意中提取标记依赖树来分析BERT捕获的句法抽象的形式和程度。以前的工作表明，单个的BERT头倾向于编码特定的依赖关系类型。我们通过显式地将BERT关系与通用依赖（UD）注释进行比较来扩展这些发现，表明它们通常不一一匹配。我们提出了一种关系识别和句法树构造的方法。与以前的工作相比，我们的方法产生了更一致的依赖树，这表明它更好地解释了BERT中的语法抽象。同时，它只需很少的监督就可以成功地应用，并且可以很好地跨语言推广。</pre></li>
<li><a href="https://arxiv.org/abs/2004.14958">A Call for More Rigor in Unsupervised Cross-lingual Learning</a> (ACL2020)
<pre>我们回顾了无监督跨语言学习的动机、定义、方法和方法，并呼吁对每一种方法采取更严格的立场。这种研究的一个现有理由是，世界上许多语言缺乏平行数据。然而，我们认为，没有任何平行数据和丰富的单语数据的情况在实践中是不现实的。我们还讨论了以前工作中使用的不同的训练信号，这些信号与纯无监督设置不同。然后，我们描述了无监督跨语言模型的调整和评估中常见的方法学问题，并介绍了最佳实践。最后，我们为这一领域的不同类型的研究（即跨语言单词嵌入、深度多语言预训练和无监督机器翻译）提供了一个统一的前景，并主张对这些模型进行比较评估。</pre></li>
<li><a href="https://arxiv.org/abs/2005.00396">Identifying Necessary Elements for BERT’s Multilinguality</a> (EMNLP2020)
<pre>已经证明，多语言BERT（mBERT）可以产生高质量的多语言表示，并实现有效的零镜头传输。这是令人惊讶的，因为mBERT在训练期间不使用任何跨语言信号。虽然最近的文献研究了这一现象，但多语言现象的原因仍然有些模糊。我们的目标是确定BERT的体系结构特性以及BERT成为多语言所必需的语言特性。为了实现快速实验，我们提出了一种基于合成和自然数据混合训练的小伯特模型的有效设置。总的来说，我们确定了影响多语言性的四个架构元素和两个语言元素。基于我们的见解，我们使用多语言预训练设置进行了实验，该设置使用VecMap修改掩蔽策略，即无监督嵌入对齐。使用三种语言在XNLI上进行的实验表明，我们的发现从小型设置转移到大型设置。</pre></li>
<li><a href="https://arxiv.org/abs/2005.00052">MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer</a>
<pre>最先进的预先训练的多语言模型（如多语言BERT和XLM-R）背后的主要目标是通过零触发或少触发跨语言传输在低资源语言中启用和引导NLP应用程序。然而，由于模型容量有限，在这种低资源语言和训练前未见过的语言上，它们的迁移性能最差。我们提出了MAD-X，这是一个基于适配器的框架，通过学习模块化语言和任务表示，它可以实现对任意任务和语言的高可移植性和参数有效传输。此外，我们还介绍了一种新的可逆适配器体系结构和一种强基线方法，用于将预先训练好的多语言模型适应新语言。在命名实体识别和因果常识推理方面，MAD-X在跨一组具有代表性的类型多样语言的跨语言迁移方面优于最新技术，并在问答方面取得了有竞争力的结果。我们的代码和适配器可从AdapterHub.ml获得</pre></li>
<li><a href="https://arxiv.org/abs/2005.00633">From Zero to Hero: On the Limitations of Zero-Shot Cross-Lingual Transfer with Multilingual Transformers</a>
<pre>具有语言建模目标（如mBERT、XLM-R）的大规模多语言转换器已经成为NLP中零触发跨语言传输的事实默认传输范式，提供了无与伦比的传输性能。然而，当前的下游评估主要在迁移环境中验证其有效性，这些迁移环境涉及到具有足够数量的训练前数据的语言，以及词汇和类型相近的语言。在这项工作中，我们分析了它们的局限性，并表明通过大规模多语言转换进行的跨语言转换，就像通过跨语言单词嵌入进行的转换一样，在资源贫乏的情况下和对于遥远的语言来说，其效率要低得多。我们的实验包括三个较低级别的任务（词性标注、依赖性分析、NER）和两个高级语义任务（NLI、QA），实验表明，迁移性能与源语言和目标语言之间的语言相似性相关，但也与目标语言预训练语料库的大小相关。我们还展示了廉价的少镜头传输（即，在源代码中进行微调后对一些目标语言实例进行微调）的惊人效果。这表明，应投入更多的研究工作，以达到超出极限零射击条件的目的。</pre></li>
<li><a href="https://arxiv.org/abs/2109.07684">Language Models are Few-shot Multilingual Learners</a>
<pre>通用语言模型已经显示出令人印象深刻的能力，在一系列的下游自然语言处理（NLP）任务和基准时，与最先进的方法相媲美，当从非常少的示例推断指令时。在此，我们评估了GPT和T5模型在无任何参数更新的情况下对非英语语言进行多类别分类的多语言技能。我们发现，在给定一些英语例子作为上下文的情况下，预先训练的语言模型不仅可以预测英语测试样本，而且可以预测非英语测试样本。最后，我们发现语言模型的上下文少镜头跨语言预测结果明显优于随机预测，并且与现有的最新跨语言模型相比具有竞争力。</pre></li>
<li><a href="https://arxiv.org/abs/2101.11109">First Align, then Predict: Understanding the Cross-Lingual Ability of Multilingual BERT</a> (EACL2021)
<pre>多语种预训练语言模型显示了显著的零射跨语言迁移能力。这种迁移是通过对一种语言感兴趣的任务进行微调，并对一种不同的语言进行评估而产生的，而在微调过程中没有看到这种情况。尽管取得了可喜的结果，但我们仍然缺乏对这种转移来源的正确理解。使用一种新的层消融技术和对模型内部表示的分析，我们表明，流行的多语言语言模型多语言BERT可以看作是两个子网络的叠加：一个多语言编码器和一个任务特定的语言不可知预测器。虽然编码器对于跨语言传输至关重要，并且在微调过程中基本保持不变，但任务预测器对传输几乎不重要，可以在微调过程中重新初始化。我们通过三个不同的任务、十七种不同类型的语言和多个领域的广泛实验来支持我们的假设。</pre></li>
<li><a href="https://arxiv.org/abs/2010.12547">Multilingual BERT Post-Pretraining Alignment</a> (NAACL2021)
<pre>我们提出了一种简单的方法来对齐多语言上下文嵌入，作为预训练后的一个步骤，以提高预训练模型的零镜头跨语言传输能力。利用并行数据，我们的方法通过最近提出的翻译语言建模目标在单词层面上对齐嵌入，并通过对比学习和随机输入洗牌在句子层面对齐嵌入。在对下游任务进行微调时，我们还使用英语执行句子级代码切换。在XNLI上，我们的最佳模型（从mBERT初始化）在零炮设置下比mBERT提高了4.7%，在使用少于18%的相同并行数据和31%的模型参数的情况下，实现了与XLM的平移序列相当的结果。在MLQA上，我们的模型优于XLM-R_基，其参数比我们的多57%。</pre></li>
<li><a href="https://arxiv.org/abs/2105.02472">XeroAlign: Zero-Shot Cross-lingual Transformer Alignment</a> (ACL2021 Findings)
<pre>预训练跨语言模型的引入为多语言NLP任务带来了决定性的改进。然而，由于缺乏标记的任务数据，需要各种方法来缩小与高资源语言的差距。特别是零炮方法，通常使用翻译后的任务数据作为训练信号，以弥合源语言和目标语言之间的性能差距。我们介绍了XeroAlign，这是一种简单的跨语言预训练变形金刚（如XLM-R）任务特定对齐方法。XeroAlign使用翻译的任务数据来鼓励模型为不同语言生成类似的句子嵌入。XeroAligned XLM-R（称为XLM-RA）在三种多语言自然语言理解任务上显示出比基线模型的强大改进，以实现最先进的零射击结果。XLM- RA的文本分类精度超过了用标记数据训练的XLM-R，并在一个跨语言的对抗性复述任务上与最先进的模型相匹配。</pre></li>
<li><a href="https://arxiv.org/abs/2106.02134">Syntax-augmented Multilingual BERT for Cross-lingual Transfer</a> (ACL2021)
<pre>近年来，我们看到了一项巨大的努力，即使用多种语言的大规模语料库对多语言文本编码器进行预培训，以促进跨语言迁移学习。然而，由于不同语言的类型差异，跨语言迁移是一个挑战。然而，语言语法，例如句法依赖，可以弥补类型上的差距。以前的工作表明，预先训练的多语言编码器，如mBERT\cite{devlin-etal-2019-bert}，可以捕获语言语法，帮助跨语言迁移。这项工作表明，显式提供语言语法和使用辅助目标对mBERT进行训练来编码通用依赖树结构有助于跨语言迁移。我们在四个NLP任务上进行了严格的实验，包括文本分类、问答、命名实体识别和面向任务的语义分析。实验结果表明，在PAWS-X和MLQA等流行基准测试中，语法增强mBERT在所有语言中的跨语言迁移平均提高了1.4和1.6个百分点。在\emph{generalized}转移设置中，性能显著提高，PAWS-X和MLQA的平均得分分别为3.9分和3.1分。</pre></li>
<li><a href="https://arxiv.org/abs/2010.10041">Language Representation in Multilingual BERT and its applications to improve Cross-lingual Generalization</a>
<pre>多语言BERT（m-BERT）中嵌入的标记包含语言和语义信息。我们发现，一种语言的表示可以通过简单地平均语言标记的嵌入来获得。通过语言表示，我们可以通过操纵标记嵌入来控制多语言BERT的输出语言，并实现无监督的标记翻译。在此基础上，我们进一步提出了一种计算成本低但有效的方法来提高m-BERT的跨语言能力。</pre></li>
<li><a href="https://openreview.net/forum?id=YjNv-hzM8BE">VECO: Variable Encoder-decoder Pre-training for Cross-lingual Understanding and Generation</a></li>
<li><a href="https://arxiv.org/abs/2004.05160">On the Language Neutrality of Pre-trained Multilingual Representations</a>
<pre>多语言上下文嵌入，如多语言BERT和XLM RoBERTa，已被证明对许多多语言任务有用。以前的工作是在形态学和句法任务中，使用零镜头迁移学习间接地探讨表征的跨语言性。相反，我们直接研究多语言上下文嵌入的语言中立性，并与词汇语义相关。我们的研究结果表明，上下文嵌入比对齐的静态单词类型嵌入更具语言中立性，并且总体上比对齐的静态单词类型嵌入更具信息性，而对齐的静态单词类型嵌入经过明确的语言中立性训练。默认情况下，上下文嵌入仍然只是适度的语言中立性，因此我们提出了两种简单的方法来实现更强的语言中立性：第一，通过无监督地集中每种语言的表示，第二，通过在小型并行数据上拟合显式投影。此外，我们还展示了如何在不使用平行数据的情况下，在语言识别方面达到最先进的准确性，并匹配统计方法在平行句子词对齐方面的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2005.09093">Are All Languages Created Equal in Multilingual BERT?</a> (ACL2020 WS)
<pre>在104种语言上训练的多语言BERT（mBERT）在几个NLP任务上表现出令人惊讶的良好跨语言性能，即使没有明确的跨语言信号。然而，这些评估侧重于高资源语言的跨语言迁移，仅涵盖mBERT所涵盖语言的三分之一。我们将探讨mBERT如何在更广泛的语言集上执行，重点关注低资源语言的表示质量，通过语言内性能来衡量。我们考虑了三个任务：命名实体识别（99种语言）、词性标注和依存句法分析（每个语言54种）。mBERT在高资源语言上的性能优于基线，或与基线相当，但在低资源语言上的性能更差。此外，这些语言的单语BERT模型做得更糟。与类似的语言搭配，单语BERT和mBERT之间的性能差距可以缩小。我们发现，低资源语言的更好模型需要更有效的预训练技术或更多的数据。</pre></li>
<li><a href="https://arxiv.org/abs/2010.12858">When Being Unseen from mBERT is just the Beginning: Handling New Languages With Multilingual Language Models</a>
<pre>基于大量原始数据的预训练语言模型的迁移学习已成为NLP中达到最先进水平的新规范。然而，对于任何可用的大规模多语种语言模型都没有涵盖的、通常只有少量原始数据可用的、看不见的语言，如何应用这种方法仍不清楚。在这项工作中，通过比较多语言和单语模型，我们证明了这种模型在看不见的语言上以多种方式表现。一些语言从迁移学习中受益匪浅，其行为与密切相关的高资源语言相似，而另一些语言显然没有。关注后者，我们表明，这种传输失败在很大程度上与用于编写此类语言的脚本的影响有关。对这些语言进行音译可以极大地提高大规模多语言模型处理下游任务的能力。</pre></li>
<li><a href="https://arxiv.org/abs/2105.02855">Adapting Monolingual Models: Data can be Scarce when Language Similarity is High</a> (ACL2021 Findings)
<pre>对于许多（少数）语言来说，培训大型模型所需的资源是不可用的。我们研究了在尽可能少的数据情况下零镜头迁移学习的性能，以及语言相似性在这一过程中的影响。我们使用来自两种低资源目标语言变体的数据对四个基于BERT的模型的词汇层进行重新训练，而转换层则在模型源语言的词性标记任务上进行独立的微调。通过结合新的词汇层和微调转换层，我们实现了两种目标语言的高任务性能。在语言相似性较高的情况下，10MB的数据似乎足以实现实质性的单语迁移性能。基于单语的BERT模型在重新训练词法层后通常比基于多语的BERT模型获得更高的下游任务性能，即使目标语言包括在多语模型中。</pre></li>
<li><a href="https://arxiv.org/abs/2007.01852">Language-agnostic BERT Sentence Embedding</a>
<pre>我们采用多语言BERT为109种语言生成语言不可知的句子嵌入许多单语和多语NLP任务的最新技术是掩蔽语言模型（MLM）预训练，然后是任务特定的微调。虽然英语句子嵌入是通过微调一个预训练的BERT模型得到的，但这种模型还没有应用于多语言句子嵌入。我们的模型将蒙面语言模型（MLM）和翻译语言模型（TLM）预训练与使用双向双编码器的翻译排序任务相结合。由此产生的多语言句子嵌入将超过112种语言的平均双文本检索准确率提高到83.7%，远远高于先前最先进的Tatoeba上的65.5%。我们的句子嵌入还建立了关于BUCC和UN bi文本检索的最新成果。</pre></li>
<li><a href="https://arxiv.org/abs/2012.14388">Universal Sentence Representation Learning with Conditional Masked Language Model</a>
<pre>本文提出了一种新的训练方法，即条件掩蔽语言建模（CMLM），以有效地学习大规模未标记语料库中的句子表示。CMLM通过对相邻句子的编码向量进行条件反射，将句子表征学习集成到传销训练中。我们的英文CMLM模型在SentEval上实现了最先进的性能，甚至优于使用监督信号学习的模型。作为一种完全无监督的学习方法，CMLM可以方便地扩展到广泛的语言和领域。我们发现，与双文本检索（BR）和自然语言推理（NLI）任务共同训练的多语言CMLM模型比以前最先进的多语言模型有很大的优势，例如跨语言语义搜索比基线模型提高了10%。我们探讨了学习表征的相同语言偏见，并提出了一种简单的、训练后和模型不可知的方法，以去除表征中的语言识别信息，同时保留句子语义。</pre></li>
<li><a href="https://arxiv.org/abs/2006.01538">WikiBERT models: deep transfer learning for many languages</a>
<pre>诸如BERT之类的深层神经语言模型在许多自然语言处理任务中实现了实质性的最新进展。由于前期培训的工作量和计算成本，语言特定模型通常只针对少量高资源语言（如英语）引入。虽然涵盖大量语言的多语言模型是可用的，但最近的研究表明，单语培训可以产生更好的模型，我们对单语和多语言培训之间权衡的理解是不完整的。在本文中，我们介绍了一个简单、全自动的管道，用于从Wikipedia数据创建特定于语言的BERT模型，并介绍了42个新的此类模型，其中大多数用于迄今为止缺乏专用深层神经语言模型的语言。我们使用最先进的UDify解析器对通用依赖数据评估这些模型的优点，并将性能与使用多语言BERT模型的结果进行对比。我们发现，使用WikiBERT模型的UDify平均优于使用mBERT的解析器，特定于语言的模型在某些语言上显示出显著的性能改进，但在其他语言上的性能改进有限或有所下降。我们还介绍了初步结果，作为理解特定语言模型最有利的条件的第一步。本工作中介绍的所有方法和模型都可以从以下位置获得开放许可证：https://github.com/turkunlp/wikibert.</pre></li>
<li><a href="https://arxiv.org/abs/2008.09112">Inducing Language-Agnostic Multilingual Representations</a>
<pre>跨语言表示有可能使NLP技术适用于世界上绝大多数语言。然而，它们目前需要大量的训练前语料库或访问类型相似的语言。在这项工作中，我们通过从多语言嵌入中移除语言身份信号来解决这些障碍。为此，我们研究了三种方法：（i）将目标语言的向量空间（全部）重新对齐到枢轴源语言；（ii）去除特定语言的平均值和差异，这将产生更好的识别性嵌入作为副产品；（iii）通过去除词法收缩和句子重新排序，增加语言间的输入相似性。我们评估了19种不同类型语言的XNLI和无参考机器翻译。我们的发现揭示了这些方法的局限性——与向量规范化不同，向量空间重新对齐和文本规范化不能在编码器和语言之间实现一致的增益。然而，由于这些方法的加性效应，它们的组合在所有任务和语言中平均减少了8.9个点（m-BERT）和18.2个点（XLM-R）。我们的代码和模型是公开的。</pre></li>
<li><a href="https://arxiv.org/abs/2011.05007">To What Degree Can Language Borders Be Blurred In BERT-based Multilingual Spoken Language Understanding?</a> (COLING2020)
<pre>本文讨论了基于BERT的多语言口语理解（SLU）模型在多大程度上可以跨语言传递知识的问题。通过实验，我们将表明，尽管它在远距离语言群体中也能很好地工作，但与理想的多语言性能仍有差距。此外，我们提出了一种新的基于BERT的对抗性模型体系结构，用于学习多语言SLU的语言共享和特定语言表示。我们的实验结果证明，该模型能够缩小差距，达到理想的多语言性能。</pre></li>
<li><a href="https://arxiv.org/abs/2010.08275">It’s not Greek to mBERT: Inducing Word-Level Translations from Multilingual BERT</a> (EMNLP2020 WS)
<pre>最近的研究表明，多语言BERT（mBERT）学习丰富的跨语言表达，允许跨语言的迁移。我们研究了嵌入在mBERT中的单词级翻译信息，并提出了两种简单的方法，无需微调即可显示出色的翻译能力。结果表明，大部分信息是以非线性方式编码的，而其中一些信息也可以用纯线性工具恢复。作为我们分析的一部分，我们检验了这样一个假设，即mBERT学习包含语言编码成分和抽象的跨语言成分的表示，并在mBERT表示中明确识别经验语言身份子空间。</pre></li>
<li><a href="https://arxiv.org/abs/2104.12250">XLM-T: A Multilingual Language Model Toolkit for Twitter</a>
<pre>语言模型在当前的自然语言处理中无处不在，其多语言能力近年来引起了广泛关注。然而，目前的分析几乎完全集中于标准基准（多语种变体），并且依赖于干净的培训前和任务特定语料库作为多语种信号。在本文中，我们介绍了XLM-T，一个用于在Twitter中使用和评估多语言模型的框架。该框架有两大特点：（1）强大的多语言基线，包括XLM-R（Conneau et al.2020）模型，该模型在超过30种语言的数百万条推文上预先训练，以及随后微调目标任务的起始代码；（2）一组统一的情绪分析Twitter数据集，使用八种不同的语言。这是一个模块化的框架，可以轻松地扩展到其他任务，并与最近也旨在使Twitter特定数据集同质化的努力相结合（Barbieri et al.2020）。</pre></li>
<li><a href="https://arxiv.org/abs/2010.12309">A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios</a>
<pre>深层神经网络和庞大的语言模型在自然语言应用中无处不在。众所周知，它们需要大量的培训数据，因此，在低资源环境下，提高性能的工作越来越多。受最近神经模型的基本变化和流行的预训练和微调范式的推动，我们综述了低资源自然语言处理的有希望的方法。在讨论了数据可用性的不同维度之后，我们对在训练数据稀疏时支持学习的方法进行了结构化概述。这包括创建附加标记数据的机制，如数据增强和远程监控，以及减少目标监控需求的转移学习设置。我们调查的目的是解释这些方法在需求方面的差异，因为了解它们对于选择适合特定低资源环境的技术至关重要。这项工作的进一步关键方面是突出未决问题，并概述未来研究的前景。</pre></li>
<li><a href="https://arxiv.org/abs/2004.04721">Translation Artifacts in Cross-lingual Transfer Learning</a> (EMNLP2020)
<pre>人类和机器翻译在跨语言迁移学习中都扮演着核心角色：许多多语言数据集是通过专业翻译服务创建的，使用机器翻译翻译测试集或训练集是一种广泛使用的迁移技术。在这篇文章中，我们展示了这样的翻译过程可以引入微妙的工件，这些工件在现有的跨语言模型中有着显著的影响。例如，在自然语言推理中，独立地翻译前提和假设可以减少它们之间的词汇重叠，而目前的模型对此非常敏感。我们发现，鉴于这一现象，跨语言迁移学习的一些研究结果需要重新考虑。基于所获得的见解，我们还将XNLI的翻译测试和零镜头方法的最新水平分别提高了4.3分和2.8分。</pre></li>
<li><a href="https://arxiv.org/abs/2004.04938">Identifying Cultural Differences through Multi-Lingual Wikipedia</a>
<pre>不同的文化或语言之间存在视角差异。不同群体对特定价值观或事件的观点缺乏相互理解可能会导致不知情的决定或偏见。自动理解组透视图可以为自然语言处理技术的许多下游应用提供必要的背景。在本文中，我们研究了共语群体，并使用语言语料库作为代理来确定他们的分布视角。我们提出了一种新的计算方法来学习共同的理解，并通过建立英语、汉语和日语的文化感知模型来测试我们的方法。在一系列不同的主题上，包括婚姻、腐败、民主，我们的模型与人类对群体内价值观和群体间差异的判断高度相关。</pre></li>
<li><a href="https://arxiv.org/abs/2004.14516">A Supervised Word Alignment Method based on Cross-Language Span Prediction using Multilingual BERT</a> (EMNLP2020)
<pre>提出了一种基于跨语言广度预测的有监督词对齐方法。我们首先将单词对齐问题形式化为从源句子中的标记到目标句子中的跨度的独立预测集合。由于这相当于一个班v2.0风格的问答任务，因此我们使用多语言BERT来解决这个问题，该BERT在手动创建的gold word对齐数据上进行了微调。通过在问题中添加标记的上下文，我们极大地提高了单词对齐的准确性。在使用汉语、日语、德语、罗马尼亚语、法语和英语的五个单词对齐数据集进行的实验中，我们表明，在不使用任何比特文本进行预训练的情况下，所提出的方法明显优于以前的有监督和无监督单词对齐方法。例如，我们的汉英数据F1得分为86.7，比之前最先进的监督方法高出13.3分。</pre></li>
<li><a href="https://arxiv.org/abs/2009.14790">BERT for Monolingual and Cross-Lingual Reverse Dictionary</a> (EMNLP2020 Findings)
<pre>反向词典的任务是根据单词描述找到合适的目标单词。在本文中，我们试图将BERT纳入这项任务中。然而，由于BERT是基于字节对编码（BPE）子字编码的，因此让BERT生成给定描述的字是非常重要的。我们提出了一种简单而有效的方法，让BERT为这个特定任务生成目标词。此外，跨语言反向词典的任务是找到用另一种语言描述的合适的目标词。以前的模型必须保留两个不同的单词嵌入，并学会对齐这些嵌入。然而，通过使用多语言BERT（Multilingual BERT，mBERT），我们可以有效地进行跨语言反向词典的一个子词嵌入，并且不需要语言之间的对齐。更重要的是，即使没有平行语料库，mBERT也可以实现显著的跨语言反向词典性能，这意味着它可以仅使用相应的单语数据来执行跨语言反向词典。该守则可于https://github.com/yhcc/BertForRD.git.</pre></li>
<li><a href="https://arxiv.org/abs/2004.14517">Bilingual Text Extraction as Reading Comprehension</a>
<pre>在本文中，我们提出了一种从有噪声的平行语料库中自动提取双语文本的方法，该方法将问题作为一个标记级的跨度预测，例如班式阅读理解。为了提取目标文档的范围，即给定源语句的翻译（span），我们使用QANet或多语言BERT。QANet可以从零开始针对特定的并行语料库进行训练，而多语言BET可以利用预先训练好的多语言表示。对于使用QANet的跨度预测方法，我们引入了一种使用整数线性规划的总体优化方法，以实现预测平行跨度的一致性。我们使用两个语言对（En-Fr和En-Ja）的模拟噪声并行语料库进行了并行句子提取实验，发现使用QANet的方法比使用两个双向RNN编码器的基线方法获得了更高的准确率，特别是对于远程语言对（En-Ja）。我们还使用En-Ja报纸文章进行了句子对齐实验，发现使用多语言BERT的方法比使用双语词典和动态规划的基线方法获得了更好的准确性。</pre></li>
<li><a href="https://arxiv.org/abs/2010.00454">Evaluating Multilingual BERT for Estonian</a>
<pre>最近，大型预先训练的语言模型，如BERT，在许多自然语言处理任务中已经达到了最先进的性能，但是对于许多语言，包括爱沙尼亚语，BERT模型还不可用。然而，存在几种可以同时处理多种语言的多语言BERT模型，并且这些模型也经过爱沙尼亚数据的训练。在本文中，我们评估了四种多语言模型——多语言BERT、多语言蒸馏BERT、XLM和XLM RoBERTa——在几个NLP任务上的性能，包括词性和词法标记、NER和文本分类。我们的目标是在这些多语言的BERT模型和这些任务的现有基线神经模型之间建立一个比较。我们的研究结果表明，多语言BERT模型在不同的爱沙尼亚NLP任务上都能很好地推广，在词性和词形标记以及文本分类方面优于所有基线模型，并且达到了与NER最佳基线相当的水平，与其他多语言模型相比，XLM RoBERTa取得了最高的结果。</pre></li>
<li><a href="https://arxiv.org/abs/2012.15613">How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models</a> (ACL2021) [<a href="https://github.com/Adapter-Hub/hgiyt">github</a>]
<pre>在这项工作中，我们提供了一个系统和全面的经验比较，预训练的多语言语言模型与他们的单语对应的单语任务性能。我们在一组五种不同的单语下游任务上研究了一组九种类型多样的语言，以及现成的预训练单语模型。首先，我们的目标是通过公平和受控的比较，确定该语言的多种语言和相应的单语表达之间是否存在差距，然后调查任何表现差异的原因。为了解开融合因素，我们在相同的数据上训练新的单语模型，使用单语和多语言训练的标记化器。我们发现，虽然预训练数据大小是一个重要因素，但指定的单语标记器在下游性能中起着同样重要的作用。我们的结果表明，在多语言模型的词汇表中充分表达的语言与单语对应的语言相比，表现出微不足道的性能下降。我们进一步发现，将原来的多语言标记器替换为专门的单语标记器可以提高几乎所有任务和语言的多语言模型的下游性能。</pre></li>
<li><a href="https://arxiv.org/abs/2109.07306">Allocating Large Vocabulary Capacity for Cross-lingual Language Model Pre-training</a> (EMNLP2021)
<pre>与单语模型相比，跨语言模型通常需要更具表现力的词汇表来充分表示所有语言。我们发现，由于词汇量有限，许多语言在最近的跨语言语言模型中的代表性不足。为此，我们提出了一种VoCap算法来确定每种语言所需的词汇量。然而，增加词汇量会显著降低训练前的速度。为了解决这个问题，我们提出了基于k-NN的目标采样来加速昂贵的softmax。我们的实验表明，使用VoCap学习的多语种词汇有利于跨语种语言模型的预训练。此外，基于k-NN的目标抽样减少了增加词汇量的副作用，同时实现了可比的性能和更快的训练前速度。代码和经过培训的多语言词汇表可在https://github.com/bozheng-hit/VoCapXLM.</pre></li>
<li><a href="https://www.aclweb.org/anthology/2021.adaptnlp-1.12/">BERTologiCoMix: How does Code-Mixing interact with Multilingual BERT?</a> (EACL2021 WS)</li>
</ul>
<h1 id="other-than-english-models">Other than English models</h1>
<ul>
<li><a href="https://arxiv.org/abs/1911.03894">CamemBERT: a Tasty French Language Model</a> (ACL2020)
<pre>在自然语言处理中，预训练语言模型是普遍存在的。尽管取得了成功，但大多数可用的模型要么接受了关于英语数据的培训，要么接受了关于以多种语言连接数据的培训。这使得在除英语以外的所有语言中实际使用此类模型非常有限。本文以法语为例，探讨了基于单语变换器的语言模型在其他语言中训练的可行性，并对我们的语言模型进行了词性标注、依存分析、命名实体识别和自然语言推理任务的评估。我们表明，使用网络爬网数据比使用维基百科数据更可取。更令人惊讶的是，我们发现一个相对较小的网络爬网数据集（4GB）所得到的结果与使用较大数据集（130+GB）得到的结果一样好。我们的最佳表现模型CamemBERT在所有四项下游任务中都达到或改进了最先进的水平。</pre></li>
<li><a href="https://arxiv.org/abs/2010.03813">On the importance of pre-training data volume for compact language models</a> (EMNLP2020)
<pre>语言建模的最新进展导致了计算密集型和资源需求型的最新模型。为了实现可持续的实践，我们研究了训练前数据量对紧凑语言模型的影响。根据逐渐增加的法语文本量训练多个基于BERT的模型。通过对法语问答数据集（FQuAD）进行微调，我们观察到只需100 MB的文本即可获得性能良好的模型。此外，我们还表明，过去极少量的训练前数据，在特定任务语料库上进行中间的训练前步骤不会产生实质性的改善。</pre></li>
<li><a href="https://arxiv.org/abs/1912.05372">FlauBERT: Unsupervised Language Model Pre-training for French</a> (LREC2020)
<pre>在许多不同的自然语言处理（NLP）任务中，语言模型已经成为实现最先进结果的关键步骤。利用目前大量的未标记文本，它们提供了一种有效的方法来预训练连续单词表示，可以针对下游任务进行微调，并在句子层面进行语境化。这一点已在英语中得到广泛证明，使用语境化表达（Dai和Le，2015；Peters等人，2018；Howard和Ruder，2018；Radford等人，2018；Devlin等人，2019；Yang等人，2019b）。在本文中，我们介绍并分享了福楼拜，这是一个在非常大且异构的法语语料库上学习的模型。不同尺寸的模型使用新的CNRS（法国国家科学研究中心）Jean Zay超级计算机进行训练。我们将我们的法语模型应用于不同的NLP任务（文本分类、释义、自然语言推理、解析、词义消歧），并表明它们在大多数情况下都优于其他预训练方法。福楼拜的不同版本以及下游任务的统一评估协议，称为烟道（法语理解评估），共享给研究社区，以便在法语NLP中进行进一步的重复性实验。</pre></li>
<li><a href="https://arxiv.org/abs/1912.07076">Multilingual is not enough: BERT for Finnish</a>
<pre>基于深度学习的语言模型在大型非注释文本语料库上进行预训练，已被证明能够为自然语言处理提供有效的迁移学习，最近的方法，如基于转换器的伯特模型，推动了各种任务的最新发展。虽然大多数关于这些模型的工作都集中在高资源语言上，特别是英语，但最近的一些工作引入了多语言模型，可以对这些模型进行微调，以处理大量不同语言的任务。然而，我们仍然缺乏对这些模型功能的透彻理解，尤其是对于资源较低的语言。本文以芬兰语为研究对象，对一系列任务中的多语言BERT模型进行了全面的评估，并将其与从头开始训练的新的芬兰语BERT模型进行了比较。新的特定于语言的模型被证明系统地、明显地优于多语言模型。虽然多语言模型在很大程度上无法达到先前提出的方法的性能，但定制的芬兰BERT模型为所有参考任务的所有语料库建立了新的最新结果：词性标记、命名实体识别和依赖项解析。我们发布了模型以及为本研究创建的所有相关资源，并在https://turkunlp.org/finbert .</pre></li>
<li><a href="https://arxiv.org/abs/1912.09582">BERTje: A Dutch BERT Model</a>
<pre>基于转换器的预训练语言模型BERT有助于提高许多自然语言处理（NLP）任务的最新性能。使用相同的架构和参数，我们开发并评估了一个名为BERTje的单语荷兰语BERT模型。与包括荷兰语但仅基于维基百科文本的多语言BERT模型相比，BERTje基于一个包含24亿代币的大型多样化数据集。BERTje在下游NLP任务（词性标注、命名实体识别、语义角色标注和情感分析）上始终优于同等大小的多语言BERT模型。我们预先培训的荷兰伯特模型在https://github.com/wietsedv/bertje.</pre></li>
<li><a href="https://arxiv.org/abs/2001.06286">RobBERT: a Dutch RoBERTa-based Language Model</a>
<pre>近年来，预训练语言模型在自然语言处理领域占据主导地位，并为各种复杂的自然语言任务带来了显著的性能提升。最突出的预训练语言模型之一是BERT，它以英语和多语言版本发布。尽管多语种的BERT在许多任务中表现良好，但最近的研究表明，在单一语言上训练的BERT模型明显优于多语种版本。因此，训练荷兰伯特模型对于荷兰NLP任务的广泛应用具有很大的潜力。以前的方法使用了早期的BERT实现来训练荷兰语版本的BERT，而我们使用RoBERTa（一种经过稳健优化的BERT方法）来训练一个名为robert的荷兰语模型。我们测量了它在各种任务上的性能以及微调数据集大小的重要性。我们还评估了特定语言标记器的重要性和模型的公平性。我们发现RobBERT改进了各种任务的最新结果，尤其是在处理较小数据集时，它的性能明显优于其他模型。这些结果表明，它是一个强大的预训练模型，适用于各种各样的荷兰语任务。预先训练和微调的模型公开提供，以支持进一步的荷兰NLP下游应用。</pre></li>
<li><a href="https://arxiv.org/abs/1905.07213">Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language</a>
<pre>本文介绍了一种针对特定语言的多语言屏蔽语言模型的自适应方法。预先训练的双向语言模型在阅读理解、自然语言推理和情感分析等广泛任务中表现出最先进的性能。目前有两种不同的方法来训练这样的模型：单语和多语。虽然特定于语言的模型表现出优异的性能，但多语言模型允许执行从一种语言到另一种语言的转换，并同时解决不同语言的任务。这项工作表明，从多语言模式到单语模式的迁移学习可以显著提高阅读理解、释义检测和情绪分析等任务的成绩。此外，单语模型的多语言初始化大大减少了训练时间。俄语的预先培训模型是开源的。</pre></li>
<li><a href="https://arxiv.org/abs/2010.15925">RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark</a> (EMNLP2020)
<pre>在本文中，我们介绍了一个先进的俄语通用语言理解评估基准——RussianGLUE。通用语言模型和变形金刚领域的最新进展要求开发一种方法，用于其广泛的诊断和测试一般智力技能——检测自然语言推理、常识推理、执行简单逻辑运算的能力，无论文本主题或词汇如何。第一次，从零开始为俄语开发了一个九项任务的基准，收集和组织类似于SuperGLUE方法。我们提供基线、人类级别的评估，一个用于评估模型的开源框架(https://github.com/RussianNLP/RussianSuperGLUE)，以及俄语变压器型号的总体排行榜。此外，我们还介绍了在自适应诊断测试集中比较多语言模型的初步结果，并提供了进一步扩展或评估独立于语言的最新模型的第一步。</pre></li>
<li><a href="https://arxiv.org/abs/2003.00104">AraBERT: Transformer-based Model for Arabic Language Understanding</a>
<pre>阿拉伯语是一种形态丰富的语言，与英语相比，资源相对较少，句法探索较少。鉴于这些局限性，阿拉伯语自然语言处理（NLP）任务，如情感分析（SA）、命名实体识别（NER）和问答（QA），已被证明是非常具有挑战性的。最近，随着基于transformers的模型的激增，基于特定语言的BERT模型已被证明在语言理解方面非常有效，只要它们是在非常大的语料库上预先训练的。这些模型能够为大多数NLP任务设定新的标准并实现最先进的结果。在本文中，我们专门针对阿拉伯语对BERT进行了预训练，以期获得与英语相同的成功。AraBERT的性能与来自Google和其他最先进方法的多语言BERT进行了比较。结果表明，新开发的AraBERT在大多数经过测试的阿拉伯语NLP任务中都达到了最先进的性能。预训练的araBERT模型在https://github.com/aub-mind/arabert 希望鼓励阿拉伯语NLP的研究和应用。</pre></li>
<li><a href="https://aclanthology.org/2021.wanlp-1.18/">ALUE: Arabic Language Understanding Evaluation</a> (EACL2021 WS) [<a href="https://www.alue.org/home">website</a>]</li>
<li><a href="https://aclanthology.org/2021.acl-long.551/">ARBERT &amp; MARBERT: Deep Bidirectional Transformers for Arabic</a> (ACL2021) [<a href="https://github.com/UBC-NLP/marbert">github</a>]</li>
<li><a href="https://arxiv.org/abs/2102.10684">Pre-Training BERT on Arabic Tweets: Practical Considerations</a>
<pre>为下游NLP任务从转换器（BERT）预训练双向编码器表示是一项非繁琐的任务。我们对5个伯特模型进行了预训练，这些模型在训练集的大小、正式和非正式阿拉伯语的混合以及语言预处理方面有所不同。所有这些都旨在支持阿拉伯语方言和社交媒体。这些实验强调了数据多样性的中心性和语言感知分割的有效性。他们还强调，更多的数据或更多的训练步骤并不需要更好的模型。我们的新模型在几个下游任务上取得了最新的成果。生成的模型以QARiB的名义发布给社区。</pre></li>
<li><a href="https://arxiv.org/abs/2003.00744">PhoBERT: Pre-trained language models for Vietnamese</a>
<pre>我们为PhoBERT提供了两个版本，PhoBERT base和PhoBERT large，这是第一个为越南语预先训练的公共大规模单语语言模型。实验结果表明，PhoBERT始终优于最近最好的预训练多语言模型XLM-R（Conneau et al.，2020），并在多个越南特定NLP任务（包括词性标注、依存分析、命名实体识别和自然语言推理）中提高了最新水平。我们发布PhoBERT以促进越南NLP的未来研究和下游应用。我们的PhoBERT型号可在https://github.com/VinAIResearch/PhoBERT</pre></li>
<li><a href="https://arxiv.org/abs/2004.00033">Give your Text Representation Models some Love: the Case for Basque</a> (LREC2020)
<pre>单词嵌入和预先训练的语言模型允许构建丰富的文本表示，并在大多数NLP任务中实现了改进。不幸的是，培训成本非常高，许多小公司和研究小组倾向于使用经过预培训并由第三方提供的模型，而不是构建自己的模型。这是次优的，因为对于许多语言来说，模型都是在较小（或较低质量）的语料库上训练的。此外，非英语语言的单语预先培训模型并不总是可用的。充其量，这些语言的模型包含在多语言版本中，其中每种语言与其他语言共享子字符串和参数的配额。对于较小的语言，如巴斯克语，情况尤其如此。在这篇论文中，我们展示了一些单语模型（FastText单词嵌入、FLAIR和BERT语言模型）使用较大的巴斯克语料库进行训练，在下游NLP任务中，包括主题分类、情感分类、词性标记和NER，产生了比公开版本更好的结果。这项工作为巴斯克的这些任务设定了新的技术水平。这项工作中使用的所有基准和模型都是公开的。</pre></li>
<li><a href="https://arxiv.org/abs/2005.12515">ParsBERT: Transformer-based Model for Persian Language Understanding</a>
<pre>经过预训练的语言模型的激增已经开启了自然语言处理（NLP）领域的一个新时代，它允许我们构建强大的语言模型。在这些模型中，基于变压器的模型（如BERT）由于其最先进的性能而越来越受欢迎。然而，这些模型通常集中在英语上，将其他语言留给资源有限的多语言模型。本文提出了一种用于波斯语的单语BERT（ParsBERT），与其他体系结构和多语言模型相比，它显示了其最先进的性能。此外，由于波斯语中NLP任务可用的数据量非常有限，因此为不同的NLP任务以及模型的预训练组成了大量数据集。ParsBERT在所有数据集（包括现有数据集和合成数据集）中获得更高的分数，并通过在情感分析、文本分类和命名实体识别任务方面优于多语言BERT和其他先前的工作，提高了最先进的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2012.11204">Leveraging ParsBERT and Pretrained mT5 for Persian Abstractive Text Summarization</a> (CSICC2021)
<pre>文本摘要是自然语言处理中最关键的任务之一。每天都有越来越多的研究在这个领域进行。预先训练的基于变压器的编码器-解码器模型已经开始在这些任务中流行起来。本文提出了两种解决这一问题的方法，并介绍了一种用于波斯语摘要的新数据集pn summary。本文采用的模型是mT5和ParsBERT模型的编译码版本（即，波斯语的单语BERT模型）。这些模型在pn摘要数据集上进行了微调。目前的工作是此类工作中的第一项，通过取得有希望的成果，可以作为今后任何工作的基准。</pre></li>
<li><a href="https://arxiv.org/abs/2006.04229">Pre-training Polish Transformer-based Language Models at Scale</a>
<pre>基于转换器的语言模型目前广泛应用于自然语言处理（NLP）。对于英语来说，这一说法尤其正确，近年来，许多使用基于变压器的体系结构的预训练模型已经发布。这推动了各种标准NLP任务（如分类、回归和序列标记）以及文本到文本任务（如机器翻译、问答或摘要）的发展。然而，对于资源不足的语言，如波兰语，情况就不同了。尽管有一些基于transformer的波兰语语言模型可用，但就语料库规模和参数数量而言，没有一个能达到最大英语语言模型的规模。在这项研究中，我们提出了两种基于流行的伯特体系结构的波兰语语言模型。更大的模型是在一个由超过10亿个波兰句子或135GB原始文本组成的数据集上训练的。我们描述了收集数据、准备语料库和预训练模型的方法。然后，我们对13项波兰语言任务评估了我们的模型，并在其中11项任务中展示了与之前方法相比的改进。</pre></li>
<li><a href="https://arxiv.org/abs/2007.01658">Playing with Words at the National Library of Sweden – Making a Swedish BERT</a>
<pre>本文介绍了瑞典国家图书馆（KB）数据驱动研究KBLab开发的瑞典BERT（“KB-BERT”）。基于最近为英语以外的语言创建基于转换器的BERT模型的努力，我们将解释如何使用KB的集合为瑞典语创建和训练新的特定于语言的BERT模型。我们还展示了我们的模型与现有模型（主要由瑞典公共就业服务局、Arbetsf \“ormedlingen和Google的多语言M-BERT生成）的比较结果，其中我们证明KB-BERT在从命名实体识别（NER）到词性标记（POS）的一系列NLP任务中都优于这些模型。我们的讨论强调了由于缺乏针对小型语言（如瑞典语）的培训数据和测试平台，仍然存在的困难。我们发布了我们的模型，以供进一步探索和研究：https://github.com/Kungbib/swedish-bert-models .</pre></li>
<li><a href="https://arxiv.org/abs/2008.03979">KR-BERT: A Small-Scale Korean-Specific Language Model</a>
<pre>自BERT出现以来，包括XLNet和RoBERTa在内的最新作品利用了大量语料库和大量参数预先训练的句子嵌入模型。由于此类模型具有大型硬件和大量数据，因此需要很长时间进行预训练。因此，尝试制作性能相对较好的小型模型非常重要。在本文中，我们利用较小的词汇表和数据集训练了一个韩国人特有的模型KR-BERT。由于朝鲜语是形态丰富的语言之一，使用非拉丁字母的资源很差，因此捕捉多语BERT模型遗漏的特定语言现象也很重要。我们测试了几个标记器，包括我们的双向字块标记器，并调整了从子字符级到字符级标记化的最小标记跨度，以便为我们的模型构建更好的词汇表。通过这些调整，我们的KR-BERT模型的表现相当，甚至比使用约1/10语料库的其他现有预训练模型更好。</pre></li>
<li><a href="https://arxiv.org/abs/2101.11363">KoreALBERT: Pretraining a Lite BERT Model for Korean Language Understanding</a> (ICPR2020)
<pre>Lite-BERT（ALBERT）被引入到自然语言的深度双向表征学习中。由于缺乏针对韩国语的预训练的ALBERT模型，最好的可用实践是多语言模型或求助于任何其他基于BERT的模型。在本文中，我们开发并预训练KoreALBERT，一个专门用于韩语理解的单语ALBERT模型。我们引入了一个新的训练目标，即语序预测（WOP），并将其与现有的MLM和SOP标准一起用于相同的体系结构和模型参数。尽管模型参数明显较少（因此训练速度更快），但我们的预训练KoreALBERT在6种不同NLU任务上的表现优于其对应的BERT。与Lan等人在英语中的实证结果一致，KoreALBERT似乎改善了涉及韩语多句编码的下游任务绩效。预先培训的KoreALBERT可公开获取，以鼓励韩国NLP的研究和应用开发。</pre></li>
<li><a href="https://arxiv.org/abs/2109.04650">What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers</a> (EMNLP2021)
<pre>GPT-3显示了在数千亿规模数据上训练的大规模语言模型（LMs）显著的上下文学习能力。在这里，我们讨论了GPT-3论文中较少报道的一些遗留问题，如非英语LM、不同大小模型的性能以及最近引入的即时优化对上下文学习的影响。为了实现这一点，我们引入HyperCLOVA，一种82B GPT-3的韩国语变体，在以韩国人为中心的560B标记语料库上进行训练。通过韩语特有的标记化，HyperCLOVA和我们的培训配置在韩语的各种下游任务中显示了最先进的零射击和少量射击学习性能。此外，我们还展示了基于即时学习的性能优势，并演示了如何将其集成到即时工程管道中。然后，我们讨论了通过引入HyperCLOVA studio（一种交互式即时工程界面），向非ML专家提供AI原型功能，实现无代码AI范式的可能性。最后，我们通过三个成功的内部应用展示了我们方法的潜力。</pre></li>
<li><a href="https://arxiv.org/abs/2105.09680">KLUE: Korean Language Understanding Evaluation</a>
<pre>我们介绍了韩语理解评估（KLUE）基准。KLUE是一个由8项韩国语自然语言理解（NLU）任务组成的集合，包括主题分类、语义-文本相似度、自然语言推理、命名实体识别、关系提取、依存分析、机器阅读理解和对话状态跟踪。在尊重版权的同时，我们从零开始从不同来源的语料库构建所有任务，以确保任何人都可以不受任何限制地访问。出于道德考虑，我们仔细设计注释协议。除了基准任务和数据外，我们还为每个任务的预训练语言模型提供合适的评估指标和微调方法。此外，我们还发布了预训练语言模型（PLM），KLUE-BERT和KLUE-RoBERTa，以帮助在KLUE上复制基线模型，从而促进未来的研究。我们从使用建议的KLUE基准套件的初步实验中得出了一些有趣的观察结果，已经证明了这个新基准套件的有用性。首先，我们发现KLUE RoBERTa large优于其他基线，包括多语言PLM和现有的开源韩国PLM。第二，即使我们从训练前的语料库中替换个人识别信息，我们也能看到性能的最小下降，这表明隐私和NLU能力并不矛盾。最后，我们发现在涉及语素级标记、检测和生成的任务中，将BPE标记化与语素级预标记化结合使用是有效的。除了加速韩国NLP研究，我们关于创建KLUE的全面文档将有助于将来为其他语言创建类似的资源。KLUE可在https://klue-benchmark.com.</pre></li>
<li><a href="https://arxiv.org/abs/2101.09635">WangchanBERTa: Pretraining transformer-based Thai Language Models</a>
<pre>基于Transformer的语言模型，更具体地说是基于BERT的体系结构，在许多下游任务中都实现了最先进的性能。然而，对于资源相对较低的语言（如泰语），模型的选择仅限于基于小得多的数据集训练基于BERT的模型或微调多语言模型，两者都会产生次优的下游性能。此外，大规模的多语种预培训没有考虑到泰语的语言特点。为了克服这些限制，我们在一个大的、消除重复的、干净的培训集（总大小为78GB）上预先培训了一个基于RoBERTa base体系结构的语言模型，该培训集由社交媒体帖子、新闻文章和其他公开可用数据集的不同领域策划。我们应用特定于泰语的文本处理规则，最重要的是保留空格，在子词标记化之前，空格是泰语中重要的块和句子边界。我们还使用较小的数据集对单词级、音节级和句子片段标记化进行了实验，以探索标记化对下游性能的影响。我们的模型wangchanberta base att spm在78.5GB数据集上未经训练，在人类注释的单语言环境中，在序列分类和标记分类任务方面优于强基线（NBSVM、CRF和ULMFit）和多语言模型（XLMR和mBERT）。</pre></li>
<li><a href="https://arxiv.org/abs/2006.07890">FinEst BERT and CroSloEngual BERT: less is more in multilingual models</a> (TSD2020)
<pre>大型预训练蒙面语言模型已经成为许多NLP问题的最新解决方案。不过，这项研究主要集中在英语方面。虽然存在大量多语言模型，但研究表明，单语模型产生的结果要好得多。我们训练两个三语类伯特模型，一个是芬兰语、爱沙尼亚语和英语，另一个是克罗地亚语、斯洛文尼亚语和英语。我们使用多语言BERT和XLM-R作为基线，评估它们在几个下游任务（NER、词性标记和依赖项解析）上的性能。新创建的BERT和CROSLONGUAL BERT在大多数单语和跨语言情况下改善了所有任务的结果</pre></li>
<li><a href="https://arxiv.org/abs/2008.12014">GREEK-BERT: The Greeks visiting Sesame Street</a> (SETN2020)
<pre>基于转换器的语言模型，如BERT及其变体，在通用基准数据集（如GLUE、SQUAD、RACE）的几个下游自然语言处理（NLP）任务中取得了最先进的性能。然而，这些模型大多应用于资源丰富的英语。在本文中，我们介绍了希腊文-BERT，一种基于单语BERT的现代希腊文语言模型。我们评估了它在三个NLP任务中的性能，即词性标注、命名实体识别和自然语言推理，获得了最先进的性能。有趣的是，在两个基准测试中，希腊文-BERT比两个基于多语言转换器的模型（M-BERT、XLM-R）以及在预先训练的单词嵌入上运行的较浅的神经基线表现出色（5%-10%）。最重要的是，我们公开了GREEK-BERT和我们的训练代码，以及说明如何为下游NLP任务微调GREEK-BERT的代码。我们期望这些资源能够促进现代希腊语的NLP研究和应用。</pre></li>
<li><a href="https://arxiv.org/abs/2009.08712">The birth of Romanian BERT</a> (EMNLP2020 Findings)
<pre>大规模预训练语言模型已成为自然语言处理中的普遍现象。然而，这些模型中的大多数要么以高资源语言（特别是英语）提供，要么以多语言模型的形式提供，这会降低单个语言的覆盖率。本文介绍了罗马尼亚语BERT，这是第一个在大型文本语料库上预训练的纯罗马尼亚语基于变压器的语言模型。我们讨论了语料库的组成和清理，模型的训练过程，以及在各种罗马尼亚数据集上对模型的广泛评估。我们不仅开放了模型本身的源代码，而且还开放了一个存储库，其中包含有关如何获取语料库、如何微调并在生产中使用该模型（带有实际示例）以及如何完全复制评估过程的信息。</pre></li>
<li><a href="https://arxiv.org/abs/2010.10906">German’s Next Language Model</a> (COLING2020 Industry Truck)
<pre>在这项工作中，我们介绍了创建基于伯特和伊莱克特拉的德语模型GBERT和GELECTRA的实验。通过改变输入训练数据、模型大小和全词掩蔽（WWM）的存在，我们能够在一组文档分类和命名实体识别（NER）任务中实现SoTA性能，适用于基本和大型模型。我们采用评估驱动的方法来训练这些模型，我们的结果表明，添加更多数据和利用WWM都可以提高模型性能。通过对现有德国模型的基准测试，我们表明这些模型是迄今为止最好的德国模型。我们经过培训的模型将向研究界公开。</pre></li>
<li><a href="https://arxiv.org/abs/2012.02110">GottBERT: a pure German Language Model</a>
<pre>最近，预先训练的语言模型推动了自然语言处理（NLP）领域的发展。变压器双向编码器（BERT）及其优化版RoBERTa的引入产生了重大影响，并增加了预训练模型的相关性。首先，该领域的研究主要是从英语数据开始的，其次是用多语种文本语料库训练的模型。然而，目前的研究表明，多语言模型不如单语模型。目前，还没有出版德语单语言RoBERTa模型，这是我们在这项工作中介绍的（GottBERT）。奥斯卡数据集中的德语部分被用作文本语料库。在评估中，我们将其在两个命名实体识别（NER）任务Conll 2003和GermEval 2014以及文本分类任务GermEval 2018（精细和粗糙）和GNAD上的性能与现有的德语单语言BERT模型和两个多语言BERT模型进行了比较。使用fairseq对GottBERT进行了与原始RoBERTa模型相关的预培训。所有下游任务都使用德国BERT基准中的超参数预设进行训练。实验是利用农场进行的。绩效以$F{1}$分数衡量。GottBERT使用RoBERTa基地架构成功地在256核TPU吊舱上进行了预训练。即使没有广泛的超参数优化，在所有的NER和一个文本分类任务中，GottBERT已经超过了所有其他经过测试的德语和多语言模型。为了支持德国NLP字段，我们在AGPLv3许可下发布了GottBERT。</pre></li>
<li><a href="https://arxiv.org/abs/2011.04784">EstBERT: A Pretrained Language-Specific BERT for Estonian</a>
<pre>本文介绍了EstBERT，一个针对爱沙尼亚语的基于大型预训练转换器的语言特定的BERT模型。最近的工作评估了爱沙尼亚任务的多语言BERT模型，发现它们优于基线。然而，基于对其他语言的现有研究，语言特定的BERT模型有望比多语言模型有所改进。我们首先描述了EstBERT预训练过程，然后给出了基于微调EstBERT的多NLP任务模型的结果，包括词性和形态标记、命名实体识别和文本分类。评估结果表明，基于EstBERT的模型在六项任务中的五项任务上优于多语言BERT模型，这进一步证明了训练特定语言的BERT模型仍然有用，即使多语言模型可用。</pre></li>
<li><a href="https://arxiv.org/abs/2103.13031">Czert – Czech BERT-like Model for Language Representation</a>
<pre>本文描述了第一个基于BERT和ALBERT体系结构的捷克单语语言表示模型的训练过程。我们对我们的模型进行了340K多个句子的预训练，这是包含捷克数据的多语言模型的50倍。在11个数据集中，我们有9个优于多语言模型。此外，我们在九个数据集上建立了最新的最新结果。最后，我们根据我们的结果讨论了单语和多语模型的性质。我们为研究社区免费发布所有经过预培训和微调的模型。</pre></li>
<li><a href="https://arxiv.org/abs/2105.11314">RobeCzech: Czech RoBERTa, a monolingual contextualized language representation model</a> (TSD2021)
<pre>我们提出了一个基于捷克语数据的单语罗伯塔语言表示模型。RoBERTa是一种基于变压器的稳健优化预训练方法。我们表明，RobeCzech显著优于同等规模的多语言和捷克训练的情境化语言表示模型，在所有五项评估NLP任务中都超过了当前的水平，并且在其中四项任务中达到了最先进的结果。该模型在以下网站公开发布：https://hdl.handle.net/11234/1-3691 和https://huggingface.co/ufal/robeczech-base.</pre></li>
<li><a href="https://arxiv.org/abs/2103.13799">Bertinho: Galician BERT Representations</a>
<pre>本文提出了加利西亚语的单语伯特模型。我们遵循最近的趋势，即即使对于资源相对较低的语言，也可以建立健壮的单语BERT模型，同时比著名的官方多语言BERT（mBERT）性能更好。更具体地说，我们发布了两个单语加利西亚伯特模型，分别使用6层和12层变压器构建；使用有限的资源进行培训（单个24GB GPU上约4500万个令牌）。然后，我们对一些任务进行了详尽的评估，如词性标记、依赖项解析和命名实体识别。为此，所有这些任务都在纯序列标签设置中转换，以便运行BERT，而无需在其上包含任何附加层（我们仅使用输出分类层将上下文表示映射到预测标签）。实验表明，我们的模型，特别是12层模型，在大多数任务中都优于mBERT的结果。</pre></li>
<li><a href="https://arxiv.org/abs/2010.12401">Pretraining and Fine-Tuning Strategies for Sentiment Analysis of Latvian Tweets</a>
<pre>在本文中，我们提出了各种预训练策略，有助于提高情绪分类任务的准确性。首先，我们使用这些策略预先训练语言表达模型，然后在下游任务中对其进行微调。在时间平衡的tweet评估上的实验结果表明，与以前的技术相比，改进是有效的。我们在拉脱维亚推特上实现了76%的真实分析准确率，这是对以前工作的重大改进</pre></li>
<li><a href="https://arxiv.org/abs/2008.09144">PTT5: Pretraining and validating the T5 model on Brazilian Portuguese data</a>
<pre>在自然语言处理（NLP）中，需要更多葡萄牙语资源，因为在最新研究中使用的大部分数据都是其他语言。在本文中，我们在BrWac语料库上预训练了一个T5模型，这是一个广泛的葡萄牙语网页集合，并在三个不同任务上与其他葡萄牙语预训练模型和多语言模型进行了性能评估。我们表明，我们的葡萄牙预训练车型比原T5车型具有更好的性能。此外，我们还展示了使用葡萄牙语词汇的积极影响。我们的代码和模型可在https://github.com/unicamp-dl/PTT5.</pre></li>
<li><a href="https://aclanthology.org/2020.findings-emnlp.445/">IndicNLPSuite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for Indian Languages</a> (EMNLP2020 Findings)</li>
<li><a href="https://arxiv.org/abs/2011.02323">Indic-Transformers: An Analysis of Transformer Language Models for Indian Languages</a> (NeurIPS2020 WS)
<pre>基于Transformer体系结构的语言模型在广泛的NLP任务（如文本分类、问答和令牌分类）上取得了最先进的性能。但是，这种性能通常在高资源语言（如英语、法语、西班牙语和德语）上进行测试和报告。另一方面，印度语言在这些基准中的代表性不足。尽管多语言变压器模型培训中包含了一些印度语言，但这些语言并不是此类工作的主要重点。为了具体评估印度语言的性能，我们通过大量的印地语、孟加拉语和泰卢固语下游任务的实验来分析这些语言模型。在这里，我们比较了微调预训练模型的模型参数与从头开始训练语言模型的效果。此外，我们从经验上反对数据集大小和模型性能之间的严格依赖关系，而是鼓励任务特定的模型和方法选择。对于文本分类任务，我们在印地语和孟加拉语上实现了最先进的性能。最后，我们提出了处理印度语言建模的有效策略，并为社区发布了模型检查点：https://huggingface.co/neuralspace-reverie.</pre></li>
<li><a href="https://www.aclweb.org/anthology/2020.coling-main.66/">IndoLEM and IndoBERT: A Benchmark Dataset and Pre-trained Language Model for Indonesian NLP</a> (COLING2020)</li>
<li><a href="https://arxiv.org/abs/2109.04607">IndoBERTweet: A Pretrained Language Model for Indonesian Twitter with Effective Domain-Specific Vocabulary Initialization</a> (EMNLP2021)
<pre>我们介绍了IndoBERTweet，这是第一个印尼Twitter的大规模预训练模型，它是通过扩展一个单语训练的印尼BERT模型和附加的特定领域词汇来训练的。我们特别关注词汇失配下的有效模型调整，并对新单词类型初始化BERT嵌入层的不同方法进行基准测试。我们发现，在七个基于Twitter的数据集上，使用平均BERT子词嵌入进行初始化使预训练速度提高了五倍，并且在外部评估方面比提出的词汇适应方法更有效。</pre></li>
<li><a href="https://arxiv.org/abs/2104.08200">IndoNLG: Benchmark and Resources for Evaluating Indonesian Natural Language Generation</a> (EMNLP2021)
<pre>自然语言生成（Natural language generation，NLG）基准为衡量进展和开发更好的NLG系统提供了重要途径。不幸的是，低资源语言缺乏公开可用的NLG基准，这对构建适用于数据量有限的语言的NLG系统构成了一个具有挑战性的障碍。在这里，我们将介绍IndoNLG，这是第一个衡量印尼三种低资源语言（印尼语、爪哇语和巽他语）自然语言生成（NLG）进展的基准。总的来说，超过1亿母语人士使用这些语言，因此构成了当今NLG系统的一个重要用例。具体来说，IndoNLG包含六项任务：摘要、问答、聊天和三对不同的机器翻译（MT）任务。我们整理了印尼语、巽他语和爪哇语数据集的干净预训练语料库Indo4B Plus，用于预训练我们的模型：IndoBART和IndoGPT。我们发现，IndoBART和IndoGPT在所有任务上都取得了具有竞争力的性能——尽管只使用了更大的多语言模型mBART LARGE的五分之一参数（Liu等人，2020年）。这一发现强调了对密切相关的本地语言进行预培训的重要性，以实现对爪哇语和巽他语等资源极低的语言的更有效学习和更快推理。</pre></li>
<li><a href="https://arxiv.org/abs/2109.04715">AfroMT: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages</a> (EMNLP2021)
<pre>可复制基准是推动机器翻译研究进展的关键。然而，现有的机器翻译基准大多局限于高资源或具有良好代表性的语言。尽管人们对低资源机器翻译越来越感兴趣，但许多非洲语言都没有标准化的可复制基准，其中许多语言被数百万人使用，但文本数据数字化程度较低。为了应对这些挑战，我们提出了AfroMT，这是一个标准化、干净、可复制的机器翻译基准，适用于八种广泛使用的非洲语言。考虑到这些语言的独特特性，我们还开发了一套用于系统诊断的分析工具。此外，我们探讨了新考虑的低资源集中预训练的情况，并开发了两种新的基于数据增强的策略，利用单词级对齐信息和伪单语数据对多语言序列到序列模型进行预训练。在对11种语言进行预训练时，我们展示了显著的改进，与强基线相比，获得了高达2个BLEU点。我们还展示了在数据受限的情况下，与跨语言传输基线相比，最多可获得12个BLEU点。所有代码和预先训练的模型都将发布，作为实现更大的非洲语言可复制基准的进一步步骤。</pre></li>
<li><a href="https://arxiv.org/abs/2203.08459">KinyaBERT: a Morphology-aware Kinyarwanda Language Model</a> (ACL2022)
<pre>像BERT这样的预训练语言模型已经成功地处理了许多自然语言处理任务。然而，这些模型中常用的无监督子词标记化方法（例如，字节对编码-BPE）在处理形态丰富的语言时是次优的。即使有一个形态分析器，将语素简单地排序到标准的BERT架构中，在捕获形态组成性和表达单词相对句法规则方面也是效率低下的。我们通过提出一个简单而有效的两层BERT体系结构来解决这些挑战，该体系结构利用形态分析器并明确表示形态组合性。尽管BERT取得了成功，但它的大多数评估都是在高资源语言上进行的，模糊了它在低资源语言上的适用性。我们在低资源形态丰富的基尼亚卢旺达语上评估了我们提出的方法，并将所提出的模型架构命名为KinyaBERT。一组稳健的实验结果表明，KinyaBERT在命名实体识别任务中的F1成绩优于坚实基线2%，在机器翻译的GLUE基准测试中的平均成绩优于坚实基线4.3%。KinyaBERT微调具有更好的收敛性，即使在存在平移噪声的情况下，也能在多个任务上获得更稳健的结果。</pre></li>
<li><a href="https://arxiv.org/abs/2010.12321">BARThez: a Skilled Pretrained French Sequence-to-Sequence Model</a>
<pre>归纳迁移学习已经席卷了整个自然语言处理领域，伯特和巴特等模型为无数自然语言处理任务创造了新的技术水平。然而，大多数可用的模型和研究都是针对英语进行的。在这项工作中，我们介绍了BARThez，法国第一个大规模预训练seq2seq模型。基于BART，BARThez特别适合于生成性任务。我们评估了BARThez在烟道基准测试中的五项区别性任务和我们为本研究创建的一个新的摘要数据集OrangeSum中的两项生成性任务。我们展示了BARThez与最先进的基于BERT的法语模型（如CamemBERT和FlauBERT）的竞争力。我们还继续在BARThez的语料库上进行多语言BART的预训练，并展示我们的结果模型mBARThez，以显著提高BARThez的生成性能。代码、数据和模型是公开的。</pre></li>
<li><a href="https://arxiv.org/abs/1909.00204">NEZHA: Neural Contextualized Representation for Chinese Language Understanding</a>
<pre>经过预训练的语言模型在各种自然语言理解（NLU）任务中取得了巨大的成功，因为它能够通过对大规模语料库的预训练捕获文本中的深层语境信息。在本技术报告中，我们介绍了我们在中文语料库上对名为NEZHA（中文理解的神经语境化表示）的语言模型进行预训练的实践，以及对中文NLU任务进行微调的实践。NEZHA的当前版本基于BERT，并有一系列经过验证的改进，包括作为有效位置编码方案的功能性相对位置编码、全词掩蔽策略、混合精度训练和训练模型的LAMB优化器。实验结果表明，NEZHA在完成命名实体识别（人民日报NER）、句子匹配（LCQMC）、中文情感分类（ChnSenti）和自然语言推理（XNLI）等具有代表性的中文任务时，达到了最先进的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2004.13922">Revisiting Pre-Trained Models for Chinese Natural Language Processing</a> (EMNLP2020 Findings)
<pre>来自Transformers的双向编码器表示（BERT）在各种NLP任务中表现出了惊人的改进，并且已经提出了连续变体来进一步改进预先训练的语言模型的性能。在本文中，我们的目标是重新审视汉语预训练语言模型，以检验其在非英语语言中的有效性，并向社区发布汉语预训练语言模型系列。我们还提出了一个简单但有效的模型，称为MacBERT，它在几个方面改进了RoBERTa，特别是采用传销作为校正（Mac）的掩蔽策略。我们对八个中文NLP任务进行了广泛的实验，以重新审视现有的预训练语言模型以及提出的MacBERT模型。实验结果表明，MacBERT可以在许多NLP任务中获得最先进的性能，并且我们还利用一些可能有助于未来研究的发现消除细节。现有资源：https://github.com/ymcui/MacBERT</pre></li>
<li><a href="https://arxiv.org/abs/2106.16038">ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information</a> (ACL2021) [<a href="https://github.com/ShannonAI/ChineseBert">github</a>]
<pre>最近的汉语预训练模型忽略了汉语特有的两个重要方面：字形和拼音，它们为语言理解提供了重要的语法和语义信息。在这项工作中，我们提出了ChineseBERT，它将汉字的{\it glyph}和{\it pinyin}信息合并到语言模型预训练中。字形嵌入是基于汉字的不同字体获得的，能够从视觉特征中捕捉字符语义，拼音嵌入是汉字发音的特征，处理了汉语中普遍存在的异名现象（同一个字符的发音不同，含义不同）.在大规模未标记中文语料库上进行预训练后，所提出的ChineseBERT模型比基线模型具有显著的性能提升，只需较少的训练步骤。Porpseed模型在广泛的中文NLP任务上实现了新的SOTA性能，包括机器阅读理解、自然语言推理、文本分类、文本分类等entence对匹配，以及命名实体识别中的竞争性能。代码和预训练模型可在https://github.com/ShannonAI/ChineseBert.</pre></li>
<li><a href="https://arxiv.org/abs/2011.14277">Intrinsic Knowledge Evaluation on Chinese Language Models</a>
<pre>最近的NLP任务从预先训练的语言模型（LM）中受益匪浅，因为它们能够对各个方面的知识进行编码。然而，目前的LM评估侧重于下游绩效，因此缺乏全面检查他们在哪些方面以及在多大程度上编码了知识。本文通过提出四项关于句法、语义、常识和事实知识的任务来解决这两个问题，总计39308美元，涵盖汉语的语言和世界知识。在整个实验过程中，我们的探针和知识数据被证明是评估预先训练的中国LMs的可靠基准。我们的工作公开于https://github.com/ZhiruoWang/ChnEval.</pre></li>
<li><a href="https://arxiv.org/abs/2012.00413">CPM: A Large-scale Generative Chinese Pre-trained Language Model</a> [<a href="https://github.com/TsinghuaAI/CPM-Generate">github</a>]
<pre>预先训练的语言模型（PLM）已被证明对各种下游NLP任务是有益的。最近，具有1750亿个参数和570GB训练数据的GPT-3由于具有少量（甚至零次）学习的能力而引起了广泛关注。然而，由于GPT-3的训练语料库主要是英语，而且参数尚未公开，因此应用GPT-3来处理中文NLP任务仍然具有挑战性。在本技术报告中，我们发布了中文预训练语言模型（CPM），该模型对大规模中文训练数据进行生成性预训练。据我们所知，拥有26亿个参数和100GB中文训练数据的CPM是最大的中文预训练语言模型，它可以促进几个后续中文NLP任务，如对话、论文生成、完形填空测试和语言理解。大量的实验表明，CPM在少数镜头（甚至是零镜头）学习的情况下，在许多NLP任务上取得了很好的性能。有关代码和参数，请访问https://github.com/TsinghuaAI/CPM-Generate.</pre></li>
<li><a href="https://arxiv.org/abs/2104.12369">PanGu-α: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation</a>
<pre>大规模预训练语言模型（PLM）已成为自然语言处理（NLP）的新范式。具有数千亿参数（如GPT-3）的PLM在自然语言理解和生成方面表现出了强大的性能，并且使用了\textit{上下文中的少量镜头}学习。在这项工作中，我们介绍了我们训练大型自回归语言模型PanGu-$\alpha$的实践，该模型具有多达2000亿个参数。PanGu-$\alpha$由MindSpore开发，并在2048 Ascend 910 AI处理器集群上进行训练。基于MindSpore Auto parallel实现了训练并行策略，该策略由五个并行维度组成，可将训练任务有效扩展到2048个处理器，包括数据并行、操作级模型并行、流水线模型并行、优化器模型并行和重物质化。为了增强PanGu-$\alpha$的泛化能力，我们从广泛的领域收集了1.1TB的高质量中文数据，对模型进行预训练。我们实证测试了PanGu-$\alpha$在各种场景下的生成能力，包括文本摘要、问答、对话生成等。此外，我们还研究了模型规模对中国NLP任务中的少数镜头性能的影响。实验结果表明，PanGu-$\alpha$在很少放炮或零放炮设置下执行各种任务的能力优越。</pre></li>
<li><a href="https://arxiv.org/abs/2003.01355">CLUECorpus2020: A Large-scale Chinese Corpus for Pre-training Language Model</a>
<pre>在本文中，我们介绍了线索组织的中文语料库CLUECorpus2020，这是一个大型语料库，可直接用于自我监督学习，如语言模型的预训练或语言生成。它拥有100克的原始语料库，包含350亿个汉字，这些汉字是从普通爬网中检索到的。为了更好地理解这个语料库，我们进行了小规模和大规模的语言理解实验，结果表明，在这个语料库上训练的模型能够在汉语上取得优异的成绩。我们发布了一个新的中文词汇表，其大小为8K，仅为谷歌发布的中文Bert词汇表大小的三分之一。它节省了计算成本和内存，同时与原始词汇表一样有效。我们还在这个语料库上发布了预训练模型的大版本和小版本。前者实现了最先进的结果，而后者在保持最高精度的同时，将训练和预测速度提高了8倍，是Bert基的8倍。为了促进汉语自我监督学习的未来工作，我们在Github上发布了我们的数据集、新词汇、代码和预先训练的模型。</pre></li>
<li><a href="https://arxiv.org/abs/2004.05986">CLUE: A Chinese Language Understanding Evaluation Benchmark</a>
<pre>自然语言理解（NLU）英语基准（如GLUE和SuperGLUE）的出现，使新的NLU模型能够在不同的任务集上进行评估。这些综合基准促进了自然语言处理（NLP）领域的广泛研究和应用。然而，问题是，大多数此类基准仅限于英语，这使得很难将英语NLU中的许多成功应用于其他语言。为了解决这个问题，我们引入了第一个大规模汉语理解评估（CLUE）基准。CLUE是一个开放式、社区驱动的项目，包含9项任务，涵盖多个成熟的单句/句子对分类任务，以及机器阅读理解，所有这些任务都基于原始中文文本。为了确定这些任务的结果，我们使用一组最先进的预先训练过的中国模型（总共9个）来报告分数。我们还介绍了一些补充数据集和其他工具，以帮助促进中国NLU的进一步进展。我们的基准发布于https://www.CLUEbenchmarks.com</pre></li>
<li><a href="https://arxiv.org/abs/2112.13610">CUGE: A Chinese Language Understanding and Generation Evaluation Benchmark</a>
<pre>实现通用语言智能一直是自然语言处理的一个长期目标，其中标准评估基准起着基础和指导作用。我们认为，对于通用语言智力评估，基准本身需要全面和系统。为此，我们提出了CUGE，这是一个汉语理解和生成评估基准，具有以下特点：（1）层次基准框架，其中数据集主要由语言能力任务数据集层次结构选择和组织。（2） 多级评分策略，其中基于层次结构框架提供不同级别的模型性能。为了方便CUGE，我们提供了一个可定制的公共排行榜，以支持灵活的模型评判标准。对有代表性的预训练语言模型的评估结果表明，在通用语言智能方面还有很大的改进空间。CUGE在CUGE上公开提供。baai。ac.cn。</pre></li>
<li><a href="https://arxiv.org/abs/2107.07498">FewCLUE: A Chinese Few-shot Learning Evaluation Benchmark</a>
<pre>预训练语言模型（PLM）在自然语言理解任务中取得了巨大的成功。尽管针对英语等语言的不同学习方案——微调、零次学习和少次学习——已经得到了广泛的探索和比较，但在汉语中，公平、全面地评估和比较这些方法的工作相对较少，因此阻碍了累积的进步。本文介绍了中国第一个综合性少数镜头学习评价基准——汉语少数镜头学习评价基准（FewCLUE）。它包括九个任务，从单句和句子对分类任务到机器阅读理解任务。我们系统地评估了五种最先进的（SOTA）少数镜头学习方法（包括PET、ADAPET、LM-BFF、P-tuning和EFL），并在新构建的FewCLUE基准上比较了它们与微调和零镜头学习方案的性能。实验结果表明：1）不同的少镜头学习方法的效果对所应用的预训练模型敏感；2） PET和P-tuning分别与RoBERTa和ERNIE一起实现最佳的整体性能。我们的基准用于NLPCC 2021的少数镜头学习竞赛。此外，我们还提供了一个用户友好的工具包，以及一个在线排行榜，以帮助促进汉语学习的进一步进展。我们提供了不同学习方法的基线性能，为今后的研究提供参考。</pre></li>
<li><a href="https://arxiv.org/abs/2009.11473">AnchiBERT: A Pre-Trained Model for Ancient ChineseLanguage Understanding and Generation</a>
<pre>古代汉语是中国文化的精髓。古汉语领域有几个自然语言处理任务，如古今汉语翻译、诗歌生成和对联生成。以往的研究通常使用监督模型，这些模型严重依赖于并行数据。然而，要获得大规模的古汉语平行数据是很困难的。为了充分利用更容易获得的单语古汉语语料库，我们发布了AnchiBERT，这是一个基于BERT体系结构的预训练语言模型，在大规模古汉语语料库上进行训练。我们从语言理解和生成任务两个方面对AnchiBERT进行评估，包括诗歌分类、古今汉语翻译、诗歌生成和对联生成。实验结果表明，AnchiBERT模型优于BERT模型和非预训练模型，并在所有情况下都达到了最先进的结果。</pre></li>
<li><a href="https://arxiv.org/abs/1909.05658">UER: An Open-Source Toolkit for Pre-training Models</a> (EMNLP2019 Demo) [<a href="https://github.com/dbiir/UER-py">github</a>]
<pre>现有的工作，包括ELMO和BERT，揭示了NLP任务预培训的重要性。虽然不存在一个在所有情况下都能发挥最佳效果的单一培训前模型，但有必要开发一个能够有效部署各种培训前模型的框架。为此，我们提出了一个按需组装预培训工具包，即通用编码器表示（UER）。UER是松散耦合的，并用丰富的模块封装。通过按需组装模块，用户可以复制最先进的培训前模型，也可以开发尚未开发的培训前模型。利用UER，我们建立了一个模型动物园，其中包含基于不同语料库、编码器和目标（目标）的预训练模型。通过适当的预训练模型，我们可以在一系列下游数据集上获得最新的结果。</pre></li>
</ul>
<h1 id="domain-specific">Domain specific</h1>
<ul>
<li><a href="https://arxiv.org/abs/2105.00827">AMMU – A Survey of Transformer-based Biomedical Pretrained Language Models</a>
<pre>基于转换器的预训练语言模型（PLM）开创了现代自然语言处理（NLP）的新纪元。这些模型结合了变压器、转移学习和自监督学习（SSL）的功能。随着这些模型在一般领域的成功，生物医学研究界开发了各种领域内PLM，从BioBERT到最新的BioELECTRA和BioALBERT模型。我们坚信有必要编写一份调查报告，对各种基于转换器的生物医学预训练语言模型（BPLM）进行全面调查。在本次调查中，我们首先简要概述了自监督学习、嵌入层和变压器编码器层等基本概念。我们讨论了基于变压器的PLM的核心概念，如预训练方法、预训练任务、微调方法以及生物医学领域特有的各种嵌入类型。我们介绍了基于transformer的BPLMs的分类，然后讨论了所有模型。我们讨论各种挑战并提出可能的解决方案。最后，我们强调了一些有待解决的问题，这些问题将推动研究界进一步改进基于变压器的BPLM。</pre></li>
<li><a href="https://arxiv.org/abs/1901.08746">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</a>
<pre>随着生物医学文档数量的快速增长，生物医学文本挖掘变得越来越重要。随着自然语言处理（NLP）技术的发展，从生物医学文献中提取有价值的信息越来越受到研究者的欢迎，而深度学习促进了有效生物医学文本挖掘模型的发展。然而，将自然语言处理的进展直接应用于生物医学文本挖掘往往会产生不令人满意的结果，因为单词分布会从一般领域语料库转移到生物医学语料库。在本文中，我们研究了最近引入的预训练语言模型BERT如何适用于生物医学语料库。我们介绍了BioBERT（来自生物医学文本挖掘变压器的双向编码器表示），这是一种在大规模生物医学语料库上预先训练的领域特定语言表示模型。使用几乎相同的任务体系结构，当对生物医学语料库进行预训练时，BioBERT在各种生物医学文本挖掘任务中的表现大大优于BERT和以前的最新模型。虽然BERT的性能与以前最先进的模型相当，但在以下三个具有代表性的生物医学文本挖掘任务中，BioBERT的性能显著优于它们：生物医学命名实体识别（F1分数提高0.62%）、生物医学关系提取（F1分数提高2.80%）生物医学问答（MRR提高12.24%）。我们的分析结果表明，对生物医学语料库进行预训练有助于理解复杂的生物医学文本。我们免费提供BioBERT的预训练重量https://github.com/naver/biobert-pretrained，以及微调BioBERT的源代码，可在https://github.com/dmis-lab/biobert.</pre></li>
<li><a href="https://aclanthology.org/2021.naacl-main.334/">Self-Alignment Pretraining for Biomedical Entity Representations</a> (NAACL2021) [<a href="https://github.com/cambridgeltl/sapbert">github</a>]</li>
<li><a href="https://aclanthology.org/2021.acl-short.72/">Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking</a> (ACL2021) [<a href="https://github.com/cambridgeltl/sapbert">github</a>]</li>
<li><a href="https://arxiv.org/abs/1906.05474">Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets</a> (ACL2019 WS)
<pre>受通用语言理解评估基准的成功启发，我们引入了生物医学语言理解评估（BLUE）基准，以促进生物医学领域培训前语言表征发展的研究。基准测试由五项任务和十个数据集组成，这些数据集涵盖生物医学和临床文本，具有不同的数据集大小和难度。我们还评估了基于BERT和ELMo的几个基线，并发现根据PubMed摘要和MIMIC-III临床笔记预先训练的BERT模型取得了最佳结果。我们使数据集、预先训练的模型和代码在https://github.com/ncbi-nlp/BLUE_Benchmark.</pre></li>
<li><a href="https://arxiv.org/abs/1908.03548">BERT-based Ranking for Biomedical Entity Normalization</a>
<pre>开发能够缓解术语变化问题的高性能实体规范化算法是生物医学界非常感兴趣的。尽管基于深度学习的方法已经成功地应用于生物医学实体规范化，但它们通常依赖于传统的上下文无关单词嵌入。最近，来自变换器（BERT）、生物医学文本挖掘BERT（BioBERT）和临床文本挖掘BERT（ClinicalBERT）的双向编码器表示被引入到使用双向变换器的上下文化单词表示模型的预训练中，提升了许多自然语言处理任务的最新水平。在本研究中，我们通过微调预训练的BERT/BioBERT/ClinicalBERT模型，提出了一种实体规范化架构，并进行了大量实验，以评估预训练模型在使用三种不同类型数据集进行生物医学实体规范化方面的有效性。我们的实验结果表明，最佳微调模型始终优于以前的方法，并提高了生物医学实体规范化的最新水平，精确度提高了1.17%。</pre></li>
<li><a href="https://arxiv.org/abs/1909.06146">PubMedQA: A Dataset for Biomedical Research Question Answering</a> (EMNLP2019)
<pre>我们介绍了PubMedQA，一个从PubMed摘要中收集的新型生物医学问答（QA）数据集。PubMedQA的任务是使用相应的摘要以是/否/可能回答研究问题（例如：术前他汀类药物是否能减少冠状动脉旁路移植术后心房颤动？）。PubMedQA有1k专家注释、61.2k未标记和211.3k人工生成的QA实例。每个PubMedQA实例由（1）一个问题组成，该问题要么是现有的研究文章标题，要么是从中派生出来的，（2）一个上下文，它是没有结论的相应摘要，（3）一个长答案，它是摘要的结论，大概回答了研究问题，以及（4）总结结论的是/否/可能答案。PubMedQA是第一个QA数据集，需要对生物医学研究文本进行推理，尤其是其定量内容，以回答问题。我们表现最好的模型是多阶段微调BioBERT，使用单词统计的长答案袋作为额外监督，与单个人的准确率78.0%和大多数基线的准确率55.2%相比，实现了68.1%的准确率，留下了很大的改进空间。PubMedQA可在以下网址公开获取：https://pubmedqa.github.io.</pre></li>
<li><a href="https://arxiv.org/abs/1909.08229">Pre-trained Language Model for Biomedical Question Answering</a>
<pre>最近问答系统的成功很大程度上归功于预先训练的语言模型。然而，由于语言模型大多是在一般领域语料库（如维基百科）上预先训练的，因此它们通常难以理解生物医学问题。在本文中，我们研究了BioBERT，一个预先训练好的生物医学语言模型，在回答生物医学问题时的性能，包括factoid、list和yes/no类型的问题。BioBERT在各种问题类型中使用几乎相同的结构，并在第七次BioASQ挑战（任务7b，阶段B）中取得最佳成绩。BioBERT在SQuAD或SQuAD 2.0上接受过预先训练，其表现轻松超越了之前的最先进模型。当BioBERT对问题、段落和答案使用适当的前/后处理策略时，其表现最佳。</pre></li>
<li><a href="https://arxiv.org/abs/1911.00712">How to Pre-Train Your Model? Comparison of Different Pre-Training Models for Biomedical Question Answering</a>
<pre>在小规模数据集上使用深度学习模型会导致过度拟合。为了克服这个问题，预训练模型并将其微调到小规模数据集的过程在图像处理等领域中得到了广泛的应用。同样，对于问题回答，可以通过几种方式进行预培训和微调。阅读理解模型通常用于预训练，但我们发现其他类型的预训练效果更好。我们比较了两种基于阅读理解和开放领域问答模型的预训练模型，并确定了在BIOASQ问答数据集上进行微调和测试时的性能。我们发现开放领域问答模式比阅读理解模式更适合这项任务。</pre></li>
<li><a href="https://arxiv.org/abs/2004.11157">On Adversarial Examples for Biomedical NLP Tasks</a>
<pre>预训练单词嵌入的成功促使其在生物医学领域的任务中使用。BERT语言模型在命名实体识别（NER）和语义文本相似性（STS）等任务的标准性能指标方面取得了显著的成果，这在NLP领域取得了重大进展。然而，目前尚不清楚这些系统在法律或医疗等关键领域是否运行良好。因此，在这项工作中，我们针对两个著名的医学NER和STS数据集提出了一个对抗性评估方案。我们提出了两种攻击类型，其灵感来自自然拼写错误和人类造成的拼写错误。我们还提出了另一种使用医学术语同义词的攻击。在这些对抗性设置下，模型的准确性显著下降，我们量化了这种性能损失的程度。我们还表明，通过使用对抗性示例对模型进行训练，可以显著提高模型的鲁棒性。我们希望我们的工作将激励使用对抗性示例来评估和开发模型，从而增强医疗任务的鲁棒性。</pre></li>
<li><a href="https://arxiv.org/abs/2005.02799">An Empirical Study of Multi-Task Learning on BERT for Biomedical Text Mining</a> (ACL2020 WS)
<pre>多任务学习（MTL）在自然语言处理应用中取得了显著的成功。在这项工作中，我们针对各种生物医学和临床自然语言处理任务（如文本相似性、关系提取、命名实体识别和文本推理）研究了一个具有多个解码器的多任务学习模型。我们的实证结果表明，MTL微调模型在生物医学和临床领域的表现分别比最先进的变压器模型（例如，BERT及其变体）好2.0%和1.3%。成对MTL进一步展示了关于哪些任务可以改善或减少其他任务的更多细节。这在研究人员为新问题选择合适模型的过程中尤其有用。代码和模型可在https://github.com/ncbi-nlp/bluebert</pre></li>
<li><a href="https://arxiv.org/abs/2007.15779">Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing</a> [<a href="https://microsoft.github.io/BLURB/">github</a>]
<pre>预训练大型神经语言模型，如BERT，在许多自然语言处理（NLP）任务中取得了令人印象深刻的成果。然而，大多数培训前的工作都集中在一般的领域语料库上，如新闻专线和Web。一个普遍的假设是，即使是特定领域的预培训也可以从通用领域语言模型开始。在本文中，我们挑战了这一假设，证明了对于具有大量未标记文本的领域，如生物医学，从头开始的预训练语言模型比一般领域语言模型的持续预训练有很大的收益。为了促进这项研究，我们从公开的数据集汇编了一个全面的生物医学NLP基准。我们的实验表明，特定领域的前训练是一个坚实的基础，为广泛的生物医学NLP任务，导致新的国家的最先进的结果全面。此外，在对建模选择进行全面评估（包括预培训和特定任务的微调）时，我们发现一些常见的做法对于BERT模型是不必要的，例如在命名实体识别（NER）中使用复杂的标记方案。为了帮助加快生物医学NLP的研究，我们为社区发布了最先进的预训练和任务特定模型，并在https://aka.ms/BLURB.</pre></li>
<li><a href="https://arxiv.org/abs/2104.10344">Improving Biomedical Pretrained Language Models with Knowledge</a> (BioNLP2021)
<pre>预训练语言模型在许多自然语言处理任务中都取得了成功。许多作品探索将知识整合到语言模型中。在生物医学领域，专家们花费了几十年的努力来建立大规模的知识库。例如，统一医学语言系统（UMLS）包含数百万个实体及其同义词，并定义了数百个实体之间的关系。利用这些知识可以使各种下游任务受益，例如命名实体识别和关系提取。为此，我们提出了KeBioLM，这是一个生物医学预训练语言模型，它明确地利用了UMLS知识库中的知识。具体来说，我们从PubMed摘要中提取实体并将它们链接到UMLS。然后，我们训练一个知识感知语言模型，该模型首先应用纯文本编码层来学习实体表示，然后应用文本实体融合编码来聚合实体表示。此外，我们还增加了实体检测和实体链接两个训练目标。在BLURB基准中的命名实体识别和关系提取实验证明了该方法的有效性。对收集的探测数据集的进一步分析表明，我们的模型具有更好的医学知识建模能力。</pre></li>
<li><a href="https://arxiv.org/abs/2010.06060">BioMegatron: Larger Biomedical Domain Language Model</a> (EMNLP2020) [<a href="https://developer.nvidia.com/blog/building-state-of-the-art-biomedical-and-clinical-nlp-models-with-bio-megatron/">website</a>]
<pre>生物医学领域特定的语言模型大量涌入，表明在生物医学领域基准测试中，经过生物医学文本预训练的语言模型比在维基百科和书籍等一般领域文本语料库中训练的语言模型表现得更好。然而，大多数工作并没有深入研究影响各个领域语言应用的因素。此外，关于特定领域模型的模型大小的研究也大多缺失。我们实证研究和评估了影响领域语言应用性能的几个因素，如子词词汇集、模型大小、预训练语料库和领域迁移。我们通过在更大的领域语料库上训练更大的BioMegatron模型，在基准测试上显示出一致的改进，这有助于我们理解领域语言模型的应用。我们展示了在命名实体识别、关系提取和问答的标准生物医学NLP基准上，比以前的最新技术（SOTA）有显著改进。模型检查点和代码可在[https://ngc.nvidia.com]及[https://github.com/NVIDIA/NeMo].</pre></li>
<li><a href="https://www.aclweb.org/anthology/2020.clinicalnlp-1.17/">Pretrained Language Models for Biomedical and Clinical Tasks: Understanding and Extending the State-of-the-Art</a> (EMNLP2020 WS)</li>
<li><a href="https://arxiv.org/abs/2005.07202">A pre-training technique to localize medical BERT and enhance BioBERT</a> [<a href="https://github.com/sy-wada/blue_benchmark_with_transformers">github</a>]
<pre>在原始文本上预训练大规模神经语言模型对于改进自然语言处理（NLP）中的迁移学习做出了重要贡献。随着基于转换器的语言模型的引入，例如来自转换器的双向编码器表示（BERT），NLP从自由文本中提取信息的性能在普通领域和医学领域都有了显著提高；然而，对于很少有高质量和大型公共可用数据库的领域，很难训练出性能良好的特定BERT模型。我们假设这个问题可以通过对特定领域的语料库进行上采样并以平衡的方式使用它对更大的语料库进行预训练来解决。我们提出的方法包括一个单一的干预和一个选项：在上采样和扩大词汇量后同时进行预训练。我们进行了三次实验，并对结果进行了评估。我们确认，我们的日语医学BERT在医学文档分类任务方面优于传统基线和其他BERT模型，并且我们的英语BERT使用通用和医学领域语料库进行预训练，在生物医学语言理解方面表现良好，可供实际使用评估（蓝色）基准。此外，我们的增强生物医学BERT模型（在训练前未使用临床笔记）表明，BLUE基准的临床和生物医学得分均比未使用我们提出的方法训练的消融模型高0.3分。通过从适合于目标任务的语料库中提取上采样实例进行均衡的预训练，我们可以构建一个高性能的BERT模型。</pre></li>
<li><a href="https://aclanthology.org/2020.findings-emnlp.129/">exBERT: Extending Pre-trained Models with Domain-specific Vocabulary Under Constrained Training Resources</a> [<a href="https://github.com/cgmhaicenter/exBERT">github</a>] (EMNLP2020 Findings)</li>
<li><a href="https://arxiv.org/abs/2006.15222">BERTology Meets Biology: Interpreting Attention in Protein Language Models</a> (ICLR2021)
<pre>Transformer架构已被证明可以学习用于蛋白质分类和生成任务的有用表示。然而，这些表述对解释性提出了挑战。在这项工作中，我们展示了一套通过注意镜头分析蛋白质转换器模型的方法。我们表明注意：（1）捕获蛋白质的折叠结构，连接在基础序列中相距很远但在三维结构中空间上接近的氨基酸，（2）靶向结合位点，蛋白质的关键功能成分，以及（3）重点关注随着层深度的增加而逐渐复杂的生物物理特性。我们发现这一行为在三个转换器架构（BERT、ALBERT、XLNet）和两个不同的蛋白质数据集上是一致的。我们还展示了注意和蛋白质结构之间相互作用的三维可视化。有关可视化和分析的代码，请访问https://github.com/salesforce/provis.</pre></li>
<li><a href="https://arxiv.org/abs/1904.05342">ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission</a>
<pre>临床笔记包含的患者信息超出了实验室值和药物等结构化数据。然而，相对于结构化数据而言，临床笔记使用不足，因为笔记是高维和稀疏的。这项工作开发和评估的代表性临床笔记使用双向变压器（临床医生）。临床医生揭示了人类判断的医学概念之间的高质量关系。临床医生使用出院总结和重症监护病房前几天的记录，在30天医院再入院预测方面优于基线。代码和模型参数可用。</pre></li>
<li><a href="https://arxiv.org/abs/2007.07562">Predicting Clinical Diagnosis from Patients Electronic Health Records Using BERT-based Neural Networks</a> (AIME2020)
<pre>在这篇文章中，我们研究了从文本电子健康记录（EHR）数据预测临床诊断的问题。我们展示了这个问题在医学界的重要性，并对这个问题和提出的方法进行了全面的历史回顾。作为主要的科学贡献，我们提出了一种改进的序列分类变压器双向编码器表示（BERT）模型，该模型实现了一种新的全连接（FC）层合成方法和仅对域数据进行预训练的BERT模型。为了实证验证我们的模型，我们使用了一个大规模的俄罗斯EHR数据集，该数据集包含大约400万次独特的患者就诊。这是对俄语进行的最大的此类研究，也是全球最大的研究之一。我们在对ICD-10的265个疾病子集进行多类分类的任务上，与其他文本表示模型进行了大量对比实验。实验表明，与其他基线相比，我们的模型的性能有所提高，包括一个微调的俄罗斯伯特（RuBERT）变体。我们还与经验丰富的医学专家小组展示了我们模型的可比性能。这使我们希望该系统的实施将减少误诊。</pre></li>
<li><a href="https://arxiv.org/abs/1904.03323">Publicly Available Clinical BERT Embeddings</a> (NAACL2019 WS)
<pre>最近几个月，ELMo（Peters et al.，2018）和BERT（Devlin et al.，2018）等上下文单词嵌入模型极大地提高了许多自然语言处理（NLP）任务的性能。然而，这些模型在专业语料库上的探索很少，例如临床文本；此外，在临床领域，还不存在公开的预训练BERT模型。在这项工作中，我们通过探索和发布临床文本的BERT模型来解决这一需求：一个用于一般临床文本，另一个用于出院总结。我们证明，与非特异性嵌入相比，使用领域特定模型在三种常见的临床NLP任务上产生性能改进。这些领域特定的模型在两项临床去识别任务中表现不佳，并认为这是去识别源文本和综合性非去识别任务文本之间差异的自然结果。</pre></li>
<li><a href="https://arxiv.org/abs/2010.10391">UmlsBERT: Clinical Domain Knowledge Augmentation of Contextual Embeddings Using the Unified Medical Language System Metathesaurus</a> (NAACL2021)
<pre>背景词嵌入模型，如BioBERT和Bio_-Albert，通过将预训练过程集中于特定领域的语料库，在生物医学自然语言处理任务中取得了最新成果。然而，这种模型没有考虑专家领域知识。在这项工作中，我们引入了UmlsBERT，这是一种上下文嵌入模型，通过一种新的知识扩充策略在预训练过程中集成了领域知识。更具体地说，使用统一医学语言系统（UMLS）元叙词表对UmlsBERT进行了两种方式的扩充：i）连接UMLS中具有相同基本“概念”的单词，以及ii）利用UMLS中的语义组知识创建具有临床意义的输入嵌入。通过应用这两种策略，UmlsBERT可以将临床领域知识编码到单词嵌入中，并在常见命名实体识别（NER）和临床自然语言推理临床NLP任务上优于现有的领域特定模型。</pre></li>
<li><a href="https://arxiv.org/abs/2004.10220">MT-Clinical BERT: Scaling Clinical Information Extraction with Multitask Learning</a>
<pre>临床笔记包含大量关于患者的重要但不易获取的信息。自动提取这些信息的系统依赖于大量的培训数据，而这些数据的创建资源是有限的。此外，它们是联合开发的；这意味着任务特定系统之间不能共享任何信息。这一瓶颈不必要地使实际应用复杂化，降低了每个解决方案的性能，并使管理多个信息提取系统的工程负担相关联。我们通过开发多任务临床BET来应对这些挑战：一个单一的深度学习模型，通过在任务之间共享表示，同时执行八项临床任务，包括实体提取、PHI识别、语言蕴涵和相似性。我们发现，我们的单一系统与所有最先进的任务特定系统具有竞争力，同时还受益于推理时的大量计算优势。</pre></li>
<li><a href="https://www.medrxiv.org/content/10.1101/2020.07.07.20148585v1">A clinical specific BERT developed with huge size of Japanese clinical narrative</a></li>
<li><a href="https://arxiv.org/abs/2005.00574">Clinical Reading Comprehension: A Thorough Analysis of the emrQA Dataset</a> (ACL2020) [<a href="https://github.com/xiangyue9607/CliniRC">github</a>]
<pre>近年来，由于大规模的标注数据集，机器阅读理解取得了巨大的进步。然而，在临床领域，由于注释所需的领域专业知识，创建此类数据集相当困难。最近，Pampari等人（EMNLP'18）通过使用专家注释的问题模板和现有的i2b2注释来创建emrQA，这是第一个基于临床笔记的大规模问答（QA）数据集，从而解决了这个问题。在本文中，我们对这一数据集和临床阅读理解（CliniRC）任务进行了深入分析。从我们的定性分析中，我们发现（i）emrQA的答案通常不完整，（ii）emrQA问题通常可以在不使用领域知识的情况下回答。从我们的定量实验中，令人惊讶的结果包括：（iii）使用一个小样本子集（5%-20%），我们可以获得与在整个数据集上训练的模型大致相同的性能，（iv）该性能接近人类专家的性能，（v）伯特模型没有击败性能最好的基础模型。根据我们对emrQA的分析，我们进一步探讨了CliniRC系统的两个期望方面：利用临床领域知识的能力和概括不可见问题和上下文的能力。我们认为，在创建未来的数据集时，应该同时考虑这两种情况。</pre></li>
<li><a href="https://arxiv.org/abs/2008.10327">Knowledge-Empowered Representation Learning for Chinese Medical Reading Comprehension: Task, Model and Resources</a>
<pre>机器阅读理解（MRC）的目的是从文章中提取问题的答案。近年来，它得到了广泛的研究，尤其是在开放领域。然而，由于缺乏大规模的训练数据，对封闭域MRC的研究很少。本文介绍了一种面向医学领域的多目标MRC任务，其目标是同时预测医学信息源中的医学问题答案和相应的支持句，以确保医学知识服务的高可靠性。为此，人工构建了一个高质量的数据集，称为多任务中国医学MRC数据集（CMedMRC），并进行了详细的分析。我们进一步提出了用于任务的中国医学BERT模型（CMedBERT），该模型通过异构特征的动态融合机制和多任务学习策略，将医学知识融合到预先训练的语言模型中。实验表明，通过融合上下文感知和知识感知的令牌表示，CMedBERT始终优于强基线。</pre></li>
<li><a href="https://arxiv.org/abs/2105.06752">Classifying Long Clinical Documents with Pre-trained Transformers</a>
<pre>自动分型是一项识别符合预定义标准的患者队列的任务。表型分型通常涉及对包含数千个标记的长临床文档进行分类。同时，最新的基于transformer的预训练语言模型将输入限制为几百个令牌（例如，对于BERT，512个令牌）。我们评估了几种将预先训练的句子编码器整合到临床文本文档级表示中的策略，发现没有预先训练的层次变换器与任务预先训练的模型具有竞争力。</pre></li>
<li><a href="https://arxiv.org/abs/2005.06634">Detecting Adverse Drug Reactions from Twitter through Domain-Specific Preprocessing and BERT Ensembling</a>
<pre>社交媒体中药物不良反应（ADR）检测的自动化将彻底改变药物警戒的实践，支持药物监管机构、制药行业和公众确保日常实践中处方药物的安全性。根据2019年8月发布的社交媒体健康挖掘（SMM4H）应用研讨会和共享任务会议记录，我们旨在开发一个深度学习模型，对包含药物提及的Twitter推文中的ADR进行分类。我们的方法包括微调$BERT_{LARGE}$和两个特定于域的BERT实现$BioBERT$和$Bio+clinicalBERT$，应用特定于域的预处理器，并开发最大预测集成方法。我们的最终模型在$F_1$分数（0.6681）和召回率（0.7700）方面都取得了最先进的性能，超过了2019年SMM4H提交的所有模型以及迄今为止的后评估。</pre></li>
<li><a href="https://arxiv.org/abs/1910.05786">Progress Notes Classification and Keyword Extraction using Attention-based Deep Learning Models with BERT</a>
<pre>已经开发了各种深度学习算法来分析不同类型的临床数据，包括临床文本分类和从“自由文本”中提取信息等。然而，从临床笔记中自动提取关键词仍然是一个挑战。这些挑战包括处理嘈杂的临床笔记，其中包含各种缩写、可能的打字错误和无结构的句子。本研究的目的是研究基于注意的深度学习模型，对从真实EHR系统中提取的未识别临床进展记录进行分类。基于注意的深度学习模型可用于解释模型并理解导致临床进展记录正确或错误分类的关键词语。本研究中基于注意的模型能够呈现人类可解释文本分类模型。结果表明，带注意层的精细调整的BERT分类模型可以达到97.6%的分类精度，高于基线精细调整的BERT分类模型。在本研究中，我们还证明了基于注意的模型可以识别与临床进展记录类别密切相关的相关关键词。</pre></li>
<li><a href="https://arxiv.org/abs/2006.03685">BERT-XML: Large Scale Automated ICD Coding Using BERT Pretraining</a>
<pre>临床互动最初记录并记录在自由文本医学笔记中。ICD编码是对与患者就诊相关的所有诊断、症状和程序进行分类和编码的任务。这个过程通常是手工的，对于医院来说非常耗时和昂贵。在本文中，我们提出了一个机器学习模型，BERT-XML，用于从EHR notes中进行大规模自动ICD编码，利用最近开发的无监督预训练，在各种NLP任务上取得了最先进的性能。我们在EHR笔记上从头开始训练一个BERT模型，使用更适合EHR任务的词汇进行学习，因此优于现成的模型。我们采用BERT结构进行多标签注意的ICD编码。当其他工作集中于小型公共医疗数据集时，我们使用数百万EHR注释生成了第一个大规模ICD-10分类模型，以预测数千个独特的ICD代码。</pre></li>
<li><a href="https://arxiv.org/abs/2008.10492">Prediction of ICD Codes with Clinical BERT Embeddings and Text Augmentation with Label Balancing using MIMIC-III</a>
<pre>本文使用MIMIC-III数据集实现了ICD代码预测任务的最新结果。这是通过使用临床BERT实现的（Alsentzer等人，2019年）。嵌入、文本增强和标签平衡，以提高ICD章节和ICD疾病代码的F1分数。我们认为，性能的提高主要是由于在训练过程中使用了新颖的文本增强来改变句子的顺序。与F1得分为0.76的前32名ICD代码预测（徐克阳等人）相比，我们最终的F1得分为0.75，但前50名ICD代码的总得分为0.75。</pre></li>
<li><a href="https://arxiv.org/abs/2010.03746">Infusing Disease Knowledge into BERT for Health Question Answering, Medical Inference and Disease Name Recognition</a> (EMNLP2020)
<pre>疾病知识包括疾病的各个方面的信息，如症状和体征、诊断和治疗。这种疾病知识对于许多与健康相关的生物医学任务至关重要，包括消费者健康问题回答、医学语言推理和疾病名称识别。虽然像BERT这样的预先训练过的语言模型在从文本中获取句法、语义和世界知识方面取得了成功，但我们发现它们可以通过症状、诊断、治疗和其他疾病方面的知识等特定信息得到进一步补充。因此，我们将BERT与疾病知识相结合，以改进这些重要任务。具体地说，我们提出了一种新的疾病知识注入训练程序，并在一组伯特模型上对其进行评估，包括伯特、BioBERT、SciBERT、ClinicalBERT、BlueBERT和ALBERT。对这三项任务的实验表明，这些模型在几乎所有情况下都能得到增强，证明了疾病知识灌输的可行性。例如，BioBERT在消费者健康问题回答方面的准确率从68.29%提高到72.09%，同时在两个数据集中观察到了新的SOTA结果。我们免费提供我们的数据和代码。</pre></li>
<li><a href="https://arxiv.org/abs/2004.09167">CheXbert: Combining Automatic Labelers and Expert Annotations for Accurate Radiology Report Labeling Using BERT</a> (EMNLP2020)
<pre>从放射学文本报告中提取标签可以对医学成像模型进行大规模培训。现有的报告标签方法通常依赖于基于医学领域知识的复杂特征工程或专家手动注释。在这项工作中，我们介绍了一种基于BERT的医学图像报告标签方法，该方法利用了现有基于规则的系统的规模和专家注释的质量。我们展示了一个生物医学预训练的BERT模型的优越性能，该模型首先在基于规则的标签机的注释上进行训练，然后在一组小的专家注释上进行微调，并通过自动反译进行增强。我们发现，我们的最终模型CheXbert能够在统计显著性方面优于以前基于最佳规则的标签机，为最大的胸部x光数据集之一的报告标签设置了新的SOTA。</pre></li>
<li><a href="https://arxiv.org/abs/2006.11991">Students Need More Attention: BERT-based Attention Model for Small Data with Application to Automatic Patient Message Triage</a> (MLHC2020)
<pre>在基于深度学习模型训练分类器时，医疗保健领域常见的小而不平衡的数据集是一个挑战。因此，我们提出了一个基于BioBERT（生物医学文本挖掘变压器的双向编码器表示）的新框架。具体而言，（i）我们在BERT的每一层中引入标签嵌入以实现自我关注，我们称之为LESA-BERT，（ii）通过将LESA-BERT提取为较小的变量，我们旨在减少处理小数据集时的过度拟合和模型大小。作为一个应用程序，我们的框架用于构建患者门户消息分类模型，该模型将消息的紧急程度分为三类：非紧急、中等和紧急。实验表明，我们的方法在宏F1分数方面比几种强基线分类器有4.3%的显著优势。此项目的代码可在\url公开获取{https://github.com/shijing001/text_classifiers}.</pre></li>
<li><a href="https://arxiv.org/abs/2005.12833">Med-BERT: pre-trained contextualized embeddings on large-scale structured electronic health records for disease prediction</a> [<a href="https://github.com/ZhiGroup/Med-BERT">github</a>]
<pre>来自电子健康记录（EHR）的基于深度学习（DL）的预测模型在许多临床任务中提供了令人印象深刻的性能。然而，大型训练群组通常需要达到高精度，这妨碍了在训练数据量有限的场景中采用基于DL的模型。最近，变换器的双向编码器表示（BERT）和相关模型在自然语言处理领域取得了巨大的成功。在一个非常大的训练语料库上对BERT进行预训练，生成情境化嵌入，可以提高在较小数据集上训练的模型的性能。我们提出了Med BERT，它采用BERT框架对来自28490650例EHR患者数据集的结构化诊断数据进行预训练上下文嵌入模型。对两项疾病预测任务进行了微调实验：（1）糖尿病患者心衰预测和（2）从两个临床数据库预测胰腺癌。Med BERT显著提高了预测精度，将接收机工作特性曲线（AUC）下的面积提高了2.02-7.12%。特别是，预先训练的Med BERT通过非常小的微调训练集（300-500个样本）显著提高了任务的性能，将AUC提高了20%以上，或相当于10倍大训练集的AUC。我们相信，Med BERT将有助于使用小型本地训练数据集进行疾病预测研究，减少数据收集费用，并加快人工智能辅助医疗的步伐。</pre></li>
<li><a href="https://aclanthology.org/D19-1371/">SciBERT: Pretrained Contextualized Embeddings for Scientific Text</a> (EMNLP2019) [<a href="https://github.com/allenai/scibert">github</a>]</li>
<li><a href="https://aclanthology.org/2020.acl-main.207/">SPECTER: Document-level Representation Learning using Citation-informed Transformers</a> (ACL2020) [<a href="https://github.com/allenai/specter">github</a>]</li>
<li><a href="https://arxiv.org/abs/2103.02410">OAG-BERT: Pre-train Heterogeneous Entity-augmented Academic Language Models</a> [<a href="https://github.com/thudm/oag-bert">github</a>]
<pre>用领域知识丰富语言模型是关键但困难的。基于世界上最大的公共学术图开放学术图（OAG），我们预先训练了一个学术语言模型，即OAG-BERT，该模型集成了大量异构实体，包括论文、作者、概念、地点和从属关系。为了更好地赋予OAG-BERT捕捉实体信息的能力，我们开发了新的预训练策略，包括异构实体类型嵌入、实体感知2D位置编码和跨域感知实体掩蔽。对于零炮推断，我们设计了一种特殊的解码策略，允许OAG-BERT从头开始生成实体名称。我们评估了各种下游学术任务的OAG-BERT，包括NLP基准测试、零镜头实体推理、异构图链接预测和作者姓名消歧。结果表明，所提出的预培训方法对于理解学术文本和从异构实体建模知识都是有效的。OAG-BERT已被部署到多个实际应用程序中，如AMiner系统中的审阅者推荐和论文标记。它也可以通过CogDL包向公众提供。</pre></li>
<li><a href="https://arxiv.org/abs/1906.02124">PatentBERT: Patent Classification with Fine-Tuning a pre-trained BERT Model</a>
<pre>在这项工作中，我们专注于微调预先训练的伯特模型，并将其应用于专利分类。当应用于拥有超过两百万项专利的大型数据集时，我们的方法优于使用CNN和单词嵌入的最新方法。此外，我们关注专利文件中没有其他部分的专利权利要求。我们的贡献包括：（1）基于预训练的BERT模型和专利分类微调的最新方法，（2）CPC子类级别的大型数据集USPTO-3M，以及可供未来研究人员使用的SQL语句，（3）表明仅专利申请就足以完成分类任务，与传统智慧相反。</pre></li>
<li><a href="https://arxiv.org/abs/2006.08097">FinBERT: A Pretrained Language Model for Financial Communications</a>
<pre>背景预训练语言模型，如BERT（Devlin et al.，2019），通过对大量未标记文本资源的培训，在各种NLP任务中取得了重大突破。金融部门也积累了大量的金融沟通文本。但是，目前还没有预训练的金融特定语言模型可用。在这项工作中，我们通过使用大规模的金融通信语料库对金融领域特定的伯特模型FinBERT进行预训练来解决这一需求。在三个金融情绪分类任务上的实验证实了FinBERT模型相对于一般领域BERT模型的优势。代码和预训练模型可在https://github.com/yya518/FinBERT. 我们希望这将对从事金融NLP任务的从业者和研究人员有用。</pre></li>
<li><a href="https://arxiv.org/abs/2010.02559">LEGAL-BERT: The Muppets straight out of Law School</a> (EMNLP2020 Findings)
<pre>BERT在几个NLP任务中取得了令人印象深刻的性能。然而，在专门领域对其适应指南的调查有限。在这里，我们关注法律领域，在这里我们探索了几种将伯特模型应用于下游法律任务的方法，在多个数据集上进行评估。我们的研究结果表明，以前的预培训和微调指南往往是盲目遵循的，但在法律领域并不总是一概而论。因此，我们建议对在专业领域应用BERT时可用的策略进行系统调查。这些是：（a）使用开箱即用的原始BET，（b）通过在特定领域语料库上进行额外的预培训来调整BET，以及（c）在特定领域语料库上从头开始对BET进行预培训。在对下游任务进行微调时，我们还提出了更广阔的超参数搜索空间，并发布了LEGAL-BERT，这是一系列旨在协助法律NLP研究、计算法和法律技术应用的BERT模型。</pre></li>
<li><a href="https://arxiv.org/abs/2105.03887">Lawformer: A Pre-trained Language Model for Chinese Legal Long Documents</a>
<pre>法律人工智能（LegalAI）旨在利用人工智能技术，特别是自然语言处理（NLP），使法律系统受益。最近，受预训练语言模型（PLM）在通用领域的成功启发，许多LegalAI研究人员致力于将PLM应用于法律任务。然而，利用PLM处理法律任务仍然具有挑战性，因为法律文件通常由数千个令牌组成，远远超过主流PLM可以处理的长度。在本文中，我们发布了基于Longformer的预训练语言模型Lawformer，用于中文法律长文档的理解。我们在各种LegalAI任务中评估Lawformer，包括判断预测、相似案例检索、法律阅读理解和法律问题回答。实验结果表明，我们的模型可以在以长文档为输入的任务上取得很好的改进。</pre></li>
<li><a href="https://arxiv.org/abs/2009.02835">E-BERT: A Phrase and Product Knowledge Enhanced Language Model for E-commerce</a>
<pre>诸如BERT之类的预先训练的语言模型在广泛的自然语言处理任务中取得了巨大的成功。然而，由于缺乏两个层次的领域知识，即短语层次和产品层次，BERT不能很好地支持与电子商务相关的任务。一方面，许多电子商务任务需要准确理解领域短语，而此类细粒度短语级知识并没有通过BERT的训练目标显式建模。另一方面，产品级知识（如产品关联）可以增强电子商务的语言建模，但它们不是事实知识，因此不加区别地使用它们可能会引入噪声。为了解决这个问题，我们提出了一个统一的预训练框架，即E-BERT。具体来说，为了保留短语级知识，我们引入了自适应混合掩蔽，该掩蔽允许模型根据两种模式的拟合过程，从学习初始单词知识自适应地切换到学习复杂短语。为了利用产品级知识，我们引入了邻域产品重构，它训练E-BERT通过去噪交叉注意层预测产品的相关邻域。我们的调查显示，在四个下游任务中，即基于评论的问题回答、方面提取、方面情感分类和产品分类，都有很好的结果。</pre></li>
<li><a href="https://arxiv.org/abs/2012.09807">BERT Goes Shopping: Comparing Distributional Models for Product Representations</a>
<pre>通过~\textit{prod2vec}，单词嵌入（例如word2vec）已成功应用于电子商务产品。受上下文化嵌入带来的几个NLP任务的最近性能改进的启发，我们建议将类似于BERT的体系结构转移到电子商务：我们的模型--~\textit{Prod2BERT}经过训练，可以通过屏蔽会话建模生成产品的表示。通过在多个车间、不同任务和一系列设计选择上的广泛实验，我们系统地比较了~\textit{Prod2BERT}和~\textit{prod2vec}嵌入的准确性：虽然~\textit{Prod2BERT}在几种情况下都具有优势，我们强调资源和超参数在最佳性能模型中的重要性。最后，我们为实践者提供了在各种计算和数据约束下培训嵌入的指南。</pre></li>
<li><a href="https://arxiv.org/abs/2102.04887">NewsBERT: Distilling Pre-trained Language Model for Intelligent News Application</a>
<pre>像BERT这样的预训练语言模型（plm）在自然语言处理方面取得了巨大的进步。新闻文章通常包含丰富的文本信息，PLMs有潜力为各种智能新闻应用（如新闻推荐和检索）增强新闻文本建模。然而，大多数现有的PLM都是具有数亿个参数的大型PLM。许多在线新闻应用程序需要以低延迟容忍度为数百万用户提供服务，这给在这些场景中整合PLM带来了巨大挑战。知识提取技术可以将一个较大的PLM压缩为一个较小的PLM，同时保持良好的性能。然而，现有的语言模型是在像维基百科这样的通用语料库上预先训练和提炼的，维基百科与新闻领域有一些差距，对于新闻智能来说可能是次优的。在本文中，我们提出了NewsBERT，它可以提取PLM以实现高效的新闻智能。在我们的方法中，我们设计了一个师生联合学习和提炼框架，以协作学习教师和学生模型，其中学生模型可以从教师模型的学习经验中学习。此外，我们还提出了一种动量蒸馏方法，将教师模型的梯度加入到学生模型的更新中，以更好地传递教师模型所学的有用知识。在两个具有三个任务的真实数据集上进行的大量实验表明，NewsBERT可以在各种智能新闻应用程序中有效地提高模型性能，而模型要小得多。</pre></li>
<li><a href="https://arxiv.org/abs/2005.01634">Code and Named Entity Recognition in StackOverflow</a> (ACL2020) [<a href="https://github.com/lanwuwei/BERTOverflow">github</a>]
<pre>随着大型编程文本语料库在互联网上的普及，人们对将自然语言和计算机代码结合起来研究越来越感兴趣。例如，StackOverflow目前有850万用户编写了超过1500万个与编程相关的问题。同时，仍然缺乏识别自然语言句子中出现的代码标记或软件相关命名实体的基本NLP技术。在本文中，我们介绍了一个新的计算机编程领域命名实体识别（NER）语料库，由15372个句子和20种细粒度实体类型组成。我们对StackOverflow中的1.52亿句句子进行了域BERT表示（BERTOverflow）训练，这导致与现成的BERT相比，绝对增加+10 F-1分数。我们还介绍了Softer模型，该模型在StackOverflow数据上实现了代码和命名实体识别的总分数为79.10 F$1$。我们的Softer模型结合了一个上下文无关的代码标记分类器和语料库级特征，以改进基于BERT的标记模型。我们的代码和数据可从以下网址获得：https://github.com/jeniyat/StackOverflowNER/</pre></li>
<li><a href="https://arxiv.org/abs/2005.10200">BERTweet: A pre-trained language model for English Tweets</a> (EMNLP2020 Demo)
<pre>我们介绍BERTweet，这是第一个针对英语推文的公共大规模预培训语言模型。我们的BERTweet具有与BERT base相同的架构（Devlin等人，2019年），使用RoBERTa预培训程序进行培训（Liu等人，2019年）。实验表明，BERTweet优于强基线RoBERTa-base和XLM-R-base（Conneau et al.，2020），在三项推特NLP任务（词性标记、命名实体识别和文本分类）上产生了比以前最先进的模型更好的性能结果。我们在麻省理工学院的许可下发布BERTweet，以促进未来对推特数据的研究和应用。我们的BERTweet可在https://github.com/VinAIResearch/BERTweet</pre></li>
<li><a href="https://arxiv.org/abs/2010.11091">TweetBERT: A Pretrained Language Representation Model for Twitter Text Analysis</a>
<pre>Twitter是一个著名的微博社交网站，用户可以在其中实时表达自己的观点和意见。因此，推特往往包含有价值的信息。随着自然语言处理领域深度学习的发展，从tweet中提取有意义的信息成为自然语言研究者越来越感兴趣的话题。应用现有的语言表示模型从Twitter中提取信息通常不会产生好的结果。此外，目前还没有专门针对社交媒体领域的文本分析语言表示模型。因此，在本文中，我们将介绍两个TweetBERT模型，它们是特定于领域的语言表示模型，在数百万条Tweet上预先训练。我们发现，在每个Twitter数据集上，TweetBERT模型在Twitter文本挖掘任务中的表现明显优于传统的BERT模型，超过7%。我们还通过评估31个不同数据集上的7个BERT模型，提供了广泛的分析。我们的结果验证了我们的假设，即在twitter语料库上持续训练语言模型有助于提高twitter的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2104.07944">A Million Tweets Are Worth a Few Points: Tuning Transformers for Customer Service Tasks</a>
<pre>在在线领域特定的客户服务应用程序中，由于数据集的可用性和噪音有限，许多公司难以成功部署高级NLP模型。虽然先前的研究表明，为特定领域的任务迁移大型开放领域预培训模型具有潜力，但在此类社交媒体客户服务环境中，特别是在多语言条件下，尚未严格评估适当的（预）培训策略。我们通过收集包含客户服务对话（865k推特）的多语言社交媒体语料库，比较各种预培训和微调方法，将其应用于5项不同的最终任务，来解决这一差距。我们表明，在对特定的最终任务进行微调之前，在域内数据集上预先训练通用多语言转换器模型，可以持续提高性能，特别是在非英语环境中。</pre></li>
<li><a href="https://arxiv.org/abs/2104.10259">Analyzing COVID-19 Tweets with Transformer-based Language Models</a>
<pre>本文描述了一种使用基于转换器的语言模型（Transformer-based Language Models，TLM）从社交媒体帖子中理解公众意见的方法。在这种方法中，我们在几个新冠病毒-19推特语料库上训练了一组GPT模型，这些语料库反映了具有不同观点的用户群体。然后，我们使用基于提示的查询来探索这些模型，以揭示用户的偏见和观点。我们演示了如何使用这种方法来产生类似于就各种社会、政治和公共卫生问题对公众进行投票的结果。对新冠病毒-19推特数据的研究结果表明，transformer语言模型是有希望的工具，可以帮助我们在一定程度上理解社交媒体上的公众意见。</pre></li>
<li><a href="https://arxiv.org/abs/2010.01150">Cost-effective Selection of Pretraining Data: A Case Study of Pretraining BERT on Social Media</a> (EMNLP2020 Findings)
<pre>最近对领域特定的BERT模型的研究表明，当模型在领域内数据上进行预训练时，可以提高下游任务的有效性。通常，这些模型中使用的预训练数据是根据其主题选择的，例如生物学或计算机科学。考虑到使用社交媒体文本的应用范围及其独特的语言多样性，我们分别对tweet和论坛文本进行了预训练，并实证证明了这两种资源的有效性。此外，我们还研究了如何使用相似性度量来指定域内预训练数据。我们公开在https://bit.ly/35RpTf0.</pre></li>
</ul>
<h1 id="multi-modal">Multi-modal</h1>
<ul>
<li><a href="https://arxiv.org/abs/2012.12556">A Survey on Visual Transformer</a>
<pre>Transformer是一种主要基于自我注意机制的深层神经网络，首次应用于自然语言处理领域。由于其强大的表示能力，研究人员正在寻找将transformer应用于计算机视觉任务的方法。在各种可视化基准测试中，基于变压器的模型的性能与其他类型的网络（如卷积网络和递归网络）相似或更好。由于它的高性能和较少的视觉特定的电感偏置需要，变压器正受到越来越多的关注，从计算机视觉界。在本文中，我们回顾了这些视觉转换器模型，将它们分为不同的任务，并分析了它们的优缺点。我们探讨的主要类别包括主干网络、高/中级视觉、低级视觉和视频处理。我们还包括有效的变压器方法，用于将变压器推进基于设备的实际应用。此外，我们还简要介绍了计算机视觉中的自我注意机制，因为它是变压器的基本组成部分。在本文的最后，我们讨论了视觉转换器面临的挑战，并提供了几个进一步的研究方向。</pre></li>
<li><a href="https://arxiv.org/abs/2101.01169">Transformers in Vision: A Survey</a>
<pre>从自然语言任务的变换模型中得到的惊人结果引起了视觉界的兴趣，他们开始研究它们在计算机视觉问题中的应用。在其显著的优点中，变压器能够对输入序列元素之间的长期依赖性进行建模，并支持序列的并行处理，与循环网络（如长短时记忆（LSTM））相比。与卷积网络不同，变压器的设计需要最小的电感偏置，并且自然适合作为设定函数。此外，变压器的简单设计允许使用类似的处理块处理多种模式（例如，图像、视频、文本和语音），并展示了对超大容量网络和大型数据集的极好可扩展性。这些优势使得使用Transformer networks的许多愿景任务取得了令人振奋的进展。本调查旨在全面概述计算机视觉学科中的变压器模型。我们首先介绍变压器成功背后的基本概念，即自我关注、大规模预培训和双向编码。然后，我们介绍了变形金刚在视觉中的广泛应用，包括流行的识别任务（例如，图像分类、对象检测、动作识别和分割）、生成建模、多模式任务（例如，视觉问答、视觉推理和视觉接地）、视频处理（例如，活动识别、视频预测）、低级视觉（例如，图像超分辨率、图像增强和彩色化）和3D分析（例如，点云分类和分割）我们比较了流行技术在建筑设计和实验价值方面各自的优势和局限性。最后，我们对开放的研究方向和未来可能的工作进行了分析。</pre></li>
<li><a href="https://arxiv.org/abs/1904.01766">VideoBERT: A Joint Model for Video and Language Representation Learning</a> (ICCV2019)
<pre>自我监督学习对于利用YouTube等平台上丰富的未标记数据变得越来越重要。鉴于大多数现有的方法学习低级表征，我们提出了一种联合视觉语言模型来学习高级特征，而无需任何明确的监督。特别是，受其最近在语言建模方面的成功启发，我们基于伯特模型学习视觉和语言标记序列上的双向联合分布，分别来自视频数据的矢量量化和现成的语音识别输出。我们在许多任务中使用VideoBERT，包括动作分类和视频字幕。我们证明了它可以直接应用于开放式词汇分类，并证实了大量的训练数据和跨模态信息对性能至关重要。此外，我们在视频字幕方面的表现优于最新技术，定量结果验证了该模型能够学习高级语义特征。</pre></li>
<li><a href="https://arxiv.org/abs/1908.02265">ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks</a> (NeurIPS2019)
<pre>我们提出了ViLBERT（Vision and Language BERT的缩写），一个学习图像内容和自然语言的任务不可知联合表示的模型。我们将流行的BERT体系结构扩展到多模态双流模型，在通过共同注意转换层交互的独立流中处理视觉和文本输入。我们在自动收集的大型概念标题数据集上通过两个代理任务对模型进行预训练，然后将其转移到多个已建立的视觉和语言任务——视觉问答、视觉常识推理、引用表达式、，以及基于字幕的图像检索——只需对基础架构进行少量添加。与现有的任务特定模型相比，我们观察到任务之间的显著改进——在所有四项任务上都达到了最先进的水平。我们的工作表明，从仅仅作为任务训练一部分的视觉和语言之间的学习基础转变为将视觉基础视为一种可训练和可转移的能力。</pre></li>
<li><a href="https://arxiv.org/abs/1908.03557">VisualBERT: A Simple and Performant Baseline for Vision and Language</a>
<pre>我们提出了Visual伯特，一个简单而灵活的框架，用于建模广泛的视觉和语言任务。VisualBERT由一堆转换器层组成，这些层隐式地将输入文本的元素和关联输入图像中的区域与自我注意对齐。我们进一步提出了两个视觉基础语言模型目标，用于图像字幕数据的预训练。在包括VQA、VCR、NLVR2和Flickr30K在内的四项视觉和语言任务上的实验表明，VisualBERT在显著简化的同时，优于或与最先进的模型相抗衡。进一步的分析表明，Visual伯特可以在没有任何明确监督的情况下将语言的元素固定到图像区域，甚至对句法关系非常敏感，例如，跟踪动词和对应于其参数的图像区域之间的关联。</pre></li>
<li><a href="https://arxiv.org/abs/1906.02940">Selfie: Self-supervised Pretraining for Image Embedding</a>
<pre>我们介绍了一种称为Selfie的预训练技术，它代表Selfie监督的图像嵌入。Selfie利用对比预测编码损失（Oord et al.，2018），将伯特的蒙面语言建模概念（Devlin et al.，2019）推广到连续数据，如图像。给定输入图像中的遮罩面片，我们的方法学习从同一图像中采样的其他“干扰”面片中选择正确的面片来填充遮罩位置。这种分类目标避免了预测目标面片的精确像素值的需要。自拍的预训练体系结构包括一个卷积块网络，用于处理补丁，然后是一个注意池网络，用于在预测遮罩补丁之前总结未遮罩补丁的内容。在微调过程中，我们重用通过预训练找到的卷积权重。我们在三个基准（CIFAR-10、ImageNet 32 x 32和ImageNet 224 x 224）上使用不同数量的标记数据（从5%到100%的训练集）评估自拍。与同一网络的标准监督训练相比，我们的预训练方法在所有设置中对ResNet-50提供了一致的改进。值得注意的是，在ImageNet 224 x 224上，每类60个示例（5%），我们的方法将ResNet-50的平均精度从35.6%提高到46.7%，绝对精度提高了11.1个点。我们的预训练方法还提高了ResNet-50训练稳定性，特别是在低数据状态下，通过显著降低不同运行测试精度的标准偏差。</pre></li>
<li><a href="https://arxiv.org/abs/2001.07966">ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data</a>
<pre>本文介绍了一种新的视觉语言预训练模型——ImageBERT，用于图像-文本联合嵌入。我们的模型是一个基于变压器的模型，它以不同的模态作为输入，并对它们之间的关系进行建模。该模型同时在四个任务上进行预训练：蒙蔽语言建模（MLM）、蒙蔽对象分类（MOC）、蒙蔽区域特征回归（MRFR）和图像文本匹配（ITM）。为了进一步提高预训练质量，我们从Web上收集了一个大规模的弱监督图像文本（LAIT）数据集。我们首先在此数据集上预训练模型，然后对概念标题和SBU标题进行第二阶段预训练。我们的实验表明，多阶段预训练策略优于单阶段预训练策略。我们还针对图像检索和文本检索任务对预先训练好的ImageBERT模型进行了微调和评估，并在MSCOCO和Flickr30k数据集上取得了最新成果。</pre></li>
<li><a href="https://arxiv.org/abs/2108.10904">SimVLM: Simple Visual Language Model Pretraining with Weak Supervision</a> (ICLR2022)
<pre>随着视觉和文本表征联合建模的最新进展，视觉语言预训练（VLP）在许多多模态下游任务中取得了令人印象深刻的性能。然而，对昂贵注释（包括清晰的图像标题和区域标签）的要求限制了现有方法的可扩展性，并且由于引入了多个特定于数据集的目标，使预训练过程变得复杂。在这项工作中，我们放松了这些限制，并提出了一个最低限度的预训练框架，名为简单视觉语言模型（SimVLM）。与之前的工作不同，SimVLM通过利用大规模弱监督来降低训练复杂性，并且使用单前缀语言建模目标进行端到端训练。在不使用额外数据或特定任务定制的情况下，生成的模型显著优于以前的预训练方法，并在一系列区分性和生成性视觉语言基准上实现了最新的结果，包括VQA（+3.74%VQA分数）、NLVR2（+1.17%准确度）、SNLI-VE（+1.37%准确度）和图像字幕任务（+10.1%苹果酒平均分数）。此外，我们还证明了SimVLM具有很强的泛化和迁移能力，能够实现零镜头行为，包括开放式视觉问答和跨模态迁移。</pre></li>
<li><a href="https://arxiv.org/abs/2107.07651">Align before Fuse: Vision and Language Representation Learning with Momentum Distillation</a> (NeurIPS2021) [<a href="https://github.com/salesforce/ALBEF">github</a>]
<pre>大规模的视觉和语言表征学习在各种视觉语言任务上取得了很大的进步。大多数现有方法使用基于转换器的多模式编码器来联合建模视觉标记（基于区域的图像特征）和单词标记。由于视觉标记和单词标记是不对齐的，因此对于多模态编码器来说，学习图像-文本交互是一个挑战。在本文中，我们引入了一种对比损失法，在通过跨模态注意将图像和文本表征融合（ALBEF）之前，将它们对齐，从而实现更基础的视觉和语言表征学习。与大多数现有方法不同，我们的方法不需要边界框注释，也不需要高分辨率图像。为了提高从含噪网络数据中学习的能力，我们提出了动量蒸馏，这是一种从动量模型产生的伪目标中学习的自训练方法。我们从互信息最大化的角度对ALBEF进行了理论分析，表明不同的训练任务可以被解释为生成图像-文本对视图的不同方式。ALBEF在多个下游视觉语言任务上实现了最先进的性能。在图像文本检索方面，ALBEF优于在数量级较大的数据集上预先训练的方法。在VQA和NLVR$^2$上，与最新技术相比，ALBEF实现了2.37%和3.84%的绝对改善，同时具有更快的推理速度。代码和预先培训的模型可在https://github.com/salesforce/ALBEF/.</pre></li>
<li><a href="https://arxiv.org/abs/1906.05743">Contrastive Bidirectional Transformer for Temporal Representation Learning</a>
<pre>本文提出了一种视频特征的自监督学习方法，与现有方法相比，该方法可以显著提高下游任务（如视频分类、字幕和分割）的性能。我们的方法将文本序列的BERT模型扩展到实值特征向量序列的情况，用噪声对比估计（NCE）代替softmax损失。我们还展示了如何从ASR（自动语音识别）衍生的视觉特征序列和单词序列中学习表示，并展示了这种跨模态训练（如果可能）的帮助。</pre></li>
<li><a href="https://arxiv.org/abs/1908.05787">M-BERT: Injecting Multimodal Information in the BERT Structure</a>
<pre>最近基于转换器的上下文单词表示，包括BERT和XLNet，在NLP的多个学科中显示了最先进的性能。在特定于任务的数据集上微调经过培训的上下文模型一直是实现优异性能的关键。虽然对这些预先训练好的模型进行微调对于词汇应用程序（仅具有语言模态的应用程序）来说非常简单，但对于多模态语言（NLP中的一个日益增长的领域，重点是对面对面交流进行建模）来说，这并不是一件小事。预先训练过的模特没有必要的组件来接受视觉和听觉两种额外的模式。在本文中，我们提出了一种连接到BERT和XLNet的多模自适应门（MAG）。MAG允许BERT和XLNet在微调期间接受多模态非语言数据。它通过转换到BERT和XLNet的内部表示来实现这一点；以视觉和听觉方式为条件的转变。在我们的实验中，我们研究了用于多模态情绪分析的常用CMU-MOSI和CMU-MOSEI数据集。微调MAG-BERT和MAG XLNet显著提高了情感分析的性能，超过了之前的基线以及仅语言微调的BERT和XLNet。在CMU-MOSI数据集上，MAG XLNet首次在NLP社区实现了人的多模态情感分析性能。</pre></li>
<li><a href="https://arxiv.org/abs/1908.05787">Integrating Multimodal Information in Large Pretrained Transformers</a>
<pre>最近基于转换器的上下文单词表示，包括BERT和XLNet，在NLP的多个学科中显示了最先进的性能。在特定于任务的数据集上微调经过培训的上下文模型一直是实现优异性能的关键。虽然对这些预先训练好的模型进行微调对于词汇应用程序（仅具有语言模态的应用程序）来说非常简单，但对于多模态语言（NLP中的一个日益增长的领域，重点是对面对面交流进行建模）来说，这并不是一件小事。预先训练过的模特没有必要的组件来接受视觉和听觉两种额外的模式。在本文中，我们提出了一种连接到BERT和XLNet的多模自适应门（MAG）。MAG允许BERT和XLNet在微调期间接受多模态非语言数据。它通过转换到BERT和XLNet的内部表示来实现这一点；以视觉和听觉方式为条件的转变。在我们的实验中，我们研究了用于多模态情绪分析的常用CMU-MOSI和CMU-MOSEI数据集。微调MAG-BERT和MAG XLNet显著提高了情感分析的性能，超过了之前的基线以及仅语言微调的BERT和XLNet。在CMU-MOSI数据集上，MAG XLNet首次在NLP社区实现了人的多模态情感分析性能。</pre></li>
<li><a href="https://arxiv.org/abs/1908.07490">LXMERT: Learning Cross-Modality Encoder Representations from Transformers</a> (EMNLP2019)
<pre>视觉和语言推理要求理解视觉概念、语言语义，最重要的是，理解这两种模式之间的对齐和关系。因此，我们提出LXMERT（从变压器学习跨模态编码器表示）框架来学习这些视觉和语言连接。在LXMERT中，我们构建了一个大型转换器模型，该模型由三个编码器组成：对象关系编码器、语言编码器和跨模态编码器。接下来，为了使我们的模型具有连接视觉和语言语义的能力，我们通过五个不同的代表性预训练任务对模型进行了大量的图像和句子对预训练：掩蔽语言建模、掩蔽对象预测（特征回归和标签分类）、跨模态匹配、，形象问答。这些任务有助于学习模态内和模态间的关系。在对预先训练的参数进行微调后，我们的模型在两个可视化问答数据集（即VQA和GQA）上实现了最先进的结果。我们还通过将预先训练的跨模态模型应用于具有挑战性的视觉推理任务NLVR2，并将之前的最佳结果绝对值提高了22%（54%至76%），从而证明了该模型的通用性。最后，我们展示了详细的消融研究，以证明我们的新模型组件和预训练策略对我们的强大结果有显著贡献；并给出了不同编码器的几种注意可视化。代码和预先培训的模型可在以下网站公开获取：https://github.com/airsplay/lxmert</pre></li>
<li><a href="https://arxiv.org/abs/2010.12831">Unsupervised Vision-and-Language Pre-training Without Parallel Images and Captions</a> (NAACL2021)
<pre>经过预先培训的上下文视觉和语言（V&L）模型在各种基准上取得了令人印象深刻的性能。然而，现有的模型需要大量的并行图像字幕数据进行预训练。此类数据收集成本高昂，需要繁琐的管理。受无监督机器翻译的启发，我们研究了在没有图像字幕语料库的情况下，是否可以通过无监督预训练来学习强V&L表示模型。特别是，我们建议对纯文本和纯图像语料库进行“掩蔽和预测”预训练，并将对象识别模型检测到的对象标记作为锚定点，连接两种模式。我们发现，这样一种简单的方法在四个英语V&L基准上实现的性能接近于使用对齐数据预先训练的模型。我们的工作挑战了广泛接受的观点，即V&L预培训需要一致的数据，同时大大减少了V&L模型所需的监督量。</pre></li>
<li><a href="https://arxiv.org/abs/2009.11278">X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers</a> (EMNLP2020)
<pre>与蒙面语言模型的成功相呼应，ViLBERT、LXMERT和UNITER等视觉和语言同行在视觉问答和视觉接地等多种多模态辨别任务上实现了最先进的表现。最近的工作也成功地将这些模型应用于图像字幕的生成任务。这就引出了一个问题：这些模型是否可以反过来从文本片段生成图像？我们对这个模型家族中的一个流行代表——LXMERT——的分析发现，它无法用当前的训练设置生成丰富且语义有意义的图像。我们介绍了X-LXMERT，它是LXMERT的一个扩展，具有训练改进，包括：离散化视觉表示，使用具有大范围掩蔽率的均匀掩蔽，并将正确的预训练数据集与正确的目标对齐，从而使其能够绘制。X-LXMERT的图像生成能力可与最先进的生成模型相媲美，而其问答和字幕能力仍与LXMERT不相上下。最后，我们通过在UNITER中添加图像生成功能来生成X-UNITER，从而证明这些训练改进的通用性。</pre></li>
<li><a href="https://arxiv.org/abs/2005.07486">Adaptive Transformers for Learning Multimodal Representations</a> (ACL2020SRW) [<a href="https://github.com/prajjwal1/adaptive_transformer">github</a>]
<pre>变形金刚的使用已经从学习语言语义发展到形成有意义的语言表征。这些体系结构通常参数化过度，需要大量计算。在这项工作中，我们扩展了自适应方法，以了解更多关于模型可解释性和计算效率的信息。具体来说，我们研究了注意广度、稀疏和结构化的辍学方法，以帮助理解他们的注意力机制是如何扩展到视觉和语言任务的。我们进一步表明，这些方法可以帮助我们更多地了解网络如何感知输入序列的复杂性、不同模式的稀疏偏好以及其他相关现象。</pre></li>
<li><a href="https://arxiv.org/abs/2106.09889">GEM: A General Evaluation Benchmark for Multimodal Tasks</a> (ACL2021 Findings) [<a href="https://github.com/microsoft/GEM">github</a>]
<pre>在本文中，我们提出GEM作为多模式任务的通用评估基准。与现有的GLUE、SuperGLUE、XGLUE和XTREME等主要关注自然语言任务的数据集不同，GEM是一个大规模的视觉语言基准测试，它包括用于图像语言任务的GEM-I和用于视频语言任务的GEM-V。与现有的多模态数据集（如用于图像语言任务的MSCOCO和Flicker30K、用于视频语言任务的YouCook2和MSR-VTT）相比，GEM不仅是同时涵盖图像语言任务和视频语言任务的最大视觉语言数据集，而且还以多种语言进行标记。我们还为此基准提供了两个基准模型。我们将发布数据集、代码和基线模型，旨在推动多语言多模态研究的发展。</pre></li>
<li><a href="https://arxiv.org/abs/1908.05054">Fusion of Detected Objects in Text for Visual Question Answering</a> (EMNLP2019)
<pre>为了推进多模态环境的模型，我们引入了一种简单但功能强大的数据神经架构，该架构结合了视觉和自然语言。“文本转换器中的边界框”（B2T2）还利用引用信息将单词绑定到单个统一体系结构中的图像部分。B2T2在视觉常识推理基准上非常有效(https://visualcommonsense.com)，实现了新的技术水平，与公布的基线相比，错误率相对降低了25%，并在公共排行榜上获得了迄今为止的最佳表现（截至2019年5月22日）。详细的分析表明，将视觉特征早期集成到文本分析中是新体系结构有效性的关键。提供了我们模型的参考实现(https://github.com/google-research/language/tree/master/language/question_answering/b2t2).</pre></li>
<li><a href="https://arxiv.org/abs/2101.11272">VisualMRC: Machine Reading Comprehension on Document Images</a> (AAAI2021)
<pre>最近关于机器阅读理解的研究主要集中在文本层面的理解，但还没有达到人类对真实世界文档的视觉布局和内容的理解水平。在本研究中，我们引入了一个新的视觉机器阅读理解数据集VisualMRC，其中给定一个问题和一个文档图像，机器阅读并理解图像中的文本以自然语言回答问题。与现有的以图像形式包含文本的可视化问答（VQA）数据集相比，VisualMRC更注重发展自然语言理解和生成能力。它包含30000多对来自多个网页领域的10000多个文档图像的问题和抽象答案。我们还引入了一个新的模型，将现有的序列扩展到序列模型，并使用大规模文本语料库进行预训练，以考虑文档的视觉布局和内容。使用VisualMRC进行的实验表明，该模型优于基本序列对序列模型和最先进的VQA模型。然而，在大多数自动评估指标上，它的性能仍然低于人类。该数据集将促进旨在连接视觉和语言理解的研究。</pre></li>
<li><a href="https://openreview.net/forum?id=xTJEN-ggl1b">LambdaNetworks: Modeling long-range Interactions without Attention</a> [<a href="https://github.com/gsarti/lambda-bert">github</a>]</li>
<li><a href="http://openaccess.thecvf.com/content_WACV_2020/html/Yang_BERT_representations_for_Video_Question_Answering_WACV_2020_paper.html">BERT representations for Video Question Answering</a> (WACV2020)</li>
<li><a href="https://arxiv.org/abs/2009.08043">Self-supervised pre-training and contrastive representation learning for multiple-choice video QA</a> (AAAI2021)
<pre>视频问答（videoqa）需要对视频和语言模式进行细粒度的理解，才能回答给定的问题。在本文中，我们提出了一种新的选择题视频问答训练方案，该方案包括一个自我监督的预训练阶段和一个作为辅助学习的主阶段监督对比学习。在自我监督的预训练阶段，我们将预测正确答案的原始问题格式转换为预测相关问题的格式，以提供具有更广泛上下文输入的模型，而无需任何进一步的数据集或注释。对于主要阶段的对比学习，我们将掩蔽噪声添加到对应于地面真值答案的输入，并将地面实况答案的原始输入视为正样本，同时将其余的处理为负样本。通过将正样本映射到更接近屏蔽输入的位置，我们表明模型性能得到了改善。我们进一步利用局部对齐的注意力更有效地关注与给定的相应字幕句子特别相关的视频帧。我们在与多项选择视频质量保证（TVQA、TVQA+和TVQA）相关的高度竞争的基准数据集上评估我们提出的模型。实验结果表明，我们的模型在所有数据集上都达到了最先进的性能。我们还通过进一步的分析来验证我们的方法。</pre></li>
<li><a href="https://arxiv.org/abs/2012.15409">UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning</a> (ACL2021)
<pre>现有的预训练方法要么侧重于单一模态任务，要么侧重于多模态任务，不能有效地相互适应。它们只能使用单一模式数据（即文本或图像）或有限的多模式数据（即图像-文本对）。在这项工作中，我们提出了一个统一的模态预训练体系结构，即UNIMO，它可以有效地适应单模态和多模态的理解和生成任务。大规模的自由文本语料库和图像集合可用于提高视觉和文本理解能力，跨模态对比学习（CMCL）可用于在图像-文本对语料库上将文本和视觉信息对齐到统一的语义空间。由于非配对的单模态数据非常丰富，我们的模型可以利用更大的数据规模来学习更多的广义表示。此外，文本知识和视觉知识可以在统一的语义空间中相互增强。实验结果表明，UNIMO显著提高了多个单峰和多峰下游任务的性能。我们的代码和预先培训的模型在https://github.com/PaddlePaddle/Research/tree/master/NLP/UNIMO</pre></li>
<li><a href="https://arxiv.org/abs/2201.12086">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</a> [<a href="https://github.com/salesforce/BLIP">github</a>]
<pre>视觉语言预训练（VLP）提高了许多视觉语言任务的表现。然而，大多数现有的预训练模型只擅长于基于理解的任务或基于生成的任务。此外，性能改进在很大程度上是通过使用从web收集的有噪声的图像-文本对来扩展数据集实现的，这是一个次优的监督来源。在本文中，我们提出了BLIP，这是一个新的VLP框架，它可以灵活地转换到视觉语言理解和生成任务。BLIP通过引导字幕有效地利用了有噪声的网络数据，字幕员生成合成字幕，过滤器去除有噪声的字幕。我们在广泛的视觉语言任务上取得了最先进的成果，例如图像文本检索（平均增长2.7%）recall@1)图像字幕（苹果酒中为2.8%）和VQA（VQA分数中为1.6%）。当以零拍方式直接转换到视频语言任务时，BLIP也表现出很强的泛化能力。代码、模型和数据集发布于https://github.com/salesforce/BLIP.</pre></li>
<li><a href="https://arxiv.org/abs/2201.04026">Uni-EDEN: Universal Encoder-Decoder Network by Multi-Granular Vision-Language Pre-training</a>
<pre>视觉语言预训练是一个新兴且发展迅速的研究课题，它将多模态知识从资源丰富的预训练任务转移到资源有限的下游任务。与主要学习单个通用编码器的现有作品不同，我们提出了一个可预训练的通用编码器-解码器网络（Uni-EDEN），以促进视觉语言感知（例如，视觉问答）和生成（例如，图像字幕）。Uni-EDEN是一个基于两流转换器的结构，由三个模块组成：分别学习每个模态表示的对象和句子编码器，以及通过模态间交互实现多模态推理和句子生成的句子解码器。考虑到每个图像的语言表示可以跨越这个层次结构中的不同粒度，包括从简单到全面的单个标签、一个短语和一个自然句子，我们通过多粒度视觉语言代理任务对Uni-EDEN进行预训练：掩蔽对象分类（MOC）、掩蔽区域短语生成（MRPG）、图像句子匹配（ISM），和隐藏句子生成（MSG）。通过这种方式，Uni-EDEN被赋予了多模态表示提取和语言建模的能力。大量的实验证明了Uni-EDEN具有令人信服的通用性，方法是将其微调为四种视觉语言感知和生成下游任务。</pre></li>
<li><a href="https://arxiv.org/abs/2007.13135">Contrastive Visual-Linguistic Pretraining</a>
<pre>最近提出了几种多模态表示学习方法，如LXMERT和ViLBERT。由于在大规模多模态预训练过程中捕获了高级语义信息，因此这种方法可以获得优异的性能。然而，由于ViLBERT和LXMERT采用视觉区域回归和分类损失，基于视觉基因组数据集上预先训练的视觉特征，他们经常遇到域间隙和噪声标签问题。为了克服这些问题，我们提出了无偏对比视觉语言预训练（CVLP），它构建了一个基于对比学习的视觉自我监督损失。我们在几个下游任务（包括VQA、GQA和NLVR2）上评估了CVLP，以验证对比学习在多模态表征学习中的优势。我们的代码可从以下网址获得：https://github.com/ArcherYunDong/CVLP-.</pre></li>
<li><a href="https://arxiv.org/abs/2010.07999">What is More Likely to Happen Next? Video-and-Language Future Event Prediction</a> (EMNLP2020)
<pre>给出一段有对齐对话的视频，人们通常可以推断下一步更可能发生什么。做出这样的预测不仅需要深入理解视频和对话背后的丰富动态，还需要大量的常识知识。在这项工作中，我们探索人工智能模型是否能够学习做出这样的多模态常识下一个事件预测。为了支持这方面的研究，我们收集了一个名为视频和语言事件预测（VLEP）的新数据集，其中包含来自10234个不同电视节目和YouTube生活方式Vlog视频剪辑的28726个未来事件预测示例（及其理据）。为了促进非平凡挑战性示例的收集，我们采用了对抗性的人与模型在环数据收集过程。我们还提供了一个强大的基线，包括来自视频、对话和常识的信息。实验表明，每种类型的信息对于这项具有挑战性的任务都是有用的，并且与VLEP上的高人类性能相比，我们的模型提供了一个良好的起点，但为未来的工作留下了很大的空间。我们的数据集和代码位于：https://github.com/jayleicn/VideoLanguageFuturePred</pre></li>
<li><a href="https://arxiv.org/abs/2102.10407">VisualGPT: Data-efficient Image Captioning by Balancing Visual Input and Linguistic Knowledge from Pretraining</a>
<pre>从少量训练数据中快速学习的能力拓宽了机器学习应用的范围。在本文中，我们提出了一个数据高效的图像字幕模型VisualGPT，该模型利用了大型预训练语言模型（LM）中的语言知识。一个关键的挑战是如何在图像中视觉信息的使用和预先训练中获得的语言知识之间取得平衡。我们设计了一种新的自恢复编码器-解码器注意机制，用于在少量域内训练数据上快速适应预训练LM作为语言解码器。建议的自复活激活单元产生稀疏激活，但降低了对零梯度的敏感性。我们在0.1%、0.5%和1%的MSCOCO和概念性字幕训练数据上训练提出的模型VisualGPT。在这些条件下，我们的表现优于最佳基线模型，在MS COCO上高达10.8%的苹果酒，在概念性标题上高达5.4%的苹果酒。此外，VisualGPT在医学报告生成数据集IUX-ray上实现了最先进的结果。据我们所知，这是第一项利用单峰数据预训练的LM提高图像字幕数据效率的工作。我们的代码可从以下网址获得：https://github.com/Vision-CAIR/VisualGPT.</pre></li>
<li><a href="https://arxiv.org/abs/2003.01473">XGPT: Cross-modal Generative Pre-Training for Image Captioning</a>
<pre>尽管许多基于BERT的跨模态预训练模型在图像文本检索和VQA等下游理解任务上产生了优异的结果，但它们不能直接应用于生成任务。在本文中，我们提出了一种新的跨模态生成预训练图像字幕的方法XGPT，该方法旨在通过三种新的生成任务，包括图像条件蒙蔽语言建模（IMLM）、图像条件去噪自动编码（IDA）和，和文本条件图像特征生成（TIFG）。因此，预先训练的XGPT可以进行微调，而无需任何特定于任务的体系结构修改，以创建用于图像字幕的最先进模型。实验表明，XGPT在基准数据集上获得了最新的结果，包括COCO字幕和Flickr30k字幕。我们还使用XGPT生成新的图像标题，作为图像检索任务的数据补充，并在所有召回指标上实现显著改进。</pre></li>
<li><a href="https://arxiv.org/abs/2111.12233">Scaling Up Vision-Language Pre-training for Image Captioning</a>
<pre>近年来，基于视觉语言预训练（VLP）的图像字幕任务取得了显著的性能提升。规模被认为是这一进步的一个重要因素。然而，大多数现有工作只关注大约400万张图像上中等大小（例如12或24层）的预训练变压器。在本文中，我们介绍了LEMON，一个大规模的图像字幕，并提供了第一个关于VLP在图像字幕中缩放行为的实证研究。我们使用最先进的VinVL模型作为参考模型，该模型由图像特征提取程序和变压器模型组成，并对变压器进行上下缩放，模型大小从1300万到6.75亿个参数不等。在数据方面，我们对多达2亿对图像文本进行了实验，这些图像文本对是根据图像的alt属性（称为ALT200M）自动从web上收集的。广泛的分析有助于描述模型大小和训练前数据大小增加时的性能趋势。我们还比较了不同的训练方法，尤其是针对大规模噪声数据的训练。因此，LEMON在几个主要的图像字幕基准上达到了新的水平，包括COCO字幕、nocaps和概念性字幕。我们还展示了LEMON在以零镜头方式使用时，可以生成带有长尾视觉概念的字幕。</pre></li>
<li><a href="https://arxiv.org/abs/2112.05230">Injecting Semantic Concepts into End-to-End Image Captioning</a> (CVPR2022)
<pre>近年来，在开发更好的图像字幕模型方面取得了巨大进展，但大多数模型都依赖于单独的目标检测器来提取区域特征。最近的视觉语言研究正在转向无检测器趋势，利用网格表示进行更灵活的模型训练和更快的推理速度。然而，这种发展主要集中在图像理解任务上，而对字幕生成任务的研究较少。在本文中，我们研究了一种性能更好的无检测器图像字幕模型，并提出了一种基于纯视觉转换器的图像字幕模型，称为ViTCAP，该模型使用网格表示，而不提取区域特征。为了提高性能，我们引入了一种新的概念令牌网络（CTN）来预测语义概念，然后将它们合并到端到端字幕中。具体而言，CTN是基于视觉转换器构建的，旨在通过分类任务预测概念标记，其中包含的丰富语义信息对字幕任务非常有利。与以前基于检测器的模型相比，ViTCAP大大简化了体系结构，同时在各种具有挑战性的图像字幕数据集上实现了具有竞争力的性能。尤其值得一提的是，ViTCAP在COCO caption Karpath split上的苹果酒得分为138.1分，在nocaps和Google CC字幕数据集上的苹果酒得分分别为93.8分和108.6分。</pre></li>
<li><a href="https://arxiv.org/abs/1909.11059">Unified Vision-Language Pre-Training for Image Captioning and VQA</a> (AAAI2020) [<a href="https://github.com/LuoweiZhou/VLP">github</a>]
<pre>本文提出了一个统一的视觉语言预训练（VLP）模型。该模型的统一之处在于：（1）它可以针对视觉语言生成（例如，图像字幕）或理解（例如，视觉问答）任务进行微调；（2）它使用共享的多层转换网络进行编码和解码，这与许多现有方法不同，其中编码器和解码器使用单独的模型实现。统一的VLP模型使用两个任务的无监督学习目标在大量图像-文本对上进行预训练：双向和序列到序列（seq2seq）掩蔽视觉语言预测。这两项任务仅在预测条件的上下文中有所不同。这是通过为共享变压器网络使用特定的自我注意面具来控制的。据我们所知，VLP是第一个报告的模型，它在视觉语言生成和理解任务方面实现了最先进的结果，与图像字幕和视觉问答一样，跨越三个具有挑战性的基准数据集：COCO字幕、Flickr30k字幕和VQA 2.0。代码和预先培训的模型可在https://github.com/LuoweiZhou/VLP.</pre></li>
<li><a href="https://arxiv.org/abs/2012.04638">TAP: Text-Aware Pre-training for Text-VQA and Text-Caption</a>
<pre>在本文中，我们提出了文本VQA和文本标题任务的文本感知预训练（TAP）。这两项任务的目的是阅读和理解图像中的场景文本，分别用于问答和图像字幕生成。与无法捕获场景文本及其与视觉和文本模式的关系的传统视觉语言预训练不同，TAP在预训练中明确地将场景文本（由OCR引擎生成）合并。通过三项预训练任务，包括蒙面语言建模（MLM）、图像-文本（对比）匹配（ITM）和相对（空间）位置预测（RPP），TAP有效地帮助模型在三种模式（文本-单词、视觉对象和场景-文本）之间学习更好的对齐表示。由于这种对齐表示学习，即使是在同一下游任务数据集上预先训练，TAP已经将TextVQA数据集的绝对准确度提高了+5.4%，而非TAP基线。为了进一步提高性能，我们基于概念性字幕数据集OCR-CC构建了一个大规模数据集，其中包含140万个场景文本相关的图像-文本对。在这个OCR-CC数据集上预先训练后，我们的方法在多个任务上大幅度优于最新技术，即TextVQA的准确率为+8.3%，ST-VQA的准确率为+8.6%，TextCaps的苹果酒分数为+10.2。</pre></li>
<li><a href="https://arxiv.org/abs/2109.05014">An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA</a> (AAAI2022)
<pre>基于知识的视觉问答（VQA）涉及回答需要图像中不存在的外部知识的问题。现有的方法首先从外部资源中检索知识，然后对所选知识、输入图像和问题进行推理以预测答案。然而，这种两步方法可能会导致不匹配，从而可能限制VQA性能。例如，检索到的知识可能有噪声，与问题无关，推理过程中重新嵌入的知识特征可能会偏离其在知识库（KB）中的原始含义。为了应对这一挑战，我们提出了基于知识的VQA的PICa，这是一种简单而有效的方法，通过使用图像字幕提示GPT3。受GPT-3在知识检索和问答方面的强大功能的启发，我们不再像以前那样使用结构化知识库，而是将GPT-3视为一个隐式的非结构化知识库，可以共同获取和处理相关知识。具体地说，我们首先将图像转换为GPT-3可以理解的标题（或标签），然后调整GPT-3，通过提供一些上下文中的VQA示例，以几种简单的方式解决VQA任务。我们通过仔细研究来进一步提高性能：（i）哪些文本格式最能描述图像内容，以及（ii）如何更好地选择和使用上下文中的示例。PICa首次将GPT-3用于多模式任务。通过仅使用16个示例，PICa在OK-VQA数据集上的绝对+8.6分超过了受监督的最新水平。我们还在VQAv2上对PICa进行了基准测试，在VQAv2上，PICa还显示了良好的几次拍摄性能。</pre></li>
<li><a href="https://arxiv.org/abs/2102.10772">Transformer is All You Need: Multimodal Multitask Learning with a Unified Transformer</a>
<pre>我们提出了UniT，一个统一的转换器模型，用于同时学习不同领域中最突出的任务，从目标检测到自然语言理解和多模态推理。基于transformer编码器-解码器架构，我们的单元模型使用编码器对每个输入模态进行编码，并使用编码输入表示的共享解码器对每个任务进行预测，然后是特定于任务的输出头。整个模型是端到端联合训练的，每个任务都有损失。与以前使用transformers进行多任务学习的工作相比，我们在所有任务中共享相同的模型参数，而不是单独微调特定于任务的模型，并在不同领域处理更多种类的任务。在我们的实验中，我们在8个数据集上共同学习了7项任务，在每个任务上都取得了很好的性能，而且参数明显较少。我们的代码以MMF格式提供，网址为https://mmf.sh.</pre></li>
<li><a href="https://arxiv.org/abs/2004.10796">VisualCOMET: Reasoning about the Dynamic Context of a Still Image</a> (ECCV2020) [<a href="http://visualcomet.xyz">website</a>]
<pre>即使是从静止图像的单个帧，人们也可以对图像在帧之前、之后和帧之外的动态故事进行推理。例如，如果有一个人在水中挣扎着漂浮，我们可以推断他是在过去某个时候掉进水里的，他现在的目的是为了活下去，他在不久的将来需要帮助，否则他会被冲走。我们提出了VisualMet，这是视觉常识推理任务的新框架，用于预测之前可能发生的事件、接下来可能发生的事件以及当前人们的意图。为了支持对视觉常识推理的研究，我们介绍了第一个大规模的视觉常识图存储库，该存储库由超过140万个视觉常识推理的文本描述组成，这些文本描述在60000个不同的图像集上仔细注释，每个图像与之前和之后的简短视频摘要配对。此外，我们在图像中出现的人和文本常识描述中提到的人之间提供了人员基础（即共同参考链接），允许图像和文本之间更紧密的集成。我们在这项任务上建立了强大的基线性能，并证明了视觉和文本常识推理之间的整合是关键，并战胜了非整合方案。</pre></li>
<li><a href="https://arxiv.org/abs/1912.02379">Large-scale Pretraining for Visual Dialog: A Simple State-of-the-Art Baseline</a>
<pre>VisualDialog之前的工作集中于在VisDial上单独训练深层神经模型。相反，我们提出了一种在转换到VisualDialog之前利用相关vision语言数据集的预训练的方法。我们采用最近提出的ViLBERT（Lu等人，2019）模型进行多回合视觉接地对话。我们的模型在概念性标题和可视化问答数据集上进行了预训练，并在VisDial上进行了微调。我们最好的单一模型在NDCG和MRR方面的绝对性能比先前发表的作品（包括模型集合）高出1%以上。接下来，我们发现在Visdail中使用“密集”注释的额外微调会导致更高的NDCG——比我们的基础模型高出10%以上——但会损害MRR——比我们的基础模型低出17%以上！这突出了两个主要指标——NDCG和MRR——之间的权衡，我们发现这是由于密集的注释与问题的原始基本真相答案没有很好的相关性。</pre></li>
<li><a href="https://arxiv.org/abs/2004.13278">VD-BERT: A Unified Vision and Dialog Transformer with BERT</a> (EMNLP2020)
<pre>可视化对话是一项具有挑战性的视觉语言任务，对话代理需要通过对图像内容和对话历史的推理来回答一系列问题。以前的工作主要集中在各种注意机制来模拟这种复杂的互动。相比之下，在这项工作中，我们提出了VD-BERT，一个简单而有效的统一视觉对话转换器框架，它利用预先训练的BERT语言模型来完成视觉对话任务。该模型的统一之处在于：（1）它使用单个流转换器编码器捕获图像和多回合对话之间的所有交互；（2）它通过相同的体系结构无缝地支持答案排序和答案生成。更重要的是，我们通过基于视觉的培训使BERT能够有效地融合视觉和对话内容。无需对外部视觉语言数据进行预培训，我们的模型产生了新的技术状态，在视觉对话排行榜上的单模型和集成设置（74.54和75.35 NDCG分数）中均达到最高位置。我们的代码和预训练模型发布于https://github.com/salesforce/VD-BERT.</pre></li>
<li><a href="https://arxiv.org/abs/1908.08530">VL-BERT: Pre-training of Generic Visual-Linguistic Representations</a> (ICLR2020)
<pre>我们为视觉语言任务引入了一种新的可预先训练的泛型表示，称为视觉语言BERT（简称VL-BERT）。VL-BERT采用简单但功能强大的变换器模型作为主干，并将其扩展为以视觉和语言嵌入特征作为输入。其中，输入的每个元素要么是来自输入句子的单词，要么是来自输入图像的感兴趣区域（RoI）。它的设计适合大多数视觉语言任务。为了更好地利用泛型表示，我们在大规模概念标题数据集和纯文本语料库上预训练VL-BERT。大量的实证分析表明，预训练过程能够更好地整合视觉语言线索，有利于后续任务，如视觉常识推理、视觉问答和指称表达理解。值得注意的是，VL-BERT在VCR基准测试排行榜上取得了单一型号的第一名。代码发布于\url{https://github.com/jackroos/VL-BERT}.</pre></li>
<li><a href="https://arxiv.org/abs/1908.06066">Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training</a>
<pre>我们提出Unicoder VL，这是一种通用编码器，旨在通过预先培训的方式学习视觉和语言的联合表示。借鉴XLM和Unicoder等跨语言预训练模型的思想，将视觉和语言内容输入到跨模式预训练的多层转换器中，其中使用了三个预训练任务，包括蒙面语言建模（MLM）、蒙面对象分类（MOC）和视觉语言匹配（VLM）。前两个任务学习基于语言和视觉内容的输入标记上下文感知表示。最后一项任务试图预测图像和文本是否相互描述。在对大规模图像字幕对进行预训练后，我们将Unicoder VL转换为基于字幕的图像文本检索和视觉常识推理，只需增加一个输出层。我们在这两项任务上都取得了最先进的或可比的结果，并展示了跨模态预训练的强大能力。</pre></li>
<li><a href="https://arxiv.org/abs/1909.11740">UNITER: Learning UNiversal Image-TExt Representations</a>
<pre>联合图像-文本嵌入是大多数视觉和语言（V+L）任务的基础，在这些任务中，多模态输入被同时处理以实现联合视觉和文本理解。在本文中，我们介绍了UNITER，一种通用的图像-文本表示，通过对四个图像-文本数据集（COCO、视觉基因组、概念性字幕和SBU字幕）的大规模预训练学习，它可以通过联合多模式嵌入为异构下游V+L任务提供支持。我们设计了四个预训练任务：蒙蔽语言建模（MLM）、蒙蔽区域建模（MRM，有三个变体）、图像文本匹配（ITM）和词区域对齐（WRA）。与之前将联合随机掩蔽应用于两种模式的工作不同，我们在训练前任务中使用条件掩蔽（即，掩蔽语言/区域建模以图像/文本的完全观察为条件）。除了用于全局图像文本对齐的ITM之外，我们还通过使用最优传输（OT）提出了WRA，以明确鼓励在预训练期间在单词和图像区域之间进行细粒度对齐。综合分析表明，条件掩蔽和基于OT的WRA都有助于更好的预训练。我们还进行了彻底的消融研究，以找到训练前任务的最佳组合。大量实验表明，UNITER在六个V+L任务（超过九个数据集）中实现了最新水平，包括视觉问答、图像文本检索、引用表达理解、视觉常识推理、视觉蕴涵和NLVR$^2$。代码可在https://github.com/ChenRocks/UNITER.</pre></li>
<li><a href="https://arxiv.org/abs/2102.03334">ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision</a>
<pre>视觉和语言预培训（VLP）提高了各种联合视觉和语言下游任务的绩效。当前的VLP方法严重依赖于图像特征提取过程，其中大多数涉及区域监控（例如，目标检测）和卷积结构（例如，ResNet）。尽管在文献中被忽略，但我们发现它在（1）效率/速度方面存在问题，简单地提取输入特征需要比多模态交互步骤更多的计算；（2）表达能力，因为它是视觉嵌入器及其预定义视觉词汇表的表达能力的上限。在本文中，我们提出了一个最小的VLP模型，即视觉和语言转换器（ViLT），在这个意义上，视觉输入的处理被大大简化为与我们处理文本输入相同的无卷积方式。我们表明，ViLT比以前的VLP模型快几十倍，但具有竞争力或更好的下游任务性能。我们的代码和预先训练的重量可在https://github.com/dandelin/vilt.</pre></li>
<li><a href="https://arxiv.org/abs/1909.02950">Supervised Multimodal Bitransformers for Classifying Images and Text</a>
<pre>自监督双向变换器模型（如BERT）在各种文本分类任务中取得了显著的改进。然而，现代数字世界越来越多模态，文本信息通常伴随着其他模态，如图像。我们引入了一个有监督的多模比特变换器模型，该模型融合了来自文本和图像编码器的信息，并在各种多模分类基准任务上获得了最先进的性能，优于强基线，包括专门用于测量多模性能的硬测试集。</pre></li>
<li><a href="https://arxiv.org/abs/2003.13198">InterBERT: Vision-and-Language Interaction for Multi-modal Pretraining</a>
<pre>学习高级多模态表征的多模态预训练是向深度学习和人工智能迈进的又一步。在这项工作中，我们提出了一个新的模型，即InterBERT（用于交互的BERT），这是我们系列多模态预训练方法M6（多模态到多模态多任务巨型变压器）的第一个模型。该模型对不同形态的信息流之间的交互具有很强的建模能力。单流交互模块能够有效地处理多模态的信息，顶部的两流模块保持了各模态的独立性，避免了单模态任务的性能下降。我们使用三个预训练任务对模型进行预训练，包括遮罩分段建模（MSM）、遮罩区域建模（MRM）和图像文本匹配（ITM）；并在一系列视觉和语言下游任务上对模型进行微调。实验结果表明，InterBERT优于一系列强基线，包括最新的多模态预训练方法，分析表明MSM和MRM对预训练是有效的，我们的方法在单模态任务中可以达到与BERT相当的性能。此外，我们提出了一个大规模的中文多模态预训练数据集，并开发了中文InterBERT，这是第一个中文多模态预训练模型。我们从中国最大的电子商务平台——移动淘宝网（mobile Taobao）提出的310万对图文数据集上预训练中文InterBERT。我们对基于文本的图像检索模型进行了微调，最近我们在线部署了基于主题的推荐模型。</pre></li>
<li><a href="https://arxiv.org/abs/2011.15124">Multimodal Pretraining Unmasked: A Meta-Analysis and a Unified Framework of Vision-and-Language BERTs</a> (TACL2021)
<pre>在计算机视觉和自然语言处理中，大规模的预训练和特定任务的微调现在是许多任务的标准方法。最近，人们提出了多种方法来训练视觉和语言BERT，以应对人工智能这两个关键领域交叉点的挑战。这些模型可以分为单流或双流编码器。我们研究了这两个范畴之间的差异，并展示了如何在单一的理论框架下将它们统一起来。然后，我们进行对照实验，以识别五个V&L Bert之间的经验差异。我们的实验表明，训练数据和超参数导致了报告结果之间的大部分差异，但它们也揭示了嵌入层在这些大规模模型中起着关键作用。</pre></li>
<li><a href="https://arxiv.org/abs/2103.07829">SemVLP: Vision-Language Pre-training by Aligning Semantics at Multiple Levels</a>
<pre>大规模图像-文本对的视觉语言预训练（VLP）在跨模态表征的学习方面取得了快速的进展。现有的预训练方法要么直接将特征级的图像表示和文本表示连接起来作为单个流转换器的输入，要么使用两个流交叉模式转换器将图像-文本表示在高级语义空间对齐。在真实的图像-文本数据中，我们观察到，一些图像-文本对很容易在两种模式上对齐简单语义，而其他图像-文本对可能在更高级别的抽象后相关。因此，在本文中，我们提出了一种新的预训练方法SemVLP，它将图像和文本表示之间的低层和高层语义联合起来。该模型采用两种流行的方式进行迭代预训练：单流预训练以在细粒度特征级别上对齐，两流预训练以对齐高级语义，方法是使用带有可插拔跨模态注意模块的共享转换器网络。在四个成熟的视觉语言理解任务上进行了大量的实验，以证明所提出的SemVLP在将跨模态表示与不同语义粒度对齐方面的有效性。</pre></li>
<li><a href="https://arxiv.org/abs/2111.07991">LiT : Zero-Shot Transfer with Locked-image Text Tuning</a> (CVPR2022)
<pre>本文介绍了对比调整，这是一种简单的方法，使用对比训练来对齐图像和文本模型，同时仍然利用它们的预训练。在我们的实证研究中，我们发现锁定的预训练图像模型和解锁的文本模型效果最好。我们将这种对比调整的实例称为“锁定图像调整”（LiT），它只是教文本模型从预先训练好的图像模型中读出新任务的良好表示。LiT模型可以将零镜头转移到新的视觉任务中，如图像分类或检索。建议的LiT具有广泛的适用性；它可以可靠地使用多种预训练方法（有监督和无监督），并跨不同的体系结构（ResNet、Vision Transformers和MLP Mixer），使用三种不同的图像文本数据集。使用基于变压器的预训练ViT-g/14模型，LiT模型在ImageNet测试集上实现了84.5%的零炮传输精度，在具有挑战性的分布外ObjectNet测试集上实现了81.1%的零炮传输精度。</pre></li>
<li><a href="https://arxiv.org/abs/2103.06561">WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training</a>
<pre>近年来，人们对多模态预训练模型进行了深入的探索，以架起视觉和语言之间的桥梁。然而，大多数模型通过假设文本和图像模态之间存在很强的语义相关性，明确地模拟了图像-文本对之间的跨模态交互。由于这种强假设在现实场景中往往无效，我们选择隐式建模大规模多模态预训练的跨模态相关性，这是我们团队领导的中国项目“文兰”的重点。具体来说，基于图像-文本对的弱相关性假设，我们在跨模态对比学习框架下提出了一个双塔预训练模型BriVL。与采用简单对比学习方法的OpenAI CLIP不同，我们通过将最新的方法MoCo应用到跨模态场景中，设计了更高级的算法。通过构建一个大型的基于队列的字典，我们的BriVL可以在有限的GPU资源中包含更多的负样本。我们进一步构建了一个名为RUC CAS WenLan的大型中文多源图像文本数据集，用于对BriVL模型进行预训练。大量实验表明，预先训练的BriVL模型在各种下游任务上都优于UNITER和OpenAI CLIP。</pre></li>
<li><a href="https://arxiv.org/abs/2106.13488">Probing Inter-modality: Visual Parsing with Self-Attention for Vision-Language Pre-training</a> (NeurIPS2021)
<pre>视觉语言预训练（VLP）旨在从图像-文本对中学习多模态表示，并以微调方式为下游视觉语言任务服务。主流VLP模型采用CNN转换器架构，该架构将图像嵌入CNN，然后将图像和文本与转换器对齐。视觉内容之间的视觉关系在图像理解中起着重要作用，是跨模态对齐学习的基础。然而，CNN在视觉关系学习方面存在局限性，因为局部感受野在建模长期依赖性方面的弱点。因此，学习视觉关系和跨模态对齐这两个目标被封装在同一个变压器网络中。这样的设计可能会忽略每个目标的特殊特性，从而限制变压器中的跨模态对准学习。为了解决这个问题，我们提出了一种用于VLP的完全变换视觉嵌入，以更好地学习视觉关系，并进一步促进跨模态对齐。具体来说，我们提出了一个称为跨模态流（IMF）的指标来衡量视觉和语言模态（即跨模态）之间的相互作用。我们还在Transformer中设计了一种新的掩蔽优化机制，名为掩蔽特征回归（MFR），以进一步促进模态间学习。据我们所知，这是第一项探索变形金刚对VLP视觉特征学习的益处的研究。我们在广泛的视觉语言任务中验证了我们的方法，包括图像文本检索、视觉问答（VQA）、视觉蕴涵和视觉推理。我们的方法不仅优于最先进的VLP性能，而且在IMF指标上也显示出优势。</pre></li>
<li><a href="https://arxiv.org/abs/2106.01804">E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning</a> (ACL2021)
<pre>大规模图像-文本对的视觉语言预训练（VLP）在跨模态下游任务中取得了巨大成功。现有的预训练方法主要采用两步训练法，首先使用预训练的目标检测器提取基于区域的视觉特征，然后将图像表示和文本嵌入连接起来作为变压器的输入进行训练。然而，这些方法面临着使用特定对象检测器的任务特定视觉表示进行通用跨模态理解的问题，以及两级流水线的计算效率低下的问题。在本文中，我们提出了第一个用于V+L理解和生成的端到端视觉语言预训练模型，即E2E-VLP，在该模型中，我们构建了一个统一的转换框架来共同学习视觉表示以及图像和文本之间的语义对齐。我们采用统一的Transformer编码器-解码器体系结构，将目标检测和图像字幕纳入预训练任务，以增强视觉学习。一组广泛的实验已经在成熟的视觉语言下游任务上进行，以证明这种新的VLP范式的有效性。</pre></li>
<li><a href="https://arxiv.org/abs/2203.09067">UNIMO-2: End-to-End Unified Vision-Language Grounded Learning</a> (ACL2022)
<pre>视觉语言预训练（VLP）在各种跨模态下游任务中取得了令人印象深刻的成绩。然而，大多数现有方法只能从对齐的图像字幕数据中学习，并且严重依赖昂贵的区域特征，这极大地限制了它们的可扩展性和性能。在本文中，我们提出了一个端到端的统一模式预训练框架，即UNIMO-2，用于对齐图像字幕数据和未对齐的纯图像和纯文本语料库的联合学习。我们建立了一个统一的转换器模型，共同学习图像和文本之间的视觉表示、文本表示和语义对齐。特别是，我们建议通过共享扎根空间对图像和文本进行扎根学习，这有助于连接未对齐的图像和文本，并在不同类型的语料库上对齐视觉和文本语义空间。实验表明，我们的扎根学习方法可以改善文本和视觉语义对齐，从而提高在各种跨模态任务中的表现。此外，得益于对不同类型语料库的有效联合建模，我们的模型在单模态视觉和文本任务上也取得了令人印象深刻的性能。我们的代码和模型在UNIMO项目页面上公开https://unimo-ptm.github.io/.</pre></li>
<li><a href="https://arxiv.org/abs/2112.03857">Grounded Language-Image Pre-training</a> [<a href="https://github.com/microsoft/GLIP">github</a>]
<pre>本文提出了一个用于学习对象级、语言感知和语义丰富的视觉表征的扎根语言图像预训练（GLIP）模型。GLIP将目标检测和短语基础统一起来用于预训练。这种统一带来了两个好处：1）它允许GLIP从检测和接地数据中学习，以改进这两项任务，并引导一个良好的接地模型；2） GLIP可以通过以自我训练的方式生成基础框来利用大量的图像-文本对，从而使学习到的表示语义丰富。在我们的实验中，我们在27M基础数据上预训练GLIP，包括3M人类注释和24M网络爬网图像-文本对。学习到的表示法显示出很强的零拍和少量镜头可转移到各种对象级识别任务。1） 当直接在COCO和LVIS上进行评估时（在训练前没有看到COCO中的任何图像），GLIP分别达到49.8 AP和26.9 AP，超过了许多监督基线。2） 在COCO上进行微调后，GLIP在val上达到60.8 AP，在测试开发上达到61.5 AP，超过了之前的SoTA。3） 当转移到13个下游目标检测任务时，一个单发的GLIP与一个完全受监督的动态头部相匹敌。代码将在https://github.com/microsoft/GLIP.</pre></li>
<li><a href="https://arxiv.org/abs/2111.02358">VLMO: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts</a> [<a href="https://github.com/microsoft/unilm/tree/master/vlmo">github</a>]
<pre>我们提出了一个统一的视觉语言预训练模型（VLMo），该模型通过模块化变压器网络联合学习双编码器和融合编码器。具体来说，我们介绍了混合模式专家（MoME）转换器，其中每个模块包含一组特定于模式的专家和一个共享的自我注意层。由于MoME的建模灵活性，预训练VLMo可以作为视觉语言分类任务的融合编码器进行微调，或者用作高效图像文本检索的双编码器。此外，我们还提出了一种分段预训练策略，该策略有效地利用了图像-文本对之外的大规模纯图像和纯文本数据。实验结果表明，VLMo在各种视觉语言任务（包括VQA和NLVR2）上取得了最先进的结果。代码和预训练模型可在https://aka.ms/vlmo.</pre></li>
<li><a href="https://arxiv.org/abs/2101.00529">VinVL: Revisiting Visual Representations in Vision-Language Models</a>
<pre>本文详细研究了视觉语言（VL）任务中改进的视觉表示，并开发了一种改进的目标检测模型，以提供以对象为中心的图像表示。与最广泛使用的\emph{bottom-up and top-down}模型\cite{Anderson 2018bottom}相比，新模型更大，更好地设计用于VL任务，并且在结合多个公共注释对象检测数据集的更大训练语料库上预训练。因此，它可以生成更丰富的视觉对象和概念集合的表示。虽然以前的VL研究主要集中在改进视觉语言融合模型上，而不涉及目标检测模型的改进，但我们发现视觉特征在VL模型中非常重要。在我们的实验中，我们将新的目标检测模型生成的视觉特征输入基于转换器的VL融合模型\oscar\cite{li2020oscar}，并利用改进的方法\short\对VL模型进行预训练，并在广泛的下游VL任务中对其进行微调。我们的结果表明，新的视觉功能显著提高了所有VL任务的性能，在七个公共基准上创造了最新的结果。我们将向公众发布新的目标检测模型。</pre></li>
<li><a href="https://arxiv.org/abs/2111.02387">An Empirical Study of Training End-to-End Vision-and-Language Transformers</a> (CVPR2022) [<a href="https://github.com/zdou0830/METER">github</a>]
<pre>视觉和语言（VL）预培训已被证明对各种VL下游任务非常有效。虽然最近的研究表明，完全基于变压器的VL模型比以前基于区域特征的方法更有效，但它们在下游任务中的性能往往会显著下降。在本文中，我们介绍了一种多模式端到端变压器框架METER，通过它我们研究了如何以端到端的方式设计和预训练一个完全基于变压器的VL模型。具体来说，我们从多个维度剖析了模型设计：视觉编码器（例如，CLIP-ViT、Swin transformer）、文本编码器（例如，RoBERTa、DeBERTa）、多模式融合模块（例如，合并注意与共同注意）、架构设计（例如，仅编码器与编码器-解码器）和预训练目标（例如，蒙面图像建模）。我们进行了全面的实验，并就如何培训高性能VL变压器提供了见解。在VQAv2测试标准集上，仅使用4M图像进行预训练，METER的准确率达到77.64%，比最先进的基于区域特征的模型高出1.04%，比之前最好的完全基于变压器的模型高出1.6%。值得注意的是，进一步放大后，我们最好的VQA模型的准确率达到了80.54%。代码和预先训练的模型在https://github.com/zdou0830/METER.</pre></li>
<li><a href="https://arxiv.org/abs/2111.12085">Crossing the Format Boundary of Text and Boxes: Towards Unified Vision-Language Modeling</a>
<pre>在本文中，我们提出了UNICORN，一种视觉语言（VL）模型，它将文本生成和包围盒预测统一到一个单一的体系结构中。具体来说，我们将每个框量化为四个离散的框标记，并将它们序列化为一个序列，可以与文本标记集成。我们将所有VL问题描述为一个生成任务，其中目标序列由集成的文本和框标记组成。然后，我们训练变压器编码器-解码器以自回归的方式预测目标。有了这样一个统一的框架和输入输出格式，UNICORN在7个VL基准上实现了与任务相关的最新水平相当的性能，包括视觉基础、基础字幕、视觉问答和图像字幕任务。当使用多任务微调进行训练时，独角兽可以用一组参数处理不同的VL任务，从而跨越下游任务边界。我们表明，使用单一模型不仅可以节省参数，还可以进一步提高模型在某些任务上的性能。最后，UNICORN展示了推广到ImageNet对象定位等新任务的能力。</pre></li>
<li><a href="https://arxiv.org/abs/2111.10023">UFO: A UniFied TransfOrmer for Vision-Language Representation Learning</a>
<pre>在本文中，我们提出了一种单统一变换器（UFO），它能够处理单峰输入（如图像或语言）或多峰输入（如图像和问题的串联），用于视觉语言（VL）表征学习。现有方法通常为每个模态设计一个单独的网络和/或为多模态任务设计一个特定的融合网络。为了简化网络结构，我们使用了一个单变压器网络，并在VL预训练期间实施多任务学习，包括图像-文本对比损失、图像-文本匹配损失以及基于双向和seq2seq注意掩码的蒙面语言建模损失。在不同的预训练任务中，相同的变压器网络用作图像编码器、文本编码器或融合网络。根据经验，我们观察到不同任务之间的冲突较少，并在视觉问答、COCO图像字幕（交叉熵优化）和nocaps（SPICE）方面达到了新的水平。在其他下游任务上，例如图像文本检索，我们也取得了有竞争力的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2111.11432">Florence: A New Foundation Model for Computer Vision</a>
<pre>对我们多样而开放的世界的自动化视觉理解要求计算机视觉模型能够很好地进行概括，并对特定任务进行最小程度的定制，类似于人类视觉。计算机视觉基础模型是解决现实世界计算机视觉应用的关键任务，它在不同的大规模数据集上进行训练，可以适应广泛的下游任务。虽然现有的视觉基础模型（如CLIP、ALIGN和Wu Dao 2.0）主要关注将图像和文本表示映射到跨模态共享表示，但我们引入了一个新的计算机视觉基础模型Florence，将表示从粗略（场景）扩展到精细（对象），从静态（图像）扩展到动态（视频），从RGB到多种模式（标题、深度）。通过结合Web级图像文本数据的通用视觉语言表示，我们的Florence模型可以轻松地适应各种计算机视觉任务，例如分类、检索、目标检测、VQA、图像标题、视频检索和动作识别。此外，Florence在许多类型的迁移学习中表现出色：全采样微调、线性探测、少镜头迁移和零镜头迁移，用于新图像和对象。所有这些属性对于我们的vision foundation模型服务于通用vision任务至关重要。Florence在44个具有代表性的基准测试中取得了最新的最新成果，例如ImageNet-1K零炮分类，前1位精度为83.74，前5位精度为97.18，COCO微调的mAP为62.4，VQA的mAP为80.36，Kinetics-600的mAP为87.8。</pre></li>
<li><a href="https://arxiv.org/abs/2006.06195">Large-Scale Adversarial Training for Vision-and-Language Representation Learning</a> (NeurIPS2020)
<pre>我们介绍了VILLA，这是第一个已知的针对视觉和语言（V+L）表征学习的大规模对抗性训练。VILLA包括两个训练阶段：（i）任务不可知对抗性预训练；其次是（ii）针对特定任务的对抗性微调。我们建议在每个模态的嵌入空间中执行对抗性训练，而不是在图像像素和文本标记上添加对抗性干扰。为了实现大规模训练，我们采用了“自由”对抗训练策略，并将其与基于KL散度的正则化相结合，以提高嵌入空间的不变性。我们将VILLA应用于当前性能最佳的V+L模型，并在广泛的任务上实现了新的技术水平，包括视觉问答、视觉常识推理、图像文本检索、引用表达式理解、视觉蕴涵和NLVR2。</pre></li>
<li><a href="https://arxiv.org/abs/2102.08981">Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts</a>
<pre>大规模图像字幕和视觉答疑数据集的可用性极大地促进了最近在视觉和语言预培训方面取得的成功。然而，收集这些数据集时，通常会有从其原始目标任务（例如，图像标题生成）继承的过度限制性需求，这限制了结果数据集的规模和多样性。我们通过放松概念说明3M（CC3M）[Sharma等人，2018]中使用的数据收集管道，进一步推动视觉和语言训练前数据的限制，并引入概念说明12M（CC12M），这是一个具有1200万图像-文本对的数据集，专门用于视觉和语言训练前。我们对该数据集进行了分析，并针对CC3M在多个下游任务中的有效性进行了基准测试，重点是长尾视觉识别。我们的结果清楚地说明了扩大视觉和语言任务的训练前数据的好处，正如nocaps和概念性字幕基准的最新结果所表明的那样。</pre></li>
<li><a href="https://arxiv.org/abs/2102.02779">Unifying Vision-and-Language Tasks via Text Generation</a>
<pre>现有的视觉和语言学习方法通常需要为每个任务设计特定于任务的体系结构和目标。例如，用于视觉问答的多标签答案分类器、用于引用表达式理解的区域记分器和用于图像字幕的语言解码器等。为了减轻这些麻烦，在这项工作中，我们提出了一个统一的框架，该框架在同一语言建模目标的单个体系结构中学习不同的任务，即，多模式条件文本生成，其中我们的模型学习基于视觉和文本输入生成文本标签。在7个流行的视觉和语言基准上，包括视觉问答、参考表达理解、视觉常识推理（其中大部分以前被建模为区别性任务），我们的生成方法（具有单一的统一架构）达到与最近任务特定的最先进的愿景和语言模型相当的性能。此外，我们的生成性方法在答案很少的问题上表现出更好的泛化能力。此外，我们还表明，我们的框架允许在单个体系结构中使用单个参数集进行多任务学习，实现了与单独优化的单个任务模型类似的性能。我们的代码可在以下网站公开获取：https://github.com/j-min/VL-T5</pre></li>
<li><a href="https://arxiv.org/abs/2101.11562">Scheduled Sampling in Vision-Language Pretraining with Decoupled Encoder-Decoder Network</a> (AAAI2021)
<pre>尽管使用基于BERT的编码器对视觉语言（VL）进行了令人印象深刻的预训练，用于VL理解和生成的通用编码器-解码器的预训练仍然具有挑战性。困难源于两个学科固有的不同特性，例如，VL理解任务利用跨模式的无限制消息传递，而生成任务仅采用视觉到文本的消息传递。在本文中，我们从编码器-解码器结构的两流解耦设计开始，其中两个解耦的跨模式编码器和解码器分别执行每种类型的代理任务，以同时理解VL和生成预训练。此外，对于VL预训练，主要的方法是用掩码令牌替换一些输入的视觉/文字令牌，并强制使用多模式编码器/解码器重建原始令牌，但在对下游任务进行微调时不涉及掩码令牌。作为替代方案，我们提出了一种主要的定时采样策略，该策略通过预训练编码器-解码器以两次通过的方式优雅地缓解这种差异。大量的实验表明，通过对四个VL理解和生成下游任务进行微调，我们的预训练编码器-解码器具有令人信服的通用性。源代码位于\url{https://github.com/YehLi/TDEN}.</pre></li>
<li><a href="https://arxiv.org/abs/2006.16934">ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph</a>
<pre>我们提出了一种知识增强的方法，ERNIE ViL，它结合了从场景图中获得的结构化知识来学习视觉语言的联合表示。ERNIE ViL试图在视觉和语言之间建立详细的语义联系（对象、对象属性和对象之间的关系），这对于视觉语言跨模态任务至关重要。ERNIE ViL利用视觉场景的场景图，在预训练阶段构造场景图预测任务，即对象预测、属性预测和关系预测任务。具体来说，这些预测任务是通过预测从句子解析的场景图中不同类型的节点来实现的。因此，ERNIE ViL可以学习描述跨视觉和语言的详细语义对齐的联合表示。在对大规模图像-文本对齐数据集进行预训练后，我们验证了ERNIE ViL在5个跨模式下游任务中的有效性。厄尼·维尔在所有这些任务上都取得了最先进的表现，在录像机排行榜上排名第一，绝对提高了3.7%。</pre></li>
<li><a href="https://arxiv.org/abs/2012.07000">KVL-BERT: Knowledge Enhanced Visual-and-Linguistic BERT for Visual Commonsense Reasoning</a>
<pre>推理是实现完全视觉理解的关键能力。为了开发具有认知水平的视觉理解和推理能力的机器，引入了视觉常识推理（VCR）任务。在VCR中，给定一个关于图像的具有挑战性的问题，机器必须正确回答，然后提供一个理由来证明其答案。采用强大的BERT模型作为主干学习图像内容和自然语言的联合表示的方法在VCR上显示了良好的改进。然而，现有的方法都没有在视觉常识推理中使用常识知识，我们相信这将对这项任务有很大帮助。在常识知识的支持下，即使图像中没有描述所需的信息，复杂的问题也可以通过认知推理来回答。因此，我们将常识知识融入到跨模态的BERT中，并提出了一种新的知识增强的视觉和语言BERT（简称KVL-BERT）模型。除了以视觉和语言内容为输入外，从概念网中提取的外部常识知识被集成到多层转换器中。为了保留原始句子的结构信息和语义表示，我们提出使用相对位置嵌入和掩蔽自我注意来削弱注入的常识知识与输入序列中其他无关成分之间的影响。与其他任务特定模型和一般任务不可知预训练模型相比，我们的KVL-BERT的性能大大优于它们。</pre></li>
<li><a href="https://arxiv.org/abs/2012.08673">A Closer Look at the Robustness of Vision-and-Language Pre-trained Models</a>
<pre>大规模预先培训的多模式变压器，如维尔伯特和UNITER，将视觉和语言（V+L）研究的最新水平推向了一个新水平。尽管迄今为止在标准任务上取得了令人印象深刻的性能，但这些预先训练的模型的健壮性仍不清楚。为了调查，我们对现有的预训练模型进行了大量的全面评估，评估了4种不同类型的V+L特定模型的稳健性：（i）语言变异；二逻辑推理；（iii）视觉内容操纵；（iv）回答分布转移。有趣的是，通过标准的模型微调，预训练的V+L模型已经比许多任务特定的最新方法表现出更好的鲁棒性。为了进一步增强模型的鲁棒性，我们提出了Mango，这是一种通用而有效的方法，可以在嵌入空间中学习多模态对抗性噪声发生器来愚弄预先训练的V+L模型。与以往专注于一种特定类型的鲁棒性的研究不同，Mango是任务不可知的，能够在不同的任务中提升预先训练的模型的通用性能，以评估鲁棒性的广泛方面。综合实验表明，Mango在9个健壮性基准中有7个达到了最新水平，大大超过了现有方法。作为对V+L稳健性的第一次全面研究，本研究将预训练模型的稳健性置于更加突出的位置，为未来的研究指明了新的方向。</pre></li>
<li><a href="https://arxiv.org/abs/2011.10652">Self-Supervised learning with cross-modal transformers for emotion recognition</a> (SLT2020)
<pre>由于野外标记数据集的可用性有限，情感识别是一项具有挑战性的任务。在语音和自然语言等领域，自监督学习在标记数据集有限的任务上取得了改进。像BERT这样的模型学习在单词嵌入中融入上下文，这可以提高下游任务（如问答）的性能。在这项工作中，我们将自我监督训练扩展到多模式应用。我们学习多模态表示使用变压器培训的蒙面语言建模任务与音频，视频和文本功能。该模型对情感识别的下游任务进行了微调。我们在CMU-MOSEI数据集上的结果表明，与基线相比，这种预训练技术可以将情绪识别性能提高3%。</pre></li>
<li><a href="https://arxiv.org/abs/2010.06775">Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision</a> (EMNLP2020)
<pre>人类通过听、说、写、读以及与多模态现实世界的互动来学习语言。现有的语言预训练框架显示了纯文本自我监督的有效性，同时本文探讨了视觉监督语言模型的概念。我们发现阻碍这一探索的主要原因是视觉基础语言数据集和纯语言语料库在数量和分布上的巨大差异。因此，我们开发了一种名为“vokenization”的技术，通过上下文将语言标记映射到它们的相关图像（我们称之为“vokens”），将多模态对齐外推到纯语言数据。“vokenizer”在相对较小的图像字幕数据集上进行训练，然后我们将其应用于为大型语言语料库生成vokens。通过使用这些上下文生成的Voken进行训练，我们的视觉监督语言模型在多个纯语言任务（如GLUE、SQuAD和SWAG）上显示出与自我监督备选方案相比的一致改进。代码和预先培训的模型可在https://github.com/airsplay/vokenization</pre></li>
<li><a href="https://arxiv.org/abs/1912.02315">12-in-1: Multi-Task Vision and Language Representation Learning</a>
<pre>许多视觉和语言研究集中在一组小而多样的独立任务和支持数据集上，这些任务和支持数据集通常是单独研究的；然而，成功完成这些任务所需的视觉基础语言理解技能有很大的重叠。在这项工作中，我们通过建立一个大规模的多任务训练体系来研究视觉和语言任务之间的关系。我们的方法最终在一个单一模型上实现，该模型包含四大类任务的12个数据集，包括视觉问答、基于字幕的图像检索、基础引用表达式和多模式验证。与独立训练的单任务模型相比，这意味着从大约30亿个参数减少到2.7亿个，同时任务的平均性能提高了2.05个点。我们使用我们的多任务框架对联合训练不同任务的效果进行深入分析。此外，我们还表明，从我们的单任务多任务模型中微调特定于任务的模型可以带来进一步的改进，达到或超过最先进水平的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2103.08849">Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models</a> (NAACL2021)
<pre>本文研究了视觉语言模型的零镜头跨语言迁移。具体来说，我们关注多语言文本到视频的搜索，并提出了一个基于转换器的模型来学习上下文化的多语言多模态嵌入。在零镜头设置下，我们实证证明，当我们使用非英语句子查询多语言文本视频模型时，性能会显著下降。为了解决这个问题，我们引入了一种多语言多模式的预训练策略，并收集了一个新的多语言教学视频数据集（MultiHowTo100M）用于预训练。在VTT上的实验表明，我们的方法在没有附加注释的情况下显著改进了非英语语言的视频搜索。此外，当多语言注释可用时，我们的方法在VTT和VATEX上的多语言文本到视频搜索方面比最近的基线有很大的优势；以及在Multi30K上进行多语言文本到图像搜索。我们的型号和多模式100米可在http://github.com/berniebear/Multi-HT100M.</pre></li>
<li><a href="https://arxiv.org/abs/2006.02635">M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-training</a> (CVPR2021)
<pre>我们提出了M3P，一个多任务多语言多模式预训练模型，它通过多任务预训练将多语言预训练和多模式预训练结合到一个统一的框架中。我们的目标是学习通用表示法，可以将以不同方式出现的对象或以不同语言表达的文本映射到一个公共语义空间。此外，为了明确鼓励图像和非英语语言之间的细粒度对齐，我们还提出了多模态代码切换训练（MCT），通过代码切换策略将单语预训练和多模态预训练结合起来。在两个基准数据集（包括MSCOCO和Multi30K）上对多语言图像检索任务进行了实验。M3P可以实现英语的可比结果和非英语语言的最新结果。</pre></li>
<li><a href="https://arxiv.org/abs/2104.00332">UC2: Universal Cross-lingual Cross-modal Vision-and-Language Pre-training</a>
<pre>视觉和语言预培训在学习视觉和语言之间的多模态表示方面取得了令人印象深刻的成功。为了将这一成功推广到非英语语言，我们引入了UC2，这是第一个用于跨语言跨模态表示学习的机器翻译增强框架。为了解决图像数据集多语言标题的稀缺性问题，我们首先通过机器翻译（MT）用其他语言扩充现有的纯英语数据集。然后，我们将标准的蒙面语言建模和图像-文本匹配训练目标扩展到多语言设置，其中通过共享的视觉上下文（即使用图像作为轴心）捕获不同语言之间的对齐。为了便于学习图像和所有感兴趣的语言的联合嵌入空间，我们进一步提出了两个新的训练前任务，即掩蔽区域到标记建模（MRTM）和视觉翻译语言建模（VTLM），利用机器翻译增强的翻译数据。对多语言图像文本检索和多语言视觉问答基准的评估表明，我们提出的框架在不同的非英语基准上实现了最新水平，同时在英语任务上保持了与单语预先训练模型相当的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2003.12137">Cycle Text-To-Image GAN with BERT</a>
<pre>我们在最先进的GAN架构的基础上，探索从各自的标题生成图像的新方法。特别是，我们使用基于注意的GANs作为模型的基线，学习从单词到图像特征的注意映射。为了更好地捕捉描述的特征，我们构建了一个新的循环设计，该设计学习一个反函数，将图像映射回原始标题。此外，我们将最近开发的BERT预训练单词嵌入作为我们的初始文本特征化器，并观察到与注意力基线相比，在定性和定量性能方面有显著改进。</pre></li>
<li><a href="https://arxiv.org/abs/1912.03063">Weak Supervision helps Emergence of Word-Object Alignment and improves Vision-Language Tasks</a>
<pre>近年来，大量采用自我关注（即变压器模型）和类似于伯特的训练原则，在大量视觉和语言问题（如视觉问答（VQA）、图像检索等）上产生了许多高性能的模型。在本文中，我们声称这些最先进的方法（SOTA）在构造单一模态内的信息方面表现得相当好，但尽管它们的性能令人印象深刻，但它们往往难以识别细粒度的模态间关系。事实上，这种关系通常被认为是在培训过程中从特定于应用程序的损失中隐式学习到的，主要是用于分类的交叉熵。虽然最近的研究通过交叉注意模块为情态间关系提供了归纳偏见，但在本研究中，我们证明（1）后一种假设不成立，即情态对齐不一定自动出现，以及（2）在视觉对象和文字之间的对齐上添加弱监督可以提高需要推理的任务上学习模型的质量。特别是，我们将对象词对齐丢失集成到SOTA视觉语言推理模型中，并在两个任务VQA和语言驱动的图像比较中对其进行评估。我们表明，所提出的细粒度模态间监督显著提高了这两个任务的性能。特别是，这种新的学习信号允许使用预先训练的模型在GQA数据集（VQA任务）上获得SOTA级性能，而无需对任务进行微调，以及在NLVR2数据集（图像的语言驱动比较）上获得新的SOTA级性能。最后，我们还通过可视化注意分布来说明贡献对模型推理的影响。</pre></li>
<li><a href="https://arxiv.org/abs/2004.06165">Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks</a>
<pre>在图像-文本对上学习跨模态表示的大规模预训练方法在视觉语言任务中越来越流行。现有的方法只是将图像区域特征和文本特征连接起来作为模型的输入进行预训练，并使用自我注意以蛮力的方式学习图像-文本语义对齐，本文提出了一种新的学习方法Oscar（Object Semantics Aligned pre training），它使用图像中检测到的对象标记作为定位点，大大简化了路线的学习。我们的方法的动机是观察到图像中的显著对象可以被准确地检测到，并且在成对的文本中经常提到。我们在650万对文本图像的公共语料库上预先训练了一个奥斯卡模型，并在下游任务上对其进行了微调，在六个成熟的视觉语言理解和生成任务上创造了新的技术水平。</pre></li>
<li><a href="https://arxiv.org/abs/2009.13682">VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning</a>
<pre>生成能够描述在标题标记的训练数据中看不到的新对象的图像标题是非常理想的，但也具有挑战性，这一能力在“新对象字幕挑战”（nocaps）中进行了评估。在这个挑战中，除了Coco字幕之外，不允许为模型训练提供额外的图像字幕训练数据。因此，传统的视觉语言预训练（VLP）方法无法应用。本文介绍了在没有字幕注释的情况下进行预训练的视觉词汇预训练（VIVO）。通过打破VLP中成对图像字幕训练数据的依赖性，维梧可以利用大量成对图像标签数据来学习视觉词汇。这是通过预先训练多层变换器模型来完成的，该模型学习将图像级别标记与其相应的图像区域特征对齐。为了解决图像标签的无序性，维梧使用匈牙利匹配损失和蒙面标签预测来进行预训练。我们通过微调预训练的图像字幕模型来验证VIVO的有效性。此外，我们还对我们的模型推断出的视觉文本对齐进行了分析。结果表明，我们的模型不仅可以生成描述新对象的流畅的图像字幕，还可以识别这些对象的位置。我们的单一模式在nocaps上取得了最新的先进成果，并超过了人类苹果酒评分。</pre></li>
<li><a href="https://arxiv.org/abs/2008.06884">DeVLBert: Learning Deconfounded Visio-Linguistic Representations</a> (ACMMM2020)
<pre>在本文中，我们建议研究域外visio语言预训练问题，其中预训练数据分布不同于下游数据的分布，在下游数据上预训练模型将被微调。现有的解决该问题的方法纯粹是基于似然的，当转移到域外下游任务时，会导致虚假的相关性并损害泛化能力。所谓伪相关，我们的意思是，给定另一个标记（对象或单词）的一个标记（对象或单词）的条件概率可能很高（由于数据集偏差），它们之间没有可靠的（因果）关系。为了缓解这种数据集偏见，我们提出了一个非结构化的Visio语言Bert框架，简称为DeVLBert，用于执行基于干预的学习。我们借鉴因果关系研究领域的后门调整思想，提出了几种基于神经网络的Bert式域外预训练体系结构。对图像检索（IR）、零镜头检索（Zero-shot IR）和视觉问答三个下游任务的定量结果表明，DeVLBert通过提高泛化能力而有效。</pre></li>
<li><a href="https://arxiv.org/abs/2011.13922">A Recurrent Vision-and-Language BERT for Navigation</a>
<pre>视觉与语言（V&L）的应用极大地提高了许多视觉语言任务的准确性。然而，它在视觉和语言导航（VLN）任务中的应用仍然有限。其中一个原因是难以使BERT体系结构适应VLN中存在的部分可观测马尔可夫决策过程，需要依赖于历史的注意力和决策。在本文中，我们提出了一个用于VLN的时间感知的递归BERT模型。具体地说，我们为BERT模型配备了一个递归函数，用于维护代理的跨模态状态信息。通过在R2R和Revrie上的大量实验，我们证明了我们的模型可以取代更复杂的编码器-解码器模型，以获得最先进的结果。此外，我们的方法可以推广到其他基于转换器的体系结构，支持预训练，并且能够同时解决导航和引用表达式任务。</pre></li>
<li><a href="https://arxiv.org/abs/2002.10832">BERT Can See Out of the Box: On the Cross-modal Transferability of Text Representations</a>
<pre>经过预训练的语言模型最近为NLP任务的重大进展做出了贡献。最近，已经开发了多模态版本的BERT，使用大量的预训练，依赖于大量对齐的文本和图像数据，主要应用于分类任务，如VQA。在本文中，我们感兴趣的是通过避免对补充数据进行预训练来评估BERT开箱即用的视觉能力。我们选择研究视觉问题生成，这是扎根对话非常感兴趣的一项任务，能够研究每种模式的影响（因为输入可以是视觉和/或文本）。此外，任务的生成方面需要自适应，因为BERT主要设计为编码器。我们介绍了BERT-gen，一种基于BERT的文本生成体系结构，能够利用单模态或多模态表示。在不同配置下报告的结果表明，BERT gen具有天生的适应多模态数据和文本生成的能力，即使可用数据很少，也可以避免昂贵的预训练。在两个已建立的VG数据集上，所提出的模型比最新技术有了实质性的改进。</pre></li>
<li><a href="https://arxiv.org/abs/2104.03135">Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning</a> (CVPR2021)
<pre>我们研究了卷积神经网络（CNN）和Transformer for vision language pre training（VLPT）的联合学习，旨在从数百万图像-文本对中学习跨模式对齐。最先进的方法提取显著图像区域，并逐步将区域与单词对齐。由于基于区域的视觉特征通常代表图像的一部分，现有的视觉语言模型很难完全理解成对自然语言的语义。在本文中，我们建议SOHO“开箱即看”，以整个图像作为输入，并以端到端的方式学习视觉语言表示。SOHO不需要边界框注释，其推理速度比基于区域的方法快10倍。特别是，SOHO学习通过视觉词典（VD）提取全面而紧凑的图像特征，这有助于跨模态理解。VD被设计用来表示语义相似的一致的视觉抽象。它是动态更新的，并用于我们提出的训练前任务掩蔽视觉建模（MVM）。我们按照标准的VLPT设置，在四个成熟的视觉语言任务上进行实验。特别是，SOHO实现了2.0%的绝对收益R@1MSCOCO文本检索5k测试分割得分，NLVR$^2$test-P分割准确率为1.5%，SNLI-VE测试分割准确率为6.7%。</pre></li>
<li><a href="https://arxiv.org/abs/2109.04448">Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers</a> (EMNLP2021)
<pre>预训练视觉和语言BERTs旨在学习结合两种模式信息的表征。我们提出了一种基于跨模态输入的诊断方法，以评估这些模型实际集成跨模态信息的程度。该方法涉及基于跨模态接地对准，完全或选择性地烧蚀一个模态的输入，并评估另一模态的模型预测性能。模型性能通过反映模型预训练目标的特定于模态的任务来衡量（例如，文本的蒙面语言建模）。当一个模态缺少输入时，已经学会使用这两种模态构造跨模态表示的模型的性能预计会更差。我们发现，最近提出的模型在去除视觉信息时预测文本比在去除文本时预测视觉对象类别相对困难得多，这表明这些模型不是对称的交叉模态模型。</pre></li>
<li><a href="https://arxiv.org/abs/2004.00849">Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers</a>
<pre>我们建议像素伯特通过深度多模态变换器将图像像素与文本对齐，这些变换器在统一的端到端框架中共同学习视觉和语言嵌入。我们的目标是直接从图像和句子对建立图像像素和语言语义之间更准确、更彻底的联系，而不是将基于区域的图像特征作为最新的视觉和语言任务。我们提出的像素级的语义连接解决了视觉和语言任务中任务特定视觉表示的局限性。它还减轻了包围盒标注的成本，克服了视觉任务中语义标签与语言语义之间的不平衡。为了更好地表示下游任务，我们预先训练了一个通用的端到端模型，该模型包含来自可视基因组数据集和MS-COCO数据集的图像和句子对。我们建议使用随机像素采样机制来增强视觉表示的鲁棒性，并将蒙面语言模型和图像文本匹配作为预训练任务。利用我们预先训练好的模型对下游任务进行的大量实验表明，我们的方法在下游任务中发挥了最大的作用，包括视觉问答（VQA）、图像文本检索、真实视觉推理自然语言（NLVR）。特别是，在公平比较的情况下，与SOTA相比，我们在VQA任务中单个模型的性能提高了2.17个点。</pre></li>
<li><a href="https://arxiv.org/abs/2201.11732">IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages</a>
<pre>为可复制性和全面性设计的可靠评估基准推动了机器学习的进步。然而，由于缺乏多语言基准，视觉和语言研究主要集中在英语任务上。为了填补这一空白，我们引入了基于图像的语言理解评估基准。IGLUE通过聚合现有数据集和创建新数据集，将20种不同语言的可视化问答、跨模式检索、扎根推理和扎根蕴涵任务结合在一起。我们的基准测试能够评估转移学习的多语言多模态模型，不仅在零镜头设置中，而且在新定义的少数镜头学习设置中。基于对现有最先进模型的评估，我们发现翻译测试迁移优于零镜头迁移，并且很少的镜头学习很难用于许多任务。此外，下游性能部分由可用于预训练的未标记文本数据量解释，而仅由目标源语言的类型距离解释。我们希望通过向社区发布基准来鼓励未来在这一领域的研究工作。</pre></li>
<li><a href="https://www.aclweb.org/anthology/2020.acl-main.674/">Understanding Advertisements with BERT</a> (ACL2020)</li>
<li><a href="https://arxiv.org/abs/2007.07229">BERTERS: Multimodal Representation Learning for Expert Recommendation System with Transformer</a>
<pre>专家推荐系统的目标是跟踪一组候选人的专业知识和偏好，识别他们的专业知识模式，并识别专家。本文介绍了一种用于专家推荐系统（BERTERS）的多模式分类方法。在我们提出的系统中，模式来自文本（候选人发表的文章）和图形（他们的合著者联系）信息。BERTERS使用Transformer（BERT）的双向编码器表示将文本转换为向量。此外，一种称为ExEm的图表示技术用于从合著者网络中提取候选特征。候选对象的最终表示是这些向量和其他特征的串联。最后，在特征拼接的基础上构建分类器。这种多模态方法可用于学术界和社区问答。为了验证BERTERS的有效性，我们分析了它在多标签分类和可视化任务中的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2005.09801">FashionBERT: Text and Image Matching with Adaptive Loss for Cross-modal Retrieval</a> (SIGIR2020)
<pre>在这篇文章中，我们讨论了服装行业跨模式检索中的文本和图像匹配问题。与一般领域的匹配不同，服装匹配需要更多地关注服装图像和文本中的细粒度信息。先锋方法从图像中检测感兴趣区域（即RoI），并使用RoI嵌入作为图像表示。一般来说，ROI倾向于表示时尚图像中的“对象级别”信息，而时尚文本倾向于描述更详细的信息，例如样式、属性。因此，对于时尚文本和图像匹配而言，ROI不够细粒度。为此，我们提出FashionBERT，它利用补丁作为图像特征。FashionBERT将预先训练好的BERT模型作为主干网络，学习文本和图像的高级表示。同时，在FashionBERT模型中，我们提出了一种自适应损失来平衡多任务学习。两个任务（即，文本和图像匹配和跨模态检索）被合并来评估FashionBERT。在公共数据集上，实验表明FashionBERT在性能上比基线和最先进的方法有显著的改进。在实践中，FashionBERT应用于一个具体的跨模态检索应用程序。我们提供了详细的匹配性能和推理效率分析。</pre></li>
<li><a href="https://arxiv.org/abs/2103.16110">Kaleido-BERT: Vision-Language Pre-training on Fashion Domain</a> (CVPR2021)
<pre>我们提出了一个新的视觉语言（VL）预训练模型，称为Kaleido-BERT，该模型引入了一种新的Kaleido策略，用于从变形金刚中进行时装跨模态表示。与最近VL模型的随机掩蔽策略不同，我们设计了对齐引导掩蔽，以共同关注图像-文本语义关系。为此，我们进行了五项新任务，即旋转、拼图、伪装、灰色到颜色和空白到颜色，以在不同规模的补丁上进行自我监督VL预训练。Kaleido-BERT在概念上简单，易于扩展到现有的BERT框架，它在四个下游任务（包括文本检索）上大幅度获得了最新的结果(R@1：4.03%绝对改善），图像检索(R@1：7.13%abs imv.）、类别识别（ACC:3.28%abs imv.）和时尚字幕（Bleu4:1.2 abs imv。）。我们在广泛的电子商务网站上验证了Kaleido BERT的效率，展示了其在现实世界应用中更广泛的潜力。</pre></li>
<li><a href="https://arxiv.org/abs/1912.13318">LayoutLM: Pre-training of Text and Layout for Document Image Understanding</a> (KDD2020) [<a href="https://github.com/microsoft/unilm/tree/master/layoutlm">github</a>]
<pre>近年来，各种NLP任务都成功地验证了预训练技术。尽管NLP应用程序广泛使用预训练模型，但它们几乎只关注文本级操作，而忽略了对文档图像理解至关重要的布局和样式信息。在本文中，我们提出\textbf{LayoutLM}来联合建模跨扫描文档图像的文本和布局信息之间的交互，这有助于大量真实文档图像理解任务，例如从扫描文档中提取信息。此外，我们还利用图像特征将单词的视觉信息合并到LayoutLM中。据我们所知，这是第一次在文档级预培训的单一框架中联合学习文本和布局。它在几个下游任务中取得了最新的成果，包括表单理解（从70.72到79.27）、收据理解（从94.02到95.24）和文档图像分类（从93.07到94.42）。代码和经过预培训的LayoutLM模型可在\url公开获取{https://aka.ms/layoutlm}.</pre></li>
<li><a href="https://arxiv.org/abs/2012.14740">LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding</a> (ACL2021)
<pre>由于其有效的模型体系结构和大规模未标记扫描/数字生成文档的优势，文本和布局的预培训在各种视觉丰富的文档理解任务中被证明是有效的。在本文中，我们在一个多模式框架中通过预训练文本、布局和图像来呈现\textbf{LayoutLMv2}，其中利用了新的模型架构和预训练任务。具体而言，LayoutLMv2不仅使用现有的蒙面视觉语言建模任务，而且在训练前阶段使用新的文本-图像对齐和文本-图像匹配任务，从而更好地学习跨模态交互。同时，它还将空间感知的自我注意机制集成到Transformer架构中，使模型能够充分理解不同文本块之间的相对位置关系。实验结果表明，LayoutLMv2的性能优于强基线，并在多种下游视觉丰富的文档理解任务上实现了最新的结果，包括FUNSD（0.7895->0.8420）、CORD（0.9493->0.9601）、SROIE（0.9524->0.9781）、Kleister NDA（0.834->0.852）、RVL-CDIP（0.9443->0.9564）和DocVQA（0.7295->0.8672）。预先培训的LayoutLMv2模型可在https://aka.ms/layoutlmv2.</pre></li>
<li><a href="https://arxiv.org/abs/2104.08836">LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding</a>
<pre>具有文本、布局和图像的多模式预培训最近在视觉丰富的文档理解任务中实现了SOTA性能，这表明了跨不同模式联合学习的巨大潜力。在本文中，我们提出了LayoutXLM，一个用于多语言文档理解的多模式预训练模型，它旨在为视觉丰富的文档理解跨越语言障碍。为了准确评估LayoutXLM，我们还引入了一个名为XFUND的多语言表单理解基准数据集，其中包括7种语言（中文、日语、西班牙语、法语、意大利语、德语、葡萄牙语）的表单理解示例，并为每种语言手动标记键值对。实验结果表明，LayoutXLM模型在XFUND数据集上的性能明显优于现有的SOTA跨语言预训练模型。预先培训的LayoutXLM模型和XFUND数据集可在https://aka.ms/layoutxlm.</pre></li>
<li><a href="https://arxiv.org/abs/2104.08405">LAMPRET: Layout-Aware Multimodal PreTraining for Document Understanding</a>
<pre>文档布局包括结构和视觉（例如字体大小）信息，这些信息非常重要，但常常被机器学习模型忽略。现有的几种使用布局信息的模型只考虑文本内容，忽视了图像等其他形式的内容的存在。此外，版面中呈现内容的空间交互从未真正得到充分利用。为了弥补这一差距，我们将文档解析为内容块（如文本、表格、图像），并提出一种新颖的布局感知多模态层次结构框架LAMPreT，对这些块和整个文档进行建模。我们的LAMPreT在较低级别使用多模式转换器对每个块进行编码，并在较高级别使用专门设计的转换器聚合块级表示和连接。我们设计了分层的预训练目标，其中低层模型的训练与多模态接地模型类似，而高层模型的训练与我们提出的新的布局感知目标类似。我们在两个布局感知任务——文本块填充和图像建议上对所提出的模型进行了评估，并展示了我们所提出的层次结构和预训练技术的有效性。</pre></li>
<li><a href="https://openreview.net/forum?id=punMXQEsPr0">BROS: A Pre-trained Language Model for Understanding Texts in Document</a></li>
<li><a href="https://arxiv.org/abs/2109.10282">TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models</a>
<pre>文本识别是文档数字化的一个长期研究问题。现有的文本识别方法通常基于CNN进行图像理解，基于RNN进行字符级文本生成。此外，作为后处理步骤，通常需要另一种语言模型来提高总体准确性。在本文中，我们提出了一种端到端文本识别方法，该方法使用预先训练好的图像转换器和文本转换器模型，即TrOCR，它利用转换器体系结构进行图像理解和字词级文本生成。TrOCR模型简单但有效，可以使用大规模合成数据进行预训练，并使用人类标记的数据集进行微调。实验表明，TrOCR模型在打印和手写文本识别任务上都优于当前最先进的模型。代码和模型将在https://aka.ms/TrOCR.</pre></li>
<li><a href="https://arxiv.org/abs/2108.11591">LayoutReader: Pre-training of Text and Layout for Reading Order Detection</a> (EMNLP2021)
<pre>阅读顺序检测是理解视觉丰富文档（例如收据和表单）的基础。不幸的是，现有的研究没有利用先进的深度学习模型，因为对足够大的数据集进行注释太费力了。我们观察到WORD文档的阅读顺序嵌入到它们的XML元数据中；同时，将WORD文档转换为PDF或图像也很容易。因此，我们以自动化的方式构建了ReadingBank，这是一个基准数据集，包含500000个文档图像的阅读顺序、文本和布局信息，涵盖了广泛的文档类型。这个有史以来第一个大规模数据集释放了深层神经网络用于阅读顺序检测的能力。具体来说，我们建议的LayoutReader使用seq2seq模型捕获文本和版面信息，用于预测阅读顺序。在我们的实验中，它在阅读顺序检测方面表现得几乎完美，并显著提高了开源和商用OCR引擎在文本行排序方面的性能。我们将在\url发布数据集和模型{https://aka.ms/layoutreader}.</pre></li>
<li><a href="https://arxiv.org/abs/1912.01127">BERT for Large-scale Video Segment Classification with Test-time Augmentation</a> (ICCV2019WS)
<pre>本文介绍了我们对第三届YouTube-8M视频理解比赛的方法，该比赛要求参赛者将视频级别的标签定位到标签实际出现的视频中的精确时间。我们的模型是帧级模型（如GatedNetVLAD和NeXtVLAD）和各种具有测试时间扩展的BERT模型的集合。我们探索了多种方法来将BERT输出作为视频表示，以及多种方法来组合视觉和音频信息。我们建议将测试时间增加为将视频帧移动到一个左或右单位，这增加了预测的多样性，并且事实上显示了评估指标的改进。我们首先在4M训练视频级数据上对模型进行预训练，然后在237K注释视频段级数据上对模型进行微调。我们实现MAP@100K0.7871，在私人测试视频片段数据中，在283个团队中排名第9。</pre></li>
<li><a href="https://arxiv.org/abs/2102.05095">Is Space-Time Attention All You Need for Video Understanding?</a>
<pre>我们提出了一种无卷积的视频分类方法，该方法完全基于空间和时间上的自我注意。我们的方法称为“时间转换器”，通过直接从一系列帧级补丁中学习时空特征，使标准转换器结构适应视频。我们的实验研究比较了不同的自我注意方案，并表明“分散注意”（时间注意和空间注意分别应用于每个块）在所考虑的设计选择中可以获得最佳的视频分类精度。尽管采用了全新的设计，TimeSformer在多个动作识别基准上取得了最先进的结果，包括Kinetics-400和Kinetics-600的最佳报告准确性。最后，与3D卷积网络相比，我们的模型训练速度更快，测试效率显著提高（精度略有下降），并且还可以应用于更长的视频剪辑（超过一分钟）。有关代码和型号，请访问：https://github.com/facebookresearch/TimeSformer.</pre></li>
<li><a href="https://arxiv.org/abs/2004.07093">lamBERT: Language and Action Learning Using Multimodal BERT</a>
<pre>近年来，基于变换器的双向编码表示（BERT）模型因其在语言理解相关任务中的高性能而受到自然语言处理领域的广泛关注。BERT模型通过无监督的方式使用大型语料库进行预训练，学习能够适应各种任务的语言表示。本研究提出了使用多模态伯特（lamBERT）模型的语言和动作学习，该模型通过1）将伯特模型扩展到多模态表示，2）将其与强化学习相结合，从而实现语言和动作的学习。为了验证所提出的模型，在网格环境中进行了一个实验，该环境需要理解语言才能使代理正常工作。因此，与其他模型（如基于卷积神经网络的模型和未经预训练的lamBERT模型）相比，lamBERT模型在多任务设置和转移设置中获得了更高的回报。</pre></li>
<li><a href="https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf">Generative Pretraining from Pixels</a> [<a href="https://github.com/openai/image-gpt">github</a>] [<a href="https://openai.com/blog/image-gpt/">website</a>]</li>
<li><a href="https://arxiv.org/abs/2006.03677">Visual Transformers: Token-based Image Representation and Processing for Computer Vision</a>
<pre>计算机视觉通过（a）将图像表示为均匀排列的像素阵列和（b）卷积高度局部化的特征而取得了显著的成功。然而，卷积对所有图像像素都一视同仁，不管其重要性如何；明确建模所有图像中的所有概念，而不考虑内容；并努力将空间上遥远的概念联系起来。在这项工作中，我们通过（a）将图像表示为语义视觉标记和（b）运行转换器来密集地建模标记关系来挑战这种范式。关键的是，我们的视觉转换器在语义标记空间中运行，根据上下文明智地处理不同的图像部分。这与像素空间变换形成鲜明对比，像素空间变换需要更多数量级的计算。使用先进的训练方法，我们的VTs显著优于卷积测试，在使用更少的触发器和参数的同时，将ImageNet top-1上的ResNet精度提高了4.6到7个点。对于LIP和COCO内容的语义分割，基于VT的特征金字塔网络（FPN）实现了高0.35点的mIoU，同时将FPN模块的失败次数减少了6.5倍。</pre></li>
<li><a href="https://openreview.net/forum?id=YicbFdNTTy">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a> (ICLR2021)</li>
<li><a href="https://arxiv.org/abs/2106.08254">BEiT: BERT Pre-Training of Image Transformers</a>
<pre>我们介绍了一种自监督视觉表示模型BEiT，它代表来自图像转换器的双向编码器表示。继自然语言处理领域的BERT之后，我们提出了一个蒙面图像建模任务来预训练视觉变换器。具体来说，在我们的预训练中，每个图像都有两个视图，即图像块（如16x16像素）和视觉标记（即离散标记）。我们首先将原始图像“标记化”为视觉标记。然后我们随机屏蔽一些图像块，并将它们输入主干变压器。预训练的目标是基于损坏的图像补丁恢复原始视觉标记。在预训练BEiT之后，我们通过在预训练编码器上附加任务层，直接微调下游任务上的模型参数。在图像分类和语义分割方面的实验结果表明，该模型与以前的预训练方法相比取得了较好的效果。例如，基本尺寸BEiT在ImageNet-1K上达到83.2%的top-1精度，在相同设置下显著优于从头开始的DeiT训练（81.8%）。此外，大型BEiT仅使用ImageNet-1K就获得了86.3%，甚至在ImageNet-22K的监督预训练中也超过了ViT-L（85.2%）。代码和预训练模型可在https://aka.ms/beit.</pre></li>
<li><a href="https://arxiv.org/abs/2102.12092">Zero-Shot Text-to-Image Generation</a> [<a href="https://github.com/openai/DALL-E">github</a>] [<a href="https://openai.com/blog/dall-e/">website</a>]
<pre>文本到图像生成传统上侧重于寻找更好的建模假设，以便在固定数据集上进行训练。这些假设可能涉及复杂的体系结构、辅助损失或辅助信息，如培训期间提供的对象零件标签或分割遮罩。我们描述了一种基于转换器的简单方法，该转换器将文本和图像标记自动回归建模为单个数据流。有了足够的数据和规模，我们的方法在以零拍方式评估时与以前的领域特定模型具有竞争力。</pre></li>
<li><a href="https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf">Learning Transferable Visual Models From Natural Language Supervision</a> [<a href="https://github.com/openai/CLIP">github</a>] [<a href="https://openai.com/blog/clip/">website</a>]</li>
<li><a href="https://arxiv.org/abs/2107.06383">How Much Can CLIP Benefit Vision-and-Language Tasks?</a>
<pre>大多数现有的视觉和语言（V&L）模型依赖预先训练的视觉编码器，使用相对较小的人工标注数据集（与网络爬网数据相比）来感知视觉世界。然而，据观察，大规模的预训练通常可以产生更好的泛化性能，例如，在大量图像标题对上训练的CLIP（对比语言图像预训练）在各种视觉任务中表现出很强的零镜头能力。为了进一步研究CLIP带来的优势，我们建议在两种典型场景中使用CLIP作为各种V&L模型的视觉编码器：1）将CLIP插入特定于任务的微调；2） 将CLIP与V&L预培训相结合，并转移到下游任务。我们发现，CLIP显著优于使用域内注释数据（如自下而上自上而下）训练的广泛使用的视觉编码器。我们在各种V&L任务上取得了具有竞争力或更好的结果，同时在视觉问答、视觉蕴涵和V&L导航任务上取得了最新的成果。我们在https://github.com/clip-vil/CLIP-ViL.</pre></li>
<li><a href="https://arxiv.org/abs/2109.04699">EfficientCLIP: Efficient Cross-Modal Pre-training by Ensemble Confident Learning and Language Modeling</a>
<pre>虽然大规模的学前培训在弥合视力和语言之间的差距方面取得了巨大成就，但它仍然面临一些挑战。首先，预培训的成本很高。其次，没有有效的方法来处理数据噪声，这会降低模型性能。第三，以前的方法只利用有限的图像-文本配对数据，而忽略了更丰富的单模态数据，这可能导致对单模态下游任务的泛化能力较差。在这项工作中，我们提出了一种通过集成自信学习获得低噪声数据子集的高效IP方法。超丰富的非成对单模态文本数据用于增强文本分支的泛化。与CLIP和WenLan相比，我们仅使用1/10的训练资源就实现了中文跨模态检索任务的最新性能，同时对单模态任务（包括文本检索和文本分类）表现出了出色的泛化能力。</pre></li>
<li><a href="https://arxiv.org/abs/2203.06386">Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation</a> (ACL2022)
<pre>最近的大规模视觉语言预训练（VLP）的双流体系结构（如CLIP）具有大量的图像-文本对数据，在各种多模式对齐任务中显示了其优越性。尽管取得了成功，但由于文本编码能力弱，生成的模型无法完成多模态生成任务。为了解决这个问题，我们建议通过视觉语言知识提取（VLKD）将双流VLP模型扩展为文本预训练语言模型（PLM），从而实现多模式生成。与从头开始的预训练相比，VLKD具有相当高的数据效率和计算效率。实验结果表明，该模型在开放式视觉问答和图像字幕等多模式生成任务中具有很强的零拍性能。例如，它在VQAv2数据集上实现了44.5%的零炮精度，超过了以前最先进的零炮模型，参数减少了7倍。此外，在VLKD之后，PLM的原始文本语言理解和生成能力得以保持，这使得我们的模型在多模态和单模态任务中都具有通用性。</pre></li>
<li><a href="https://arxiv.org/abs/2103.17249">StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery</a>
<pre>受StyleGAN在不同领域生成高度真实图像的能力的启发，最近的许多工作都集中在理解如何使用StyleGAN的潜在空间来操纵生成的和真实的图像。然而，发现语义上有意义的潜在操作通常涉及人类对多个自由度的仔细检查，或者为每个期望的操作创建一个带注释的图像集合。在这项工作中，我们探索如何利用最近引入的对比语言图像预训练（CLIP）模型的力量，为StyleGAN图像处理开发一个基于文本的界面，而不需要手动操作。我们首先介绍一种优化方案，该方案利用基于片段的丢失来修改输入潜在向量，以响应用户提供的文本提示。接下来，我们描述了一个潜在映射器，该映射器为给定的输入图像推断文本引导的潜在操作步骤，从而允许更快、更稳定的基于文本的操作。最后，我们提出了一种将文本提示映射到StyleGAN样式空间中输入不可知方向的方法，从而实现交互式文本驱动的图像操作。大量的结果和比较证明了我们方法的有效性。</pre></li>
<li><a href="https://arxiv.org/abs/2102.05644">Training Vision Transformers for Image Retrieval</a>
<pre>《变形金刚》在自然语言理解方面，以及最近在图像分类方面，都取得了卓越的成果。我们在此扩展了这项工作，并提出了一种基于变换器的图像检索方法：我们采用视觉变换器生成图像描述符，并使用度量学习目标训练生成的模型，该目标将对比损失与微分熵正则化器相结合。我们的结果表明，与基于卷积的方法相比，变压器的性能得到了一致和显著的改进。特别是，我们的方法在几个类别级检索的公共基准（即斯坦福在线产品、店内产品和CUB-200）上优于最新水平。此外，我们在ROxford和RParis上的实验还表明，在类似的设置下，Transformer在特定的对象检索方面具有竞争力，特别是在短矢量表示和低分辨率图像的情况下。</pre></li>
<li><a href="https://arxiv.org/abs/2103.08784">LightningDOT: Pre-training Visual-Semantic Embeddings for Real-Time Image-Text Retrieval</a> (NAACL2021)
<pre>多模态预训练推动了视觉和语言研究的巨大进步。这些大规模的预训练模型虽然成功，但由于变压器结构中的跨模态注意造成的巨大计算成本，其推理速度很慢。当应用于实际应用时，这种延迟和计算需求严重阻碍了预训练模型的实际使用。在本文中，我们研究了图像文本检索（ITR），这是V+L应用程序中最成熟的场景，甚至在最近的预训练模型出现之前就已经得到了广泛的研究。我们提出了一种简单而高效的方法LightningDOT，它在不牺牲准确性的情况下，将ITR的推理时间加快了数千倍。LightningDOT通过对三个新的学习目标进行预训练，离线提取特征索引，并采用即时点积匹配和进一步重新排序，消除了耗时的跨模式注意，从而显著加快了检索过程。事实上，LightningDOT在Flickr30k、COCO和Multi30K等多个ITR基准测试中达到了新的技术水平，优于现有的预训练模型，这些模型消耗的计算时间是现有模型的1000倍。代码和培训前检查点位于https://github.com/intersun/LightningDOT.</pre></li>
<li><a href="https://arxiv.org/abs/2102.04432">Colorization Transformer</a> (ICLR2021) [<a href="https://github.com/google-research/google-research/tree/master/coltran">github</a>]
<pre>我们提出了一种新的基于自我注意的高保真图像彩色化方法——彩色化变换器。给定灰度图像，着色分三步进行。我们首先使用条件自回归变换器来产生灰度图像的低分辨率粗着色。我们的架构采用条件转换层来有效地调节灰度输入。随后的两个完全并行的网络将粗彩色低分辨率图像向上采样为精细彩色高分辨率图像。根据FID结果和机械Turk测试中的人类评估，从着色变压器采样产生的各种颜色的保真度优于先前着色ImageNet的最新技术。值得注意的是，在超过60%的案例中，人类评估者更喜欢三种生成颜色中评级最高的颜色，而不是真实的颜色。着色转换器的代码和预先培训的检查点可在https://github.com/google-research/google-research/tree/master/coltran</pre></li>
<li><a href="https://arxiv.org/abs/2005.08271">A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal Transformer</a> [<a href="https://v-iashin.github.io/bmt">website</a>]
<pre>密集视频字幕旨在定位和描述未剪辑视频中的重要事件。现有的方法主要通过只利用视觉特征来解决这一问题，而完全忽略了音频轨迹。只有少数先前的工作使用了这两种模式，但它们显示了较差的结果，或者显示了具有特定领域的数据集的重要性。在本文中，我们介绍了双模变压器，它概括了双模输入的变压器结构。我们展示了所提出的音频和视频模式模型在密集视频字幕任务中的有效性，但该模块能够在序列到序列任务中消化任何两种模式。我们还表明，作为双模转换器的一部分，预先训练的双模编码器可以用作简单提案生成模块的特征提取器。该性能在一个具有挑战性的ActivityNet字幕数据集上进行了演示，我们的模型在该数据集上实现了出色的性能。代码可用：v-iashin.github.io/bmt</pre></li>
<li><a href="https://arxiv.org/abs/2011.11760">Multimodal Pretraining for Dense Video Captioning</a> (AACL-IJCNLP2020)
<pre>通过教学视频学习烹饪、汽车维修和家庭维修等具体实践技能的情况越来越多。众所周知，通过元信息（如所涉及的主要步骤的时间戳注释）可以改善此类视频的用户体验。自动生成这样的注释是一项挑战，我们在这里描述了两个相关的贡献。首先，我们构建并发布了一个新的密集视频字幕数据集——视频时间线标签（ViTT），该数据集以各种教学视频和时间戳注释为特征。其次，我们探讨了几种多模态序列到序列预训练策略，这些策略利用了大量无监督的视频数据集和类似字幕的文本。我们使用YouCook2和ViTT对密集视频字幕模型进行预训练，然后进行微调。我们证明了这样的模型具有良好的泛化性，并且在各种各样的教学视频中都具有很强的鲁棒性。</pre></li>
<li><a href="https://arxiv.org/abs/2102.05095">Is Space-Time Attention All You Need for Video Understanding?</a>
<pre>我们提出了一种无卷积的视频分类方法，该方法完全基于空间和时间上的自我注意。我们的方法称为“时间转换器”，通过直接从一系列帧级补丁中学习时空特征，使标准转换器结构适应视频。我们的实验研究比较了不同的自我注意方案，并表明“分散注意”（时间注意和空间注意分别应用于每个块）在所考虑的设计选择中可以获得最佳的视频分类精度。尽管采用了全新的设计，TimeSformer在多个动作识别基准上取得了最先进的结果，包括Kinetics-400和Kinetics-600的最佳报告准确性。最后，与3D卷积网络相比，我们的模型训练速度更快，测试效率显著提高（精度略有下降），并且还可以应用于更长的视频剪辑（超过一分钟）。有关代码和型号，请访问：https://github.com/facebookresearch/TimeSformer.</pre></li>
<li><a href="https://arxiv.org/abs/2102.06183">Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling</a> (CVPR2021) [<a href="https://github.com/jayleicn/ClipBERT">github</a>]
<pre>视频和语言学习的规范方法（例如，视频问答）规定了一个神经模型，用于从视觉模型和语言模型的文本特征中离线提取密集视频特征。这些特征提取器是独立训练的，通常用于不同于目标域的任务，使得这些固定特征对于下游任务来说是次优的。此外，由于密集视频特征的高计算过载，通常很难（或不可行）将特征提取器直接插入现有方法中以便于微调。为了弥补这一困境，我们提出了一个通用框架ClipBERT，该框架通过采用稀疏采样，实现视频和语言任务的可负担的端到端学习，其中每个训练步骤仅使用视频中的一个或几个稀疏采样的短片段。在六个数据集上进行文本到视频检索和视频问答的实验表明，ClipBERT优于（或等同于）现有的利用全长视频的方法，这表明，仅使用少量稀疏采样片段的端到端学习通常比使用从全长视频中密集提取的离线特征更准确，证明了众所周知的“少即是多”原则。数据集中的视频来自相当不同的域和长度，从3秒的通用域GIF视频到180秒的YouTube人类活动视频，显示了我们方法的泛化能力。我们提供了全面的消融研究和彻底的分析，以剖析导致这一成功的因素。我们的代码在https://github.com/jayleicn/ClipBERT</pre></li>
<li><a href="https://arxiv.org/abs/2105.09996">VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding</a> (ACL2021 Findings)
<pre>我们提出了一种简化的、任务无关的多模式预训练方法，可以接受视频或文本输入，也可以同时接受视频或文本输入，用于各种终端任务。现有的预培训是针对特定任务的，通过采用需要两种模式的单个跨模式编码器，限制其用于检索式结束任务，或采用两个单模式编码器进行更复杂的多任务学习，限制早期跨模式融合。相反，我们引入了新的预训练掩蔽方案，可以更好地跨模式混合（例如，通过强制文本掩蔽来预测最近的视频嵌入），同时还保持可分性（例如，有时需要单峰预测，而不使用所有输入）。实验结果表明，与以前的任何方法相比，在更广泛的任务范围内，该方法具有很强的性能，通常优于特定任务的预训练。该守则可于https://github.com/pytorch/fairseq/tree/main/examples/MMPT.</pre></li>
<li><a href="https://arxiv.org/abs/2109.14084">VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding</a> (EMNLP2021)
<pre>我们提出了VideoCLIP，这是一种对比方法，用于预训练零镜头视频和文本理解的统一模型，而不在下游任务上使用任何标签。VideoCLIP通过对比时间重叠的正视频文本对和最近邻检索的硬负片，训练视频和文本的转换器。我们对一系列下游任务（包括序列级文本视频检索、视频QA、令牌级动作定位和动作分割）的实验显示了最先进的性能，超过了先前的工作，在某些情况下甚至优于监督方法。该守则可于https://github.com/pytorch/fairseq/tree/main/examples/MMPT.</pre></li>
<li><a href="https://arxiv.org/abs/2012.02128">BERT-hLSTMs: BERT and Hierarchical LSTMs for Visual Storytelling</a>
<pre>视觉故事讲述是一项富有创造性和挑战性的任务，旨在为一系列图像自动生成类似故事的描述。以前的视觉故事化方法所产生的描述缺乏连贯性，因为他们使用词级序列生成方法，并没有充分考虑句子级依赖性。为了解决这个问题，我们提出了一个新的分层视觉讲故事框架，该框架分别对句子级和单词级语义进行建模。我们使用基于变换器的BERT来获得句子和单词的嵌入。然后，我们采用分层LSTM网络：底部LSTM接收来自BERT的句子向量表示作为输入，以学习与图像对应的句子之间的依赖关系，顶部LSTM负责生成相应的单词向量表示，从底部LSTM获取输入。实验结果表明，在自动评估指标BLEU和CIDEr下，我们的模型的性能优于最密切相关的基线，也表明了我们的方法在人类评估中的有效性。</pre></li>
<li><a href="https://arxiv.org/abs/1910.11559">SpeechBERT: Cross-Modal Pre-trained Language Model for End-to-end Spoken Question Answering</a>
<pre>尽管最近人们对口语理解任务的各种端到端模型进行了探索，但本文可能是第一次挑战非常困难的端到端口语问答任务（SQA）的已知尝试。从各种文本处理任务的非常成功的BERT模型中学习，我们提出了一个音频和文本联合学习的SpeechBERT模型。该模型优于传统的级联ASR方法和以下文本问答（TQA）模型，用于包括ASR错误的数据集，因为端到端模型能够在ASR产生错误之前从音频数据中提取信息。将所提出的端到端模型与级联体系结构进行集成时，可以获得更好的性能。除了端到端SQA的潜力之外，SpeechBERT也可以用于许多其他口语理解任务，就像许多文本处理任务的BERT一样。</pre></li>
<li><a href="https://arxiv.org/abs/2005.12142">An Audio-enriched BERT-based Framework for Spoken Multiple-choice Question Answering</a>
<pre>在口语多项选择题回答（SMCQA）任务中，给定一段文字、一个问题和多项选择都是以语音形式出现的，机器需要选择正确的选项来回答问题。虽然音频可能包含SMCQA的有用提示，但在系统开发中通常只使用自动转录的文本。由于大规模的预训练语言表示模型，例如来自transformers（BERT）的双向编码器表示，只有自动转录文本的系统仍然可以达到一定的性能水平。然而，以前的研究已经证明，声级统计可以抵消自动语音识别系统造成的文本不准确或隐藏在单词嵌入生成器中的表示不足，从而使SMCQA系统具有鲁棒性。沿着研究路线，本研究致力于设计一个基于伯特的SMCQA框架，该框架不仅继承了伯特学习的语境化语言表示的优点，而且将从音频中提取的互补声级信息与文本级信息集成在一起。因此，提出了一种基于音频丰富的BERT的SMCQA框架。一系列实验表明，在已公布的中国SMCQA数据集上，与选定的基线和SOTA系统相比，精确度有显著提高。</pre></li>
<li><a href="https://arxiv.org/abs/1910.05453">vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations</a>
<pre>我们建议vq-wav2vec通过wav2vec风格的自监督上下文预测任务学习音频片段的离散表示。该算法使用gumbel-softmax或在线k-means聚类对密集表示进行量化。离散化可以直接应用NLP社区中需要离散输入的算法。实验表明，BERT预训练在TIMIT音素分类和WSJ语音识别方面取得了新的进展。</pre></li>
<li><a href="https://arxiv.org/abs/1911.03912">Effectiveness of self-supervised pre-training for speech recognition</a>
<pre>我们比较了显式量化音频数据或不量化学习表示的自监督表示学习算法。我们发现前者更准确，因为它通过vq-wav2vec[1]建立了良好的数据词汇表，以便在随后的训练中学习有效的表示。与以前的工作不同，我们直接使用连接主义时间分类（CTC）损失对转录语音上预先训练的BERT模型进行微调，而不是将表示输入到特定于任务的模型中。我们还提出了一种直接从连续音频数据学习的伯特式模型，并将原始音频的预训练与频谱特征进行了比较。使用vq-wav2vec词汇表在10小时的标记Librispeech数据上微调BERT模型几乎与在testclean上对100小时标记数据进行训练的最著名报告系统一样好，同时在其他测试上实现25%的功率降低。当仅使用10分钟的标记数据时，其他测试的WER为25.2，清洁测试的WER为16.3。这表明，自我监督可以使语音识别系统在转录数据量接近零的情况下进行训练。</pre></li>
<li><a href="https://arxiv.org/abs/2006.11477">wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations</a>
<pre>我们首次证明，仅从语音音频中学习强大的表示，然后对转录语音进行微调，可以在概念上更简单的同时，胜过最佳的半监督方法。wav2vec 2.0屏蔽潜在空间中的语音输入，并解决在联合学习的潜在表征量化上定义的对比任务。使用Librispeech所有标记数据的实验在干净/其他测试集上达到1.8/3.3 WER。当将标记数据量降低到1小时时，wav2vec 2.0在100小时子集上的性能优于以前的技术状态，同时使用的标记数据减少了100倍。仅使用10分钟的标记数据和对53k小时的未标记数据进行预训练，仍然可以达到4.8/8.2 WER。这证明了使用有限数量的标记数据进行语音识别的可行性。</pre></li>
<li><a href="https://arxiv.org/abs/2012.12121">Applying wav2vec2.0 to Speech Recognition in various low-resource languages</a>
<pre>有几个领域拥有相应的广泛使用的特征提取器，如ResNet、BERT和GPT-x。这些模型通常通过自我监督对大量未标记数据进行预训练，可以有效地应用于下游任务。在语音领域，wav2vec2.0开始在有声读物领域的Librispeech语料库上展示其强大的表示能力和超低资源语音识别的可行性。然而，除了英语之外，wav2vec2.0还没有在真实的口语场景和语言中进行过测试。为了验证它在语言上的普遍性，我们应用预先训练好的模型来解决各种口语中的低资源语音识别任务。与以前的工作相比，我们在六种语言方面实现了20%以上的相对改进。在这些语言中，英语增长了52.4%。此外，使用粗粒度的建模单元（如子词或字符）比使用细粒度的建模单元（如电话或信件）获得更好的结果。</pre></li>
<li><a href="https://arxiv.org/abs/2010.10504">Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition</a>
<pre>我们结合半监督学习在自动语音识别中的最新发展，利用Libri Light数据集的未标记音频，在LibriSpeech上获得最先进的结果。更准确地说，我们使用使用wav2vec 2.0预训练的巨型一致性模型，使用SPECAMULTE进行有噪声的学生训练。通过这样做，我们能够在LibriSpeech测试/测试其他集上实现字错误率（WER）1.4%/2.6%，而当前最先进的WER为1.7%/3.3%。</pre></li>
<li><a href="https://arxiv.org/abs/2102.00291">Speech Recognition by Simply Fine-tuning BERT</a> (ICASSP2021)
<pre>我们提出了一种通过微调BERT实现自动语音识别（ASR）的简单方法，这是一种在大规模未标记文本数据上训练的语言模型（LM），可以生成丰富的上下文表示。我们的假设是，给定一个历史上下文序列，一个强大的LM可以缩小可能的选择范围，语音信号可以用作简单的线索。因此，与从零开始训练强大声学模型（AM）的传统ASR系统相比，我们相信，通过简单地微调BERT模型，语音识别是可能的。作为一项初步研究，我们在AISHELL数据集上证明了所提出的思想的有效性，并表明在BERT上叠加一个非常简单的AM可以产生合理的性能。</pre></li>
<li><a href="https://arxiv.org/abs/1909.10924">Understanding Semantics from Speech Through Pre-training</a>
<pre>端到端口语理解（SLU）是一种直接从语音特征中推断语义而不需要中间文本表示的方法。尽管端到端SLU系统的声学模型组件可以使用自动语音识别（ASR）目标进行预训练，但SLU组件只能从有限的特定任务训练数据中学习语义特征。在本文中，我们首次提出对端到端SLU系统中的SLU组件进行大规模无监督预训练，以便SLU组件能够从大量未标记的音频数据中保留语义特征。由于声学模型组件（即音素后验序列）的输出与文本序列有很大的不同，我们提出了一种新的预训练模型BERT-PLM，该模型通过排列语言建模表示来自变换器的双向编码器表示。BERT-PLM通过相当于部分置换语言建模目标的回归目标对未标记数据上的SLU组件进行训练，同时利用BERT网络的完整双向上下文信息。实验结果表明，我们的方法比最先进的端到端系统的性能要好，误差降低了12.5%以上。</pre></li>
<li><a href="https://arxiv.org/abs/1910.10387">Speech-XLNet: Unsupervised Acoustic Model Pretraining For Self-Attention Networks</a>
<pre>自我注意网络（SAN）可以通过BERT和XLNet等无监督的预训练范式从双向表征学习中获益匪浅。在本文中，我们提出了一种类似XLNet的预训练方案“语音XLNet”，用于无监督声学模型预训练，以使用SAN学习语音表示。预训练的SAN在混合SAN/HMM框架下进行了微调。我们推测，通过洗牌语音帧顺序，语音XLNet中的排列可以作为一个强大的正则化器，鼓励SAN通过其注意力权重关注全局结构进行推断。此外，Speech XLNet还允许该模型探索双向上下文，以便进行有效的语音表示学习。在TIMIT和WSJ上的实验表明，与随机初始化权值训练相比，语音XLNet在收敛速度和识别精度方面都大大提高了SAN/HMM的性能。我们的最佳系统在TIMIT和WSJ任务上分别实现了11.9%和8.3%的相对改善。特别是，最佳系统在TIMIT测试集上实现了13.3%的电话错误率（PER），据我们所知，这是从单个系统获得的最低PER。</pre></li>
<li><a href="https://arxiv.org/abs/2007.04134">Learning Speech Representations from Raw Audio by Joint Audiovisual Self-Supervision</a> (ICML2020 WS)
<pre>视听模式之间的直观交互对于跨模式的自我监督学习很有价值。这一概念已在视频动作识别和声学场景分类等一般视听任务中得到验证。然而，视听讲话的自我监督仍有待探索。我们提出了一种从原始音频波形中学习自监督语音表示的方法。我们通过将纯音频自我监控（通过预测信息音频属性）与视觉自我监控（通过从音频生成对话人脸）相结合来训练原始音频编码器。视觉借口任务驱动音频表示来捕获与嘴唇运动相关的信息。这丰富了音频编码器的视觉信息和编码器可以用于评估没有视觉模态。我们的方法在已建立的孤立词分类基准上与现有的自监督音频特征相比具有竞争力，并且在从较少的标签学习方面显著优于其他方法。值得注意的是，我们的方法也优于完全监督训练，因此为语音相关任务提供了强大的初始化能力。我们的研究结果证明了多模态自我监控在视听讲话中学习良好音频表征的潜力。</pre></li>
<li><a href="https://arxiv.org/abs/2010.13826">Semi-Supervised Spoken Language Understanding via Self-Supervised Speech and Language Model Pretraining</a>
<pre>最近关于口语理解（SLU）的许多工作至少局限于以下三种方式中的一种：模型是在oracle文本输入上训练的，忽略了ASR错误；模型是在没有时隙值的情况下仅预测意图；或者模型是在大量内部数据上训练的。在本文中，我们提出了一个干净和通用的框架，从转录或未翻译的语音中直接通过半监督从语音中学习语义，以解决这些问题。我们的框架建立在预训练的端到端（E2E）ASR和自监督语言模型（如BERT）的基础上，并在有限数量的目标SLU数据上进行微调。我们研究了ASR组件的两种半监督设置：转录语音的监督预训练，以及用自监督语音表示（如wav2vec）替换ASR编码器的无监督预训练。同时，我们确定了评估SLU模型的两个基本标准：环境噪声鲁棒性和E2E语义评估。ATIS的实验表明，我们的SLU框架与语音作为输入可以执行与那些使用Oracle文本作为语义理解的输入，即使环境噪声存在，并且有限数量的标记语义数据可用于训练。</pre></li>
<li><a href="https://arxiv.org/abs/2010.12283">ST-BERT: Cross-modal Language Model Pre-training For End-to-end Spoken Language Understanding</a>
<pre>语言模型预训练在各种下游任务中显示出良好的效果。在此背景下，我们引入了一个跨模态的预训练语言模型，称为语音文本伯特（ST-BERT），用于处理端到端口语理解（E2E SLU）任务。ST-BERT以音素后级和子词级文本为输入，通过我们提出的两个训练前任务：跨模态掩蔽语言建模（CM-MLM）和跨模态条件语言建模（CM-CLM）学习语境化跨模态对齐。在三个基准上的实验结果表明，我们的方法对于各种SLU数据集都是有效的，并且即使在1%的训练数据可用的情况下，性能也会显著下降。此外，我们的方法还通过使用特定于域的语音文本对数据进行域自适应预训练，进一步提高了SLU的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2011.08238">End-to-end spoken language understanding using transformer networks and self-supervised pre-trained features</a>
<pre>变压器网络和自我监督预培训一直在自然语言处理（NLP）领域取得最新成果；然而，它们在口语理解（SLU）领域的优点仍需进一步研究。在本文中，我们介绍了一种基于模块化端到端（E2E）SLU变压器网络的架构，该架构允许使用自我监督的预训练声学特征、预训练模型初始化和多任务训练。使用ATIS数据集进行了几个SLU实验，用于预测意图和实体标签/值。这些实验研究了预训练模型初始化和多任务训练与传统滤波器组或自监督预训练声学特征的交互作用。结果表明，在几乎所有的实验中，自监督预训练声学特征都优于滤波器组特征，而且当这些特征与多任务训练结合使用时，它们几乎消除了预训练模型初始化的必要性。</pre></li>
<li><a href="https://arxiv.org/abs/2102.06283">Speech-language Pre-training for End-to-end Spoken Language Understanding</a>
<pre>端到端（E2E）口语理解（SLU）可以直接从语音信号中推断语义，而无需将自动语音识别器（ASR）与自然语言理解（NLU）模块级联。然而，成对的话语记录和相应的语义可能并不总是可用的，也不足以在实际生产环境中训练E2E SLU模型。在本文中，我们建议将优化的E2E ASR编码器（语音）和预训练的语言模型编码器（语言）统一到转换器解码器中。通过使用条件掩蔽语言模型（MLM）目标，在目标域的有限标记数据上不断增强统一语音语言预训练模型（SLP），从而可以有效地为推理中的给定输入语音生成意图序列、时隙类型和时隙值。在两个公共语料库上的实验结果表明，该方法优于传统的级联方法。它还优于目前最先进的E2E SLU方法，具有更少的成对数据。</pre></li>
<li><a href="https://arxiv.org/abs/2005.11640">Jointly Encoding Word Confusion Network and Dialogue Context with BERT for Spoken Language Understanding</a> (Interspeech2020)
<pre>口语理解（SLU）将自动语音识别器（ASR）中的假设转换为结构化语义表示。ASR识别错误会严重降低后续SLU模块的性能。为了解决这个问题，单词混淆网络（WCN）被用来编码SLU的输入，它包含比1-best或n-best假设列表更丰富的信息。为了进一步消除歧义，对话上下文的最后一个系统动作也被用作附加输入。本文提出了一种新的基于BERT的SLU模型（WCN-BERT SLU），用于对WCN和对话上下文进行联合编码。它可以将WCN的结构信息和ASR后验概率集成到BERT结构中。在SLU的一个基准DSTC2上的实验表明，该方法是有效的，并且能够显著优于以前的最新模型。</pre></li>
<li><a href="https://arxiv.org/abs/2106.13043">AudioCLIP: Extending CLIP to Image, Text and Audio</a>
<pre>在过去，快速发展的声音分类领域得益于其他领域方法的应用。今天，我们观察到将特定领域的任务和方法融合在一起的趋势，这为社区提供了新的优秀模型。在这项工作中，我们提出了剪辑模型的扩展，除了处理文本和图像外，还处理音频。我们提出的模型使用AudioSet数据集将ESResNeXt音频模型合并到CLIP框架中。这种组合使所提出的模型能够执行双峰和单峰分类和查询，同时保持CLIP以零镜头推理方式推广到看不见的数据集的能力。AudioCLIP在环境声音分类（ESC）任务中取得了最新的成果，在UrbanSound8K和ESC-50数据集上的准确率分别达到90.07%和97.15%，超过了其他方法。此外，它在相同的数据集（分别为68.78%和69.40%）上设置了零炮ESC任务的新基线。最后，我们还评估了所提出模型的跨模式查询性能以及完全和部分训练对结果的影响。为了再现性，我们发布了代码。</pre></li>
<li><a href="https://arxiv.org/abs/2005.08575">Audio ALBERT: A Lite BERT for Self-supervised Learning of Audio Representation</a>
<pre>对于自监督语音处理，使用预训练模型作为语音表示提取器是至关重要的。在最近的工作中，为了获得更好的性能，增加模型的大小已被用于声学模型训练。在本文中，我们提出了音频阿尔伯特，一个精简版本的自我监督语音表示模型。我们使用两个下游任务的表示，说话人识别和音素分类。我们表明，Audio ALBERT能够在下游任务中实现与那些大型模型竞争的性能，同时使用的参数减少91%。此外，我们使用一些简单的探测模型来测量说话人和音素的信息在潜在表征中被编码了多少。在探测实验中，我们发现潜在表征编码的音素和说话人信息都比最后一层丰富。</pre></li>
<li><a href="https://arxiv.org/abs/2006.13979">Unsupervised Cross-lingual Representation Learning for Speech Recognition</a>
<pre>本文提出了一种XLSR方法，该方法通过从多种语言的原始语音波形中预训练单个模型来学习跨语言语音表征。我们建立在wav2vec 2.0的基础上，通过解决掩盖潜在语音表征的对比任务来训练wav2vec 2.0，并共同学习跨语言共享的潜在语音量化。实验表明，跨语言预训练显著优于单语预训练。在CommonVoice基准测试中，XLSR显示，与最著名的结果相比，相对音素错误率降低了72%。在BABEL上，我们的方法比同类系统提高了16%的字错误率。我们的方法实现了一个单一的多语言语音识别模型，它可以与强大的单个模型竞争。分析表明，潜在的离散语音表示是跨语言共享的，而相关语言的共享则有所增加。我们希望通过发布XLSR-53，这是一个用53种语言预训练的大型模型，来促进低资源语音理解的研究。</pre></li>
<li><a href="https://arxiv.org/abs/2004.10093">Curriculum Pre-training for End-to-End Speech Translation</a> (ACL2020)
<pre>端到端语音翻译给编码器带来了沉重的负担，因为它必须同时转录、理解和学习跨语言语义。为了获得功能强大的编码器，传统的方法在ASR数据上对其进行预训练以捕获语音特征。然而，我们认为仅通过简单的语音识别对编码器进行预训练是不够的，应该考虑高级语言知识。受此启发，我们提出了一种课程前培训方法，其中包括一门转录学习的初级课程和两门理解两种语言中的话语和映射单词的高级课程。这些课程的难度逐渐增加。实验表明，我们的课程预训练方法显著提高了En-De和En-Fr语音翻译基准。</pre></li>
<li><a href="https://arxiv.org/abs/2010.11445">MAM: Masked Acoustic Modeling for End-to-End Speech-to-Text Translation</a>
<pre>端到端语音到文本翻译（E2E-ST）是一种将源语言语音直接翻译成目标语言文本的翻译方法，在实际应用中有着广泛的应用，但传统的级联翻译方法（ASR+MT）往往会遇到错误传播的问题。另一方面，现有的端到端解决方案在很大程度上依赖于源语言转录进行预训练或具有自动语音识别（ASR）的多任务训练。相反，我们提出了一种简单的技术，仅在语音端以自监督方式学习鲁棒语音编码器，它可以利用语音数据而无需转录。这种被称为掩蔽声学建模（MAM）的技术不仅为改进E2E-ST提供了一种替代解决方案，而且还可以对任何声学信号（包括非语音信号）进行无注释的预训练。我们在8个不同的翻译方向上进行实验。在不使用任何转录的情况下，我们的技术实现了+1.1 BLEU和+2.3 BLEU的平均改善（MAM预训练）。使用任意声学信号对MAM进行预训练，这些语言的平均改善率为+1.6 BLEU。与ASR多任务学习解决方案相比，我们的预训练MAM模型不使用转录，在训练过程中响应转录，达到了相似的精度。</pre></li>
<li><a href="https://arxiv.org/abs/2010.12829">Multilingual Speech Translation with Efficient Finetuning of Pretrained Models</a> (ACL2021)
<pre>我们提出了一种简单而有效的方法，通过预训练语音编码器和文本解码器的有效迁移学习来构建多语种语音到文本（ST）的翻译。我们的主要发现是，一个最低限度的LNA（LayerNorm and Attention）微调可以通过微调小于10%的预训练参数来实现零镜头跨语言和跨模态传输能力。这可以有效地利用大型预训练模型，降低训练成本。使用wav2vec 2.0进行声学建模，使用mBART生成多语言文本，我们的方法在大规模多语言ST基准CoVoST 2上为34个翻译方向（其中23个方向超过级联ST）提供了最新技术（+6.4 BLEU在15个En-X方向上的平均值，以及+5.1 BLEU在19个X-En方向上的平均值）。我们的方法在多对多语言模型中展示了强大的零炮性能（+5.7 BLEU在18个非英语方向上的平均值），使其成为获得高质量语音翻译并提高参数和数据效率的吸引人的方法。</pre></li>
<li><a href="https://arxiv.org/abs/2103.03541">Multilingual Byte2Speech Text-To-Speech Models Are Few-shot Spoken Language Learners</a>
<pre>为了将神经语音合成扩展到各种实际语言，我们提出了一个多语言端到端框架，将字节输入映射到频谱图，从而允许任意输入脚本。除了对40多种语言的强大结果外，该框架还展示了在极低资源、甚至只有40秒转录记录的少数镜头场景下适应新语言的能力，而不需要词汇、额外语料库、辅助模型或语言专业知识等每种语言资源，从而确保可扩展性。虽然它保留了令人满意的清晰度和自然匹配丰富的资源模型。进行了详尽的对比研究，以揭示该框架对低资源语言的潜力。此外，为了更好地理解其机制，我们提出了一种在多语言模型中提取特定于语言的子网络的新方法。</pre></li>
<li><a href="https://arxiv.org/abs/1906.07307">Towards Transfer Learning for End-to-End Speech Synthesis from Deep Pre-Trained Language Models</a>
<pre>现代文本到语音（TTS）系统能够生成声音几乎与人类语音一样自然的音频。然而，开发高质量的TTS系统的门槛仍然很高，因为通常需要一组相当大的演播室质量<text，audio>对。与用于开发最先进系统的商业数据相比，公开可用的数据通常在质量和规模方面都较差。根据公开数据训练的TTS系统生成的音频不仅听起来不那么自然，而且还表现出更多的背景噪声。在这项工作中，我们的目标是降低TTS系统对高质量数据的依赖性，通过在训练过程中为它们提供由深度预训练语言模型提取的文本知识。特别是，我们研究了使用BERT辅助训练Tacotron-2，这是一种由编码器和基于注意的解码器组成的最先进的TTS。从大量未标记文本数据中学习到的BERT表示包含关于输入文本的非常丰富的语义和句法信息，并且有可能被TTS系统用来弥补高质量数据的不足。我们将BERT作为一个平行分支合并到Tacotron-2编码器中，该编码器具有自己的注意头。对于输入文本，它同时被传递到BERT和Tacotron-2编码器。由两个分支提取的表示被连接起来，然后馈送到解码器。作为一项初步研究，虽然我们还没有发现将BERT纳入Tacotron-2能够在人类可感知水平上产生更自然或更干净的语音，我们观察到其他方面的改进，例如模型在知道何时停止解码方面明显更好，这样在合成音频结束时就不会有太多的胡言乱语，并且在训练期间会更快地收敛。</pre></li>
<li><a href="https://arxiv.org/abs/2008.01551">To BERT or Not To BERT: Comparing Speech and Language-based Approaches for Alzheimer’s Disease Detection</a> (Interspeech2020)
<pre>鉴于阿尔茨海默病的高发病率和传统方法的高成本，与自动检测阿尔茨海默病（AD）相关的研究非常重要。由于广告对自然语言的内容和声学有显著影响，自然语言处理和机器学习为可靠地检测AD提供了有前途的技术。我们在最近的ADReSS挑战数据集上比较和对比了两种AD检测方法的性能：1）使用基于领域知识的手工特征，捕捉语言和声学现象，以及2）基于变换器（BERT）的序列分类模型的微调双向编码器表示。我们还比较了挑战中神经心理学评分任务的多重基于特征的回归模型。我们观察到，考虑到语言学在认知损伤检测中的相对重要性，微调的BERT模型在AD检测任务上优于基于特征的方法。</pre></li>
<li><a href="https://arxiv.org/abs/2010.10892">BERT for Joint Multichannel Speech Dereverberation with Spatial-aware Tasks</a>
<pre>我们提出了一种具有两个空间感知任务的联合多信道语音去冗余方法：波达方向（DOA）估计和语音分离。所提出的方法将涉及的任务作为序列到序列的映射问题来处理，这对于各种前端语音增强任务来说已经足够普遍了。该方法的灵感来自于变压器双向编码器表示（BERT）出色的序列建模能力。我们不是以自我监督的方式使用预训练的显式表示，而是以监督的方式使用转换器编码的隐藏表示。对变长语音的多通道谱幅度和谱相位信息进行编码。实验结果证明了该方法的有效性。</pre></li>
</ul>
<h1 id="model-compression">Model compression</h1>
<ul>
<li><a href="https://arxiv.org/abs/2008.05221">Compression of Deep Learning Models for Text: A Survey</a>
<pre>近年来，自然语言处理（NLP）和信息检索（IR）领域取得了巨大进展，这得益于递归神经网络（RNN）、选通递归单元（GRU）和长短时记忆（LSTMs）网络等深度学习模型，以及Transformer[120]基于变压器的双向编码器表示（BERT）[24]、生成预训练变压器（GPT-2）[94]、多任务深度神经网络（MT-DNN）[73]、超长网络（XLNet）[134]、文本到文本传输变压器（T5）[95]、T-NLG[98]和GShard[63]等模型。但是这些模型的尺寸很大。另一方面，现实世界的应用要求模型尺寸小、响应时间短和计算功率低。在这篇综述中，我们讨论了六种不同类型的方法（剪枝、量化、知识提炼、参数共享、张量分解和基于次二次变换的方法）用于压缩此类模型，以便在实际行业NLP项目中进行部署。考虑到使用高效小型模型构建应用程序的迫切需要，以及该领域最近发表的大量工作，我们相信这项调查组织了“NLP深度学习”社区在过去几年所做的大量工作，并将其作为一个连贯的故事呈现出来。</pre></li>
<li><a href="https://arxiv.org/abs/1903.12136">Distilling Task-Specific Knowledge from BERT into Simple Neural Networks</a>
<pre>在自然语言处理文献中，神经网络正变得越来越深刻和复杂。这一趋势的最新代表是深层语言表示模型，包括伯特、埃尔莫和GPT。这些发展使人们相信，上一代用于语言理解的较浅的神经网络已经过时。然而，在本文中，我们证明了在不改变体系结构、外部训练数据或附加输入特征的情况下，基本的、轻量级的神经网络仍然具有竞争力。我们建议将BERT（一种最先进的语言表示模型）中的知识提取到单层BiLSTM中，以及用于句子对任务的暹罗对应模型中。在释义、自然语言推理和情感分类的多个数据集中，我们使用ELMo获得了可比的结果，同时使用的参数大约减少了100倍，推理时间减少了15倍。</pre></li>
<li><a href="https://arxiv.org/abs/1908.09355">Patient Knowledge Distillation for BERT Model Compression</a> (EMNLP2019)
<pre>经过预训练的语言模型（如BERT）已被证明对自然语言处理（NLP）任务非常有效。然而，在训练这些模型时对计算资源的高需求阻碍了它们在实践中的应用。为了缓解大规模模型训练中的资源短缺问题，我们提出了一种患者知识提取方法，将原始的大型模型（教师）压缩为同样有效的轻量级浅层网络（学生）。与以前的知识提取方法不同，我们的学生模型只使用教师网络最后一层的输出进行提取，我们的学生模型耐心地从教师模型的多个中间层学习增量知识提取，遵循两种策略：（$i$）PKD last：从最后的$k$层学习；和（$ii$）PKD Skip：从每$k$层学习。这两种患者蒸馏方案能够利用教师隐藏层中的丰富信息，并鼓励学生模型通过多层蒸馏过程耐心地向教师学习和模仿。从经验上看，这可以转化为多个NLP任务的改进结果，在不牺牲模型准确性的情况下显著提高训练效率。</pre></li>
<li><a href="https://arxiv.org/abs/1909.00100">Small and Practical BERT Models for Sequence Labeling</a> (EMNLP2019)
<pre>我们提出了一个实用的方案来训练一个单一的多语言序列标记模型，该模型可以产生最先进的结果，并且小而快，可以在单个CPU上运行。从公共多语言BERT检查点开始，我们的最终模型比最先进的多语言基线小6倍，快27倍，精确度更高。我们证明了我们的模型在低资源语言上的表现尤其出色，并且可以处理codemixed输入文本，而无需对codemixed示例进行显式训练。我们通过对70个树状词库和48种语言的词性标注和形态预测的报告，展示了我们方法的有效性。</pre></li>
<li><a href="https://arxiv.org/abs/1909.10351">TinyBERT: Distilling BERT for Natural Language Understanding</a> [<a href="https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT">github</a>]
<pre>语言模型预训练（如BERT）显著提高了许多自然语言处理任务的性能。然而，预先训练的语言模型通常计算量很大，因此很难在资源受限的设备上有效地执行它们。为了加快推理速度，减少模型尺寸，同时保持准确性，我们首先提出了一种新的变压器蒸馏方法，该方法专门用于变压器模型的知识蒸馏（KD）。通过利用这种新的KD方法，可以有效地将大量编码在大型教师BERT中的知识转移到小型学生BERT中。然后，我们为TinyBERT引入了一个新的两阶段学习框架，该框架在训练前和任务特定学习阶段都执行变压器蒸馏。该框架确保TinyBERT能够捕获BERT中的一般领域以及特定于任务的知识。TinyBERT有4层，在经验上是有效的，在GLUE基准测试中，其性能达到其教师BERTBASE的96.8%以上，同时推理速度是其教师Bertbert的7.5倍和9.4倍。4层TinyBERT也明显优于4层最先进的BERT蒸馏基线，只有约28%的参数和约31%的推断时间。此外，TinyBERT具有6层，其性能与其基础相当。</pre></li>
<li><a href="https://arxiv.org/abs/1910.01108">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</a> (NeurIPS2019 WS) [<a href="https://github.com/huggingface/transformers/tree/master/examples/distillation">github</a>]
<pre>随着大规模预训练模型的迁移学习在自然语言处理（NLP）中变得越来越普遍，在边缘和/或受限制的计算训练或推理预算中操作这些大型模型仍然具有挑战性。在这项工作中，我们提出了一种方法来预先训练一个较小的通用语言表示模型，称为DistilBERT，然后可以对其进行微调，使其在广泛的任务（如较大的任务）上具有良好的性能。虽然之前的大多数工作都研究了使用蒸馏来构建特定于任务的模型，但我们在训练前阶段利用了知识蒸馏，并表明可以将BERT模型的大小减少40%，同时保留97%的语言理解能力，并加快60%。为了利用大模型在训练前学习到的归纳偏差，我们引入了一种结合语言建模、蒸馏和余弦距离损失的三重损失。我们的更小、更快、更轻的模型比预训练更便宜，我们在概念验证实验和比较性的设备研究中展示了其在设备上计算的能力。</pre></li>
<li><a href="https://arxiv.org/abs/2009.14167">Contrastive Distillation on Intermediate Representations for Language Model Compression</a> (EMNLP2020)
<pre>现有的语言模型压缩方法大多使用一个简单的L2损失来将大型BERT模型的中间表示形式中的知识提取到较小的BERT模型。尽管被广泛使用，但这一设计目标假设隐藏表示的所有维度都是独立的，无法捕获教师网络中间层中的重要结构知识。为了获得更好的蒸馏效果，我们提出了基于中间表征的对比蒸馏（CoDIR），这是一个原则性的知识蒸馏框架，在该框架中，学生通过对比目标通过教师的中间层进行知识蒸馏。CoDIR通过学习区分大量正样本和负样本，有助于学生利用教师隐藏层中的丰富信息。CoDIR可以很容易地应用于在预训练和微调阶段压缩大规模语言模型，并在GLUE基准上实现了卓越的性能，优于最先进的压缩方法。</pre></li>
<li><a href="https://arxiv.org/abs/1910.03723">Knowledge Distillation from Internal Representations</a> (AAAI2020)
<pre>知识提炼通常是通过训练一个小模型（学生）来模仿一个大而笨重的模型（老师）。其思想是通过使用输出概率作为软标签来优化学生，从而压缩来自教师的知识。然而，当教师人数相当大时，无法保证教师的内部知识会转移到学生身上；即使学生与软标签非常匹配，其内部表示也可能有很大不同。这种内部不匹配会破坏原本打算从教师转移到学生身上的泛化能力。在本文中，我们建议将一个大型模型（如BERT）的内部表示提取为它的简化版本。我们制定了两种提取这种表示的方法和各种算法来进行提取。我们使用GLUE基准测试中的数据集进行了实验，结果一致表明，从内部表示添加知识提取比仅使用软标签提取更有效。</pre></li>
<li><a href="https://arxiv.org/abs/2012.06048">Reinforced Multi-Teacher Selection for Knowledge Distillation</a> (AAAI2021)
<pre>在自然语言处理（NLP）任务中，推理速度慢和GPU占用空间大仍然是在生产中应用预先训练好的深度模型的瓶颈。作为一种流行的模型压缩方法，知识提取将知识从一个或多个大型（教师）模型转移到一个小型（学生）模型。当蒸馏中有多个教师模型可用时，最先进的方法会在整个蒸馏过程中为教师模型指定一个固定的权重。此外，现有的大多数方法对每一个教师模型都给予同等的权重。在本文中，我们观察到，由于训练示例的复杂性和学生模型能力的差异，从教师模型中进行差异学习可以提高学生模型的性能。我们系统地开发了一种增强方法，针对不同的训练实例动态分配教师模型的权重，并优化学生模型的性能。我们在几个NLP任务上的大量实验结果清楚地验证了我们方法的可行性和有效性。</pre></li>
<li><a href="https://arxiv.org/abs/2012.14022">ALP-KD: Attention-Based Layer Projection for Knowledge Distillation</a> (AAAI2021)
<pre>知识提取被认为是一种训练和压缩策略，其中教师和学生两个神经网络在训练过程中耦合在一起。教师网络被认为是一个值得信赖的预测者，学生试图模仿其预测。通常，我们会选择一个结构更轻的学生，这样我们就可以实现压缩，同时提供高质量的结果。在这种情况下，蒸馏只发生在最终预测中，而学生也可以从老师对内部组件的监督中获益。基于此，我们研究了中间层的精馏问题。由于学生层和教师层之间可能没有一对一的对齐，现有技术跳过了一些教师层，只从其中的一个子集提取。这一缺点直接影响质量，因此我们提出了一种依赖注意的组合技术。我们的模型融合了教师方面的信息，并考虑了每一层的重要性，然后在组合的教师层和学生层之间进行提炼。使用我们的技术，我们将一个12层的BERT（Devlin等人，2019年）蒸馏成6层、4层和2层的对应物，并在粘合任务中对其进行评估（Wang等人，2018年）。实验结果表明，我们的组合方法能够优于其他现有技术。</pre></li>
<li><a href="https://arxiv.org/abs/2109.11295">Dynamic Knowledge Distillation for Pre-trained Language Models</a> (EMNLP2021)
<pre>知识提取（KD）已被证明是压缩大规模预训练语言模型的有效方法。然而，现有方法静态地进行KD，例如，学生模型将其输出分布与预定义培训数据集上选定教师模型的输出分布对齐。在这篇论文中，我们探讨了一种动态的知识提炼是否能够使学生根据自己的能力调整学习过程，并考虑到学生的表现和学习效率。我们从教师模式采用、数据选择和KD目标适应三个方面探讨了动态调整。实验结果表明：（1）选择合适的教师模式可以提高学生模式的学习效果；（2） 使用10%的信息量实例进行KD，在大大加快培训速度的同时，取得了相当的绩效；（3） 通过调整不同对齐目标的监督贡献，可以提高学生的成绩。我们发现动态知识提取是有希望的，并讨论了未来更有效的知识发现方法的潜在方向。我们的代码可在https://github.com/lancopku/DynamicKD.</pre></li>
<li><a href="https://arxiv.org/abs/2109.08359">Distilling Linguistic Context for Language Model Compression</a> (EMNLP2021)
<pre>最近语言表征学习的成功背后是一个计算量大且记忆密集的神经网络。知识提取是在资源匮乏的环境中部署如此庞大的语言模型的一项主要技术，它可以不受限制地将所学的单个单词表示的知识转移。在本文中，受最近观察到的语言表征相对定位且整体上具有更多语义知识的启发，我们提出了一个新的语言表征学习知识提取目标，该目标通过两种类型的跨表征关系传递上下文知识：单词关系和层转换关系。与语言模型的其他最新蒸馏技术不同，我们的上下文蒸馏对教师和学生之间的架构更改没有任何限制。我们验证了我们的方法在具有挑战性的语言理解任务基准测试中的有效性，不仅在各种规模的体系结构中，而且结合最近提出的自适应规模修剪方法DynaBERT。</pre></li>
<li><a href="https://arxiv.org/abs/2012.06153">Improving Task-Agnostic BERT Distillation with Layer Mapping Search</a>
<pre>知识提取（KD）是一种将知识从大的教师模型转移到小的学生模型的方法，近年来被广泛应用于BERT模型的压缩。除了原始KD中输出的监督外，最近的工作表明，层级监督对student-BERT模型的性能至关重要。然而，以前的工作是启发式地设计层映射策略（例如，统一层或最后一层），这可能导致性能低下。在本文中，我们建议使用遗传算法（GA）自动搜索最佳图层映射。为了加快搜索过程，我们进一步提出了一种代理设置，其中一小部分训练语料被采样进行蒸馏，并选择三个代表性任务进行评估。在获得最佳层映射后，我们对整个语料库执行任务不可知的BERT蒸馏，以构建一个紧凑的学生模型，该模型可以直接对下游任务进行微调。对评估基准的综合实验表明：1）层映射策略对任务不可知性有显著影响，不同的层映射会导致不同的性能；2） 从提出的搜索过程中得到的最优层映射策略始终优于其他启发式策略；3） 通过优化层映射，我们的学生模型在粘合任务上实现了最先进的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2001.08950">PoWER-BERT: Accelerating BERT inference for Classification Tasks</a>
<pre>我们开发了一种新的方法，称为功率伯特（PoWER-BERT），以提高流行的伯特模型的推理时间，同时保持准确性。它的工作原理是：a）利用与字向量（中间编码器输出）相关的冗余并消除冗余向量。b） 基于自我注意机制，通过制定一种策略来衡量哪些词向量的重要性，从而确定要消除哪些词向量。c） 通过增加BERT模型和损失函数，学习要消除多少词向量。在标准GLUE基准测试上的实验表明，与BERT相比，PoWER BERT在推理时间上减少了4.5倍，准确度损失小于1%。我们表明，与以前的方法相比，幂BERT在准确性和推理时间之间提供了更好的折衷。我们证明，我们的方法在应用于高度压缩的BERT版本ALBERT时，推理时间减少了6.8倍，准确度损失小于1%。PoWER BERT的代码可在https://github.com/IBM/PoWER-BERT.</pre></li>
<li><a href="https://arxiv.org/abs/1912.06638">WaLDORf: Wasteless Language-model Distillation On Reading-comprehension</a>
<pre>基于转换器的超大型语言模型（VLLMs），如BERT、XLNet和RoBERTa，最近在各种自然语言理解（NLU）任务中表现出了巨大的性能。然而，由于它们的尺寸，这些VLLM是资源密集型的，并且在生产时部署起来非常麻烦。最近的一些出版物研究了各种方法，将基于转换器的VLLM（最常见的是BERT库）中的知识提取到一个更小的模型中，该模型在推理时可以运行得更快。在这里，我们提出了一组新的技术，这些技术共同产生了一个特定于任务的混合卷积和变换模型，WaLDORf，该模型实现了最先进的推理速度，同时仍然比以前的蒸馏模型更精确。</pre></li>
<li><a href="https://arxiv.org/abs/1909.11687">Extremely Small BERT Models from Mixed-Vocabulary Training</a> (EACL2021)
<pre>像BERT这样的预训练语言模型在NLP任务上取得了很好的效果，但由于内存占用，在资源有限的设备上是不切实际的。这一足迹的很大一部分来自具有大量输入词汇表和嵌入维度的输入嵌入。现有的用于模型压缩的知识提取方法不能直接用于训练词汇量较小的学生模型。为此，我们提出了一种蒸馏方法，通过混合词汇训练来调整教师和学生的嵌入。我们的方法将BERT-LARGE压缩为具有较小词汇和隐藏维度的任务不可知模型，该模型比其他蒸馏的BERT模型小一个数量级，并且在语言理解基准以及实际对话任务上提供了更好的尺寸精度权衡。</pre></li>
<li><a href="https://arxiv.org/abs/2002.02925">BERT-of-Theseus: Compressing BERT by Progressive Module Replacing</a> (EMNLP2020)
<pre>在本文中，我们提出了一种新的模型压缩方法，通过渐进模块替换来有效地压缩BERT。我们的方法首先将原始的BERT划分为几个模块，并构建它们的紧凑替代品。然后，我们用它们的替代品随机替换原始模块，以训练紧凑模块模仿原始模块的行为。我们通过培训逐步增加更换的可能性。通过这种方式，我们的方法在原始模型和紧凑模型之间带来了更深层次的交互。与以前的知识提取方法相比，我们的方法没有引入任何额外的损失函数。我们的方法在GLUE基准上优于现有的知识提取方法，显示了模型压缩的新视角。</pre></li>
<li><a href="https://arxiv.org/abs/2002.08307">Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning</a> (ACL2020 SRW)
<pre>预先训练的通用特征抽取器，如用于自然语言处理的BERT和用于计算机视觉的VGG，已经成为改进深度学习模型的有效方法，而不需要更多的标记数据。虽然有效，但对于某些部署场景，像BERT这样的功能提取器可能会非常大。我们探讨了BERT的权重修剪，并问：训练前的压缩如何影响迁移学习？我们发现剪枝在三个广泛的领域影响迁移学习。低水平的修剪（30-40%）根本不会影响培训前的损失或转移到下游任务。中等水平的修剪会增加训练前的损失，并防止有用的训练前信息转移到下游任务。高水平的修剪还防止模型拟合下游数据集，从而导致进一步退化。最后，我们观察到对特定任务进行微调不会提高其可裁剪性。我们的结论是，在训练前，可以对BERT进行一次删减，而不是在不影响性能的情况下对每个任务单独进行删减。</pre></li>
<li><a href="https://arxiv.org/abs/2002.10957">MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers</a>
<pre>预先训练的语言模型（例如，BERT（Devlin et al.，2018）及其变体）在各种NLP任务中取得了显著的成功。然而，这些模型通常由数亿个参数组成，由于延迟和容量限制，这给实际应用中的微调和在线服务带来了挑战。在这项工作中，我们提出了一种简单有效的方法来压缩大型变压器（Vaswani et al.，2017），该方法基于预先训练的模型，称为深度自我注意蒸馏。小模型（学生）通过深入模仿大模型（教师）的自我注意模块进行训练，自我注意模块在变压器网络中起着至关重要的作用。具体来说，我们建议提取教师最后一层的自我注意模块，这对学生来说是有效和灵活的。此外，除了已有研究中使用的注意分布（即查询和键的缩放点积）外，我们还引入了自我注意模块中值之间的缩放点积作为新的深度自我注意知识。此外，我们还表明，引入一名教师助理（Mirzadeh等人，2019年）也有助于提取大型预培训变压器模型。实验结果表明，在不同参数大小的学生模型中，我们的单语模型优于最新的基线。特别是，它使用50%的变压器参数和教师模型的计算，在2.0班和几个胶水基准任务上保持了99%以上的准确性。在将深度自我注意蒸馏应用于多语言预训练模型方面，我们也取得了有竞争力的结果。</pre></li>
<li><a href="https://arxiv.org/abs/2104.11928">Extract then Distill: Efficient and Effective Task-Agnostic BERT Distillation</a>
<pre>任务不可知的知识提取，一个师生框架，已被证明是有效的伯特压缩。尽管在NLP任务上取得了有希望的结果，但它需要大量的计算资源。在本文中，我们提出了一种通用且灵活的策略，即先提取后提取（ETD），以重用教师的参数进行高效且有效的任务无关提取，该策略可应用于任何规模的学生。具体来说，我们介绍了两种ETD变体，ETD Rand和ETD Impt，它们分别以随机方式和遵循重要性度量来提取教师参数。这样，学生在蒸馏过程开始时就已经掌握了一些知识，这使得蒸馏过程收敛得更快。我们在GLUE基准和团队上演示了ETD的有效性。实验结果表明：（1）与没有ETD策略的基线相比，ETD可以节省70%的计算成本。此外，在使用相同的计算资源时，它比基线获得更好的结果。（2） ETD是通用的，已被证明对不同的蒸馏方法（如TinyBERT和MiniLM）和不同规模的学生有效。源代码将在发布后公开。</pre></li>
<li><a href="https://arxiv.org/abs/2002.11985">Compressing Large-Scale Transformer-Based Models: A Case Study on BERT</a>
<pre>预先训练的基于Transformer的模型已经在各种自然语言处理（NLP）任务中实现了最先进的性能。然而，这些模型通常有数十亿个参数，因此，太过消耗资源和计算密集，不适合具有严格延迟要求的低性能设备或应用程序。一个潜在的补救办法是模型压缩，这已经引起了很多研究关注。在这里，我们总结了压缩变压器的研究，重点是特别流行的伯特模型。特别是，我们调查了BERT压缩的最新技术，阐明了压缩大型变压器模型的当前最佳实践，并深入了解了各种方法的工作原理。我们的分类和分析也为实现轻量级、准确和通用的NLP模型提供了有希望的未来研究方向。</pre></li>
<li><a href="https://arxiv.org/abs/2002.11794">Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers</a>
<pre>由于硬件资源有限，训练深度学习模型的目标通常是在训练和推理的时间和内存限制下最大限度地提高准确性。我们研究了这种环境下模型大小的影响，重点研究了NLP任务中受计算限制的转换器模型：自监督预训练和高资源机器翻译。我们首先表明，即使较小的Transformer模型在每次迭代中执行得更快，但较宽和较深的模型收敛的步骤要少得多。此外，这种加速收敛的速度通常超过了使用更大模型的额外计算开销。因此，计算效率最高的训练策略是反直觉地训练非常大的模型，但在少量迭代后停止。这导致了大型变压器模型的训练效率和小型变压器模型的推理效率之间的明显权衡。然而，我们表明，与小模型相比，大模型对量化和修剪等压缩技术更具鲁棒性。因此，我们可以两全其美：高度压缩的大型模型比轻度压缩的小型模型精度更高。</pre></li>
<li><a href="https://arxiv.org/abs/1908.08962">Well-Read Students Learn Better: On the Importance of Pre-training Compact Models</a>
<pre>自然语言表示法的最新发展伴随着大量昂贵的模型，这些模型通过自我监督的预训练来利用大量的通用领域文本。由于将此类模型应用于下游任务的成本，已经提出了几种预训练语言表示的模型压缩技术（Sun等人，2019年；Sanh，2019年）。然而，令人惊讶的是，仅仅预先训练和微调紧凑模型的简单基线被忽略了。在本文中，我们首先表明，在较小的体系结构中，预训练仍然很重要，并且微调预训练的紧凑模型可以与并行工作中提出的更复杂的方法相竞争。从预先训练的紧凑模型开始，我们接着探索通过标准知识提取从大型微调模型转移任务知识。由此产生的简单但有效的通用算法，即预训练蒸馏，带来了进一步的改进。通过大量实验，我们更普遍地探讨了在两个变量（模型大小和未标记任务数据的属性）下预训练和蒸馏之间的相互作用。一个令人惊讶的观察结果是，即使顺序应用于相同的数据，它们也会产生复合效应。为了加速未来的研究，我们将公开我们的24个预先训练的微型BERT模型。</pre></li>
<li><a href="https://arxiv.org/abs/2004.02984">MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices</a> (ACL2020)
<pre>自然语言处理（NLP）最近通过使用具有数亿个参数的庞大预训练模型取得了巨大成功。但是，这些型号的型号尺寸大，延迟时间长，因此无法部署到资源有限的移动设备上。在本文中，我们提出MobileBERT来压缩和加速流行的BERT模型。与原始的BERT一样，MobileBERT是任务不可知的，也就是说，它可以通过简单的微调一般地应用于各种下游NLP任务。基本上，MobileBERT是BERT_LARGE的精简版，同时配备了瓶颈结构和精心设计的自我关注和前馈网络之间的平衡。为了训练MobileBERT，我们首先训练了一个专门设计的教师模型，一个结合了BERT_大模型的倒瓶颈模型。然后，我们从这位老师向MobileBERT进行知识转移。实证研究表明，MobileBERT比BERT_BASE小4.3倍，速度快5.5倍，同时在知名基准上取得了竞争性成果。在GLUE的自然语言推理任务中，MobileBERT在Pixel 4手机上的GLUEscore为77.7（比BERT_BASE低0.6），延迟为62毫秒。在团队v1.1/v2.0问答任务中，MobileBERT的开发F1得分为90.0/79.2（比BERT_BASE高1.5/2.1）。</pre></li>
<li><a href="https://arxiv.org/abs/2005.03848">Distilling Knowledge from Pre-trained Language Models via Text Smoothing</a>
<pre>本文研究通过师生知识提取压缩预训练的语言模型，如BERT（Devlin et al.，2019）。以前的工作通常迫使学生模型严格模仿老师预测的平滑标签。作为替代方案，我们提出了一种新的BERT提取方法，即要求教师生成平滑的单词id，而不是标签，用于教授知识提取中的学生模型。我们称这种方法为文本平滑。实际上，我们在BERT中使用蒙蔽语言模型（MLM）的softmax预测来生成给定文本的单词分布，并使用预测的软单词ID平滑这些输入文本。我们假设平滑标签和平滑文本都可以隐式地增加输入语料库，而文本平滑直观上更有效，因为它可以在一个神经网络前向步骤中生成更多的实例。GLUE和SQuAD上的实验结果表明，我们的解决方案与现有的BERT蒸馏方法相比，可以获得具有竞争力的结果。</pre></li>
<li><a href="https://arxiv.org/abs/2004.04037">DynaBERT: Dynamic BERT with Adaptive Width and Depth</a>
<pre>像BERT这样预先训练好的语言模型，虽然在许多自然语言处理任务中功能强大，但计算和内存都很昂贵。为了缓解这个问题，一种方法是在部署之前针对特定任务压缩它们。然而，最近关于BERT压缩的工作通常将大型BERT模型压缩为固定的较小尺寸。它们不能完全满足不同硬件性能的边缘器件的要求。在本文中，我们提出了一种新的动态BERT模型（简称DynaBERT），它可以通过选择自适应的宽度和深度来灵活地调整大小和延迟。DynaBERT的训练过程包括首先训练一个宽度自适应的BERT，然后通过从全尺寸模型中提取知识到小的子网络，同时允许宽度和深度自适应。网络重新布线也被用来让更多的子网络共享更重要的注意力头和神经元。在各种效率约束下的综合实验表明，我们提出的动态BERT（或RoBERTa）在其最大尺寸下具有与BERT基（或RoBERTa基）相当的性能，而在较小的宽度和深度下，其性能始终优于现有的BERT压缩方法。代码可在https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/DynaBERT.</pre></li>
<li><a href="https://arxiv.org/abs/1909.11556">Reducing Transformer Depth on Demand with Structured Dropout</a>
<pre>超参数化变压器网络在各种自然语言处理任务中取得了最新成果，如机器翻译、语言建模和问答。这些模型包含数亿个参数，需要大量计算，并且容易过度拟合。在这项工作中，我们探索LayerDrop，一种结构化的辍学形式，它在训练期间具有正则化效果，并允许在推理时进行有效的修剪。特别是，我们表明，可以从一个大型网络中选择任意深度的子网络，而无需对其进行微调，并且对性能的影响有限。我们通过改进机器翻译、语言建模、摘要、问答和语言理解基准的最新技术，证明了我们方法的有效性。此外，我们还表明，与从头开始的训练或使用蒸馏相比，我们的方法可以得到更高质量的小的类伯特模型。</pre></li>
<li><a href="https://www.aclweb.org/anthology/2020.acl-main.204/">DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference</a> (ACL2020)</li>
<li><a href="https://arxiv.org/abs/2006.04152">BERT Loses Patience: Fast and Robust Inference with Early Exit</a> [<a href="https://github.com/JetRunner/PABEE">github</a>] [<a href="https://github.com/huggingface/transformers/tree/master/examples/bert-loses-patience">github</a>]
<pre>在本文中，我们提出了基于耐心的早期退出，这是一种简单而有效的推理方法，可以作为一种即插即用技术，同时提高预训练语言模型（PLM）的效率和鲁棒性。为了实现这一点，我们的方法将内部分类器与PLM的每一层相耦合，并在内部分类器的中间预测在预定义的步骤数保持不变时动态停止推理。我们的方法提高了推理效率，因为它允许模型使用较少的层进行预测。同时，一个ALBERT模型的实验结果表明，与现有的早期退出方法相比，我们的方法可以通过防止过度思考和利用多个分类器进行预测来提高模型的准确性和鲁棒性，从而获得更好的准确性和速度权衡。</pre></li>
<li><a href="https://arxiv.org/abs/2105.13878">Accelerating BERT Inference for Sequence Labeling via Early-Exit</a> (ACL2021)
<pre>在许多实际场景中，性能和效率都是序列标记任务的关键因素。虽然预训练模型（PTM）显著提高了各种序列标记任务的性能，但其计算成本非常昂贵。为了缓解这个问题，我们扩展了最近成功的早期退出机制，以加速序列标记任务的PTM推断。然而，现有的早期退出机制是专门为序列级任务而设计的，而不是序列标记。在本文中，我们首先提出了一个简单的句子级提前退出扩展序列标记任务。为了进一步降低计算成本，我们还提出了一种令牌级提前退出机制，允许部分令牌在不同层提前退出。考虑到序列标记中固有的局部依赖性，我们采用基于窗口的标准来决定令牌是否退出。令牌级提前退出带来了训练和推理之间的差距，因此我们引入了额外的自采样微调阶段来缓解这一差距。对三种常见的序列标记任务的大量实验表明，该方法在性能降低最小的情况下，可以节省高达66%-75%的推理成本。与竞争对手的压缩模型（如DistilBERT）相比，我们的方法可以在相同的2倍、3倍和4倍加速比下获得更好的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2107.00175">Elbert: Fast Albert with Confidence-Window Based Early Exit</a>
<pre>尽管在自然语言处理（NLP）领域取得了巨大的成功，但像BERT这样的大型预训练语言模型由于参数数量多、推理速度慢而不适合于资源受限或实时应用。近年来，BERT的压缩和加速已经成为一个重要的研究课题。通过采用参数共享策略，ALBERT在实现竞争性能的同时，大大减少了参数的数量。尽管如此，阿尔伯特仍然有很长的推理时间。在这项工作中，我们提出了ELBERT，与ALBERT相比，由于提出了基于置信窗的早期退出机制，它显著提高了平均推理速度，而不引入额外的参数或额外的训练开销。实验结果表明，在不同的数据集上，与ALBERT相比，ELBERT实现了从2$\倍到10$\倍的自适应推理加速，而精度下降可以忽略不计。此外，在计算量相同的情况下，ELBERT算法比现有的早期退出算法具有更高的精度。此外，为了理解早期退出机制的原理，我们还将其决策过程可视化到ELBERT中。</pre></li>
<li><a href="https://arxiv.org/abs/2101.09755">RomeBERT: Robust Training of Multi-Exit BERT</a>
<pre>伯特在自然语言理解（NLU）任务上取得了优异的成绩。然而，BERT拥有大量的参数，需要部署一定的资源。在加速方面，最近提出了动态提前退出BERT（DeeBERT），它包含多个退出，并采用动态提前退出机制以确保有效的推理。在获得效率-性能折衷的同时，多出口BERT中早期出口的性能明显低于晚期出口。在本文中，我们利用梯度正则化自蒸馏对多出口BERT（RomeBERT）进行鲁棒训练，可以有效地解决早期和晚期出口之间的性能不平衡问题。此外，所提出的RomeBERT对多出口和BERT主干采用一阶段联合训练策略，而DeeBERT需要两个阶段，需要更多的训练时间。在GLUE数据集上进行了大量实验，以证明我们方法的优越性。我们的代码可在https://github.com/romebert/RomeBERT.</pre></li>
<li><a href="https://arxiv.org/abs/2105.11618">TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference</a> (NAACL2021)
<pre>现有的预训练语言模型（PLM）在推理方面的计算成本通常很高，这使得它们在各种资源有限的实际应用中不切实际。为了解决这一问题，我们提出了一种动态令牌缩减方法来加速PLMs的推理，称为TR-BERT，它可以灵活地调整推理中每个令牌的层数，以避免冗余计算。特别地，TR-BERT将令牌缩减过程描述为一个多步骤令牌选择问题，并通过强化学习自动学习选择策略。在几个下游NLP任务上的实验结果表明，TR-BERT能够将BERT速度提高2-5倍，以满足各种性能要求。此外，TR-BERT还可以在一组长文本任务中以较少的计算量获得更好的性能，因为它的令牌层数自适应大大加快了PLMs中的自我注意操作。本文的源代码和实验细节可以从https://github.com/thunlp/TR-BERT.</pre></li>
<li><a href="https://www.aclweb.org/anthology/2020.acl-main.537/">FastBERT: a Self-distilling BERT with Adaptive Inference Time</a> (ACL2020)</li>
<li><a href="https://arxiv.org/abs/2101.08890">Distilling Large Language Models into Tiny and Effective Students using pQRNN</a>
<pre>大型预先训练的多语言模型，如mBERT、XLM-R，可以在语言理解任务中获得最先进的结果。但是，它们不太适合服务器和边缘设备上的延迟关键型应用程序。减少这些模型所需的内存和计算资源非常重要。为此，我们提出了pQRNN，这是一种基于投影的无嵌入神经编码器，它对于自然语言处理任务来说非常微小和有效。在没有预训练的情况下，尽管PQRNN比LSTM模型小140倍，但PQRNN的性能明显优于预训练嵌入的LSTM模型。在参数数量相同的情况下，它们的性能优于变压器基线，从而显示了它们的参数效率。此外，我们还表明PQRNN是提取大型预训练语言模型的有效学生体系结构。我们进行仔细的烧蚀，研究pQRNN参数、数据增加和蒸馏设置的影响。在具有挑战性的多语言语义分析数据集MTOP上，pQRNN学生的成绩达到mBERT教师的95.9\%，但比mBERT教师小350倍。在mATIS（一种流行的语法分析任务）上，pQRNN的学生平均能够接触到97.1%的老师，同时又小了350倍。我们强有力的结果表明，我们的方法非常适合于对延迟敏感的应用程序，同时能够利用类似mBERT的大型模型。</pre></li>
<li><a href="https://arxiv.org/abs/2004.03097">Towards Non-task-specific Distillation of BERT via Sentence Representation Approximation</a>
<pre>最近，由于其有效性和通用性，BERT已成为各种NLP深度模型的重要组成部分。然而，BERT的在线部署往往因其庞大的参数和较高的计算成本而受阻。大量研究表明，知识提取能够有效地将知识从BERT转移到参数较小的模型中。然而，目前的BERT提取方法主要集中在任务特定的提取上，这种方法导致了通用性BERT的一般语义知识的丢失。在本文中，我们提出了一个面向提取的句子表示近似框架，该框架可以在不指定任务的情况下将预先训练好的BERT提取到一个简单的基于LSTM的模型中。与BERT一致，我们的蒸馏模型能够通过微调执行迁移学习，以适应任何句子级下游任务。此外，我们的模型可以进一步配合特定任务的蒸馏过程。GLUE基准测试中的多个NLP任务的实验结果表明，我们的方法优于其他特定任务的蒸馏方法或更大的模型，即ELMO，效率得到了很好的提高。</pre></li>
<li><a href="https://arxiv.org/abs/2004.04124">LadaBERT: Lightweight Adaptation of BERT through Hybrid Model Compression</a> (COLING2020)
<pre>BERT是一种由大量语料库预先训练的尖端语言表示模型，在各种自然语言理解任务中取得了优异的性能。然而，将BERT应用于在线服务的一个主要障碍问题是，它占用大量内存，导致用户请求的延迟不能令人满意，从而增加了模型压缩的必要性。现有的解决方案利用知识提取框架来学习模仿BERT行为的较小模型。然而，知识提炼的训练过程本身是昂贵的，因为它需要足够的训练数据来模拟教师模型。在本文中，我们提出了一种混合解决方案LadaBERT（通过混合模型压缩实现BERT的轻量级自适应），它结合了不同模型压缩方法的优点，包括权重修剪、矩阵分解和知识提取。LadaBERT在各种公共数据集上实现了最先进的准确性，同时培训开销可以减少一个数量级。</pre></li>
<li><a href="https://arxiv.org/abs/2004.03844">Poor Man’s BERT: Smaller and Faster Transformer Models</a>
<pre>基于Transformer的NLP模型使用数亿甚至数十亿个参数进行训练，这限制了它们在计算受限环境中的适用性。虽然参数的数量通常与性能相关，但不清楚下游任务是否需要整个网络。受最近关于修剪和提取预训练模型的工作的启发，我们探索了在预训练模型中删除层的策略，并观察了修剪对下游粘合任务的影响。我们能够将BERT、RoBERTa和XLNet模型删减高达40%，同时保持高达98%的原始性能。此外，我们表明，我们的修剪模型与使用知识蒸馏构建的，无论是在大小和性能方面。我们的实验产生了有趣的观察结果，如：（i）较低的层次对维持下游任务绩效最为关键，（ii）一些任务，如释义检测和句子相似性，对层次的下降更为稳健，以及（iii）使用不同的目标函数训练的模型表现出不同的学习模式，并且w.r.t层下降。</pre></li>
<li><a href="https://arxiv.org/abs/2005.06628">schuBERT: Optimizing Elements of BERT</a> (ACL2020)
<pre>Transformers\citep{vaswani2017attention}已逐渐成为许多最先进的自然语言表示模型的关键组件。最近一个基于转换器的模型-BERT\citep{devlin2018bert}在各种自然语言处理任务上取得了最新的成果，包括GLUE、第1.1版和第2.0版。然而，这个模型在计算上是禁止的，并且有大量的参数。在这项工作中，我们重温了BERT的架构选择，以获得更轻的模型。我们专注于减少参数的数量，但我们的方法可以应用于其他目标，如触发器或延迟。我们表明，通过减少算法选择的正确架构设计维度，而不是减少变压器-编码器层的数量，可以获得更有效的光伯特模型。特别是，我们的舒伯特在GLUE和SQuAD数据集上的平均准确度比具有三个编码器层且参数数量相同的伯特高6.6\%$。</pre></li>
<li><a href="https://arxiv.org/abs/2010.06133">BERT-EMD: Many-to-Many Layer Mapping for BERT Compression with Earth Mover’s Distance</a> (EMNLP2020) [<a href="https://github.com/lxk00/BERT-EMD">github</a>]
<pre>预先训练的语言模型（如BERT）在各种自然语言处理（NLP）任务中取得了显著的成功。然而，高存储和计算成本阻碍了预先训练的语言模型在资源受限的设备上的有效部署。在本文中，我们提出了一种新的基于多对多层映射的伯特蒸馏方法，它允许每个中间学生层从任何中间教师层学习。这样，我们的模型可以针对不同的NLP任务自适应地从不同的教师层学习受直觉的驱使，不同的NLP任务需要不同层次的语言知识，这些知识包含在BERT的中间层中。此外，我们利用地球移动器距离（EMD）计算将知识从教师网络转换为学生网络所必须支付的最小累积成本。EMD可实现多对多图层映射的有效匹配。%EMD可以应用于不同规模的网络层，有效地度量教师网络和学生网络之间的语义距离。此外，我们还提出了一种代价注意机制来自动学习EMD中使用的层权重，以进一步提高模型的性能，加快收敛速度。在GLUE benchmark上进行的大量实验表明，我们的模型在准确性和模型压缩方面与强大的竞争对手相比具有竞争力。</pre></li>
<li><a href="https://arxiv.org/abs/2106.01023">One Teacher is Enough? Pre-trained Language Model Distillation from Multiple Teachers</a> (ACL2021 Findings)
<pre>预训练语言模型（PLM）在NLP中取得了巨大的成功。然而，它们庞大的模型尺寸阻碍了它们在许多实际系统中的应用。知识提取是一种流行的PLM压缩技术，它从大型教师PLM中学习一个小的学生模型。然而，从一位教师那里学到的知识可能是有限的，甚至是有偏见的，从而导致低质量的学生模式。在本文中，我们提出了一个多教师知识提取框架MT-BERT，用于预训练的语言模型压缩，它可以从多教师PLM中训练出高质量的学生模型。在MT-BERT中，我们设计了一种多教师协同微调方法，通过共享池和预测层对下游任务中的多教师PLM进行联合微调，以调整其输出空间，实现更好的协同教学。此外，我们还提出了一种多教师隐式损失和多教师蒸馏损失，将多教师PLM中隐藏状态和软标签的有用知识转移到学生模型中。在三个基准数据集上的实验验证了MT-BERT在PLM压缩中的有效性。</pre></li>
<li><a href="https://arxiv.org/abs/2112.07198">From Dense to Sparse: Contrastive Pruning for Better Pre-trained Language Model Compression</a> (AAAI2022)
<pre>在预训练和微调范式下，预训练语言模型（PLM）在各种自然语言处理（NLP）任务中取得了巨大成功。plm具有大量的参数，计算量大，资源消耗大。因此，模型修剪被引入到大规模PLM的压缩中。然而，大多数先验方法只考虑任务特定知识对下游任务的影响，而忽略修剪过程中不必要的任务无关知识，这可能导致灾难性遗忘问题，并导致泛化能力差。为了在我们的剪枝模型中保持任务不可知和任务特定的知识，我们在预训练和微调的范式下提出了对比剪枝（CAP）。它被设计为一个通用框架，兼容结构化和非结构化修剪。CAP统一于对比学习，使剪枝模型能够从预先训练的任务不可知知识模型和微调的任务特定知识模型中学习。此外，为了更好地保持修剪模型的性能，快照（即每次修剪迭代中的中间模型）也可以作为修剪的有效监督。我们的大量实验表明，采用CAP始终会产生显著的改进，特别是在非常高的稀疏性场景中。仅保留3%的模型参数（即97%的稀疏性），CAP在QQP和MNLI任务中成功实现了原始BERT性能的99.2%和96.3%。此外，我们的探索性实验表明，CAP修剪后的模型具有更好的泛化能力。</pre></li>
<li><a href="https://arxiv.org/abs/2004.05686">TinyMBERT: Multi-Stage Distillation Framework for Massive Multi-lingual NER</a> (ACL2020)
<pre>对于各种自然语言处理任务来说，深度和大型预先训练的语言模型是最先进的。然而，这些模型的巨大规模可能会阻碍它们在实践中的应用。一些近期和并行的工作使用知识提炼将这些巨大的模型压缩为浅层模型。在这项工作中，我们研究了知识提取，重点是多语言命名实体识别（NER）。特别是，我们研究了几种蒸馏策略，并提出了一种利用教师内部表示的阶段优化方案，该方案与教师体系结构无关，并且表明它优于先前工作中采用的策略。此外，我们还研究了几个因素的作用，如未标记数据量、注释资源、模型体系结构和推理延迟等。我们表明，我们的方法导致了MBERT类教师模型的大量压缩，参数压缩高达35倍，批量推理延迟压缩高达51倍，同时保留了41种语言中95%的F1分数。</pre></li>
<li><a href="https://www.aclweb.org/anthology/2020.acl-main.202/">XtremeDistil: Multi-stage Distillation for Massive Multilingual Models</a> (ACL2020)</li>
<li><a href="https://arxiv.org/abs/2103.08809">Robustly Optimized and Distilled Training for Natural Language Understanding</a>
<pre>在本文中，我们探讨了多任务学习（MTL）作为第二个预训练步骤来学习变压器语言模型的增强通用语言表示。我们在几个自然语言理解任务中使用MTL增强表示来提高性能和泛化能力。此外，我们将知识提炼（KD）纳入MTL中，以进一步提高绩效，并设计了一种KD变体，该变体可以有效地从多名教师那里学习。通过将MTL和KD相结合，我们提出了稳健优化和提炼（ROaD）的建模框架。我们使用ROaD和ELECTRA模型来获得机器阅读理解和自然语言推理的最新结果。</pre></li>
<li><a href="https://arxiv.org/abs/1910.04732">Structured Pruning of Large Language Models</a>
<pre>大型语言模型最近在各种自然语言任务中取得了最先进的性能。与此同时，这些模型的大小和延迟都显著增加，这使得它们的使用成本很高，并提出了一个有趣的问题：语言模型需要大吗？我们从模型压缩的角度来研究这个问题。我们提出了一种通用的结构化剪枝方法，通过使用低秩因子分解对每个权重矩阵进行参数化，并在训练过程中自适应地去除秩-1分量。在语言建模任务上，我们的结构化方法在不同的压缩级别上优于其他非结构化和块结构化修剪基线，同时在训练和推理过程中实现了显著的加速。我们还证明了我们的方法可以应用于修剪大型语言模型中的自适应单词嵌入，以及在几个下游微调分类基准上修剪BERT模型。</pre></li>
<li><a href="https://arxiv.org/abs/2005.07683">Movement Pruning: Adaptive Sparsity by Fine-Tuning</a> [<a href="https://github.com/huggingface/transformers/tree/master/examples/movement-pruning">github</a>]
<pre>在纯监督学习中，幅度剪枝是一种广泛使用的减小模型大小的策略；然而，在已经成为最先进的自然语言处理应用标准的迁移学习机制中，它的效率较低。我们建议使用运动剪枝，这是一种简单、确定的一阶权重剪枝方法，更适合于预训练模型的微调。我们给出了该方法的数学基础，并将其与现有的零阶和一阶修剪方法进行了比较。实验表明，在剪枝大型预训练语言模型时，运动剪枝在高稀疏区域表现出显著的改进。当与蒸馏相结合时，该方法只需3%的模型参数即可实现最小的精度损失。</pre></li>
<li><a href="https://arxiv.org/abs/2009.08065">Efficient Transformer-based Large Scale Language Representations using Hardware-friendly Block Structured Pruning</a> (EMNLP2020 Findings)
<pre>预先训练的大规模语言模型在许多自然语言处理（NLP）任务中越来越显示出高精度。然而，硬件平台上有限的重量存储和计算速度阻碍了预训练模型的普及，尤其是在边缘计算时代。在这项工作中，我们提出了一种高效的基于转换器的大规模语言表示方法，使用硬件友好的块结构修剪。我们将重新加权的组套索合并到块结构剪枝中进行优化。除了显著减少重量存储和计算外，该方法还实现了较高的压缩率。在通用语言理解评估（GLUE）基准任务的不同模型（BERT、RoBERTa和DistilBERT）上的实验结果表明，在某些任务上，我们实现了高达5.0x的准确度，并且零或轻微降低了准确度。我们提出的方法也与现有的紧凑型预训练语言模型（如使用知识提取的DistilBERT）正交，因为在DistilBERT的基础上可以进一步获得1.79倍的平均压缩率，而精度降低为零或很小。它适用于在资源受限的边缘设备上部署最终的压缩模型。</pre></li>
<li><a href="https://www.aclweb.org/anthology/2020.findings-emnlp.64/">Pruning Redundant Mappings in Transformer Models via Spectral-Normalized Identity Prior</a> (EMNLP2020 Findings)</li>
<li><a href="https://arxiv.org/abs/2012.07463">Parameter-Efficient Transfer Learning with Diff Pruning</a>
<pre>虽然预训练网络的任务特定微调在NLP方面带来了显著的经验进步，但网络的大尺寸使得微调难以在多任务、内存受限的环境中部署。我们提出差异剪枝作为一种简单的方法，在pretrain-finetune框架内实现参数有效的迁移学习。该方法将精细调整视为学习特定于任务的差异向量，该差异向量应用于预训练参数向量之上，该参数向量保持不变，并在不同任务之间共享。在训练过程中，使用L0范数惩罚的可微近似自适应修剪微分向量，以鼓励稀疏性。随着任务数量的增加，差异修剪变得更加有效，因为它只需要存储每个任务的差异向量的非零位置和权重，而存储共享预训练模型的成本保持不变。此外，它不需要在培训期间访问所有任务，这使得它在任务以流形式到达或任务集未知的环境中具有吸引力。我们发现，使用差异修剪进行微调的模型可以匹配GLUE基准上完全微调基线的性能，而每个任务只修改0.5%的预训练模型参数。</pre></li>
<li><a href="https://arxiv.org/abs/2010.13382">FastFormers: Highly Efficient Transformer Models for Natural Language Understanding</a> (EMNLP2020 WS) [<a href="https://github.com/microsoft/fastformers">github</a>]
<pre>基于转换器的模型是自然语言理解（NLU）应用的最新技术。模型在各种任务上越来越大，越来越好。然而，变压器模型在计算上仍然具有挑战性，因为与传统方法相比，它们在推理时效率不高。在本文中，我们提出了FastFormers，这是一组方法，用于在各种NLU任务中实现基于Transformer的模型的高效推理时间性能。我们展示了如何小心地利用知识提取、结构化剪枝和数值优化可以极大地提高推理效率。我们提供有效的配方，可以指导从业者为各种NLU任务和预训练模型选择最佳设置。将建议的配方应用到SuperGLUE基准测试中，与CPU上的现成机型相比，我们实现了9.8倍到233.9倍的速度提升。在GPU上，我们还使用所提出的方法实现了高达12.4倍的速度提升。我们表明，在Azure F16s_v2实例上，FastFormers可以将服务1亿个请求的成本从4223美元大幅降低到18美元。根据SustaiNLP 2020共享任务中使用的指标，将能耗降低6.9倍-125.8倍，从而实现可持续运行。</pre></li>
<li><a href="https://arxiv.org/abs/2107.13686">AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models</a> (ACL2021) [<a href="https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/AutoTinyBERT">github</a>]
<pre>预训练语言模型（PLM）在自然语言处理中取得了巨大的成功。大多数PLM遵循BERT中架构超参数的默认设置（例如，在前馈子网络中，隐藏维度是中间维度的四分之一）（Devlin等人，2019）。很少有研究探索BERT中架构超参数的设计，特别是对于更有效的小尺寸PLM，这对于在资源受限设备上的实际部署至关重要。在本文中，我们采用一次性神经结构搜索（NAS）来自动搜索结构超参数。具体而言，我们仔细设计了一次性学习技术和搜索空间，为各种延迟约束提供了一种自适应且高效的微型PLM开发方法。我们将我们的方法命名为AutoTinyBERT，并在GLUE和SQuAD基准上评估其有效性。大量实验表明，我们的方法优于基于SOTA搜索的基线（NAS-BERT）和基于SOTA蒸馏的方法（如DistilBERT、TinyBERT、MiniLM和MobileBERT）。此外，基于所获得的体系结构，我们提出了一种比单个PLM的开发速度更快的更高效的开发方法。</pre></li>
<li><a href="https://arxiv.org/abs/2106.13474">Adapt-and-Distill: Developing Small, Fast and Effective Pretrained Language Models for Domains</a> (ACL2021 Findings)
<pre>大型预训练模型在许多自然语言处理任务中取得了巨大成功。然而，当它们应用于特定领域时，这些模型会受到领域转移的影响，并在延迟和容量限制的微调和在线服务方面带来挑战。在本文中，我们提出了一种为特定领域开发小型、快速和有效的预训练模型的通用方法。这是通过调整现成的通用预训练模型并在目标域中执行任务无关知识提取来实现的。具体来说，我们在适应阶段提出了特定领域的词汇扩展，并利用语料库级别的出现概率来自动选择增量词汇的大小。然后，我们系统地探索不同的策略来压缩特定领域的大型预训练模型。我们在生物医学和计算机科学领域进行实验。实验结果表明，在特定领域的任务中，我们的方法比BERT-BASE模型取得了更好的性能，比BERT-BASE模型小3.3倍，快5.1倍。代码和预先培训的模型可在https://aka.ms/adalm.</pre></li>
<li><a href="https://arxiv.org/abs/1910.01769">Distilling BERT into Simple Neural Networks with Unlabeled Transfer Data</a>
<pre>最近通过自我监督对大量文本进行预训练的巨大模型在各种自然语言处理任务中取得了最新的成果。然而，这些庞大且昂贵的模型很难在实践中用于下游任务。最近的一些工作使用知识提炼来压缩这些模型。然而，我们看到，与大教师相比，小学生模型的表现存在差距。在这项工作中，除了有限的标记训练实例之外，我们还利用了大量域内未标记的传输数据来弥合提取BET的差距。我们表明，简单的RNN为基础的学生模型，即使具有硬蒸馏，可以执行与给定的巨大的教师传递集。通过软蒸馏和利用教师的中间表现，学生的表现可以进一步提高。我们的研究表明，我们的学生模型可以将庞大的教师压缩高达26倍，同时在低资源环境下，使用少量的标记数据，仍然可以匹配甚至略微超过教师的表现。此外，对于XtremeDistil（慕克吉和Hassan Awadallah，2020年）的多语言扩展工作，我们展示了对多语言类BERT教师模型的大量提炼，在参数压缩方面高达35倍，在批量推理延迟加速方面高达51倍，同时在41种语言中保留95%的F1分数。</pre></li>
<li><a href="https://arxiv.org/abs/2001.04246">AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural Architecture Search</a>
<pre>大型预先训练的语言模型，如BERT，已经在各种自然语言处理任务中显示了它们的有效性。然而，巨大的参数大小使得它们很难部署在需要使用有限资源进行快速推理的实时应用程序中。现有方法将BERT压缩到小模型中，而这种压缩是任务独立的，即，对于所有不同的下游任务，相同的压缩BERT。基于面向任务的BERT压缩的必要性和优点，我们提出了一种新的压缩方法AdaBERT，该方法利用可微神经结构搜索将BERT自动压缩为特定任务的任务自适应小模型。我们结合了一个面向任务的知识提取损失来提供搜索提示和一个效率感知损失作为搜索约束，这使得任务自适应BERT压缩能够在效率和有效性之间进行很好的权衡。我们在几个NLP任务上评估了AdaBERT，结果表明，这些任务自适应压缩模型在推理时间上比BERT快12.7倍到29.3倍，在参数大小上比BERT小11.5倍到17.0倍，同时保持了可比性能。</pre></li>
<li><a href="https://arxiv.org/abs/2006.11316">SqueezeBERT: What can computer vision teach NLP about efficient neural networks?</a>
<pre>人类每天读写数千亿条信息。此外，由于大型数据集、大型计算系统和更好的神经网络模型的可用性，自然语言处理（NLP）技术在理解、校对和组织这些信息方面取得了重大进展。因此，在众多应用程序中部署NLP以帮助web用户、社交网络和企业是一个巨大的机会。特别是，我们认为智能手机和其他移动设备作为关键平台部署NLP模型的规模。然而，如今的高精度NLP神经网络模型（如BERT和RoBERTa）在计算上非常昂贵，在Pixel 3智能手机上对文本片段进行分类需要1.7秒。在这项工作中，我们观察到分组卷积等方法已经为计算机视觉网络带来了显著的加速，但其中许多技术尚未被NLP神经网络设计者采用。我们演示了如何用分组卷积替换自我注意层中的若干操作，并将此技术用于一种称为SqueezeBERT的新型网络体系结构中，该网络体系结构的运行速度比基于像素3的BERT快4.3倍，同时在粘合测试集上实现了具有竞争力的准确性。压缩码将被释放。</pre></li>
<li><a href="https://arxiv.org/abs/2010.03688">Optimizing Transformers with Approximate Computing for Faster, Smaller and more Accurate NLP Models</a>
<pre>近年来，Transformer模型通过在一系列自然语言处理（NLP）任务中提供最先进的性能而引起了人们的极大兴趣。然而，这些模型可能有超过1000亿个参数，对计算和内存的要求非常高。我们通过近似计算解决这一挑战，特别针对NLP任务中变压器的使用。变压器通常经过预先培训，然后通过转移学习专门用于特定任务。基于预先训练的变压器在几个下游NLP任务中经常被过度参数化的观察，我们提出了一个框架来创建更小、更快、在某些情况下更精确的模型。该框架的关键基石是显著性分析（SA）方法，该方法识别预先培训的变压器中对给定任务不太重要的部件，以及近似不太重要部件的技术。我们的近似方法包括对块、注意头和权重组进行修剪，对不太重要的权重进行量化，以及基于低复杂度符号匹配的注意机制。我们的框架可以根据用户的约束条件进行调整，以生成更快、更小和/或更精确的模型。我们将我们的框架应用于七个变压器模型，包括优化模型，如DistilBERT和Q8BERT，以及三个下游任务。我们证明，我们的框架生成的模型速度快4倍，小14倍（相对精度降低不到0.5%），或者精度高5.5%，同时模型大小提高9.83倍，速度提高2.94倍。</pre></li>
<li><a href="https://arxiv.org/abs/2010.08512">An Approximation Algorithm for Optimal Subarchitecture Extraction</a> [<a href="https://github.com/alexa/bort/">github</a>]
<pre>我们考虑的问题找到一组选定的深度神经网络的结构参数，这是最佳的三个指标：参数大小，推理速度和错误率。在本文中，我们正式地陈述了这个问题，并提出了一个近似算法，对于一个大的实例子集，其行为类似于一个近似误差为$\rho\leq{1-\epsilon}}的FPTA，并且以$O（|{Xi}}+{W^*| T}|（1+|{\Theta}}{B}{124;}{Xi 3}步运行，其中$\epsilon$和$s$是输入参数$|{B} |$是批量大小$|{W^*_T}|$表示最大权重集赋值的基数；和${\Xi}{124;$和${\Theta}{124;$分别是候选体系结构和超参数空间的基数。</pre></li>
<li><a href="https://arxiv.org/abs/1910.06360">Structured Pruning of a BERT-based Question Answering Model</a>
<pre>行业背景自然语言处理（NLP）研究的最新趋势是在严格的计算限制下操作大规模预训练语言模型，如BERT。虽然大多数模型压缩工作都集中在使用昂贵的预训练蒸馏来“蒸馏”通用语言表示，但较少关注创建更小的任务特定语言表示，可以说，这些语言表示在行业环境中更有用。在本文中，我们研究了通过对底层变压器模型的参数进行结构化剪枝来压缩基于BERT和RoBERTa的问答系统。我们发现，任务特定结构化剪枝和任务特定蒸馏的廉价组合，不需要预先训练蒸馏的费用，可以在一系列速度/精度折衷操作点上产生高性能的模型。我们从为2.0班或自然问题培训的现有全尺寸模型开始，介绍允许单独消除变压器选定部件的闸门。具体而言，我们研究（1）结构化修剪以减少每个变压器层中的参数数量，（2）适用于基于BERT和RoBERTa的模型，（3）适用于2.0班和自然问题，以及（4）将结构化修剪与蒸馏相结合。我们实现了推理速度的近两倍，在自然问题的简短回答准确度上损失不到0.5分。</pre></li>
<li><a href="https://arxiv.org/abs/2005.00697">DeFormer: Decomposing Pre-trained Transformers for Faster Question Answering</a> (ACL2020)
<pre>基于Transformer的QA模型在所有层都使用输入范围内的自我关注——即跨问题和输入通道——导致它们速度慢且内存密集。事实证明，我们可以在所有层面上，特别是在较低层面上，不需要投入广泛的自我关注。我们引入了变形器，一个分解的变换器，它在底层用问题范围和通道范围的自我关注来代替完全的自我关注。这允许对输入文本表示进行独立于问题的处理，从而支持预计算段落表示，从而大大减少运行时计算量。此外，由于变形器与原始模型非常相似，我们可以使用标准变换器的预训练权重初始化变形器，并直接在目标QA数据集上进行微调。我们展示了变形器版本的BERT和XLNet可以将QA的速度提高4.3倍以上，并且使用简单的基于蒸馏的损失，它们只会导致精度下降1%。我们在https://github.com/StonyBrookNLP/deformer.</pre></li>
<li><a href="https://arxiv.org/abs/1911.03829">Distilling Knowledge Learned in BERT for Text Generation</a> (ACL2020)
<pre>大规模预先训练的语言模型，如BERT，在语言理解任务中取得了巨大的成功。然而，如何利用BERT生成语言仍然是一个悬而未决的问题。在本文中，我们提出了一种新的方法，条件掩蔽语言建模（C-MLM），以实现对目标生成任务的BERT微调。微调的BERT（教师）被用作额外的监督，以改进传统的Seq2Seq模型（学生），从而获得更好的文本生成性能。通过利用BERT独特的双向性，提取在BERT中学习到的知识可以鼓励自回归Seq2Seq模型提前计划，对连贯文本生成实施全局序列级监督。实验表明，该方法在机器翻译和文本摘要等多语言生成任务上的性能明显优于强转换基线。我们提出的模型在IWSLT德语-英语和英语-越南语机器翻译数据集上也达到了新的水平。代码可在https://github.com/ChenRocks/Distill-BERT-Textgen.</pre></li>
<li><a href="https://arxiv.org/abs/2008.03822">Distilling the Knowledge of BERT for Sequence-to-Sequence ASR</a> (Interspeech2020)
<pre>基于注意的序列到序列（seq2seq）模型在自动语音识别（ASR）中取得了很好的效果。然而，当这些模型以从左到右的方式解码时，它们无法访问右侧的上下文。我们通过知识提炼，将BERT作为外部语言模型应用于seq2seq ASR，从而利用左右上下文。在我们提出的方法中，BERT生成软标签来指导seq2seq ASR的训练。此外，我们利用当前话语之外的语境作为输入。实验评估表明，我们的方法显著提高了自发日语语料库（CSJ）中seq2seq基线的ASR性能。从BERT中提取的知识要比从只关注左上下文的transformer LM中提取的知识好。我们还展示了在当前话语之外利用语境的有效性。我们的方法优于其他LM应用方法，如n-最佳重新扫描和浅层融合，但不需要额外的推理成本。</pre></li>
<li><a href="https://arxiv.org/abs/2010.13002">Pre-trained Summarization Distillation</a>
<pre>最近最先进的总结方法利用大型预训练变压器模型。将这些模型提炼成更小的学生模型对于实际应用至关重要；然而，NLP文献中提出了许多不同的蒸馏方法。最近关于分类和回归任务的提取伯特的工作显示了使用直接知识提取的强大性能。或者，机器翻译从业者使用伪标记提取，在伪标记中，小模型在大模型的翻译上进行训练。第三种更简单的方法是“收缩和微调”（SFT），它通过将参数复制到较小的学生模型中，然后进行微调，从而避免任何显式蒸馏。我们比较了Pegasus和BART的这三种提取方法、当前和以前的最新技术、预先训练的摘要模型，发现SFT在CNN/DailyMail数据集上优于知识提取和伪标记，但在更抽象的XSUM数据集上执行伪标记的能力不足。PyTorch代码和不同大小的检查点可通过拥抱面部变压器在这里获得http://tiny.cc/4iy0tz.</pre></li>
<li><a href="https://arxiv.org/abs/2007.11088">Understanding BERT Rankers Under Distillation</a> (ICTIR2020)
<pre>诸如在大型语料库上预训练的BERT等深层语言模型极大地提高了最先进的信息检索排名系统的性能。嵌入在这些模型中的知识使他们能够拾取段落和查询之间的复杂匹配信号。然而，推理过程中的高计算成本限制了它们在现实搜索场景中的部署。在本文中，我们研究了是否以及如何通过蒸馏将BERT中的搜索知识转移到较小的ranker。我们的实验表明，使用适当的蒸馏程序至关重要，该程序在保持最先进性能的同时，可产生高达九倍的加速。</pre></li>
<li><a href="https://arxiv.org/abs/2009.07531">Simplified TinyBERT: Knowledge Distillation for Document Retrieval</a>
<pre>尽管利用BERT模型进行文档排序是有效的，但这种方法的高计算成本限制了它们的使用。为此，本文首先实证研究了两种知识提取模型对文档排序任务的有效性。此外，在最近提出的TinyBERT模型的基础上，提出了两种简化方法。对两个不同且广泛使用的基准进行的评估表明，简化TinyBERT和建议的简化不仅提高了TinyBERT，而且在提供15$\倍的加速比时，显著优于BERT Base。</pre></li>
<li><a href="https://www.aclweb.org/anthology/2020.repl4nlp-1.10/">Exploring the Limits of Simple Learners in Knowledge Distillation for Document Classification with DocBERT</a> (ACL2020 WS)</li>
<li><a href="https://arxiv.org/abs/2002.12620">TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing</a> (ACL2020 Demo)
<pre>在本文中，我们介绍了TextBrewer，一个为自然语言处理而设计的开源知识提取工具包。它使用不同的神经网络模型，支持各种监督学习任务，如文本分类、阅读理解、序列标记。TextBrewer提供了一个简单而统一的工作流程，可以使用高度灵活的配置快速设置蒸馏实验。它提供了一组预定义的蒸馏方法，可以使用自定义代码进行扩展。作为一个案例研究，我们使用TextBrewer提取几个典型NLP任务的BERT。通过简单的配置，我们获得的结果与具有相似数量参数的公共模型相当，甚至更高。我们的工具包可通过以下途径获得：http://textbrewer.hfl-rc.com</pre></li>
<li><a href="https://arxiv.org/abs/2010.16407">TopicBERT for Energy Efficient Document Classification</a> (EMNLP2020 Findings)
<pre>先前的研究指出，BERT的计算成本随序列长度呈二次增长，从而导致更长的训练时间、更高的GPU内存限制和碳排放。虽然最近的工作试图在培训前解决这些可伸缩性问题，但这些问题在微调中也很突出，特别是对于文档分类等长序列任务。因此，我们的工作重点是优化文档分类微调的计算成本。我们通过在一个名为TopicBERT的统一框架中互补学习主题和语言模型来实现这一点。这大大减少了自我关注操作的数量——这是一个主要的性能瓶颈。因此，我们的模型实现了1.4倍（$\sim40\%$）的加速，减少了$CO_2$排放，同时在5个数据集上保持了$99.9\%$的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2012.06946">MiniVLM: A Smaller and Faster Vision-Language Model</a>
<pre>最近的视觉语言（VL）研究表明，通过使用转换器模型从海量图像-文本对中学习通用表示，然后对下游VL任务进行微调，取得了显著的进展。虽然现有的研究集中于通过大型预训练模型实现高精度，但构建轻量级模型在实践中具有很大的价值，但探索较少。在本文中，我们提出了一个更小、更快的VL模型MiniVLM，它可以在各种下游任务（如较大的对应任务）上进行微调，并具有良好的性能。MiniVLM由两个模块组成，一个是视觉特征提取模块，一个是基于转换器的视觉语言融合模块。我们设计了一个两阶段高效的特征提取程序（TEE），其灵感来自于一阶段高效的ET网络，与基线模型相比，显著降低了视觉特征提取的时间成本$95\%$。在比较了不同的紧凑型BERT模型后，采用MiniLM结构降低了变压器模块的计算量。此外，我们通过添加700万美元的开放图像数据来改进MiniVLM预训练，这些数据由最先进的字幕模型伪标记。我们还使用从强标记模型获得的高质量图像标记进行预训练，以增强跨模态对齐。大型模型离线使用，不会增加微调和推断的任何开销。通过上述设计选择，我们的MiniVLM将模型尺寸减少了$73\%$，推理时间成本减少了$94\%$，同时能够在多个VL任务上保持$94-97\%$的准确性。我们希望MiniVLM有助于将最先进的VL研究用于边缘应用。</pre></li>
<li><a href="https://arxiv.org/abs/2104.02096">Compressing Visual-linguistic Model via Knowledge Distillation</a>
<pre>尽管在视觉语言（VL）表征的预训练方面取得了令人兴奋的进展，但很少有人渴望使用小型VL模型。在本文中，我们研究了知识蒸馏（KD），以有效地将基于变压器的大型VL模型压缩为小型VL模型。主要的挑战来自于从教师和学生的不同检测器中提取的不一致的区域视觉标记，导致隐藏表征和注意力分布的错位。为了解决这个问题，我们使用来自学生检测器的相同区域建议对教师进行再培训和调整，而特征来自教师自己的对象检测器。通过对齐网络输入，适应的教师能够通过中间表示传递知识。具体地说，我们使用均方误差损失来模拟转换器块内的注意分布，并通过与存储在样本队列中的否定表示进行对比来呈现令牌噪声对比损失来对齐隐藏状态。为此，我们表明，我们提出的蒸馏显著提高了小型VL模型在图像字幕和视觉问答任务上的性能。它在苹果酒中的COCO字幕得分达到120.8分，比未蒸馏的同类产品提高了5.1分；VQA 2.0的准确度为69.8，比基线增加0.8。我们的大量实验和烧蚀证实了VL蒸馏在预训练和微调阶段的有效性。</pre></li>
<li><a href="https://arxiv.org/abs/2104.11832">Playing Lottery Tickets with Vision and Language</a>
<pre>基于变压器的大规模预培训最近彻底改变了视觉和语言（V+L）研究。LXMERT、ViLBERT和UNITER等模型极大地提升了各种V+L任务的最新水平。然而，这些模型中的大量参数阻碍了它们在实际中的应用。并行地，对彩票假设的研究表明，深度神经网络包含小匹配子网络，在隔离训练时，它可以实现比密集网络更高的性能。在这项工作中，我们进行了第一次实证研究，以评估这种可训练的子网络是否也存在于预先训练的V+L模型中。我们使用性能最好的V+L模型之一UNITER作为实验平台，并整合了7个具有代表性的V+L任务进行实验，包括视觉问答、视觉常识推理、视觉蕴涵、引用表达理解、图像文本检索、GQA和NLVR$^2$。通过综合分析，我们将主要发现总结如下。（$i$）很难找到与完整单元模型的性能完全匹配的子网络（即票证）。然而，令人鼓舞的是，我们可以找到50%-70%稀疏度的“轻松”中奖彩票，保持99%的准确率。通过特定任务修剪发现的（$ii$）子网络可以很好地转移到其他任务，而在训练前任务中发现的稀疏度为60%/70%的子网络可以普遍转移，在所有任务中平均匹配98%/96%的完全准确度。（$iii$）对抗性训练可进一步用于提高所发现彩票的性能。</pre></li>
<li><a href="https://arxiv.org/abs/1909.05840">Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT</a>
<pre>基于转换器的体系结构已经成为一系列自然语言处理任务的事实模型。特别是，基于伯特的模型在胶水任务、CoNLL-03和小队中取得了显著的准确性提高。然而，基于BERT的模型具有令人望而却步的内存占用和延迟。因此，在资源受限的环境中部署基于BERT的模型已成为一项具有挑战性的任务。在这项工作中，我们使用二阶Hessian信息对微调的BERT模型进行了广泛的分析，并利用我们的结果提出了一种将BERT模型量化到超低精度的新方法。特别是，我们提出了一种新的分组量化方案，并使用基于Hessian的混合精度方法进一步压缩模型。我们在SST-2、MNLI、CoNLL-03和STAND的BERT下游任务上广泛测试了我们提出的方法。我们可以实现与基线相当的性能，性能降低最多2.3\%$，即使是超低精度量化到2位，对应的模型参数压缩高达13美元倍，嵌入表压缩和激活高达4美元倍。在所有的任务中，我们观察到贝特的表现损失最高。通过探讨基于Hessian的分析和可视化，我们发现这与当前的训练/微调策略不适用于全队这一事实有关。</pre></li>
<li><a href="https://arxiv.org/abs/1910.06188">Q8BERT: Quantized 8Bit BERT</a> (NeurIPS2019 WS)
<pre>最近，基于变换器的预训练语言模型（如BERT和GPT）在许多自然语言处理（NLP）任务中都有了很大的改进。然而，这些模型包含大量的参数。GPT2和威震天等更大更精确的模型的出现，表明了大型预训练变压器模型的趋势。然而，在生产环境中使用这些大型模型是一项复杂的任务，需要大量的计算、内存和电源资源。在这项工作中，我们展示了如何在BERT的微调阶段执行量化感知训练，以便以最小的精度损失将BERT压缩$4\倍。此外，如果针对支持8位整数的硬件进行优化，生成的量化模型可以加快推理速度。</pre></li>
<li><a href="https://arxiv.org/abs/2004.07320">Training with Quantization Noise for Extreme Model Compression</a> (ICLR2021)
<pre>我们解决了生成紧凑模型的问题，最大限度地提高了给定模型大小的精度。标准解决方案是使用量化感知训练训练网络，其中权重在训练期间量化，梯度用直通估计器近似。在本文中，我们将此方法扩展到int8定点量化之外的工作，使用极端压缩方法，其中STE引入的近似是严重的，例如乘积量化。我们的建议是在每次前进过程中仅量化不同的随机权重子集，允许无偏梯度流过其他权重。控制噪声的数量及其形式可以在保持原始模型性能的同时实现极高的压缩率。因此，在自然语言处理和图像分类中，我们在准确性和模型大小之间建立了新的最先进的折衷方案。例如，将我们的方法应用于最先进的Transformer和ConvNet体系结构，通过将RoBERTa压缩到14MB，我们可以在MNLI上实现82.5%的精度，通过将EfficientNet-B3压缩到3.3MB，我们可以在ImageNet上实现80.0 top-1精度。</pre></li>
<li><a href="https://arxiv.org/abs/2103.02800">Hardware Acceleration of Fully Quantized BERT for Efficient Natural Language Processing</a>
<pre>BERT是最新的基于转换器的模型，它在各种NLP任务中实现了最先进的性能。在本文中，我们研究了用于边缘计算的FPGA上BERT的硬件加速。为了解决巨大的计算复杂性和内存占用问题，我们建议对BERT（FQ-BERT）进行完全量化，包括权重、激活、softmax、层规范化和所有中间结果。实验表明，FQ-BERT可以实现重量的7.94倍压缩，性能损失可以忽略不计。然后，我们提出了一种适合FQ-BERT的加速器，并在Xilinx ZCU102和ZCU111 FPGA上进行了评估。它可以实现每瓦3.18 fps/W的性能，分别是Intel（R）Core（TM）i7-8700 CPU和NVIDIA K80 GPU的28.91倍和12.72倍。</pre></li>
<li><a href="https://arxiv.org/abs/2012.15701">BinaryBERT: Pushing the Limit of BERT Quantization</a> (ACL2021)
<pre>大规模预训练语言模型的快速发展极大地增加了对模型压缩技术的需求，其中量化是一种流行的解决方案。在本文中，我们提出了二值化，它通过加权二值化将伯特量化推向极限。我们发现，由于二元BERT的复杂和不规则的损失情况，二元BERT比三元BERT更难直接训练。因此，我们提出了三元权重拆分，它通过从一个半尺寸的三元网络中等效拆分来初始化二进制伯特。因此，二元模型继承了三元模型的良好性能，并且可以在拆分后通过微调新的体系结构来进一步增强。实验结果表明，与全精度模型相比，我们的BinaryBERT的性能仅略有下降，但比全精度模型小24倍，在GLUE和SQuAD基准上实现了最先进的压缩结果。</pre></li>
<li><a href="https://arxiv.org/abs/2101.01321">I-BERT: Integer-only BERT Quantization</a>
<pre>基于Transformer的模型，如BERT和RoBERTa，在许多自然语言处理任务中取得了最先进的成果。然而，它们的内存占用、推理延迟和功耗在边缘甚至在数据中心都是令人望而却步的高效推理。虽然量化可能是一个可行的解决方案，但之前对基于变压器的模型进行量化的工作在推理过程中使用浮点算法，这无法有效地利用仅整数逻辑单元，如最近的图灵张量核或传统的仅整数ARM处理器。在这项工作中，我们提出了I-BERT，一种新的基于变压器的模型量化方案，该方案使用纯整数算法对整个推理进行量化。基于非线性运算的轻量级纯整数近似方法，例如GELU、Softmax和层规范化，I-BERT执行端到端纯整数的BERT推断，无需任何浮点计算。我们使用RoBERTa Base/Large来评估我们对粘合下游任务的方法。我们表明，在这两种情况下，与全精度基线相比，I-BERT获得了相似（略高）的精度。此外，与FP32推理相比，我们初步实现的I-BERT在T4 GPU系统上的INT8推理的加速比为2.4-4.0x。该框架是在PyTorch中开发的，并且是开源的。</pre></li>
<li><a href="https://arxiv.org/abs/2103.11367">ROSITA: Refined BERT cOmpreSsion with InTegrAted techniques</a> (AAAI2021)
<pre>BERT家族预先训练的语言模型定义了广泛NLP任务的艺术状态。然而，基于BERT的模型的性能主要取决于大量的参数，这阻碍了它们在资源有限的场景中的应用。面对这个问题，最近的研究一直试图将BERT压缩成一个小规模的模型。然而，大多数以前的工作主要集中在一种单一的压缩技术上，很少注意不同方法的组合。当使用集成技术对BERT进行压缩时，一个关键问题是如何设计整个压缩框架以获得最佳性能。针对这个问题，我们集成了三种压缩方法（权重剪枝、低秩因子分解和知识提取（KD）），并探索了一系列关于模型结构、KD策略、剪枝频率和学习速率调度的设计。我们发现，仔细选择设计对压缩模型的性能至关重要。基于实证结果，我们的最佳压缩模型，称为集成技术的改进BERT压缩（ROSITA），比BERT小7.5倍，同时在GLUE基准的五项任务上保持98.5%的性能，优于具有类似参数预算的先前BERT压缩方法。该守则可于https://github.com/llyx97/Rosita.</pre></li>
<li><a href="https://arxiv.org/abs/2009.12812">TernaryBERT: Distillation-aware Ultra-low Bit BERT</a> (EMNLP2020)
<pre>基于转换器的预训练模型（如BERT）在许多自然语言处理任务中取得了显著的性能。然而，这些模型的计算和内存都很昂贵，阻碍了它们部署到资源受限的设备上。在这项工作中，我们提出了三元BERT，它在一个微调的BERT模型中对权重进行三元化。具体而言，我们使用基于近似和损失感知的三元化方法，并实证研究了BERT不同部分的三元化粒度。此外，为了减少低比特容量导致的精度下降，我们在训练过程中利用了知识提取技术。在GLUE基准和SQuAD上的实验表明，我们提出的TernaryBERT量化方法优于其他BERT量化方法，甚至达到了与全精度模型相当的性能，同时比全精度模型小14.9倍。</pre></li>
<li><a href="https://arxiv.org/abs/2011.14203">EdgeBERT: Optimizing On-Chip Inference for Multi-Task NLP</a>
<pre>基于转换器的语言模型（如BERT）为许多自然语言处理（NLP）任务提供了显著的准确性改进。然而，它们巨大的计算和内存需求使得它们难以部署到具有严格延迟要求的资源受限边缘平台。我们介绍了EdgeBERT，一种用于多任务NLP延迟感知能量优化的深入算法硬件协同设计。EdgeBERT采用基于熵的早期退出预测，以便在句子粒度上执行动态电压频率缩放（DVFS），以便在遵守规定的目标延迟的同时将能耗降至最低。通过采用自适应注意广度、选择性网络修剪和浮点量化的校准组合，计算和内存占用开销得到进一步缓解。此外，为了最大限度地发挥这些算法在常开和中间边缘计算环境中的协同效益，我们专门设计了一个12nm可扩展硬件加速器系统，集成了快速开关低压差稳压器（LDO）、全数字锁相环（ADPLL）以及，高密度嵌入式非易失性存储器（ENVM），其中小心地存储共享多任务参数的稀疏浮点位编码。总之，与Nvidia Jetson Tegra X2移动GPU上的传统无提前停止推理、无延迟无限制提前退出方法和CUDA自适应相比，EdgeBERT硬件系统上的延迟感知多任务NLP推理加速产生的能量分别降低了7倍、2.5倍和53倍。</pre></li>
<li><a href="https://arxiv.org/abs/2102.06621">Optimizing Inference Performance of Transformers on CPUs</a>
<pre>Transformer架构彻底改变了自然语言处理（NLP）领域。基于Transformers的模型（例如，BERT）为许多重要的Web服务提供了动力，如搜索、翻译、问答等。虽然大量研究关注这些模型的训练，但在提高其推理性能方面所做的努力相对较少。本文通过对基于CPU的变压器模型的可扩展性和性能的实证分析来解决这一差距。重点关注广受欢迎的BERT模型，我们确定了变压器体系结构中的关键组件，在这些组件中进行了大量计算，并提出了三种优化方法来加快计算速度。使用HuggingFace的推理基准对优化进行评估，结果表明，优化的加速比高达x2.37。所考虑的优化不需要对模型的实现进行任何更改，也不会影响其准确性。</pre></li>
</ul>
<h1 id="misc.">Misc.</h1>
<ul>
<li><a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf">Language Models are Unsupervised Multitask Learners</a> [<a href="https://github.com/openai/gpt-2">github</a>]</li>
<li><a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> (NeurIPS2020) [<a href="https://github.com/openai/gpt-3">github</a>]
<pre>最近的工作表明，通过对大量文本进行预训练，然后对特定任务进行微调，在许多NLP任务和基准测试方面取得了实质性进展。虽然这种方法在体系结构中通常是任务不可知的，但仍然需要数千或上万个示例的特定于任务的微调数据集。相比之下，人类通常只能通过几个例子或简单的指令来执行一项新的语言任务，而目前的NLP系统仍然很难做到这一点。在这里，我们展示了扩展语言模型极大地提高了任务无关性、少镜头的性能，有时甚至可以与先前最先进的微调方法相媲美。具体地说，我们训练了GPT-3，一个具有1750亿个参数的自回归语言模型，比以前任何非稀疏语言模型都多10倍，并在少数镜头设置下测试了它的性能。对于所有任务，GPT-3应用时没有任何梯度更新或微调，任务和少量镜头演示仅通过与模型的文本交互指定。GPT-3在许多NLP数据集上实现了强大的性能，包括翻译、问答和完形填空任务，以及一些需要动态推理或领域适应的任务，如解读单词、在句子中使用新单词或执行3位数算术。同时，我们还确定了一些数据集，其中GPT-3的少量镜头学习仍然困难，以及一些GPT-3面临与大型网络语料库培训相关的方法学问题的数据集。最后，我们发现GPT-3可以生成新闻文章的样本，而人类评价者很难将其与人类撰写的文章区分开来。我们讨论了这一发现和GPT-3的广泛社会影响。</pre></li>
<li><a href="https://arxiv.org/abs/2008.06239">Language Models as Few-Shot Learner for Task-Oriented Dialogue Systems</a>
<pre>面向任务的对话系统使用四个相互连接的模块，即自然语言理解（NLU）、对话状态跟踪（DST）、对话策略（DP）和自然语言生成（NLG）。考虑到数据收集的高成本，研究挑战是以最少的样本量（即，几次拍摄）学习每个模块。解决这一问题的最常见和有效的技术是迁移学习，在迁移学习中，对文本或任务特定数据进行预训练的大型语言模型在少数样本上进行微调。这些方法需要微调步骤和每个任务的一组参数。不同的是，语言模型，如GPT-2（Radford et al.，2019）和GPT-3（Brown et al.，2020），通过用很少的例子启动模型，允许很少的镜头学习。在本文中，我们评估了NLU、DST、DP和NLG任务中语言模型的启动少数镜头能力。重要的是，我们强调了这种方法目前的局限性，并讨论了对未来工作的可能影响。</pre></li>
<li><a href="https://arxiv.org/abs/2012.07805">Extracting Training Data from Large Language Models</a>
<pre>发布在私有数据集上经过训练的大型（十亿个参数）语言模型已经变得很普遍。本文演示了在这种情况下，对手可以通过查询语言模型执行训练数据提取攻击来恢复单个训练示例。我们演示了我们对GPT-2的攻击，GPT-2是一种在公共互联网上训练的语言模型，能够从模型的训练数据中提取数百个逐字文本序列。这些提取的示例包括（公共）个人身份信息（姓名、电话号码和电子邮件地址）、IRC对话、代码和128位UUID。我们的攻击是可能的，即使上面的每个序列只包含在训练数据中的一个文档中。我们全面评估我们的提取攻击，以了解导致其成功的因素。令人担忧的是，我们发现大型模型比小型模型更容易受到攻击。最后，我们总结经验教训，并讨论培训大型语言模型的可能保障措施。</pre></li>
<li><a href="https://arxiv.org/abs/2009.03393">Generative Language Modeling for Automated Theorem Proving</a>
<pre>我们探讨了基于转换器的语言模型在自动定理证明中的应用。这项工作的动机是，与人类相比，自动定理证明器的一个主要限制——原始数学术语的生成——可能通过从语言模型生成来解决。我们为Metamath形式化语言提供了一个自动验证和证明助手GPT-f，并对其性能进行了分析。据我们所知，GPT-f发现了新的简短证明，这些证明被纳入了主要的元数学库，这是基于深度学习的系统首次提供了被正式数学社区采用的证明。</pre></li>
<li><a href="https://www.aclweb.org/anthology/2020.acl-main.314/">Do you have the right scissors? Tailoring Pre-trained Language Models via Monte-Carlo Methods</a> (ACL2020)</li>
<li><a href="https://arxiv.org/abs/2003.02249">jiant: A Software Toolkit for Research on General-Purpose Text Understanding Models</a> [<a href="https://github.com/nyu-mll/jiant/">github</a>]
<pre>我们介绍了一个开源工具包，用于对英语NLU任务进行多任务和迁移学习实验。jiant支持使用最先进的模型进行模块化和配置驱动的实验，并实现了一系列用于探索、迁移学习和多任务训练实验的任务。jiant实现了50多个NLU任务，包括所有GLUE和SuperGLUE基准任务。我们证明了jiant在各种任务和模型上重现了已发布的性能，包括BERT和RoBERTa。jiant可从以下网址获得：https://jiant.info.</pre></li>
<li><a href="https://arxiv.org/abs/1903.07785">Cloze-driven Pretraining of Self-attention Networks</a>
<pre>我们提出了一种新的预训练双向转换器模型的方法，该模型在各种语言理解问题中提供了显著的性能增益。我们的模型解决了完形填空式的单词重建任务，其中每个单词都被删除，并且必须根据文本的其余部分进行预测。实验表明，GLUE和NER以及选区分析基准测试的最新结果在性能上有很大提高，这与同时引入的BERT模型一致。我们还详细分析了一些有助于有效预训练的因素，包括数据域和大小、模型容量以及完形填空目标的变化。</pre></li>
<li><a href="https://arxiv.org/abs/1901.11373">Learning and Evaluating General Linguistic Intelligence</a>
<pre>我们将一般语言智能定义为重用先前获得的关于语言词汇、语法、语义和语用惯例的知识以快速适应新任务的能力。利用这一定义，我们分析了最先进的自然语言理解模型，并进行了广泛的实证调查，通过一系列实验评估学习过程中所获得知识的任务独立性，以对照这些标准对其进行评估。除了任务性能之外，我们还提出了一个新的基于测试数据在线编码的评估指标，该指标量化了现有代理（模型）学习新任务的速度。我们的研究结果表明，尽管该领域在推广到许多任务的模型体系结构方面取得了令人印象深刻的进展，但这些模型仍然需要大量的领域内培训示例（例如，微调、培训特定于任务的模块），并且容易发生灾难性遗忘。此外，我们发现，我们的模型远远不能解决一般任务（例如，文档问答），而是过分适合特定数据集（例如，团队）的怪癖。我们讨论了缺失的部分和关于如何向一般语言智能发展的猜测。</pre></li>
<li><a href="https://arxiv.org/abs/1903.05987">To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks</a> (ACL2019 WS)
<pre>虽然以前的大多数工作都集中在不同的预训练目标和迁移学习体系结构上，但我们问如何使预训练模型最好地适应给定的目标任务。我们关注两种最常见的自适应形式：特征提取（预训练权重被冻结）和直接微调预训练模型。我们使用两种最先进的模型对不同NLP任务的实证结果表明，微调与特征提取的相对性能取决于训练前任务和目标任务的相似性。我们探索这一发现的可能解释，并为NLP从业者提供一套适应指南。</pre></li>
<li><a href="https://www.aclweb.org/anthology/D19-1062/">Learning to Speak and Act in a Fantasy Text Adventure Game</a> (EMNLP2019)</li>
<li><a href="https://arxiv.org/abs/2005.01063">A Two-Stage Masked LM Method for Term Set Expansion</a> (ACL2020)
<pre>我们处理术语集扩展（TSE）的任务：给定一个语义类中的示例术语的小种子集，查找该类中的更多成员。这项任务具有极大的实际效用，同时也具有理论效用，因为它需要从几个例子中进行概括。以前的TSE任务方法可以描述为分布式或基于模式。我们利用神经掩蔽语言模型（MLM）的强大功能，提出了一种新的TSE算法，该算法结合了基于模式的方法和分布式方法。由于种子集的规模较小，微调方法并不有效，因此需要更具创造性地使用传销。这个想法的要点是使用传销模式首先挖掘关于种子集的信息模式，然后通过推广这些模式来获得种子类的更多成员。我们的方法优于最先进的TSE算法。可在以下网址获得实施：https://github.com/ guykush/TermSetExpansion MPB/</pre></li>
<li><a href="https://arxiv.org/abs/2010.09535">Cold-start Active Learning through Self-supervised Language Modeling</a> (EMNLP2020)
<pre>主动学习通过选择最关键的示例进行标注，努力降低注释成本。通常，主动学习策略取决于分类模型。例如，不确定性抽样取决于校准不良的模型置信分数。在冷启动环境中，由于模型不稳定和数据匮乏，主动学习是不切实际的。幸运的是，现代NLP提供了额外的信息来源：预先训练的语言模型。训练前的损失可以找到让模型吃惊的例子，应该标记为有效的微调。因此，我们将语言建模损失视为分类不确定性的代理。利用BERT，我们开发了一种基于蒙蔽语言建模损失的简单策略，该策略可以最小化文本分类的标记成本。与其他基线相比，我们的方法在更少的采样迭代次数和计算时间内达到更高的精度。</pre></li>
<li><a href="https://arxiv.org/abs/1812.06705">Conditional BERT Contextual Augmentation</a>
<pre>我们提出了一种新的标记句数据增强方法，称为条件伯特上下文增强。数据扩充方法通常用于防止过度拟合和提高深层神经网络模型的泛化能力。最近提出的上下文增广通过随机替换语言模型预测的更多样的替换词来增广标记句。BERT证明了深层双向语言模型比单向语言模型或前向和后向模型的浅层连接更强大。通过引入一个新的条件掩蔽语言模型，我们将BERT改进为条件BERT。脚注{原BERT论文中出现过一次术语“条件掩蔽语言模型”，表明上下文条件相当于术语“掩蔽语言模型”。在我们的论文中，“条件掩蔽语言模型”指示我们对“蒙版语言模型”}任务应用额外的标签条件约束。经过良好训练的条件BERT可以用于增强上下文增强。在六种不同的文本分类任务上的实验表明，我们的方法可以很容易地应用于卷积或递归神经网络分类器，从而获得明显的改进。</pre></li>
<li><a href="https://arxiv.org/abs/2003.02245">Data Augmentation using Pre-trained Transformer Models</a> (AACL-IJCNLP2020) [<a href="https://github.com/varinf/TransformersDataAugmentation">github</a>]
<pre>基于语言模型的预训练模型（如BERT）在不同的NLP任务中提供了显著的收益。在本文中，我们研究了不同类型的基于变压器的预训练模型，如用于条件数据扩充的自回归模型（GPT-2）、自动编码器模型（BERT）和seq2seq模型（BART）。我们表明，将类标签预先添加到文本序列中提供了一种简单而有效的方法来调整预先训练的模型以进行数据扩充。此外，在三个分类基准上，预先训练的Seq2Seq模型在低资源环境下优于其他数据增强方法。此外，我们还探讨了不同的基于预训练模型的数据扩充在数据多样性方面的差异，以及这些方法如何很好地保留类标签信息。</pre></li>
<li><a href="https://arxiv.org/abs/2010.02394">Mixup-Transfomer: Dynamic Data Augmentation for NLP Tasks</a> (COLING2020)
<pre>Mixup是最新的数据增强技术，它线性插值输入示例和相应的标签。通过在像素级插值图像，它在图像分类中显示了强大的有效性。受这一研究方向的启发，在本文中，我们将探讨i）如何将混合应用于自然语言处理任务，因为文本数据很难以原始格式混合；ii）如果混音在基于变压器的学习模型中仍然有效，例如，BERT。为了实现这一目标，我们将基于mixup-To-transformer的预训练体系结构（名为“mixup-transformer”）整合到一个广泛的NLP任务中，同时保持整个端到端训练系统。我们通过在GLUE基准上运行大量实验来评估所提出的框架。此外，我们还通过以一定比例减少训练数据来检验混音转换器在低资源场景中的性能。我们的研究表明，对于预先训练的语言模型，混搭是一种与领域无关的数据增强技术，从而显著提高了基于transformer的模型的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2104.08826">GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation</a>
<pre>GPT-3等大规模语言模型是优秀的少数镜头学习者，可以通过自然文本提示进行控制。最近的研究报告称，基于提示的直接分类消除了微调的需要，但缺乏数据和推理的可伸缩性。本文提出了一种新的数据增强技术，该技术利用大规模语言模型从真实样本的混合中生成真实的文本样本。我们还建议利用语言模型预测的软标签，有效地从大规模语言模型中提取知识，同时创建文本扰动。我们在不同的分类任务上进行了数据增强实验，结果表明我们的方法比现有的文本增强方法有很大的优越性。消融研究和定性分析为我们的方法提供了更多的见解。</pre></li>
<li><a href="https://arxiv.org/abs/2010.01054">Unsupervised Text Style Transfer with Padded Masked Language Models</a> (EMNLP2020)
<pre>我们提出了Masker，一种用于风格转换的无监督文本编辑方法。为了解决没有并行源目标对的情况，我们训练源域和目标域的屏蔽语言模型（MLM）。然后我们发现两个模型在可能性方面分歧最大的文本范围。这允许我们识别要删除的源标记，以转换源文本以匹配目标域的样式。删除的令牌被目标传销所取代，通过使用填充的传销变体，我们避免了必须预先确定插入令牌的数量。我们对句子融合和情感转移的实验表明，掩蔽者在完全无监督的环境中表现出竞争性。此外，在低资源环境下，在蒙版机生成的银色训练数据上对监督方法进行预训练时，可将其准确性提高10个百分点以上。</pre></li>
<li><a href="https://arxiv.org/abs/2004.12506">Assessing Discourse Relations in Language Generation from Pre-trained Language Models</a>
<pre>NLP的最新进展归因于大规模预训练语言模型的出现。特别是GPT-2，由于其从左到右的语言建模目标，它适合于生成任务，但其生成的文本的语言质量在很大程度上尚未探索。我们的工作在理解GPT-2在语篇连贯方面的输出方面迈出了一步。我们对GPT-2输出中的显性话语关系在有机生成和微调情景下的有效性进行了全面研究。结果表明，GPT-2并不总是生成包含有效语篇关系的文本；然而，它的文本更符合人类对微调场景的期望。我们提出了一种解耦策略来缓解这些问题，并强调了显式建模话语信息的重要性。</pre></li>
<li><a href="https://arxiv.org/abs/1904.00962">Large Batch Optimization for Deep Learning: Training BERT in 76 minutes</a> (ICLR2020)
<pre>在海量数据集上训练大型深层神经网络在计算上非常具有挑战性。最近，使用大批量随机优化方法来解决这个问题的兴趣激增。这一研究领域最突出的算法是LARS，它通过采用分层自适应学习速率，在几分钟内在ImageNet上训练ResNet。然而，对于像伯特这样的注意力模型，LAR的表现很差，这表明它在不同任务中的表现并不一致。在本文中，我们首先研究了一种原则性的分层自适应策略，以加速使用大批量和小批量的深层神经网络的训练。利用该策略，我们开发了一种新的分层自适应大批量优化技术LAMB；然后，我们提供了LAMB和LARS的收敛性分析，显示了在一般非凸设置下收敛到一个稳定点。我们的实证结果表明，LAMB在各种任务（如BERT和ResNet-50训练）中表现优异，超参数调整很少。特别是对于BERT培训，我们的优化器支持使用非常大的批量32868，而不会降低性能。通过将批量大小增加到TPUv3 Pod的内存限制，BERT训练时间可以从3天减少到76分钟（表1）。LAMB实现可在https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py</pre></li>
<li><a href="https://arxiv.org/abs/2006.13484">Accelerated Large Batch Optimization of BERT Pretraining in 54 minutes</a>
<pre>最近，BERT在自然语言理解（NLU）领域引起了广泛关注，并在各种NLU任务中取得了最新成果。然而，它的成功需要大量的深层神经网络和大量的数据，这导致训练时间长，阻碍了开发进程。利用随机梯度方法进行小批量训练是减少训练时间的有效手段。在这一研究路线上，LAMB是一个突出的例子，它将伯特在TPUv3吊舱上的训练时间从3天减少到76分钟。在本文中，我们提出了一种称为LAN的加速梯度方法，以提高使用大批量小批量进行训练的效率。由于理论上学习率的上界是函数的Lipschitz常数的倒数，因此不能总是通过选择更大的学习率来减少优化迭代次数。为了在不损失精度的情况下使用更大的小批量，我们开发了一种新的学习率调度器，克服了使用大学习率的困难。使用所提出的局域网方法和学习速率方案，我们在BERT预训练的第一阶段和第二阶段分别将小批量规模扩大到96K和33K。在192个AWS EC2 P3dn.24xlarge实例上需要54分钟，才能在1.1版班次上达到90.5或更高的F1目标分数，实现云中最快的BERT训练时间。</pre></li>
<li><a href="https://arxiv.org/abs/2005.02178">IsoBN: Fine-Tuning BERT with Isotropic Batch Normalization</a> (AAAI2021)
<pre>微调预训练语言模型（PTLM），如伯特及其更好的变体罗伯塔，已成为提高自然语言理解（NLU）任务性能的常见做法。表征学习的最新进展表明，各向同性（即单位方差和不相关）嵌入可以显著提高下游任务的性能，具有更快的收敛速度和更好的泛化能力。然而，PTLMs中预训练嵌入物的各向同性研究相对较少。在本文中，我们用直观的可视化方法分析了预训练的PTLM[CLS]嵌入的各向同性，并指出了两个主要问题：它们的标准偏差的高方差和不同维度之间的高相关性。我们还提出了一种新的网络正则化方法，各向同性批量归一化（IsoBN）来解决这个问题，通过动态惩罚主成分，在微调中学习更多各向同性表示。这种简单而有效的微调方法在七个NLU任务的平均值上产生约1.0的绝对增量。</pre></li>
<li><a href="https://arxiv.org/abs/2008.00177">Multi-node Bert-pretraining: Cost-efficient Approach</a>
<pre>最近，基于转换器的大规模语言模型（如BERT、GPT-2和XLNet）为许多自然语言处理（NLP）任务带来了令人振奋的最新成果。这些最新模型的一个共同趋势是模型复杂性的显著增加，这将引入更多的权重和计算。此外，随着大规模无监督数据集的出现，由于单个训练周期内数据样本量的增加，训练时间进一步延长。因此，为了在合理的时间内训练这些模型，机器学习（ML）程序员通常需要高级硬件设置，如支持高级GPU的NVIDIA DGX工作站或专用加速器，如谷歌的TPU吊舱。我们的工作解决了这一局限性，并证明通过仔细的算法和软件优化，可以在2周内在学术规模的广泛可用GPU集群上对伯特预训练模型进行训练。在本文中，我们针对如何提高单个设备的培训吞吐量、将培训工作量分配到多个节点和GPU以及克服网络上的大型数据交换带来的通信瓶颈提出了这些优化。我们表明，我们能够在合理的时间预算（12天）内，在学术环境中对BERT进行预培训，但硬件资源需求比以前基于NVIDIA DGX机器或谷歌TPU吊舱的工业环境要便宜得多，也不那么苛刻。</pre></li>
<li><a href="https://arxiv.org/abs/2104.07705">How to Train BERT with an Academic Budget</a>
<pre>虽然大型语言模型在NLP中被广泛使用，但对它们进行预培训被认为是一种奢侈品，只有少数资金雄厚的行业实验室才能负担得起。如何以更为适度的预算来培训这样的模型？我们提出了一种使用单一低端深度学习服务器在24小时内预训练蒙面语言模型的方法。我们证明，通过软件优化、设计选择和超参数调整的组合，可以以原始预培训成本的一小部分生产出与BERT竞争的基于粘合任务的模型。</pre></li>
<li><a href="https://arxiv.org/abs/1909.08053">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</a>
<pre>最近在语言建模方面的工作表明，训练大型转换器模型提高了自然语言处理应用程序的技术水平。然而，由于内存限制，非常大的模型可能很难训练。在这项工作中，我们介绍了我们用于训练大型变压器模型的技术，并实现了一种简单、高效的层内模型并行方法，该方法能够训练具有数十亿参数的变压器模型。我们的方法不需要新的编译器或库更改，与管道模型并行性是正交的和互补的，并且可以通过在本机PyTorch中插入一些通信操作来完全实现。我们通过使用512 GPU将基于变压器的模型聚合到83亿个参数来说明这种方法。我们在整个应用程序中维持15.1万亿次，扩展效率为76%，而强大的单一GPU基线维持39万亿次，是峰值浮点的30%。为了证明大型语言模型可以进一步提升技术水平（SOTA），我们训练了一个类似于GPT-2的83亿参数转换器语言模型和一个类似于BERT的39亿参数模型。我们表明，随着模型尺寸的增长，仔细关注类BERT模型中层规范化的放置对于提高性能至关重要。使用GPT-2模型，我们在WikiText103（10.8，SOTA复杂度为15.8）和LAMBADA（66.5%，SOTA准确度为63.2%）数据集上获得了SOTA结果。我们的BERT模型在RACE数据集上实现了SOTA结果（90.9%，而SOTA准确率为89.4%）。</pre></li>
<li><a href="https://arxiv.org/abs/2201.11990">Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model</a> [<a href="https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/">blog</a>]
<pre>经过预训练的通用语言模型可以通过零触发、少触发和微调技术来适应下游任务，从而在各种自然语言处理领域实现最先进的精度。由于它们的成功，这些模型的规模迅速增加，需要高性能的硬件、软件和算法技术来训练如此大的模型。作为微软和NVIDIA共同努力的结果，我们介绍了最大的基于单片变压器的语言模型威震天图灵NLG 530B（MT-NLG）的培训细节，该模型具有5300亿个参数。在本文中，我们首先关注使用DeepSpeed和威震天来训练该模型的基础设施以及3D并行方法。接下来，我们详细介绍了训练过程、训练语料库的设计和数据整理技术，我们认为这是该模型成功的关键因素。最后，我们讨论了各种评估结果，以及MT-NLG显示的其他有趣的观察结果和新特性。我们证明了MT-NLG在几个NLP基准上实现了优异的零、一和少镜头学习精度，并建立了新的最先进的结果。我们相信，我们的贡献将有助于进一步发展大规模培训基础设施、大规模语言模型和自然语言世代。</pre></li>
<li><a href="https://arxiv.org/abs/2112.11446">Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher</a>
<pre>语言建模通过利用大量人类书面知识库来更好地预测和理解世界，从而向智能通信系统迈出了一步。在本文中，我们分析了基于Transformer的语言模型在各种模型尺度上的性能——从具有数千万个参数的模型到被称为Gopher的2800亿个参数的模型。这些模型在152项不同任务中进行评估，在大多数任务中实现了最先进的性能。在阅读理解、事实核查和有毒语言识别等领域，从量表中获得的收益最大，但逻辑和数学推理的收益较小。我们提供了对训练数据集和模型行为的整体分析，涵盖了模型规模与偏差和毒性的交叉点。最后，我们讨论了语言模型在人工智能安全和减少下游危害方面的应用。</pre></li>
<li><a href="https://arxiv.org/abs/2101.03961">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</a>
<pre>在深度学习中，模型通常会对所有输入重复使用相同的参数。混合专家（MoE）对此不屑一顾，而是为每个传入的示例选择不同的参数。结果是一个稀疏激活的模型——参数数量惊人——但计算成本不变。然而，尽管MoE取得了一些显著的成功，但由于复杂性、通信成本和培训不稳定性，广泛采用受到了阻碍——我们用开关变压器解决了这些问题。我们简化了MoE路由算法，设计了直观的改进模型，降低了通信和计算成本。我们提出的训练技术有助于解决不稳定性问题，并且我们展示了大型稀疏模型可能首次以较低精度（bfloat16）格式进行训练。我们基于T5 Base和T5 Large设计模型，在相同计算资源的情况下，训练前速度提高7倍。这些改进扩展到多语言设置中，我们衡量了所有101种语言相对于mT5基本版本的收益。最后，我们通过在“庞大的干净爬网语料库”上预训练多达万亿个参数模型，提升了当前语言模型的规模，并实现了比T5-XXL模型4倍的加速。</pre></li>
<li><a href="https://arxiv.org/abs/2112.06905">GLaM: Efficient Scaling of Language Models with Mixture-of-Experts</a> [<a href="https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html">blog</a>]
<pre>用更多的数据、计算和参数扩展语言模型，推动了自然语言处理的重大进展。例如，由于规模化，GPT-3能够在情境学习任务中取得良好的效果。然而，训练这些大型密集模型需要大量的计算资源。在本文中，我们提出并开发了一系列名为GLaM（通才语言模型）的语言模型，它使用稀疏激活的混合专家体系结构来扩展模型容量，同时与密集变体相比，所需的培训成本也大大降低。最大的GLaM有1.2万亿个参数，大约是GPT-3的7倍。它只消耗训练GPT-3所用能量的1/3，推理需要一半的计算次数，同时仍然在29个NLP任务中实现更好的整体零触发和一触发性能。</pre></li>
<li><a href="https://arxiv.org/abs/2204.02311">PaLM: Scaling Language Modeling with Pathways</a> [<a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html">blog</a>]
<pre>大型语言模型已被证明可以通过少量的快照学习在各种自然语言任务中取得显著的性能，这大大减少了使模型适应特定应用程序所需的特定于任务的训练示例的数量。为了进一步了解规模对少数镜头学习的影响，我们训练了一个5400亿参数、密集激活的变形金刚语言模型，我们称之为路径语言模型PaLM。我们使用Pathways在6144 TPU v4芯片上训练PaLM，Pathways是一种新的ML系统，能够跨多个TPU吊舱进行高效训练。我们通过在数百种语言理解和生成基准上实现最先进的少镜头学习结果，展示了扩展的持续优势。在许多这些任务上，PaLM 540B实现了突破性的性能，在一系列多步骤推理任务上超过了经过微调的最先进水平，在最近发布的大基准上也超过了人类的平均性能。大量大型试验台任务显示出模型规模的不连续改进，这意味着当我们扩展到最大的模型时，性能急剧提高。PaLM在多语言任务和源代码生成方面也具有强大的能力，我们在大量基准测试中对此进行了演示。此外，我们还对偏倚和毒性进行了全面分析，并研究了训练数据记忆在模型尺度上的程度。最后，我们讨论了与大型语言模型相关的伦理考虑，并讨论了潜在的缓解策略。</pre></li>
<li><a href="https://arxiv.org/abs/2111.05972">Amazon SageMaker Model Parallelism: A General and Flexible Framework for Large Model Training</a>
<pre>随着深度学习模型规模的快速增长，需要用于大型模型培训的系统级解决方案。我们介绍了Amazon SageMaker model parallelism，这是一个与Pyrotch集成的软件库，可以使用model parallelism和其他节省内存的功能轻松训练大型模型。与现有解决方案相比，SageMaker库的实现更具通用性和灵活性，因为它可以在任意模型体系结构上自动划分和运行管道并行，而代码更改最少，并且还为tensor并行提供了通用和可扩展的框架，它支持更广泛的用例，模块化程度足以轻松应用于新的培训脚本。该库还在更大程度上保留了原生PyTorch用户体验，支持模块重用和动态图形，同时让用户完全控制培训步骤的细节。我们评估了GPT-3、RoBERTa、BERT和神经协同过滤的性能，并展示了与现有解决方案相比的竞争力。</pre></li>
<li><a href="https://arxiv.org/abs/1910.02054">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a>
<pre>大型深度学习模型可以显著提高精度，但训练数十亿到数万亿的参数是一项挑战。现有的解决方案，如数据和模型并行，在将这些模型放入有限的设备内存中，同时获得计算、通信和开发效率方面存在根本性的限制。我们开发了一种新的解决方案，零冗余优化器（Zero），用于优化内存，极大地提高了训练速度，同时增加了可以有效训练的模型大小。ZeRO消除了数据和模型并行训练中的内存冗余，同时保持了较低的通信量和较高的计算粒度，使我们能够以持续的高效率按设备数量比例调整模型大小。我们对内存需求和通信量的分析表明：使用当今的硬件，ZeRO有可能扩展超过1万亿个参数。我们实现并评估ZeRO：它在400 GPU上以超线性加速比训练超过100B参数的大型模型，实现15 PB的吞吐量。这意味着与最先进的技术相比，模型尺寸增加了8倍，可实现的性能提高了10倍。就可用性而言，ZeRO可以训练高达13B参数的大型模型（例如，大于威震天GPT 8.3B和T5 11B），而无需模型并行性，这对科学家来说是很难应用的。最后但并非最不重要的一点是，研究人员利用零的系统突破创造了世界上最大的语言模型（图灵NLG，17B参数），其精确度打破了记录。</pre></li>
<li><a href="https://arxiv.org/abs/2104.07857">ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning</a>
<pre>在过去三年中，最大的密集深度学习模型已经增长了1000倍，达到数千亿个参数，而GPU内存只增长了5倍（16 GB到80 GB）。因此，主要通过系统创新来支持模型规模的增长，系统创新使大型模型能够装入多个GPU的聚合GPU内存。然而，我们正在接近GPU内存墙。它需要800个NVIDIA V100 GPU才能适应一个万亿参数的模型进行训练，而这样的集群对于大多数数据科学家来说简直遥不可及。此外，这种规模的模型训练需要复杂的并行技术组合，这给数据科学家重构模型带来了巨大负担。在本文中，我们介绍了ZeRO Infinity，这是一种新的异构系统技术，它利用GPU、CPU和NVMe内存在有限的资源上实现前所未有的模型规模，而无需重构模型代码。同时，它实现了出色的训练吞吐量和可扩展性，不受有限的CPU或NVMe带宽的限制。ZeRO Infinity可以适合具有数十甚至数百万亿参数的模型，用于在当前一代GPU集群上进行训练。它可以用于微调单个NVIDIA DGX-2节点上的万亿参数模型，使大型模型更容易访问。在训练吞吐量和可扩展性方面，它在512个NVIDIA V100 GPU（峰值的40%）上支持超过25 PB的速度，同时还展示了超线性可扩展性。ZeRO Infinity的开源实现可通过DeepSpeed获得，这是一个深度学习优化库，使分布式培训变得简单、高效和有效。</pre></li>
<li><a href="https://arxiv.org/abs/2108.05818">PatrickStar: Parallel Training of Pre-trained Models via Chunk-based Memory Management</a> [<a href="https://github.com/Tencent/PatrickStar">github</a>]
<pre>预训练模型（PTM）正在革新人工智能（AI）技术。它可以在海量数据上学习通用语言特性，然后在特定于任务的数据上进行微调。不幸的是，PTM培训的计算硬件要求非常昂贵，这使得它成为AI社区中一小部分人的游戏。因此，我们提出了一个名为PatrickStar的系统，以降低PTM的硬件要求，并使每个人都可以访问它们。PatrickStar使用CPU-GPU异构内存空间存储模型数据。与现有的工作不同，我们首先以细粒度的方式管理模型数据，将它们组织在内存块中，并在异构内存空间中动态分布。在预热迭代中收集的运行时内存统计数据的指导下，块在异构内存中得到了有效的编排，并生成了较低的CPU-GPU数据传输量。PatrickStar与零冗余优化器共生，使用数据并行性扩展到多个GPU，具有更低的通信带宽要求和更高效的带宽利用率。该系统可以在现有工作无法完成的更大模型和更大批量上训练任务。实验结果表明，PatrickStar在8xV100和240GB CPU内存节点上训练了一个120亿参数的GPT模型，该模型是SOTA工作的模型规模限制的1.5倍，并实现了比SOTA更高的计算效率。即使在一台700美元的个人电脑上，它也能训练出7亿个参数的GPT模型。我们的代码是公开的。</pre></li>
<li><a href="https://arxiv.org/abs/2102.02888">1-bit Adam: Communication Efficient Large-Scale Training with Adam’s Convergence Speed</a>
<pre>大型模型（如BERT和GPT-3）的可扩展培训需要基于模型设计、体系结构和系统功能的仔细优化。从系统的角度来看，通信已经成为一个主要的瓶颈，特别是在使用标准TCP互连的商品系统上，这种互连提供有限的网络带宽。通信压缩是减少此类系统训练时间的重要技术。其中一种最有效的方法是误差补偿压缩，它即使在1位压缩下也能提供稳健的收敛速度。然而，最先进的误差补偿技术仅适用于基本的优化器，如SGD和动量SGD，它们与梯度呈线性关系。它们不能与Adam等基于非线性梯度的优化器配合使用，Adam为BERT等模型提供了最先进的收敛效率和精度。在本文中，我们提出了1位Adam，它将通信量减少了$5倍，提供了更好的可扩展性，并提供了与未压缩Adam相同的收敛速度。我们的主要发现是Adam的方差（非线性项）变得稳定（在热身阶段之后），并且可以用作剩余训练（压缩阶段）的固定前提条件。在多达256个GPU上进行的实验表明，1位Adam使BERT大型预训练的吞吐量提高了3.3倍，使小队微调的吞吐量提高了2.9倍。此外，我们还为我们提出的工作提供了理论分析。</pre></li>
<li><a href="https://arxiv.org/abs/2102.07988">TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models</a>
<pre>模型并行已成为训练现代大规模深层语言模型的必要条件。在这项工作中，我们从现有的模型并行方法中确定了一个新的正交维度：由于其自回归特性，可以在基于转换器的语言模型的单个训练序列中执行管道并行。与以前的工作相比，这可以实现更细粒度的管道。基于这一关键思想，我们设计了一种高性能令牌级流水线并行算法TeraPipe，用于基于变压器的语言模型的同步模型并行训练。我们开发了一种新的基于动态规划的算法来计算给定特定模型和集群配置的最佳流水线执行方案。我们表明，与最先进的模型并行方法相比，TeraPipe可以将AWS集群上具有1750亿个参数的最大GPT-3模型的训练速度提高5.0x，该集群具有48个p3.16xlarge实例。复制代码可在以下网址找到：https://github.com/zhuohan123/terapipe</pre></li>
<li><a href="https://arxiv.org/abs/2104.04473">Efficient Large-Scale Language Model Training on GPU Clusters</a>
<pre>大型语言模型在一系列任务中实现了最先进的准确性。然而，有效地训练这些模型具有挑战性，原因有两个：a）GPU内存容量有限，无法在多GPU服务器上安装大型模型；b）训练这些模型所需的计算操作数量可能导致训练时间过长。因此，提出了新的模型并行方法，如张量并行和流水线并行。不幸的是，这些方法的简单使用导致了数千个GPU的根本性扩展问题，例如，由于昂贵的跨节点通信或设备花费大量时间等待其他设备取得进展。在本文中，我们展示了如何组合不同类型的并行方法（张量、管道和数据并行），以扩展到数千个GPU和具有数万亿参数的模型。我们对流水线并行技术进行了综述，并提出了一种新的交错流水线并行调度方案，该方案可将吞吐量提高10%+，内存占用与现有方法相当。我们定量地研究了张量、管道和数据并行性之间的权衡，并提供了如何配置大型模型的分布式训练的直观性。我们的方法允许我们在3072 GPU上以502 petaFLOP/s的速度对1万亿个参数的模型执行训练迭代，每个GPU的吞吐量达到理论峰值的52%。我们的代码是开源的https://github.com/nvidia/megatron-lm.</pre></li>
<li><a href="https://arxiv.org/abs/2001.08361">Scaling Laws for Neural Language Models</a>
<pre>我们研究了交叉熵损失下语言模型性能的经验标度律。损失随着模型大小、数据集大小和用于训练的计算量呈幂律变化，有些趋势跨越七个数量级以上。其他架构细节（如网络宽度或深度）在大范围内的影响最小。简单方程控制过拟合对模型/数据集大小的依赖性以及训练速度对模型大小的依赖性。这些关系允许我们确定固定预算的最佳分配。较大模型的样本效率明显更高，因此最佳计算效率训练包括在相对较少的数据量上训练非常大的模型，并在收敛之前显著停止。</pre></li>
<li><a href="https://arxiv.org/abs/2010.14701">Scaling Laws for Autoregressive Generative Modeling</a>
<pre>我们在四个领域确定了交叉熵损失的经验标度律：生成图像建模、视频建模、多模态图像$\leftrightarrow$文本模型和数学问题解决。在所有情况下，随着模型尺寸和计算预算的增加，自回归变压器的性能都会随着幂律和常数比例律的增加而平稳提高。最佳模型大小还取决于通过幂律计算的预算，指数几乎在所有数据域中都是通用的。交叉熵损失的信息论解释为$S（$True$）+D_{\mathrm{KL}（$True$| |$Model$）$，经验标度定律表明预测了真实数据分布的熵以及真实分布和模型分布之间的KL差异。根据这种解释，十亿参数变压器几乎是YFCC100M图像分布的完美模型，其采样分辨率为8美元乘以8美元，我们可以预测实现nats/图像中任何给定可减少损失（即其他分辨率的$D{\mathrm{KL}}$）所需的模型大小。我们在特定领域中发现了一些额外的缩放定律：（a）我们确定了多模态模型中字幕和图像之间互信息的缩放关系，并展示了如何回答“一张图片值一千个单词吗？”；（b） 在数学问题解决的情况下，当外推超出训练分布时，我们确定了模型性能的标度律；（c） 我们对用于ImageNet分类的生成图像模型进行了微调，发现分类损失和错误率的平滑缩放，即使生成损失趋于平稳。综上所述，这些结果加强了这样一种情况，即标度律对神经网络性能具有重要影响，包括对下游任务的影响。</pre></li>
<li><a href="https://arxiv.org/abs/2109.10686">Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers</a>
<pre>关于变压器架构的伸缩行为，仍然存在许多悬而未决的问题。这些扩展决策和发现可能非常关键，因为培训运行通常伴随着相关的计算成本，这会对财务和/或环境产生影响。本文的目标是展示从预训练和微调变压器中获得的缩放见解。虽然Kaplan等人对变压器语言模型的缩放行为进行了全面研究，但范围仅限于上游（预培训）损耗。因此，目前尚不清楚这些发现是否会转移到pretrain finetune范式下的下游任务中。本文的主要发现如下：（1）我们表明，除了模型大小外，下游微调的模型形状也很重要，（2）缩放协议在不同的计算区域运行不同，（3）广泛采用的T5基和T5大尺寸是帕累托无效的。为此，我们提出了改进的缩放协议，与广泛采用的T5基础模型相比，我们重新设计的模型实现了类似的下游微调质量，同时参数减少了50%，训练速度加快了40%。我们公开发布了100多个不同T5配置的预训练检查点，以促进未来的研究和分析。</pre></li>
<li><a href="https://arxiv.org/abs/2101.00027">The Pile: An 800GB Dataset of Diverse Text for Language Modeling</a> [<a href="https://pile.eleuther.ai/">website</a>]
<pre>最近的研究表明，训练数据集多样性的增加提高了大规模语言模型的一般跨领域知识和下游泛化能力。考虑到这一点，我们提出了\textit{the Pile}：一个825 GiB的英语文本语料库，旨在训练大规模的语言模型。该桩由22个不同的高质量子集（包括现有和新建）构成，其中许多来自学术或专业来源。我们对GPT-2和GPT-3在桩上的非协调性能进行的评估表明，这些模型在其许多组成部分上存在困难，例如学术写作。相反，在桩上训练的模型在桩的所有组件上都比原始CC和CC-100显著提高，同时改善了下游评估的性能。通过深入的探索性分析，我们为潜在用户记录了数据的潜在方面。我们公开了其构造中使用的代码。</pre></li>
<li><a href="https://arxiv.org/abs/2107.06499">Deduplicating Training Data Makes Language Models Better</a>
<pre>我们发现现有的语言建模数据集包含许多近似重复的示例和长时间重复的子字符串。因此，在这些数据集上训练的语言模型的非提示输出中，超过1%是从训练数据中逐字复制的。我们开发了两种工具来消除训练数据集的重复数据——例如，从C4中删除一个重复超过60000次的61个单词的英语句子。重复数据消除使我们能够对发出记忆文本的模型进行训练，使其频率降低十倍，并且需要更少的训练步骤才能达到相同或更好的精确度。我们还可以减少列车测试重叠，这会影响超过4%的标准数据集验证集，从而允许更准确的评估。我们发布了代码，用于复制我们的工作并执行数据集重复数据消除https://github.com/google-research/deduplicate-text-datasets.</pre></li>
<li><a href="https://openreview.net/forum?id=HkgaETNtDB">Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models</a> (ICLR2020)</li>
<li><a href="https://openreview.net/forum?id=Syx79eBKwr">A Mutual Information Maximization Perspective of Language Representation Learning</a> (ICLR2020)</li>
<li><a href="https://arxiv.org/abs/1907.11932">Is BERT Really Robust? Natural Language Attack on Text Classification and Entailment</a> (AAAI2020)
<pre>机器学习算法通常容易受到敌对示例的攻击，这些示例与原始示例相比有着不可察觉的变化，但可以愚弄最先进的模型。通过公开恶意制作的对抗性示例，有助于评估甚至提高这些模型的健壮性。在这篇文章中，我们提出了textwooler，一个简单但强大的基线来生成自然的对抗性文本。通过将其应用于两个基本的自然语言任务，文本分类和文本蕴涵，我们成功地攻击了三个目标模型，包括强大的预训练BERT和广泛使用的卷积和递归神经网络。我们从三个方面展示了该框架的优势：（1）有效——它在成功率和干扰率方面优于最先进的攻击；（2）实用性保护——它保留语义内容和语法性，并保持人类的正确分类；（3）高效---它生成对抗性文本，计算复杂度与文本长度成线性关系*代码、预先训练的目标模型和测试示例可在https://github.com/jind11/TextFooler.</pre></li>
<li><a href="https://arxiv.org/abs/2004.06660">Weight Poisoning Attacks on Pre-trained Models</a> (ACL2020)
<pre>最近，NLP在使用大型预训练模型方面出现了激增。用户下载在大型数据集上预先训练的模型权重，然后根据自己选择的任务微调权重。这就提出了一个问题：下载不可信的预训练权重是否会构成安全威胁。在本文中，我们证明了有可能构造“权重中毒”攻击，在这种攻击中，预先训练的权重被注入漏洞，这些漏洞在微调后暴露“后门”，使得攻击者能够通过简单地注入任意关键字来操纵模型预测。我们证明，通过应用正则化方法（我们称之为RIPPLe）和初始化过程（我们称之为嵌入手术），即使数据集和微调过程的知识有限，也可能发生此类攻击。我们在情感分类、毒性检测和垃圾邮件检测方面的实验表明，这种攻击具有广泛的适用性，并构成了严重的威胁。最后，我们概述了针对此类攻击的实际防御措施。复制我们实验的代码可在https://github.com/neulab/RIPPLe.</pre></li>
<li><a href="https://arxiv.org/abs/2004.09984">BERT-ATTACK: Adversarial Attack Against BERT Using BERT</a> (EMNLP2020)
<pre>针对离散数据（如文本）的对抗性攻击已被证明比连续数据（如图像）更具挑战性，因为使用基于梯度的方法很难生成对抗性样本。目前成功的文本攻击方法通常在字符或单词层面上采用启发式替换策略，在大量可能的替换组合空间中寻找最优解，同时保持语义一致性和语言流畅性仍然是一个挑战。在本文中，我们提出了\textbf{BERT-Attack}，这是一种使用预先训练的蒙面语言模型生成对抗性样本的高质量和有效的方法，以BERT为例。我们在下游任务中将BERT与其微调模型和其他深度神经模型对立起来，这样我们就可以成功地误导目标模型进行错误预测。我们的方法在成功率和干扰百分比方面都优于最先进的攻击策略，同时生成的对抗性样本流畅且语义保持。此外，计算成本较低，因此可以进行大规模发电。该守则可于https://github.com/LinyangLee/BERT-Attack.</pre></li>
<li><a href="https://arxiv.org/abs/2106.01452">BERT-Defense: A Probabilistic Model Based on BERT to Combat Cognitively Inspired Orthographic Adversarial Attacks</a> (ACL2021 Findings)
<pre>对抗性攻击暴露了深度学习系统的重要盲点。虽然单词级和句子级攻击场景主要处理查找愚弄NLP模型的输入语义释义，但字符级攻击通常会在输入流中插入拼写错误。人们普遍认为，通过拼写纠正模块，这些更容易维护。在这项工作中，我们展示了标准拼写检查器和Pruthi et al.（2019）的方法，该方法训练防止插入、删除和交换，在Eger和Benz（2020）最近提出的字符级基准上表现不佳其中包括更具挑战性的攻击，如视觉和语音干扰以及缺失分词。相反，我们表明，一个未经训练的迭代方法，结合上下文无关的字符级信息与上下文相关的信息从伯特的蒙面语言建模可以执行与人类人群工人从亚马逊机械土耳其人（AMT）通过3-镜头学习监督。</pre></li>
<li><a href="https://arxiv.org/abs/2103.10013">Model Extraction and Adversarial Transferability, Your BERT is Vulnerable!</a> (NAACL2021)
<pre>从文本分类到文本生成的自然语言处理（NLP）任务已经被预先训练好的语言模型（如BERT）彻底改变。这使得公司能够通过封装用于下游任务的经过微调的BERT模型，轻松构建强大的API。然而，当一个经过微调的BERT模型作为一个服务部署时，它可能会遭受恶意用户发起的不同攻击。在这项工作中，我们首先介绍对手如何在有限的先验知识和查询的情况下，在多个基准数据集上窃取基于BERT的API服务（受害者/目标模型）。我们进一步表明，提取的模型可以导致针对受害者模型的高度可转移的对抗性攻击。我们的研究表明，基于BERT的API服务的潜在漏洞仍然存在，即使受害者模型和攻击模型之间存在架构不匹配。最后，我们研究了两种保护受害者模型的防御策略，发现除非牺牲受害者模型的性能，否则模型的扩展性和对抗性转移性都可以有效地损害目标模型</pre></li>
<li><a href="https://arxiv.org/abs/2003.04985">Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT</a>
<pre>越来越多的文献声称，在处理恶意创建的对抗性示例时，深层神经网络的脆弱性。然而，目前尚不清楚这些模型在通常存在{text自然而非恶意}对抗性实例的现实场景中如何运行。这项工作系统地探讨了BERT（NLP中最先进的变压器式模型）在处理噪声数据时的鲁棒性，特别是在无意中出现的键盘键入错误。对情绪分析和问答基准的深入实验表明：（i）一个句子中不同单词的拼写错误不会产生同等的影响。信息词的拼写错误会造成更严重的损害；（ii）与插入、删除等相比，打字错误是最具破坏性的因素。；（iii）人类和机器在识别对抗性攻击方面有不同的重点。</pre></li>
<li><a href="https://www.aclweb.org/anthology/2020.acl-main.245/">Robust Encodings: A Framework for Combating Adversarial Typos</a> (ACL2020)</li>
<li><a href="https://arxiv.org/abs/2005.05683">On the Robustness of Language Encoders against Grammatical Errors</a> (ACL2020)
<pre>我们进行了深入的研究，以诊断预先训练的语言编码员（ELMo、BERT和RoBERTa）在遇到自然语法错误时的行为。具体来说，我们收集非母语人士的真实语法错误，并在干净的文本数据上进行对抗性攻击来模拟这些错误。我们使用这种方法来促进在下游应用程序上调试模型。结果证实，所有测试模型的性能都受到影响，但影响程度各不相同。为了解释模型行为，我们进一步设计了一个语言可接受性任务，以揭示他们识别不合语法句子的能力和错误的位置。我们发现，固定的上下文编码器与一个简单的分类器训练的句子正确性预测能够定位错误的位置。我们还为BERT设计了一个完形填空测试，发现BERT捕获了上下文中错误和特定标记之间的交互。我们的结果有助于理解语言编码器对语法错误的鲁棒性和行为。</pre></li>
<li><a href="https://arxiv.org/abs/2108.12237">Evaluating the Robustness of Neural Language Models to Input Perturbations</a> (EMNLP2021)
<pre>高性能神经语言模型已经在广泛的自然语言处理（NLP）任务中获得了最新的结果。然而，当应用于嘈杂的真实数据时，通用基准数据集的结果通常不能反映模型的可靠性和鲁棒性。在这项研究中，我们设计并实现了各种类型的字符级和单词级扰动方法，以模拟输入文本可能有轻微噪声或与NLP系统训练的数据分布不同的真实场景。通过对不同NLP任务的综合实验，我们研究了高性能语言模型（如BERT、XLNet、RoBERTa和ELMo）处理不同类型输入扰动的能力。结果表明，语言模型对输入扰动非常敏感，即使引入微小的变化，其性能也会下降。我们强调模型需要进一步改进，当前的基准没有很好地反映模型的稳健性。我们认为，对扰动输入的评估应该常规地补充广泛使用的基准，以便更现实地理解NLP系统的鲁棒性。</pre></li>
<li><a href="https://arxiv.org/abs/2004.06100">Pretrained Transformers Improve Out-of-Distribution Robustness</a> (ACL2020) [<a href="https://github.com/camelop/NLP-Robustness">github</a>]
<pre>虽然像BERT这样的预训练变压器在分布示例中实现了高精度，但它们是否可以推广到新的分布？我们通过构建一个新的具有真实分布变化的鲁棒性基准，系统地度量了七个NLP数据集的分布外（OOD）泛化。我们测量了先前模型的泛化，包括单词袋模型、ConvNets和LSTMs，并且我们表明，预训练变压器的性能下降要小得多。预训练变压器在检测异常或OOD示例方面也更有效，而以前的许多模型往往比偶然性更差。我们研究了影响稳健性的因素，发现较大的模型不一定更稳健性，蒸馏可能有害，更多样化的预训练数据可以增强稳健性。最后，我们展示了未来的工作可以提高OOD健壮性的地方。</pre></li>
<li><a href="https://arxiv.org/abs/2004.03012">“You are grounded!”: Latent Name Artifacts in Pre-trained Language Models</a> (EMNLP2020)
<pre>预先训练的语言模型（LMs）可能会使其训练语料库中产生的对下游模型的偏见永久化。我们关注与给定名称（例如，Donald）表示相关的工件，根据语料库，这些工件可能与特定实体相关，如下一个标记预测（例如，Trump）所示。虽然在某些情况下有帮助，但在不明确或不适当的情况下也会发生根植。例如，为“Donald is a”生成的结尾与其他名字的结尾有很大的不同，并且通常具有超过平均水平的负面情绪。我们用阅读理解探针展示了名字扰动改变模型答案对下游任务的潜在影响。作为一线希望，我们的实验表明，对不同语料库进行额外的预训练可能会减轻这种偏见。</pre></li>
<li><a href="https://arxiv.org/abs/2004.07453">The Right Tool for the Job: Matching Model and Instance Complexities</a> (ACL2020) [<a href="https://github.com/allenai/sledgehammer">github</a>]
<pre>随着NLP模型变得越来越大，执行一个经过训练的模型需要大量的计算资源，从而产生货币和环境成本。为了更好地尊重给定的推理预算，我们建议修改上下文表示微调，在推理过程中，允许对简单实例提前（和快速）“退出”神经网络计算，对硬实例晚（和准确）退出。为了实现这一点，我们将分类器添加到不同的BERT层，并使用其校准的置信度分数来做出早期退出决策。我们在两个任务中对五个不同的数据集进行了测试：三个文本分类数据集和两个自然语言推理基准。我们的方法在几乎所有情况下都提供了一个良好的速度/精度权衡，生成的模型比最新技术快五倍，同时保持了它们的精度。与基线BERT模型相比，我们的方法几乎不需要额外的训练资源（时间或参数）。最后，我们的方法减轻了在不同效率水平下对多个模型进行昂贵的再培训的需要；我们允许用户通过在推理时设置单个变量，使用单个训练模型控制推理速度/精度权衡。我们公开发布我们的代码。</pre></li>
<li><a href="https://arxiv.org/abs/2004.02105">Unsupervised Domain Clusters in Pretrained Language Models</a> (ACL2020)
<pre>NLP中“域内数据”的概念通常过于简单和模糊，因为文本数据在许多细微的语言方面（如主题、风格或正式程度）有所不同。此外，域标签多次不可用，这使得构建特定于域的系统具有挑战性。我们展示了大量预先训练的语言模型隐式地学习句子表示，这些句子表示是在没有监督的情况下按域聚类的——这表明了文本数据中域的简单数据驱动定义。我们利用这一特性，提出了基于这种模型的域数据选择方法，这种方法只需要一小部分域内单语数据。我们评估我们的数据选择方法的神经机器翻译跨越五个不同的领域，其中他们胜过一个既定的方法，衡量BLUU和精度和召回的句子选择相对于甲骨文。</pre></li>
<li><a href="https://arxiv.org/abs/1910.12366">Thieves on Sesame Street! Model Extraction of BERT-based APIs</a> (ICLR2020)
<pre>我们研究了自然语言处理中的模型提取问题，其中只有对受害者模型具有查询访问权限的对手试图重建该模型的本地副本。假设对手和受害者模型都微调了大型预训练语言模型，如BERT（Devlin et al.2019），我们表明对手不需要任何真实的训练数据来成功发起攻击。事实上，攻击者甚至不需要使用语法或语义上有意义的查询：我们表明，随机单词序列加上特定于任务的启发式方法，可以有效地查询各种NLP任务的模型提取，包括自然语言推理和问答。因此，我们的工作强调了一个只有在NLP社区内转向转移学习方法才能实现的漏洞：对于几百美元的查询预算，攻击者可以提取一个性能仅略低于受害者模型的模型。最后，我们研究了两种针对模型提取的防御策略——隶属度分类和API水印——它们虽然能够成功地抵御幼稚的对手，但对于更复杂的对手却无能为力。</pre></li>
<li><a href="https://arxiv.org/abs/2001.05140">Graph-Bert: Only Attention is Needed for Learning Graph Representations</a>
<pre>主导的图神经网络（GNN）过度依赖于图链接，已经出现了一些严重的性能问题，如暂停动画问题和过度平滑问题。此外，由于内存限制限制了节点间的批处理，固有的互连特性排除了图内的并行化，这对于大型图来说变得至关重要。在本文中，我们将介绍一种新的图神经网络，即graph-BERT（graph-based BERT），它完全基于注意机制，没有任何图卷积或聚合算子。我们建议在局部上下文中用采样的无链接子图训练GRAPH-BERT，而不是用完整的大输入图来训练GRAPH-BERT。GRAPH-BERT可以在独立模式下有效地学习。同时，如果有任何有监督的标签信息或特定的面向应用的目标可用，则预先训练好的GRAPH-BERT也可以直接或通过必要的微调转移到其他应用任务。我们在几个图形基准数据集上测试了GRAPH-BERT的有效性。基于预先训练好的GRAPH-BERT和节点属性重建和结构恢复任务，我们进一步对GRAPH-BERT在节点分类和图聚类任务上进行了微调。实验结果表明，GRAPH-BERT在学习效果和效率上都优于现有的GNNs。</pre></li>
<li><a href="https://arxiv.org/abs/2006.05213">Graph-Aware Transformer: Is Attention All Graphs Need?</a>
<pre>图是表示许多领域中的关系和结构信息的自然数据结构。为了涵盖广泛的图形数据应用，包括图形分类和图形生成，需要一个通用且灵活的模型，该模型由能够处理图形数据的编码器和解码器组成。尽管具有代表性的编码器-解码器模型Transformer在各种任务中表现出优异的性能，特别是在自然语言处理中，但由于图形的非顺序特性，它不能立即用于图形。为了解决这种不兼容性，我们提出了图形感知转换器（GRAT），这是第一个基于转换器的模型，它可以端到端的方式对整个图形进行编码和解码。GRAT具有自适应于边缘信息的自注意机制和基于双路径方法的自回归解码机制，双路径方法包括子图编码路径和每个解码步骤的节点和边缘生成路径。我们在多个设置上对GRAT进行了经验评估，包括基于编码器的任务，如QM9数据集上的分子特性预测，以及基于编码器-解码器的任务，如有机分子合成领域中的分子图生成。GRAT已经展示了非常有希望的结果，包括在QM9基准测试中4个回归任务的最新性能。</pre></li>
<li><a href="https://arxiv.org/abs/2002.08155">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a> (EMNLP2020 Findings)
<pre>我们提出了CodeBERT，一种编程语言（PL）和自然语言（NL）的双模预训练模型。CodeBERT学习支持下游NL-PL应用的通用表示法，如自然语言代码搜索、代码文档生成等。我们使用基于转换器的神经架构开发CodeBERT，并使用混合目标函数对其进行训练，该混合目标函数包含替换标记检测的预训练任务，这是为了检测从发电机取样的可能的替代品。这使我们能够利用NL-PL对的双峰数据和单峰数据，前者为模型训练提供输入标记，而后者有助于学习更好的生成器。我们通过微调模型参数在两个NL-PL应用程序上评估CodeBERT。结果表明，CodeBERT在自然语言代码搜索和代码文档生成任务上都达到了最先进的性能。此外，为了研究在CodeBERT中学习到什么类型的知识，我们构建了一个NL-PL探测数据集，并在零炮设置中进行评估，其中预先训练的模型的参数是固定的。结果表明，CodeBERT在NL-PL探测上的性能优于以前的预训练模型。</pre></li>
<li><a href="https://arxiv.org/abs/2006.03511">Unsupervised Translation of Programming Languages</a>
<pre>转换编译器也称为源到源转换器，是一种将源代码从高级编程语言（如C++或Python）转换为另一种的系统。Transcompiler主要用于互操作性，并将用过时或不推荐使用的语言（如COBOL、Python 2）编写的代码库移植到现代语言。它们通常依赖于应用于源代码抽象语法树的手工重写规则。不幸的是，最终的翻译往往缺乏可读性，无法遵守目标语言的约定，需要手动修改才能正常工作。整个翻译过程非常耗时，需要源语言和目标语言方面的专业知识，这使得代码翻译项目非常昂贵。尽管神经模型在自然语言翻译中的表现明显优于基于规则的模型，但由于该领域缺乏并行数据，神经模型在跨编译中的应用受到限制。在本文中，我们建议利用无监督机器翻译的最新方法来训练一个完全无监督的神经跨编译器。我们在开源GITHUB项目上对源代码进行训练，并显示它可以在高精度的C++、java和Python之间转换函数。我们的方法完全依赖于单语源代码，不需要源语言或目标语言方面的专业知识，并且可以很容易地推广到其他编程语言。我们还构建并发布了一个由852个并行函数组成的测试集，以及用于检查翻译正确性的单元测试。我们表明，我们的模型比基于规则的商业基线有显著的优势。</pre></li>
<li><a href="https://arxiv.org/abs/2103.06333">Unified Pre-training for Program Understanding and Generation</a> (NAACL2021)
<pre>代码摘要和生成支持编程语言（PL）和自然语言（NL）之间的转换，而代码翻译则支持将遗留代码从一个PL迁移到另一个PL。本文介绍了PLBART，一种序列到序列的模型，能够执行广泛的程序和语言理解和生成任务。PLBART通过去噪自动编码在大量Java和Python函数以及相关NL文本集合上进行预训练。对英语代码摘要、代码生成和七种编程语言的代码翻译的实验表明，PLBART优于或可与最先进的模型相媲美。此外，对区分性任务（如程序修复、克隆检测和易受攻击代码检测）的实验证明了PLBART在程序理解方面的有效性。此外，分析表明，PLBART学习对程序语义至关重要的程序语法、样式（例如，标识符命名约定）、逻辑流（例如，else块内的if块等同于else if块），因此即使在注释有限的情况下也会表现优异。</pre></li>
<li><a href="https://arxiv.org/abs/2105.00377">MathBERT: A Pre-Trained Model for Mathematical Formula Understanding</a>
<pre>像BERT这样的大规模预训练模型在各种自然语言处理（NLP）任务中取得了巨大成功，但如何使它们适应与数学相关的任务仍然是一个挑战。现有的预训练模型忽略了公式及其上下文之间的结构特征和语义对应关系。为了解决这些问题，我们提出了一种新的预训练模型，即\textbf{MathBERT}，它与数学公式及其相应的上下文联合训练。此外，为了进一步捕捉公式的语义层次结构特征，设计了一个新的预训练任务来预测从公式的语义结构表示算子树（OPT）中提取的蒙面公式子结构。我们在三个下游任务上进行了各种实验，以评估MathBERT的性能，包括数学信息检索、公式主题分类和公式标题生成。实验结果表明，MathBERT在所有这三个任务上都显著优于现有方法。此外，我们定性地证明了该预训练模型有效地捕获了公式的语义层次结构信息。据我们所知，MathBERT是第一个经过预训练的数学公式理解模型。</pre></li>
<li><a href="https://arxiv.org/abs/2105.08928">Investigating Math Word Problems using Pretrained Multilingual Language Models</a>
<pre>本文从跨语言和多语言的角度重新审视了数学词汇问题。我们使用带有复制机制的序列到序列模型，在预训练的多语言模型上构造我们的MWP解算器。我们比较了MWP解算器在跨语言和多语言场景中的表现。为了便于跨语言性能的比较，我们首先将大规模英语数据集MathQA与中文数据集Math23K进行了对比。然后，我们通过机器翻译和人工标注将多个英语数据集扩展到双语数据集。我们的实验表明，即使目标表达式具有相同的运算符集和常量，MWP解算器也可能不会转换为不同的语言。但对于跨语言和多语言的情况，如果源语言和目标语言上都存在问题类型，则可以更好地将其推广。</pre></li>
<li><a href="https://arxiv.org/abs/2106.03921">Measuring and Improving BERT’s Mathematical Abilities by Predicting the Order of Reasoning</a> (ACL2021)
<pre>想象你在超市里。你篮子里有两个香蕉，想买四个苹果。你们一共有多少水果？这个看似简单的问题对于数据驱动的语言模型来说可能是一个挑战，即使是大规模培训。然而，我们期望这种通用语言模型除了具有典型的语言能力外，还具有一些数学能力。为了实现这一目标，我们调查了一个常用的语言模型，伯特，是否具有这样的数学能力，如果是，到什么程度。为此，我们在一个流行的数学问题数据集AQuA-RAT上对BERT进行了微调，并进行了一些测试，以更好地理解所学的表示法。由于我们教授用自然语言训练的模型做形式数学，我们假设这种模型将受益于解释数学结果如何推导的半形式步骤的训练。为了更好地适应这种训练，我们还提出了学习数学规则的新借口任务。我们称之为（邻居）推理顺序预测（ROP或NROP）。有了这个新模型，我们取得了比数据驱动基线更好的结果，甚至可以与更定制的模型媲美。我们还展示了如何减少这种模型中的位置偏差。</pre></li>
<li><a href="https://arxiv.org/abs/2109.04711">Pre-train or Annotate? Domain Adaptation with a Constrained Budget</a> (EMNLP2021)
<pre>最近的研究表明，领域语言模型的预训练可以提高适应新领域时的性能。然而，与预培训相关的成本提出了一个重要的问题：给定固定的预算，NLP从业者应该采取什么步骤来最大限度地提高绩效？在本文中，我们研究了预算约束下的领域适应问题，并将其作为一个介于数据注释和预培训之间的客户选择问题。具体来说，我们测量了三个过程文本数据集的注释成本和三个域内语言模型的预训练成本。然后，我们在不同的预算约束下评估预培训和数据注释的不同组合的效用，以评估哪种组合策略最有效。我们发现，对于较小的预算，将所有资金用于注释会导致最佳性能；一旦预算足够大，数据注释和域内预培训的结合将更有效。因此，我们建议在使NLP模型适应新领域时，任务特定的数据注释应该是经济策略的一部分。</pre></li>
<li><a href="https://www.aclweb.org/anthology/2020.ecnlp-1.8/">Item-based Collaborative Filtering with BERT</a> (ACL2020 WS)</li>
<li><a href="https://arxiv.org/abs/2009.13292">RecoBERT: A Catalog Language Model for Text-Based Recommendations</a>
<pre>利用未标记文本进行广泛的自我监督预训练的语言模型，最近已证明在各种语言理解任务中显著提高了最先进的表现。然而，目前尚不清楚是否以及如何利用这些最新模型来进行基于文本的建议。在这项工作中，我们介绍了RecoBERT，一种基于BERT的方法，用于学习基于文本的项目推荐的目录专用语言模型。我们建议采用新的训练和推理程序，对项目之间的相似性进行评分，而不需要项目相似性标签。训练和推理技术都旨在利用文本目录的未标记结构，并尽量减少它们之间的差异。通过在推理过程中加入四个分数，雷科伯特可以比其他技术更准确地推断基于文本的项目间相似性。此外，我们还引入了一个新的语言理解任务，使用基于专业葡萄酒评论的相似性进行葡萄酒推荐。作为另一项贡献，我们发布了由人类葡萄酒专家制作的带注释的推荐数据集。最后，我们评估了雷科伯特，并将其与各种关于葡萄酒和时尚推荐任务的最新NLP模型进行了比较。</pre></li>
<li><a href="https://arxiv.org/abs/2002.06305">Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping</a>
<pre>在自然语言处理中，对预先训练好的上下文单词嵌入模型进行微调，以使其适用于有监督的下游任务已经变得很普遍。然而，这个过程通常是脆弱的：即使具有相同的超参数值，不同的随机种子也可能导致截然不同的结果。为了更好地理解这一现象，我们对GLUE基准测试中的四个数据集进行了实验，对每个数据集进行数百次微调，同时只改变随机种子。我们发现，与以前报告的结果相比，性能有了显著提高，并且我们量化了最佳模型的性能如何随微调试验数量的变化而变化。此外，我们还考察了随机种子选择的两个影响因素：权重初始化和训练数据顺序。我们发现，这两种方法对样本外性能的差异都有相当大的贡献，并且一些权重初始化在所研究的所有任务中都表现良好。在小数据集上，我们观察到许多微调试验在训练过程中出现了部分分歧，我们为从业者提供了最佳实践，让他们尽早停止训练前景不佳的跑步。我们公开发布所有实验数据，包括2100次试验的训练和验证分数，以鼓励在微调过程中进一步分析训练动态。</pre></li>
<li><a href="https://arxiv.org/abs/1912.05877">Extending Machine Language Models toward Human-Level Language Understanding</a>
<pre>语言对人类智力至关重要，但它的作用到底是什么？我们认为语言是一个系统的一部分，用来理解和交流各种情况。人类理解和交流情境的能力是从经验中逐渐产生的，并取决于生物神经网络的领域一般原则：基于连接的学习、分布式表示和上下文敏感、基于相互约束满足的处理。当前的人工语言处理系统依赖于同一领域的一般原理，体现在人工神经网络中。事实上，这一领域的最新进展依赖于{基于查询的注意}，它扩展了这些系统利用上下文的能力，并促成了显著的突破。然而，目前大多数模型只关注语言内部任务，限制了它们执行依赖于理解情况的任务的能力。这些系统也缺乏对固定语境范围之外先前情景内容的记忆。我们描述了大脑分布式理解系统的组织结构，该系统包括一个解决记忆问题的快速学习系统。我们为未来的理解模型勾勒了一个框架，该模型同样利用认知神经科学和人工智能，并利用基于查询的注意。我们强调相关的当前方向，并考虑在计算系统中完全捕获人类级语言理解所需的进一步发展。</pre></li>
<li><a href="https://openreview.net/forum?id=GKTvAcb12b">Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data</a> (ACL2020)</li>
<li><a href="https://arxiv.org/abs/2105.06020">Are Larger Pretrained Language Models Uniformly Better? Comparing Performance at the Instance Level</a> (ACL2021 Findings) [<a href="https://github.com/ruiqi-zhong/acl2021-instance-level">github</a>]
<pre>较大的语言模型平均具有较高的准确性，但它们是否在每个实例（数据点）上都更好？一些研究表明，较大的模型具有更高的分布外稳健性，而其他研究表明，它们对稀有子群的准确性较低。为了理解这些差异，我们在单个实例的层次上研究这些模型。然而，一个主要的挑战是个体预测对训练随机性中的噪声高度敏感。我们开发了统计上严格的方法来解决这一问题，在考虑了预训练和微调噪声后，我们发现，与总体精度提高2-10%相比，MNLI、SST-2和QQP中至少有1-4%的情况下，我们的BERT-Large比BERT-Mini差。我们还发现，微调噪声随着模型大小的增加而增加，并且实例级精度具有动量：从BERT Mini到BERT Middle的改进与从BERT Middle到BERT Large的改进相关。我们的发现表明，实例级预测提供了丰富的信息来源；因此，我们建议研究人员用模型预测补充模型权重。</pre></li>
<li><a href="https://arxiv.org/abs/1901.10125">Glyce: Glyph-vectors for Chinese Character Representations</a>
<pre>直观地说，像中文这样的标识语言的NLP任务应该受益于这些语言中字形信息的使用。然而，由于字形中缺乏丰富的象形证据，以及标准计算机视觉模型对字符数据的泛化能力较弱，利用字形信息的有效方法仍有待找到。在本文中，我们通过介绍Glyce（汉字表示的字形向量）来解决这一差距。我们进行了三大创新：（1）使用历史汉字（如青铜器文字、篆书、繁体中文等），以丰富汉字的象形证据；（2） 我们设计了适合汉字图像处理的CNN结构（称为天则阁CNN）；（3）在多任务学习系统中，我们使用图像分类作为辅助任务，以提高模型的泛化能力。我们表明，基于glyph的模型能够在广泛的中文NLP任务中始终优于基于word/charid的模型。我们能够为各种中文NLP任务设置最新的结果，包括标记（NER、CWS、POS）、句子对分类、单句分类任务、依存分析和语义角色标记。例如，该模型在OntoNotes数据集NER上的F1得分为80.6，在BERT上为+1.5；在复旦语料库的文本分类中，它几乎达到了99.8%的完美准确率。代码位于https://github.com/ShannonAI/glyce.</pre></li>
<li><a href="https://arxiv.org/abs/1909.03464">Back to the Future – Sequential Alignment of Text Representations</a>
<pre>语言在许多与自然语言处理任务相关的方面随着时间的推移而发展。例如，最近在出版物中出现的标记“BERT”和“ELMO”指的是神经网络架构，而不是人。这种类型的时间信号通常会被忽略，但如果要在较长时间内部署机器学习模型，这一点很重要。特别是，在连续的决策任务中，语言进化导致数据在时间步长之间漂移。此类任务的示例包括预测年度会议的论文接受情况（定期）或推特上谣言的作者立场预测（不定期）。受计算机视觉成功的启发，我们通过顺序对齐学习的表示来解决数据漂移问题。我们评估了三项具有挑战性的任务，这些任务在时间尺度、语言单位和领域方面各不相同。这些任务表明我们的方法优于几个强基线，包括使用所有可用数据。我们认为，由于其计算费用低，顺序对齐是处理语言进化的一种实用解决方案。</pre></li>
<li><a href="https://www.aclweb.org/anthology/papers/W/W19/W19-1402/">Improving Cuneiform Language Identification with BERT</a> (NAACL2019 WS)</li>
<li><a href="https://arxiv.org/abs/2005.00672">Generating Derivational Morphology with BERT</a>
<pre>预训练语言模型（PLM）能生成派生复杂词吗？我们提出了第一项研究调查这个问题，以伯特为例PLM。我们考察了BERT在不同环境下的推导能力，从使用未修改的预训练模型到完全微调。我们最好的模型，DagoBERT（衍生和生成优化的BERT），明显优于先前最先进的衍生生成（DG）。此外，我们的实验表明，输入分割对BERT的派生知识有着至关重要的影响，这表明，如果使用形态信息的单位词汇表，PLM的性能可以进一步提高。</pre></li>
<li><a href="https://arxiv.org/abs/1912.05238">BERT has a Moral Compass: Improvements of ethical and moral values of machines</a>
<pre>允许机器选择是否杀害人类将对世界和平与安全造成毁灭性的影响。但我们如何让机器具备学习道德甚至道德选择的能力呢？Jentzsch等人（2019年）表明，将机器学习应用于人类文本，可以通过使用句子嵌入计算句子层面的道德偏见分数，提取关于“对”和“错”行为的道义伦理推理。机器学会了杀死生物是令人反感的，但消磨时间是可以的；吃东西是必要的，但不能吃脏东西；传播信息很重要，但不应传播错误信息。然而，被评估的道德偏见仅限于简单的行为——一个动词——以及行为与周围环境的排名。最近，BERT——以及RoBERTa和SBERT等变体——为广泛的NLP任务设定了新的最先进性能。但伯特还有更好的道德指南针吗？在本文中，我们讨论并证明了事实确实如此。因此，语言表征的最新改进也提高了机器潜在伦理和道德价值的表征。我们认为，通过文本的高级语义表示，伯特可以让人们更好地理解文本中隐含的道德和伦理价值。这使得道德选择机器（MCM）能够更准确地提取道德选择和道德价值的印记。</pre></li>
<li><a href="https://arxiv.org/abs/2106.05630">MusicBERT: Symbolic Music Understanding with Large-Scale Pre-Training</a> (ACL2021 Findings)
<pre>符号音乐理解是指从符号数据（例如MIDI格式，但不是音频）理解音乐，它涵盖了许多音乐应用，如流派分类、情感分类和音乐片段匹配。虽然良好的音乐表现有利于这些应用，但缺乏训练数据阻碍了表现学习。受自然语言处理中预训练模型成功的启发，在本文中，我们开发了MusicBERT，一种用于音乐理解的大规模预训练模型。为此，我们构建了一个包含100多万首音乐歌曲的大规模符号音乐语料库。由于符号音乐包含更多的结构（例如，小节、位置）和多样的信息（例如，节奏、乐器和音高），简单地采用NLP到符号音乐的预训练技术只能带来边际收益。因此，我们设计了几种机制，包括八元组MIDI编码和条级掩蔽策略，以增强符号音乐数据的预训练。实验证明了MusicBERT在四项音乐理解任务上的优势，包括旋律完成、伴奏建议、体裁分类和风格分类。烧蚀研究也验证了我们设计的八元组MIDI编码和条形掩蔽策略在MusicBERT中的有效性。</pre></li>
<li><a href="https://dl.acm.org/citation.cfm?id=3342186">SMILES-BERT: Large Scale Unsupervised Pre-Training for Molecular Property Prediction</a> (ACM-BCB2019)</li>
<li><a href="https://arxiv.org/abs/2010.09885">ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction</a>
<pre>GNNs和化学指纹是表征分子性质预测的主要方法。然而，在NLP中，由于其强大的下游任务转移，变形金刚已经成为表征学习的事实标准。与此同时，围绕transformers的软件生态系统正在迅速成熟，HuggingFace和BertViz等库实现了简化的培训和反思。在这项工作中，我们首次尝试通过ChemBERTa模型系统地评估变压器在分子性质预测任务中的作用。ChemBERTa可以很好地扩展预训练数据集的大小，在MoleculeNet上提供有竞争力的下游性能和有用的基于注意力的可视化模式。我们的结果表明，变形金刚为分子表征学习和性质预测提供了一个有前途的未来工作途径。为了促进这些努力，我们从PubChem发布了一个7700万微笑的策划数据集，适用于大规模的自我监督预培训。</pre></li>
<li><a href="https://arxiv.org/abs/2007.16012">BERT Learns (and Teaches) Chemistry</a>
<pre>现代计算有机化学正变得越来越数据驱动。在这一领域仍然存在大量未解决的重要问题，例如给定反应物的产物预测、药物发现和度量优化分子合成，但近年来，使用机器学习解决这些问题的努力也有所增加。在这项工作中，我们建议使用注意从数据驱动的角度研究影响分子子结构的官能团和其他属性，在分子的字符串表示数据集上使用基于转换器的模型（BERT），并分析其注意头的行为。然后，我们将通过模型学习的官能团和原子表示应用于较小数据集的毒性、溶解度、药物相似性和合成可访问性问题，使用学习的表示作为分子图形结构上的图形卷积和注意模型的特征，以及伯特的微调。最后，我们建议使用注意力可视化作为化学从业者和学生快速识别各种化学性质中重要子结构的有用工具。</pre></li>
<li><a href="https://www.biorxiv.org/content/10.1101/2021.04.27.441365v1">Prediction of RNA-protein interactions using a nucleotide language model</a></li>
<li><a href="https://arxiv.org/abs/2005.09159">Sketch-BERT: Learning Sketch Bidirectional Encoder Representation from Transformers by Self-supervised Learning of Sketch Gestalt</a> (CVPR2020)
<pre>以往的草图研究通常考虑像素格式的草图，并利用基于CNN的模型来理解草图。从根本上说，草图存储为一系列数据点（矢量格式表示），而不是像素的真实照片图像。SketchRNN研究了长短时记忆网络（LSTM）对矢量格式草图的生成性神经表示。不幸的是，SketchRNN学习的表示主要用于生成任务，而不是识别和检索草图的其他任务。为此，受最近的BERT模型的启发，我们提出了一个从Transformer学习草图双向编码器表示的模型（草图BERT）。我们将BERT推广到素描领域，提出了新的组件和预训练算法，包括新设计的素描嵌入网络和素描格式塔的自监督学习。特别地，针对训练前的任务，我们提出了一种新的素描格式塔模型（SGM）来帮助训练素描。实验表明，草图的学习表示可以帮助和提高草图识别、草图检索和草图格式塔等下游任务的性能。</pre></li>
<li><a href="https://arxiv.org/abs/2008.04057">The Chess Transformer: Mastering Play using Generative Language Models</a>
<pre>这项工作表明，自然语言转换器可以支持更通用的战略建模，尤其是文本存档游戏。除了学习自然语言技能外，抽象变压器架构还可以在棋盘上生成有意义的动作。通过进一步的微调，transformer通过在280万个棋类游戏中使用便携式游戏符号进行训练来学习复杂的游戏性。经过30000个训练步骤后，OpenAI的生成式预训练转换器（GPT-2）优化了7.74亿个参数的权重。这台经过微调的国际象棋转换器可以生成合理的策略，并显示可识别为经典开场白的游戏队形，如英语或斯拉夫语交换。最后，在实战中，新模型展示了一个人对变压器的界面，该界面可以正确过滤非法移动，并提供了一种挑战变压器国际象棋策略的新方法。我们预计未来的工作将建立在这个转换器的承诺之上，特别是在其他战略游戏中，在这些游戏中，功能可以从简单但富有表现力的玩家注释中捕获潜在的复杂规则语法。</pre></li>
<li><a href="https://arxiv.org/abs/2007.03500">The Go Transformer: Natural Language Modeling for Game Play</a>
<pre>这项工作应用自然语言建模来生成古代围棋游戏中看似合理的战略动作。我们训练生成型预训练转换器（GPT-2）模仿以智能游戏格式（SGF）存档的围棋冠军的风格，该格式提供了移动序列的文本描述。经过训练的模型进一步生成了有效但以前看不见的围棋策略。由于GPT-2保留标点和间距，文本生成器的原始输出为游戏可视化和创造性模式提供输入，例如使用自动重放的Sabaki项目的游戏引擎。结果表明，语言建模可以捕获锦标赛围棋游戏的顺序格式和它们的策略形式。与随机游戏板相比，GPT-2微调显示了有效的开局顺序，有利于角球，而不是不太有利的中锋和边锋。游戏生成作为一项语言建模任务，为40多个其他棋盘游戏提供了新颖的方法，其中历史文本注释提供了训练数据（例如，Amazons&Connect 4/6）。</pre></li>
<li><a href="https://arxiv.org/abs/2001.00781">On the comparability of Pre-trained Language Models</a>
<pre>无监督表征学习的最新发展成功地确立了NLP中迁移学习的概念。主要有三种力量推动了这一研究领域的改进：更精细的体系结构更好地利用了上下文信息。它们不是简单地插入静态的预先训练的表示法，而是基于端到端可训练模型中的环境学习，具有更智能化的语言建模目标。除此之外，更大的语料库被用作资源，用于以自我监督的方式对大型语言模型进行预训练，然后对监督任务进行微调。并行计算和云计算的进步，使得在相同甚至更短的时间内对这些模型进行训练成为可能。这三项发展凝聚在新的最新技术（SOTA）结果中，并以越来越高的频率显示出来。这些改进的来源并不总是显而易见的，因为不可能完全理清三种驱动力的作用。我们致力于提供几个大型预培训语言模型的清晰、简明概述，这些模型在过去两年中在使用新架构和资源方面取得了SOTA成果。我们希望向读者澄清模型之间的差异，并试图进一步深入了解词法/计算改进以及架构更改的单一贡献。我们明确不打算量化这些贡献，而是将我们的工作视为一个概述，以确定基准比较的潜在起点。此外，我们暂时希望指出在开源和可复制研究领域改进的潜在可能性。</pre></li>
<li><a href="https://arxiv.org/abs/1910.03771">Transformers: State-of-the-art Natural Language Processing</a>
<pre>模型体系结构和模型预训练的发展推动了自然语言处理的最新进展。变压器结构有助于构建更高容量的模型，而预培训使其能够有效地将此容量用于各种任务\textit{Transformers}是一个开源库，目标是向更广泛的机器学习社区开放这些进步。该库包含在统一API下精心设计的最先进的Transformer架构。支持这个图书馆的是一个由社区制作并可供社区使用的预训练模型的策展收藏\textit{Transformers}被设计为可由研究人员扩展，对从业者来说简单，并且在工业部署中快速而健壮。该库位于\url{https://github.com/huggingface/transformers}.</pre></li>
<li><a href="https://arxiv.org/abs/2004.08900">The Cost of Training NLP Models: A Concise Overview</a>
<pre>我们回顾了培训大规模语言模型的成本，以及这些成本的驱动因素。目标受众包括预算模型训练实验的工程师和科学家，以及试图理解现代自然语言处理（NLP）经济学的非实践者。</pre></li>
</ul>
</body>
</html>
