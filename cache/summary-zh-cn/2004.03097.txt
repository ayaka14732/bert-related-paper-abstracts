最近，由于其有效性和通用性，BERT已成为各种NLP深度模型的重要组成部分。然而，BERT的在线部署往往因其庞大的参数和较高的计算成本而受阻。大量研究表明，知识提取能够有效地将知识从BERT转移到参数较小的模型中。然而，目前的BERT提取方法主要集中在任务特定的提取上，这种方法导致了通用性BERT的一般语义知识的丢失。在本文中，我们提出了一个面向提取的句子表示近似框架，该框架可以在不指定任务的情况下将预先训练好的BERT提取到一个简单的基于LSTM的模型中。与BERT一致，我们的蒸馏模型能够通过微调执行迁移学习，以适应任何句子级下游任务。此外，我们的模型可以进一步配合特定任务的蒸馏过程。GLUE基准测试中的多个NLP任务的实验结果表明，我们的方法优于其他特定任务的蒸馏方法或更大的模型，即ELMO，效率得到了很好的提高。