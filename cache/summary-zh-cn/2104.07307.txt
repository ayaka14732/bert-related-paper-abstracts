文本数字推理（NRoT）提出了独特的挑战，但现有的培训前目标并未很好地解决这些挑战。我们探索了五种顺序训练计划，它们适用于NRoT的预训练T5模型。我们的最终模型改编自T5，但在对文本离散推理（DROP）数据集进行微调之前，对三个数据集进行了进一步的预训练，旨在加强NRoT和一般阅读理解所需的技能。这项训练将DROP调整后的F1成绩（专注于计算的分数）从45.90提高到70.83。我们的模型接近GenBERT（72.4），这是一个自定义的基于BERT的模型，使用相同的数据集，参数显著增加。我们表明，使用多个难度越来越大的数值推理数据集训练T5多任务框架，可以在分布式模块和符号模块之间不需要手动工程划分功能的情况下实现良好的DROP性能。