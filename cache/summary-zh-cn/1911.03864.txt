多层变压器网络由交错的自注意子层和前馈子层组成。以不同的模式排列子层是否会导致更好的性能？我们生成随机排序的变压器，并使用语言建模目标对其进行训练。我们观察到，其中一些模型能够实现比交错基线更好的性能，并且那些成功的变体往往在底部有更多的自我关注，在顶部有更多的前馈子层。我们提出了一种新的转换器模式，即三明治转换器，并表明它在不消耗参数、内存或训练时间的情况下，改善了多单词级和字符级语言建模基准的复杂性。然而，正如我们在机器翻译模型上所展示的那样，三明治重新排序模式并不能保证每个任务的性能提高。相反，我们建议需要进一步探索任务特定的子层重新排序，以释放额外的收益。