在本文中，我们发现Devlin et al.（2018）发布的多语言BERT（M-BERT）作为从104种语言的单语语料库中预先训练的单一语言模型，在零镜头跨语言模型迁移方面出人意料地出色，其中，使用一种语言中特定于任务的注释来微调模型，以便使用另一种语言进行评估。为了理解其中的原因，我们进行了大量的探索性实验，结果表明，即使对不同脚本中的语言，迁移也是可能的，在类型相似的语言之间，迁移效果最好，单语语料库可以训练代码转换模型，并且模型可以找到翻译对。从这些结果中，我们可以得出结论，M-BERT确实创建了多语言表示，但这些表示显示出影响某些语言对的系统缺陷。