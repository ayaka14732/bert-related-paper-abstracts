本文介绍了SimCSE，一个简单的对比学习框架，它极大地促进了最先进的句子嵌入。我们首先描述了一种无监督的方法，该方法采用输入句子并在对比目标中预测自身，仅使用标准差作为噪声。这个简单的方法令人惊讶地很好地完成，与以前的监督同行一致。我们发现，辍学充当了最小的数据扩充，删除它会导致表示崩溃。然后，我们提出了一种有监督的方法，该方法将自然语言推理数据集中的注释对纳入我们的对比学习框架，使用“蕴涵”对作为肯定词，“矛盾”对作为硬否定词。我们在标准语义文本相似性（STS）任务中评估SimCSE，我们使用BERT-base的无监督和监督模型分别实现了76.3%和81.6%的Spearman相关性，与之前的最佳结果相比，分别提高了4.2%和2.2%。我们还从理论和经验上证明，对比学习目标将预训练嵌入的各向异性空间正则化，使其更加均匀，并且在有监督信号可用时，它可以更好地对齐正对。