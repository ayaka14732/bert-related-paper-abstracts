众所周知，大型语言模型（LMs）在其参数中对世界知识进行编码，因为它们对大量web语料库进行预训练，通常用于执行与知识相关的下游任务，如问答、事实检查和开放对话。在现实场景中，存储在LMs中的世界知识可能会随着世界的变化而迅速过时，但要避免灾难性遗忘并可靠地获取新知识，同时保持不变知识是非常重要的。为了推动社区更好地维护不断变化的LMs，我们提出了一个新的持续学习（CL）问题，称为持续知识学习（CKL）。我们构建了一个新的基准和度量来量化时不变世界知识的保留、过时知识的更新和新知识的获取。我们采用文献中适用的最新方法来创建几个强基线。通过大量实验，我们发现CKL具有独特的