预训练多语言模型（PMM）通过跨语言迁移实现零镜头学习，在预训练期间表现最好。虽然有一些方法可以提高看不见的语言的性能，但它们几乎完全是使用世界上一小部分语言的原始文本进行评估的。在本文中，我们使用一个可用于1600多种语言的资源：新约，来评估现有方法使PMM适应新语言的性能。这具有挑战性，原因有两个：（1）语料库规模小，（2）领域狭窄。尽管所有方法的性能都有所下降，但令人惊讶的是，与XLM-R相比，在所有语言中，词性标注的准确率高达17.69\%$，NER的平均准确率高达6.29$。另一个意外的发现是，最简单的持续预训练方法的性能最好。最后，我们进行了一个案例研究，以理清域和大小的影响，并阐明微调源语言的影响。