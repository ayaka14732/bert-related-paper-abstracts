众所周知，dot产品自我关注是最先进变压器型号的核心和不可或缺的部分。但这真的需要吗？本文研究了基于点积的自我注意机制对变压器模型性能的真正重要性和贡献。通过大量实验，我们发现：（1）随机对齐矩阵令人惊讶地表现出相当的竞争力；（2）从令牌（查询键）交互中学习注意权重是有用的，但毕竟不是那么重要。为此，我们提出\textsc{Synthesizer}，这是一种学习合成注意权重的模型，无需令牌交互。在我们的实验中，我们首先展示了简单合成器在一系列任务（包括机器翻译、语言建模、文本生成和GLUE/SuperGLUE基准测试）中，与vanilla Transformer模型相比，能够获得极具竞争力的性能。当用点积注意力合成时，我们发现合成器始终优于变压器。此外，我们还对合成器与动态卷积进行了额外的比较，结果表明，简单的随机合成器不仅比动态卷积快60\%$，而且相对提高了3.5\%$的复杂度。最后，我们展示了简单因子合成器在仅编码任务上的性能优于LINFORMER。