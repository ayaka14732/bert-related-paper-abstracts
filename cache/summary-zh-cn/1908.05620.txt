语言模型预训练，如BERT，在许多NLP任务中取得了显著的效果。然而，尚不清楚为什么预训练-然后微调范式可以提高不同任务的性能和泛化能力。在本文中，我们建议在特定数据集上可视化损失景观和微调BERT的优化轨迹。首先，我们发现，与从头开始的培训相比，预培训在下游任务中达到了一个良好的初始点，这导致了更广泛的优化和更容易的优化。我们还证明了微调过程对过度拟合的鲁棒性，即使对于下游任务，BERT是高度过度参数化的。第二，可视化结果表明，由于训练损失面和泛化误差面之间的一致性以及平坦、宽的最优解，微调BERT更易于泛化。第三，在微调过程中，BERT的较低层更具不变性，这表明靠近输入的层可以学习更多可转换的语言表示。