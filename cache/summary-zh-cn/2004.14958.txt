我们回顾了无监督跨语言学习的动机、定义、方法和方法，并呼吁对每一种方法采取更严格的立场。这种研究的一个现有理由是，世界上许多语言缺乏平行数据。然而，我们认为，没有任何平行数据和丰富的单语数据的情况在实践中是不现实的。我们还讨论了以前工作中使用的不同的训练信号，这些信号与纯无监督设置不同。然后，我们描述了无监督跨语言模型的调整和评估中常见的方法学问题，并介绍了最佳实践。最后，我们为这一领域的不同类型的研究（即跨语言单词嵌入、深度多语言预训练和无监督机器翻译）提供了一个统一的前景，并主张对这些模型进行比较评估。