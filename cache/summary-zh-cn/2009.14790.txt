反向词典的任务是根据单词描述找到合适的目标单词。在本文中，我们试图将BERT纳入这项任务中。然而，由于BERT是基于字节对编码（BPE）子字编码的，因此让BERT生成给定描述的字是非常重要的。我们提出了一种简单而有效的方法，让BERT为这个特定任务生成目标词。此外，跨语言反向词典的任务是找到用另一种语言描述的合适的目标词。以前的模型必须保留两个不同的单词嵌入，并学会对齐这些嵌入。然而，通过使用多语言BERT（Multilingual BERT，mBERT），我们可以有效地进行跨语言反向词典的一个子词嵌入，并且不需要语言之间的对齐。更重要的是，即使没有平行语料库，mBERT也可以实现显著的跨语言反向词典性能，这意味着它可以仅使用相应的单语数据来执行跨语言反向词典。该守则可于https://github.com/yhcc/BertForRD.git.