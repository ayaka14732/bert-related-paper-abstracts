近年来，自然语言处理（NLP）和信息检索（IR）领域取得了巨大进展，这得益于递归神经网络（RNN）、选通递归单元（GRU）和长短时记忆（LSTMs）网络等深度学习模型，以及Transformer[120]基于变压器的双向编码器表示（BERT）[24]、生成预训练变压器（GPT-2）[94]、多任务深度神经网络（MT-DNN）[73]、超长网络（XLNet）[134]、文本到文本传输变压器（T5）[95]、T-NLG[98]和GShard[63]等模型。但是这些模型的尺寸很大。另一方面，现实世界的应用要求模型尺寸小、响应时间短和计算功率低。在这篇综述中，我们讨论了六种不同类型的方法（剪枝、量化、知识提炼、参数共享、张量分解和基于次二次变换的方法）用于压缩此类模型，以便在实际行业NLP项目中进行部署。考虑到使用高效小型模型构建应用程序的迫切需要，以及该领域最近发表的大量工作，我们相信这项调查组织了“NLP深度学习”社区在过去几年所做的大量工作，并将其作为一个连贯的故事呈现出来。