我们介绍了一种简单而有效的方法，将上下文嵌入与常识图嵌入相结合，称为BERT注入图：与其他嵌入相匹配。首先，我们引入一种预处理方法来提高查询知识库的速度。然后，我们开发了一种从每个知识库创建知识嵌入的方法。我们介绍了一种在两种未对齐的标记化方法之间对齐标记的方法。最后，结合知识库嵌入，提出了一种基于知识库嵌入的文本化方法。我们还显示了BERTs倾向于纠正精度较低的问题类型。我们的模型实现了比BERT更高的准确性，我们在共享任务的官方排行榜上排名第五，在没有任何额外语言模型预训练的情况下得分最高。