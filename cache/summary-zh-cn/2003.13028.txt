预先训练的序列到序列（seq-to-seq）模型显著提高了一些语言生成任务的准确性，包括抽象摘要。尽管通过对这些模型进行微调，抽象摘要的流畅性得到了极大的提高，但尚不清楚它们是否能够识别出摘要中要包含的源文本的重要部分。在这项研究中，我们通过大量实验研究了将识别源文本重要部分的显著性模型与预先训练的seq-to-seq模型相结合的有效性。我们还提出了一种新的组合模型，包括从源文本中提取标记序列的显著性模型和将该序列作为额外输入文本的seq-to-seq模型。实验结果表明，即使seq-to-seq模型在大规模语料库上预先训练，大多数组合模型在CNN/DM和XSum数据集上都优于简单的微调seq-to-seq模型。此外，对于CNN/DM数据集，建议的组合模型在ROUGE-L上比之前的最佳执行模型高1.33个点。