自然语言理解（NLU）的最新进展推动了信息检索（IR）的快速发展，这在很大程度上归功于用于文档排名的微调深度语言模型（LMs）。虽然非常有效，但基于这些LMs的排名模型比以前的方法增加了几个数量级的计算成本，特别是因为它们必须通过大规模神经网络为每个查询文档对提供数据，以计算单个相关性得分。为了解决这个问题，我们提出了ColBERT，一种新的排序模型，它采用了深度LMs（特别是BERT）来实现高效检索。ColBERT引入了一种后期交互体系结构，该体系结构使用BERT对查询和文档进行独立编码，然后采用一种廉价但功能强大的交互步骤对它们的细粒度相似性进行建模。通过延迟并保留这种细粒度的交互，ColBERT可以利用深度LMs的表达能力，同时获得离线预计算文档表示的能力，从而大大加快查询处理。除了降低对传统模型检索的文档重新排序的成本外，ColBERT的修剪友好交互机制还可以利用向量相似性索引直接从大型文档集合进行端到端检索。我们使用两个最近的文章搜索数据集对科尔伯特进行了广泛的评估。结果表明，ColBERT的有效性与现有的基于BERT的模型具有竞争性（并且优于每个非BERT基线），同时执行速度快两个数量级，每个查询所需的失败次数少四个数量级。