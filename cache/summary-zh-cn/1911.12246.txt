我们研究了在预训练的变形金刚语言模型（如BERT和RoBERTa）中，个体注意头隐式捕捉句法依赖关系的程度。我们采用两种方法——取最大注意权重和计算最大生成树——从每个层/头的注意权重中提取隐式依赖关系，并将其与基本真理通用依赖（UD）树进行比较。我们发现，对于某些UD关系类型，存在能够比解析英语文本上的基线更好地恢复依赖类型的头部，这表明一些自我注意头部充当句法结构的代理。我们还对两个数据集——面向语法的COA和面向语义的MNLI——进行了微调分析，以研究微调是否会影响他们的自我注意模式，但我们没有观察到使用我们的方法提取的总体依赖关系存在实质性差异。我们的结果表明，这些模型有一些跟踪个别依赖类型的专家注意头，但没有一个通才注意头能够比普通基线更好地执行整体解析，而且，直接分析注意力权重可能不会揭示出伯特式模型所能学到的很多语法知识。