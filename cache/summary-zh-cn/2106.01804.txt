大规模图像-文本对的视觉语言预训练（VLP）在跨模态下游任务中取得了巨大成功。现有的预训练方法主要采用两步训练法，首先使用预训练的目标检测器提取基于区域的视觉特征，然后将图像表示和文本嵌入连接起来作为变压器的输入进行训练。然而，这些方法面临着使用特定对象检测器的任务特定视觉表示进行通用跨模态理解的问题，以及两级流水线的计算效率低下的问题。在本文中，我们提出了第一个用于V+L理解和生成的端到端视觉语言预训练模型，即E2E-VLP，在该模型中，我们构建了一个统一的转换框架来共同学习视觉表示以及图像和文本之间的语义对齐。我们采用统一的Transformer编码器-解码器体系结构，将目标检测和图像字幕纳入预训练任务，以增强视觉学习。一组广泛的实验已经在成熟的视觉语言下游任务上进行，以证明这种新的VLP范式的有效性。