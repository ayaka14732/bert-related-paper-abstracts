针对基于方面的情绪分析（ABSA）中的任务，分析了从BERT评论中学习到的预训练隐藏表示。我们的工作受到ABSA基于BERT语言模型的最新进展的推动。然而，目前尚不清楚在未标记语料库上训练的（蒙面）语言模型的一般代理任务（proxy task of）如何为ABSA中的下游任务提供重要特征。通过利用ABSA中的注释数据集，我们研究了在评论中预先训练的BERT的注意和学习表示。我们发现，BERT很少使用自我注意头来编码上下文词（如表示方面的介词或代词）和观点词。方面表示中的大多数特性都致力于领域（或产品类别）和方面本身的细粒度语义，而不是从其上下文中携带总结的观点。我们希望这项研究能对改进ABSA的自监督学习、无监督学习和微调的未来研究有所帮助。预先培训的模型和代码可在https://github.com/howardhsu/BERT-for-RRC-ABSA.