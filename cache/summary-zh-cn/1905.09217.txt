神经网络为自动学习复杂的语言模式和查询文档关系提供了新的可能性。神经IR模型在学习查询文档相关模式方面取得了很好的效果，但在理解查询或文档的文本内容方面却很少有人做过探索。本文研究利用最近提出的上下文神经语言模型BERT，为IR提供更深入的文本理解。实验结果表明，BERT的上下文文本表示比传统的单词嵌入更有效。与单词袋检索模型相比，上下文语言模型可以更好地利用语言结构，大大改进了用自然语言编写的查询。将文本理解能力与搜索知识相结合，可以得到一个增强的预训练BERT模型，该模型可以在训练数据有限的情况下对相关的搜索任务有所帮助。