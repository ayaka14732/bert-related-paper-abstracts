本文研究在语境空间中学习跨语言表征的问题。我们提出了跨语言BERT变换（CLBT），这是一种基于公开的预训练BERT模型生成跨语言语境化单词嵌入的简单有效方法（Devlin等人，2018）。在这种方法中，从上下文词对齐中学习线性变换，以对齐用不同语言独立训练的上下文化嵌入。我们证明了这种方法在零炮跨语言迁移分析中的有效性。实验表明，我们的嵌入大大优于以前使用静态嵌入的最新技术。我们进一步将我们的方法与XLM（Lample和Conneau，2019）进行了比较，XLM是最近提出的一种跨语言语言模型，使用大量并行数据进行训练，并取得了极具竞争力的结果。