使用蒙面语言模型（MLM）目标的变体对NLP模型进行预训练，最近在许多任务上取得了显著的改进。本文考察了作为下游任务中使用的训练样本数量函数的预训练模型的好处。在几个文本分类任务中，我们发现，随着训练示例的数量增加到数百万，基于finetunning-BERT的模型和训练vanilla-LSTM之间的精度差距从无到有地缩小到1%以内。我们的研究结果表明，基于传销的模型可能会达到一个递减的回报点，因为监督数据的大小显著增加。