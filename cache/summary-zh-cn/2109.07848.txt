当一个句子的开头与多种句法分析兼容时，会出现暂时的句法歧义。我们检验了在处理暂时模糊的输入时，神经语言模型（LMs）在何种程度上表现出这种分析的不确定性，以及这种不确定性是如何通过消除歧义的线索来调节的。我们通过从中生成来探测LM的期望：我们使用随机解码来导出一组句子完成，并根据完成过程中解析的分布来估计LM分配给每个解释的概率。与基于评分的有针对性的句法评估方法不同，这种技术使研究人员能够探索未事先假设的补全。我们运用这种方法，利用人类句子处理实验的材料，研究了两种LMs（GPT2和LSTM）在三种类型的暂时歧义上的行为。我们发现LMs可以同时跟踪多个分析；不确定性的程度因结构和上下文而异。作为对消除歧义线索的回应，LMs通常选择正确的解释，但偶尔出现的错误指出了潜在的改进领域。