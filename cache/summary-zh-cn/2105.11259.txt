经过微调的预训练语言模型（PLM）在几乎所有NLP任务上都取得了令人敬畏的性能。通过使用额外的提示来微调PLMs，我们可以进一步激发PLMs中分布的丰富知识，以便更好地为下游任务服务。在情感分类和自然语言推理等少数类别分类任务中，即时调优取得了令人满意的结果。然而，手动设计大量的语言提示既麻烦又容易出错。对于那些自动生成的提示，在非少镜头场景中验证它们的有效性也是昂贵和耗时的。因此，及时调整以解决许多类分类任务仍然是一个挑战。为此，我们提出了针对多类文本分类的规则提示调优（PTR），并应用逻辑规则构造具有多个子提示的提示。通过这种方式，PTR能够将每个类的先验知识编码到即时调优中。我们在关系分类这一典型且复杂的多类分类任务上进行了实验，结果表明，PTR可以显著且持续地优于现有的最新基线。这表明PTR是一种很有前途的方法，可以利用人类的先验知识和PLMs来完成那些复杂的分类任务。