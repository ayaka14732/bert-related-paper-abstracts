尽管诸如Bert和XLNet等预先训练的语言模型在许多自然语言处理任务中迅速提高了技术水平，但它们仅依赖于语料库中单词之间的表面信息来隐含语义。直觉上，背景知识会影响理解的效果。受这一常识的启发，我们专注于通过利用显性知识改进模型预训练。与最近通过知识掩蔽策略优化预训练模型的研究不同，我们提出了一种简单而通用的方法，将显性知识与预训练相结合。具体地说，我们首先从知识图（KG）中匹配知识事实，然后直接向transformer添加一个知识禁令层，而不改变其架构。本研究旨在发现显性知识对培训的直接影响。我们针对不同的下游任务在不同的数据集上进行实验。实验结果表明，仅通过向transformer添加外部知识，就可以提高许多NLP任务的学习性能。