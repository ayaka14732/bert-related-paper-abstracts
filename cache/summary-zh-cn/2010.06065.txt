本文考虑了零炮实体链接问题，其中测试时间内的链接可能不存在于训练中。在基于BERT的研究成果的基础上，我们发现一种简单而有效的方法是扩展长程序列建模。与以前的许多方法不同，我们的方法不需要对具有长位置嵌入的BERT进行昂贵的预训练。相反，我们提出了一种有效的位置嵌入初始化方法，称为嵌入重复，它基于BERT基初始化较大的位置嵌入。在Wikia的零炮EL数据集上，我们的方法将SOTA从76.06%提高到79.08%，对于其长数据，相应的提高从74.57%提高到82.14%。我们的实验表明，在不重新训练伯特模型的情况下，长程序列建模是有效的。