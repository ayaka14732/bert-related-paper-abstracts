Mixup是最新的数据增强技术，它线性插值输入示例和相应的标签。通过在像素级插值图像，它在图像分类中显示了强大的有效性。受这一研究方向的启发，在本文中，我们将探讨i）如何将混合应用于自然语言处理任务，因为文本数据很难以原始格式混合；ii）如果混音在基于变压器的学习模型中仍然有效，例如，BERT。为了实现这一目标，我们将基于mixup-To-transformer的预训练体系结构（名为“mixup-transformer”）整合到一个广泛的NLP任务中，同时保持整个端到端训练系统。我们通过在GLUE基准上运行大量实验来评估所提出的框架。此外，我们还通过以一定比例减少训练数据来检验混音转换器在低资源场景中的性能。我们的研究表明，对于预先训练的语言模型，混搭是一种与领域无关的数据增强技术，从而显著提高了基于transformer的模型的性能。