基于转换器的模型是自然语言理解（NLU）应用的最新技术。模型在各种任务上越来越大，越来越好。然而，变压器模型在计算上仍然具有挑战性，因为与传统方法相比，它们在推理时效率不高。在本文中，我们提出了FastFormers，这是一组方法，用于在各种NLU任务中实现基于Transformer的模型的高效推理时间性能。我们展示了如何小心地利用知识提取、结构化剪枝和数值优化可以极大地提高推理效率。我们提供有效的配方，可以指导从业者为各种NLU任务和预训练模型选择最佳设置。将建议的配方应用到SuperGLUE基准测试中，与CPU上的现成机型相比，我们实现了9.8倍到233.9倍的速度提升。在GPU上，我们还使用所提出的方法实现了高达12.4倍的速度提升。我们表明，在Azure F16s_v2实例上，FastFormers可以将服务1亿个请求的成本从4223美元大幅降低到18美元。根据SustaiNLP 2020共享任务中使用的指标，将能耗降低6.9倍-125.8倍，从而实现可持续运行。