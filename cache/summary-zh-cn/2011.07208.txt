近年来，人们发现，为大型数据集中的语言建模任务预先训练一个基于转换器的模型，然后为下游任务对其进行微调非常有用。这种预先训练的语言模型的一个主要优点是，它们可以有效地吸收句子中每个单词的上下文。然而，对于诸如答案选择任务这样的任务，预先训练的语言模型还没有被广泛使用。为了研究它们在此类任务中的有效性，本文采用了来自Transformer（BERT）语言模型的预训练双向编码器表示，并在两个问答（QA）数据集和三个社区问答（CQA）数据集上对其进行微调，以完成答案选择任务。我们发现，对回答选择任务的BERT模型进行微调非常有效，与之前的最新技术相比，QA数据集和CQA数据集的最大改善率分别为13.1%和18.7%。