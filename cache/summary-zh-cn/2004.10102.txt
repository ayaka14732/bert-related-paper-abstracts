注意力是Transformers的关键组成部分，Transformers最近在自然语言处理方面取得了相当大的成功。因此，人们对注意力进行了广泛的研究，以调查变形金刚的各种语言能力，重点是分析注意力权重与特定语言现象之间的相似性。本文表明，注意权重只是决定注意输出的两个因素之一，并提出了一种基于范数的分析方法，该方法结合了第二个因素，即转换输入向量的范数。我们对伯特和基于变压器的神经机器翻译系统的基于规范的分析的结果包括：（i）与先前的研究相反，伯特对特殊令牌的关注不够，并且（ii）可以从变压器的注意机制中提取合理的词对齐。这些发现提供了变压器内部工作的见解。