最近基于转换器的上下文单词表示，包括BERT和XLNet，在NLP的多个学科中显示了最先进的性能。在特定于任务的数据集上微调经过培训的上下文模型一直是实现优异性能的关键。虽然对这些预先训练好的模型进行微调对于词汇应用程序（仅具有语言模态的应用程序）来说非常简单，但对于多模态语言（NLP中的一个日益增长的领域，重点是对面对面交流进行建模）来说，这并不是一件小事。预先训练过的模特没有必要的组件来接受视觉和听觉两种额外的模式。在本文中，我们提出了一种连接到BERT和XLNet的多模自适应门（MAG）。MAG允许BERT和XLNet在微调期间接受多模态非语言数据。它通过转换到BERT和XLNet的内部表示来实现这一点；以视觉和听觉方式为条件的转变。在我们的实验中，我们研究了用于多模态情绪分析的常用CMU-MOSI和CMU-MOSEI数据集。微调MAG-BERT和MAG XLNet显著提高了情感分析的性能，超过了之前的基线以及仅语言微调的BERT和XLNet。在CMU-MOSI数据集上，MAG XLNet首次在NLP社区实现了人的多模态情感分析性能。