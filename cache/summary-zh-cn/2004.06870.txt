像BERT这样的语言表示模型能够有效地从纯文本中获取上下文语义信息，并且已经被证明在许多下游NLP任务中通过适当的微调可以获得有希望的结果。然而，大多数现有的语言表征模型不能明确地处理共指现象，这对于连贯理解整个语篇至关重要。为了解决这个问题，我们提出了CorefBERT，一种新的语言表示模型，它可以捕捉上下文中的相关关系。实验结果表明，与现有的基线模型相比，CorefBERT可以在需要协同推理的各种下游NLP任务上取得一致的显著改进，同时在其他常见NLP任务上保持与先前模型相当的性能。本文的源代码和实验细节可以从https://github.com/thunlp/CorefBERT.