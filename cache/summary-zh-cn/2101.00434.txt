预训练语言模型的引入将许多复杂的特定于任务的NLP模型简化为简单的轻量级层。这种趋势的一个例外是共指消解，在预训练的transformer编码器中附加了复杂的特定于任务的模型。该模型虽然非常有效，但内存占用非常大，这主要是由于动态构造的跨距和跨距对表示，这妨碍了完整文档的处理以及在单个批处理中对多个实例进行训练的能力。我们引入了一个轻量级的端到端协同引用模型，该模型消除了对跨度表示、手工制作的特性和启发式的依赖。我们的模型与当前的标准模型相比具有竞争力，同时更简单、更高效。