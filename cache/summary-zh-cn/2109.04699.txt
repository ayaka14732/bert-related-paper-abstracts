虽然大规模的学前培训在弥合视力和语言之间的差距方面取得了巨大成就，但它仍然面临一些挑战。首先，预培训的成本很高。其次，没有有效的方法来处理数据噪声，这会降低模型性能。第三，以前的方法只利用有限的图像-文本配对数据，而忽略了更丰富的单模态数据，这可能导致对单模态下游任务的泛化能力较差。在这项工作中，我们提出了一种通过集成自信学习获得低噪声数据子集的高效IP方法。超丰富的非成对单模态文本数据用于增强文本分支的泛化。与CLIP和WenLan相比，我们仅使用1/10的训练资源就实现了中文跨模态检索任务的最新性能，同时对单模态任务（包括文本检索和文本分类）表现出了出色的泛化能力。