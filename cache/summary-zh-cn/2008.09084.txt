最近的许多工作表明，合并依赖树中的语法信息可以改进特定于任务的转换器模型。然而，将依赖树信息合并到预先训练的转换器模型（例如，BERT）中的效果仍然不清楚，特别是考虑到最近强调这些模型如何隐式编码语法的研究。在这项工作中，我们系统地研究了将依赖树合并到预先训练好的变换器中对三个具有代表性的信息提取任务的效用：语义角色标记（SRL）、命名实体识别和关系提取。我们提出并研究了两种不同的合并依赖结构的策略：后期融合方法（将图形神经网络应用于转换器的输出）和联合融合方法（将语法结构注入转换器注意层）。这些策略代表了先前的工作，但我们引入了额外的模型设计元素，这些元素对于获得更好的性能是必要的。我们的实证分析表明，这些注入语法的转换器在SRL和关系提取任务上获得了最先进的结果。然而，我们的分析也揭示了这些模型的一个关键缺点：我们发现，它们的性能提高在很大程度上取决于人类注释依赖解析的可用性，这就提出了关于语法增强转换器在实际应用中的可行性的重要问题。