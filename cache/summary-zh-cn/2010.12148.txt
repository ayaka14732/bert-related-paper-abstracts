粗粒度的语言信息，如命名实体或短语，有助于在预训练中进行充分的表征学习。以前的工作主要集中在将BERT的掩蔽语言建模（MLM）的目标从掩蔽单个令牌扩展到n个令牌的连续序列。我们认为这种连续掩蔽方法忽略了对粗粒度语言信息的内部依赖和相互关系的建模。作为替代方案，我们提出了ERNIE-Gram，一种显式n-Gram掩蔽方法，以增强粗粒度信息与预训练的集成。在ERNIE-Gram中，n-Gram直接使用显式n-Gram恒等式而不是n个令牌的连续序列来屏蔽和预测。此外，ERNIE-Gram使用生成器模型对合理的n-Gram身份进行采样，作为可选的n-Gram掩码，并以粗粒度和细粒度方式对其进行预测，以实现全面的n-Gram预测和关系建模。我们对ERNIE Gram进行了中英文文本语料库的预训练，并对19项下游任务进行了微调。实验结果表明，ERNIE-Gram在很大程度上优于以前的预训练模型，如XLNet和RoBERTa，并与最先进的方法取得了相当的结果。源代码和预先培训的模型已在https://github.com/PaddlePaddle/ERNIE.