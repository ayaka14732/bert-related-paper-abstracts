语言模型（如BERT、XLNet等）的最新进展使得在阅读理解等复杂NLP任务上的表现超过了人类。然而，用于培训的标记数据集大多以英语提供，这使得很难确认其他语言的进步。幸运的是，模型现在已经在数百种语言的未标记数据上进行了预训练，并展示了有趣的从一种语言到另一种语言的转换能力。在本文中，我们证明了多语言BERT自然能够将抽取式问答任务（eQA）从英语迁移到其他语言。更具体地说，它的表现优于此前最为人所知的转移到日本和法国的基线。此外，使用最近发布的大型eQA法语数据集，我们能够进一步证明：（1）零射击转移提供的结果非常接近于目标语言的直接训练；（2）转移和目标训练相结合是总体上的最佳选择。最后，我们给出了一个实际应用：一个名为Kate的多语言会话代理，它可以直接从内部网页面的内容中用多种语言回答与人力资源相关的问题。