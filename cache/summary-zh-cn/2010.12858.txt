基于大量原始数据的预训练语言模型的迁移学习已成为NLP中达到最先进水平的新规范。然而，对于任何可用的大规模多语种语言模型都没有涵盖的、通常只有少量原始数据可用的、看不见的语言，如何应用这种方法仍不清楚。在这项工作中，通过比较多语言和单语模型，我们证明了这种模型在看不见的语言上以多种方式表现。一些语言从迁移学习中受益匪浅，其行为与密切相关的高资源语言相似，而另一些语言显然没有。关注后者，我们表明，这种传输失败在很大程度上与用于编写此类语言的脚本的影响有关。对这些语言进行音译可以极大地提高大规模多语言模型处理下游任务的能力。