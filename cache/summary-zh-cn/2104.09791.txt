预训练和微调在许多下游自然语言处理（NLP）任务中取得了显著的成功。最近，为信息检索（IR）量身定制的预训练方法也得到了探索，最新的成功是PROP方法，它在各种特定检索基准上达到了新的SOTA。PROP的基本思想是构造\textit{representative words prediction}（ROP）任务，用于受查询似然模型启发的预训练。尽管PROP的性能令人兴奋，但其有效性可能受到ROP任务构造过程中采用的经典单语言模型的限制。为了解决这个问题，我们提出了一种基于BERT的自举预训练方法（即B-PROP），用于adhoc检索。其关键思想是使用强大的上下文语言模型BERT来代替经典的单语言模型来构建ROP任务，并将BERT自身重新训练到为IR定制的目标上。具体来说，我们引入了一种新的对比方法，受随机性思想的启发，利用BERT的自我注意机制从文档中抽取代表性单词。通过对下游ad-hoc检索任务的进一步微调，我们的方法在没有预训练或使用其他预训练方法的情况下实现了基线的显著改进，并进一步推动了SOTA在各种ad-hoc检索任务上的应用。