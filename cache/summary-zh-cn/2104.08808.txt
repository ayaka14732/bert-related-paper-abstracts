随着时间的推移不断扩展知识并利用知识快速概括到新任务的能力是人类语言智能的一个关键特征。然而，追求对新任务快速概括的现有模型（例如，很少有镜头学习方法）大多在固定数据集上进行单镜头训练，无法动态扩展其知识；而连续学习算法并不是专门为快速泛化而设计的。我们提出了一种新的学习设置，即少数镜头学习者的持续学习（CLIF），以解决统一设置中两种学习设置的挑战。CLIF假设模型从顺序到达的不同NLP任务序列中学习，积累知识以改进对新任务的泛化，同时保持先前学习的任务的性能。我们研究了在连续学习环境中泛化能力是如何受到影响的，评估了许多连续学习算法，并提出了一种新的正则化适配器生成方法。我们发现，灾难性遗忘对泛化能力的影响程度小于对所见任务的影响程度；而连续学习算法仍然可以给泛化能力带来相当大的好处。