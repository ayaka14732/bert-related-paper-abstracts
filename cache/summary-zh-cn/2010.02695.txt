虽然已经进行了大量分析，以证明在深层NLP模型中学习到的表征所捕获的语言知识，但很少关注单个神经元。我们使用预测形态学、语法和语义的核心语言任务，在预先训练的语言模型上进行神经元级分析，有这样的问题：i）预训练模型中的单个神经元是否捕捉到语言信息？ii）网络的哪些部分可以更多地了解某些语言现象？iii）信息的分布或集中程度如何？和iv）不同的体系结构在学习这些属性方面有什么不同？我们发现了预测语言任务的小神经元亚群，与预测语法的高水平任务相比，低水平任务（如形态学）局限于较少的神经元。我们的研究还揭示了有趣的跨架构比较。例如，我们发现XLNet中的神经元在预测属性时比BERT和其他神经元更局部化和不相交，因为它们更分散和耦合。