经过预先训练的深层语境表征已经在各种常识NLP任务上提升了最新水平，但我们对这些模型的能力缺乏具体的理解。因此，我们调查和挑战伯特的常识表达能力的几个方面。首先，我们探讨了BERT对各种对象属性的分类能力，证明了BERT在其嵌入空间中对各种常识特征进行编码的能力很强，但在许多领域仍然存在不足。接下来，我们表明，通过使用与缺陷属性相关的额外数据来增加BERT的预训练数据，我们能够在使用最少数据量的情况下提高下游常识推理任务的性能。最后，我们开发了一种微调知识图嵌入的方法，并展示了显式知识图的持续重要性。