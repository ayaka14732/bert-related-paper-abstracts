众所周知，开放领域问答（QA）涉及一些基础知识和推理挑战，但模型在接受基准任务培训时是否真正学习了这些知识？为了研究这一点，我们引入了几个新的挑战性任务，以探索最先进的QA模型是否具有关于单词定义和一般分类推理的一般知识，这两种知识对于更复杂的推理形式来说都是基本的，并且在基准数据集中广泛存在。作为昂贵的众包的替代方案，我们引入了一种从各种类型的专家知识（如知识图和词汇分类法）自动构建数据集的方法，允许系统地控制产生的探测，并进行更全面的评估。我们发现自动构造的探测容易受到注释工件的攻击，我们对此进行了仔细的控制。我们的评估证实，基于transformer的QA模型已经倾向于识别某些类型的结构词汇知识。然而，它也揭示了一个更微妙的画面：它们的性能大幅下降，甚至在基本分类层次结构中的啤酒花数量略有增加，或者随着更具挑战性的干扰物候选答案的引入。此外，即使这些模型在标准实例级评估中获得成功，但在语义连接探测集群级评估时（例如，关于概念的所有Isa问题），它们仍有很大的改进空间。