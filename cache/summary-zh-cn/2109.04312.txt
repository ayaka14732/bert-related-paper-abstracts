这项工作提出了一种稀疏注意转换器体系结构，用于对包含大型表的文档进行建模。表格在网络上无处不在，而且信息丰富。然而，web上超过20%的关系表具有20行或更多行（Cafarella等人，2008），这些大型表对电流互感器模型提出了挑战，电流互感器模型通常限于512个令牌。在这里，我们提出MATE，一种新的转换器体系结构，用于对web表的结构进行建模。MATE使用稀疏注意力的方式，使头部能够有效地关注表中的行或列。该体系结构在速度和内存方面呈线性扩展，可以使用当前加速器处理包含8000多个令牌的文档。MATE还对表格数据具有更合适的归纳偏差，并为三个表格推理数据集设置了新的最新技术。对于HybridQA（Chen等人，2020b），一个涉及包含表格的大型文档的数据集，我们将最佳先验结果提高了19个点。