预训练大型神经语言模型，如BERT，在许多自然语言处理（NLP）任务中取得了令人印象深刻的成果。然而，大多数培训前的工作都集中在一般的领域语料库上，如新闻专线和Web。一个普遍的假设是，即使是特定领域的预培训也可以从通用领域语言模型开始。在本文中，我们挑战了这一假设，证明了对于具有大量未标记文本的领域，如生物医学，从头开始的预训练语言模型比一般领域语言模型的持续预训练有很大的收益。为了促进这项研究，我们从公开的数据集汇编了一个全面的生物医学NLP基准。我们的实验表明，特定领域的前训练是一个坚实的基础，为广泛的生物医学NLP任务，导致新的国家的最先进的结果全面。此外，在对建模选择进行全面评估（包括预培训和特定任务的微调）时，我们发现一些常见的做法对于BERT模型是不必要的，例如在命名实体识别（NER）中使用复杂的标记方案。为了帮助加快生物医学NLP的研究，我们为社区发布了最先进的预训练和任务特定模型，并在https://aka.ms/BLURB.