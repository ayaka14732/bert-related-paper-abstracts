经过预训练的语言模型BERT在一系列自然语言处理任务中带来了显著的性能改进。由于该模型是在大量不同主题的语料库上训练的，因此对于训练（源数据）和测试（目标数据）中的数据分布不同而共享相似性的领域转移问题，它表现出稳健的性能。尽管与以前的模型相比，它有了很大的改进，但由于域转移，它仍然存在性能下降的问题。为了缓解这些问题，我们提出了一种简单而有效的无监督领域自适应方法，即竞争性区分领域自适应与知识提取相结合的竞争性领域自适应（AAD）。我们在30个领域对的跨领域情感分类任务中评估了我们的方法，提高了文本情感分类中无监督领域适应的最新性能。