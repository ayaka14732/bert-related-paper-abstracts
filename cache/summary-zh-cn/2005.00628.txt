虽然像BERT这样的预训练模型在自然语言理解任务中表现出了巨大的优势，但在对目标任务进行微调之前，可以通过在数据丰富的中间任务中进一步训练模型来提高其性能。然而，对于中间任务训练在什么时候以及为什么对给定的目标任务有益，人们仍然知之甚少。为了研究这一点，我们对预训练的RoBERTa模型进行了大规模研究，其中包括110个中间目标任务组合。我们进一步评估了所有经过培训的模型，其中包括25项探测任务，旨在揭示驱动转移的特定技能。我们观察到，需要高级推理和推理能力的中级任务往往效果最好。我们还观察到，目标任务绩效与更高层次的能力（如共指消解）密切相关。然而，我们没有观察到探测和目标任务性能之间更细微的相关性，这突出了需要在广泛的探测基准上开展进一步的工作。我们还观察到有证据表明，在训练前遗忘所学知识可能会限制我们的分析，这突出了在这些环境下进一步研究迁移学习方法的必要性。