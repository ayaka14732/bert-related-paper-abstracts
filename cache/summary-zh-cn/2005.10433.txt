我们研究了数据到文本任务的预训练+微调策略。我们的实验表明，采用T5形式的文本到文本预训练能够使简单的、基于端到端转换器的模型优于为数据到文本生成定制的流水线神经结构，以及基于语言模型的替代预训练技术，如BERT和GPT-2。重要的是，T5预训练可以带来更好的泛化，这一点可以从域外测试集的大量改进中得到证明。我们希望我们的工作能为未来的研究提供一个有用的基线，因为数据到文本任务中的迁移学习变得越来越普遍。