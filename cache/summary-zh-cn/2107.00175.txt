尽管在自然语言处理（NLP）领域取得了巨大的成功，但像BERT这样的大型预训练语言模型由于参数数量多、推理速度慢而不适合于资源受限或实时应用。近年来，BERT的压缩和加速已经成为一个重要的研究课题。通过采用参数共享策略，ALBERT在实现竞争性能的同时，大大减少了参数的数量。尽管如此，阿尔伯特仍然有很长的推理时间。在这项工作中，我们提出了ELBERT，与ALBERT相比，由于提出了基于置信窗的早期退出机制，它显著提高了平均推理速度，而不引入额外的参数或额外的训练开销。实验结果表明，在不同的数据集上，与ALBERT相比，ELBERT实现了从2$\倍到10$\倍的自适应推理加速，而精度下降可以忽略不计。此外，在计算量相同的情况下，ELBERT算法比现有的早期退出算法具有更高的精度。此外，为了理解早期退出机制的原理，我们还将其决策过程可视化到ELBERT中。