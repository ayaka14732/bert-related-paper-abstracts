许多视觉和语言研究集中在一组小而多样的独立任务和支持数据集上，这些任务和支持数据集通常是单独研究的；然而，成功完成这些任务所需的视觉基础语言理解技能有很大的重叠。在这项工作中，我们通过建立一个大规模的多任务训练体系来研究视觉和语言任务之间的关系。我们的方法最终在一个单一模型上实现，该模型包含四大类任务的12个数据集，包括视觉问答、基于字幕的图像检索、基础引用表达式和多模式验证。与独立训练的单任务模型相比，这意味着从大约30亿个参数减少到2.7亿个，同时任务的平均性能提高了2.05个点。我们使用我们的多任务框架对联合训练不同任务的效果进行深入分析。此外，我们还表明，从我们的单任务多任务模型中微调特定于任务的模型可以带来进一步的改进，达到或超过最先进水平的性能。