最先进的参数有效微调方法依赖于在预训练语言模型的层之间引入适配器模块。但是，这些模块针对每个任务分别进行培训，因此无法在任务之间共享信息。在本文中，我们表明，我们可以通过使用共享超网络生成所有层和任务的适配器参数来学习这些参数，而共享超网络是以transformer模型中的任务、适配器位置和层id为条件的。这个参数高效的多任务学习框架允许我们通过超网络在任务之间共享知识，同时通过特定于任务的适配器使模型适应每个单独的任务，从而实现两个世界的最佳效果。在著名的GLUE基准测试上的实验表明，多任务学习的性能有所提高，而每个任务只添加了0.29%的参数。此外，我们还展示了在各种任务中的少数镜头域泛化方面的显著性能改进。我们的代码在https://github.com/rabeehk/hyperformer.