近年来，大量采用自我关注（即变压器模型）和类似于伯特的训练原则，在大量视觉和语言问题（如视觉问答（VQA）、图像检索等）上产生了许多高性能的模型。在本文中，我们声称这些最先进的方法（SOTA）在构造单一模态内的信息方面表现得相当好，但尽管它们的性能令人印象深刻，但它们往往难以识别细粒度的模态间关系。事实上，这种关系通常被认为是在培训过程中从特定于应用程序的损失中隐式学习到的，主要是用于分类的交叉熵。虽然最近的研究通过交叉注意模块为情态间关系提供了归纳偏见，但在本研究中，我们证明（1）后一种假设不成立，即情态对齐不一定自动出现，以及（2）在视觉对象和文字之间的对齐上添加弱监督可以提高需要推理的任务上学习模型的质量。特别是，我们将对象词对齐丢失集成到SOTA视觉语言推理模型中，并在两个任务VQA和语言驱动的图像比较中对其进行评估。我们表明，所提出的细粒度模态间监督显著提高了这两个任务的性能。特别是，这种新的学习信号允许使用预先训练的模型在GQA数据集（VQA任务）上获得SOTA级性能，而无需对任务进行微调，以及在NLVR2数据集（图像的语言驱动比较）上获得新的SOTA级性能。最后，我们还通过可视化注意分布来说明贡献对模型推理的影响。