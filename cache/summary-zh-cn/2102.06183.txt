视频和语言学习的规范方法（例如，视频问答）规定了一个神经模型，用于从视觉模型和语言模型的文本特征中离线提取密集视频特征。这些特征提取器是独立训练的，通常用于不同于目标域的任务，使得这些固定特征对于下游任务来说是次优的。此外，由于密集视频特征的高计算过载，通常很难（或不可行）将特征提取器直接插入现有方法中以便于微调。为了弥补这一困境，我们提出了一个通用框架ClipBERT，该框架通过采用稀疏采样，实现视频和语言任务的可负担的端到端学习，其中每个训练步骤仅使用视频中的一个或几个稀疏采样的短片段。在六个数据集上进行文本到视频检索和视频问答的实验表明，ClipBERT优于（或等同于）现有的利用全长视频的方法，这表明，仅使用少量稀疏采样片段的端到端学习通常比使用从全长视频中密集提取的离线特征更准确，证明了众所周知的“少即是多”原则。数据集中的视频来自相当不同的域和长度，从3秒的通用域GIF视频到180秒的YouTube人类活动视频，显示了我们方法的泛化能力。我们提供了全面的消融研究和彻底的分析，以剖析导致这一成功的因素。我们的代码在https://github.com/jayleicn/ClipBERT