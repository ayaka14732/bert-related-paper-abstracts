视觉语言预训练（VLP）提高了许多视觉语言任务的表现。然而，大多数现有的预训练模型只擅长于基于理解的任务或基于生成的任务。此外，性能改进在很大程度上是通过使用从web收集的有噪声的图像-文本对来扩展数据集实现的，这是一个次优的监督来源。在本文中，我们提出了BLIP，这是一个新的VLP框架，它可以灵活地转换到视觉语言理解和生成任务。BLIP通过引导字幕有效地利用了有噪声的网络数据，字幕员生成合成字幕，过滤器去除有噪声的字幕。我们在广泛的视觉语言任务上取得了最先进的成果，例如图像文本检索（平均增长2.7%）recall@1)图像字幕（苹果酒中为2.8%）和VQA（VQA分数中为1.6%）。当以零拍方式直接转换到视频语言任务时，BLIP也表现出很强的泛化能力。代码、模型和数据集发布于https://github.com/salesforce/BLIP.