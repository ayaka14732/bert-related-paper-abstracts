单词嵌入和预先训练的语言模型允许构建丰富的文本表示，并在大多数NLP任务中实现了改进。不幸的是，培训成本非常高，许多小公司和研究小组倾向于使用经过预培训并由第三方提供的模型，而不是构建自己的模型。这是次优的，因为对于许多语言来说，模型都是在较小（或较低质量）的语料库上训练的。此外，非英语语言的单语预先培训模型并不总是可用的。充其量，这些语言的模型包含在多语言版本中，其中每种语言与其他语言共享子字符串和参数的配额。对于较小的语言，如巴斯克语，情况尤其如此。在这篇论文中，我们展示了一些单语模型（FastText单词嵌入、FLAIR和BERT语言模型）使用较大的巴斯克语料库进行训练，在下游NLP任务中，包括主题分类、情感分类、词性标记和NER，产生了比公开版本更好的结果。这项工作为巴斯克的这些任务设定了新的技术水平。这项工作中使用的所有基准和模型都是公开的。