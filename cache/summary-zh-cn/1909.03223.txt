文本压缩在摘要、阅读理解和文本编辑等方面有着广泛的应用。然而，几乎所有现有的方法都需要手工制作的特性、语法标签或并行数据。即使是在无监督的环境下完成此任务，其体系结构也需要特定于任务的自动编码器。此外，这些模型只为每个源输入生成一个压缩句子，因此适应最终输出的不同风格要求（例如长度）通常意味着从头开始重新训练模型。在这项工作中，我们提出了一个完全无监督的模型Deleter，该模型能够发现任意句子的“最佳删除路径”，其中路径上的每个中间序列都是前一个序列的相干子序列。该方法完全依赖于预训练的双向语言模型（BERT），根据生成句子的平均复杂度对每个候选删除进行评分，并执行渐进式贪婪前瞻搜索，为每个步骤选择最佳删除。我们将Deleter应用于抽取句子压缩的任务中，发现我们的模型与在102万个具有相似压缩比的领域示例上训练的最新监督模型具有竞争力。定性分析以及自动和人工评估都验证了我们的模型能够产生高质量的压缩。