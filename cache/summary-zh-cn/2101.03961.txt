在深度学习中，模型通常会对所有输入重复使用相同的参数。混合专家（MoE）对此不屑一顾，而是为每个传入的示例选择不同的参数。结果是一个稀疏激活的模型——参数数量惊人——但计算成本不变。然而，尽管MoE取得了一些显著的成功，但由于复杂性、通信成本和培训不稳定性，广泛采用受到了阻碍——我们用开关变压器解决了这些问题。我们简化了MoE路由算法，设计了直观的改进模型，降低了通信和计算成本。我们提出的训练技术有助于解决不稳定性问题，并且我们展示了大型稀疏模型可能首次以较低精度（bfloat16）格式进行训练。我们基于T5 Base和T5 Large设计模型，在相同计算资源的情况下，训练前速度提高7倍。这些改进扩展到多语言设置中，我们衡量了所有101种语言相对于mT5基本版本的收益。最后，我们通过在“庞大的干净爬网语料库”上预训练多达万亿个参数模型，提升了当前语言模型的规模，并实现了比T5-XXL模型4倍的加速。