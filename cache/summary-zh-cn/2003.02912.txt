近年来，自然语言处理（NLP）在许多领域取得了令人瞩目的进展，这得益于新颖的、经过训练的上下文表示模型的出现。特别是，Devlin等人（2019年）提出了一种称为BERT（来自变压器的双向编码器表示）的模型，该模型使研究人员能够通过微调数据集和任务的表示，在众多NLP任务中获得最先进的性能，无需开发和培训高度特定的体系结构。作者还发布了多语言BERT（multilingual BERT，mBERT），这是一个在104种语言的语料库上训练的模型，可以作为通用语言模型。该模型在跨语言自然推理任务中取得了令人印象深刻的结果。在BERT模型潜力的驱动下，NLP社区已经开始调查并生成大量的BERT模型，这些模型在特定的语言上进行训练，并在特定的数据域和任务上进行测试。这使我们能够通过将mBERT与这些更具体的模型的性能进行比较来评估其作为通用语言模型的真正潜力。本文介绍了特定语言的BERT模型的最新进展，提供了不同维度（即体系结构、数据域和任务）的总体情况。我们的目标是提供语言特定（语言特定）BERT模型和mBERT之间的共性和差异的直接概述。我们还提供了一个互动和不断更新的网站，可用于探索我们收集的信息，网址：https://bertlang.unibocconi.it.