Transformer基于编码器-解码器框架，在几个自然语言生成任务上实现了最先进的性能。编码器将输入句子中的单词映射成一系列隐藏状态，然后将这些隐藏状态反馈给解码器以生成输出句子。这些隐藏状态通常对应于输入单词，并侧重于捕获本地信息。然而，很少对全局（句子级）信息进行探索，这为提高生成质量留下了空间。在本文中，我们提出了一种新的全局表示增强变压器（GRET）来显式地建模变压器网络中的全局表示。具体而言，在所提出的模型中，从编码器生成全局表示的外部状态。然后在解码过程中将全局表示融合到解码器中，以提高生成质量。我们在两个文本生成任务中进行了实验：机器翻译和文本摘要。在四个WMT机器翻译任务和LCSTS文本摘要任务上的实验结果表明了该方法对自然语言生成的有效性。