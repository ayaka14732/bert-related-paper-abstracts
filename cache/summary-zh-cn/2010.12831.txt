经过预先培训的上下文视觉和语言（V&L）模型在各种基准上取得了令人印象深刻的性能。然而，现有的模型需要大量的并行图像字幕数据进行预训练。此类数据收集成本高昂，需要繁琐的管理。受无监督机器翻译的启发，我们研究了在没有图像字幕语料库的情况下，是否可以通过无监督预训练来学习强V&L表示模型。特别是，我们建议对纯文本和纯图像语料库进行“掩蔽和预测”预训练，并将对象识别模型检测到的对象标记作为锚定点，连接两种模式。我们发现，这样一种简单的方法在四个英语V&L基准上实现的性能接近于使用对齐数据预先训练的模型。我们的工作挑战了广泛接受的观点，即V&L预培训需要一致的数据，同时大大减少了V&L模型所需的监督量。