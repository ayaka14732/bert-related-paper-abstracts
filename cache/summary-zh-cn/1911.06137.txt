阅读理解（RC）已经在各种数据集中进行了研究，深度神经网络带来了更高的性能。然而，这些模型在不同领域的泛化能力尚不清楚。为了缓解这个问题，我们将研究RC上的无监督域自适应，其中在标记的源域上训练模型，并仅使用未标记的样本应用于目标域。我们首先表明，即使使用强大的BERT上下文表示，当在一个数据集上训练的模型直接应用于另一个目标数据集时，性能仍然不能令人满意。为了解决这个问题，我们提出了一种新的条件对抗式自我训练方法（CASe）。具体而言，我们的方法利用在源数据集上微调的BERT模型以及置信度过滤，在目标域中生成可靠的伪标记样本以进行自我训练。另一方面，它通过跨领域的条件对抗学习进一步减少了领域分布差异。大量实验表明，我们的方法在多个大规模基准数据集上达到了与监督模型相当的精度。