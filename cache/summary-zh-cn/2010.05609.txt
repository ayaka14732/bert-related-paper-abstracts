预训练的基于Transformer的模型在各种自然语言处理数据集上实现了最先进的结果。然而，这些模型的大小通常是它们在实际生产应用程序中部署的一个缺点。对于多语言模型，大多数参数位于嵌入层。因此，减少词汇量应该对参数总数产生重要影响。在本文中，我们建议根据目标语料库生成处理较少语言的较小模型。我们在XNLI数据集上对多语言BERT的较小版本进行了评估，但我们认为该方法可能适用于其他多语言变压器。获得的结果证实，我们可以生成更小的模型，保持可比结果，同时减少多达45%的参数总数。我们将我们的模型与DistilmBERT（一种多语言BERT的蒸馏版本）进行了比较，结果表明，与语言简化不同，蒸馏导致XNLI数据集的总体准确性下降了1.7%到6%。所提供的模型和代码是公开的。