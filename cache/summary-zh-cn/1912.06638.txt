基于转换器的超大型语言模型（VLLMs），如BERT、XLNet和RoBERTa，最近在各种自然语言理解（NLU）任务中表现出了巨大的性能。然而，由于它们的尺寸，这些VLLM是资源密集型的，并且在生产时部署起来非常麻烦。最近的一些出版物研究了各种方法，将基于转换器的VLLM（最常见的是BERT库）中的知识提取到一个更小的模型中，该模型在推理时可以运行得更快。在这里，我们提出了一组新的技术，这些技术共同产生了一个特定于任务的混合卷积和变换模型，WaLDORf，该模型实现了最先进的推理速度，同时仍然比以前的蒸馏模型更精确。