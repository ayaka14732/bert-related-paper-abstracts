故事生成是一项重要但具有挑战性的任务，即从主导上下文生成合理的故事。尽管在流畅性和局部连贯性建模方面取得了成功，但现有的神经语言生成模型（如GPT-2）仍然存在重复、逻辑冲突以及生成的故事缺乏长期连贯性的问题。我们推测，这是因为很难将相关常识知识联系起来，理解因果关系，并以适当的时间顺序规划实体和事件。在本文中，我们设计了一个知识增强的常识故事生成预训练模型。我们建议利用来自外部知识库的常识知识来生成合理的故事。为了进一步捕捉合理故事中句子之间的因果关系和时间依赖关系，我们采用了多任务学习，该学习结合了一个区分目标，在微调过程中区分真假故事。自动和手动评估表明，我们的模型可以生成比最先进的基线更合理的故事，特别是在逻辑和全球一致性方面。