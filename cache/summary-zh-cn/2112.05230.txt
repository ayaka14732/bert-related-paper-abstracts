近年来，在开发更好的图像字幕模型方面取得了巨大进展，但大多数模型都依赖于单独的目标检测器来提取区域特征。最近的视觉语言研究正在转向无检测器趋势，利用网格表示进行更灵活的模型训练和更快的推理速度。然而，这种发展主要集中在图像理解任务上，而对字幕生成任务的研究较少。在本文中，我们研究了一种性能更好的无检测器图像字幕模型，并提出了一种基于纯视觉转换器的图像字幕模型，称为ViTCAP，该模型使用网格表示，而不提取区域特征。为了提高性能，我们引入了一种新的概念令牌网络（CTN）来预测语义概念，然后将它们合并到端到端字幕中。具体而言，CTN是基于视觉转换器构建的，旨在通过分类任务预测概念标记，其中包含的丰富语义信息对字幕任务非常有利。与以前基于检测器的模型相比，ViTCAP大大简化了体系结构，同时在各种具有挑战性的图像字幕数据集上实现了具有竞争力的性能。尤其值得一提的是，ViTCAP在COCO caption Karpath split上的苹果酒得分为138.1分，在nocaps和Google CC字幕数据集上的苹果酒得分分别为93.8分和108.6分。