由于其有效的模型体系结构和大规模未标记扫描/数字生成文档的优势，文本和布局的预培训在各种视觉丰富的文档理解任务中被证明是有效的。在本文中，我们在一个多模式框架中通过预训练文本、布局和图像来呈现\textbf{LayoutLMv2}，其中利用了新的模型架构和预训练任务。具体而言，LayoutLMv2不仅使用现有的蒙面视觉语言建模任务，而且在训练前阶段使用新的文本-图像对齐和文本-图像匹配任务，从而更好地学习跨模态交互。同时，它还将空间感知的自我注意机制集成到Transformer架构中，使模型能够充分理解不同文本块之间的相对位置关系。实验结果表明，LayoutLMv2的性能优于强基线，并在多种下游视觉丰富的文档理解任务上实现了最新的结果，包括FUNSD（0.7895->0.8420）、CORD（0.9493->0.9601）、SROIE（0.9524->0.9781）、Kleister NDA（0.834->0.852）、RVL-CDIP（0.9443->0.9564）和DocVQA（0.7295->0.8672）。预先培训的LayoutLMv2模型可在https://aka.ms/layoutlmv2.