虽然大规模语言模型（LMs）能够很好地模拟自然语言的分布以生成真实的文本，但很难控制它们生成的分布区域。这一问题尤其严重，因为用于训练大型LMs的数据集通常包含显著的毒性、仇恨、偏见和负面性。我们提出GeDi作为一种有效的方法，使用较小的LMs作为生成鉴别器来引导从较大的LMs生成，从而使它们更安全、更可控。GeDi通过规范化两类条件分布，通过贝叶斯规则计算所有可能的下一个令牌的分类概率，在每一步指导生成；一个以所需属性或控制代码为条件，另一个以不需要的属性或反控制代码为条件。我们发现，GeDi比最先进的方法具有更强的可控性，同时发电速度提高了30倍以上。此外，只对GeDi进行四个主题的培训，使我们能够从一个关键字中可控地生成新主题，从而解锁以前可控生成方法所不具备的新功能。最后，我们证明了GeDi可以使GPT-2（1.5B参数）在不牺牲语言质量的情况下显著降低毒性，使其成为目前最实用的解决大型语言模型问题的方法，同时保持快速生成速度。