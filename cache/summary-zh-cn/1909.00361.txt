尽管社区在机器阅读理解（MRC）任务方面取得了很大的进展，但以前的工作大多是解决基于英语的MRC问题，而在其他语言方面的工作很少，主要是因为缺乏大规模的训练数据。在本文中，我们提出了非英语语言的跨语言机器阅读理解（CLMRC）任务。首先，我们为CLMRC任务提供了几种简单易行的反译方法。然而，将答案准确地对齐到另一种语言是困难的，并且可能会引入额外的噪音。在此背景下，我们提出了一种新的模型Dual-BERT，它利用丰富的资源语言（如英语）提供的大规模训练数据，在双语环境中学习短文和问题之间的语义关系，然后利用所学知识提高低资源语言的阅读理解能力。我们在两个中文机器阅读理解数据集CMRC 2018和DRCD上进行了实验。结果表明，与各种最先进的系统相比，CLMRC任务有了很大程度的一致和显著的改进，这表明了CLMRC任务的潜力。现有资源：https://github.com/ymcui/Cross-Lingual-MRC