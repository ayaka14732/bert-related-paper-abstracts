在本文中，我们建议研究域外visio语言预训练问题，其中预训练数据分布不同于下游数据的分布，在下游数据上预训练模型将被微调。现有的解决该问题的方法纯粹是基于似然的，当转移到域外下游任务时，会导致虚假的相关性并损害泛化能力。所谓伪相关，我们的意思是，给定另一个标记（对象或单词）的一个标记（对象或单词）的条件概率可能很高（由于数据集偏差），它们之间没有可靠的（因果）关系。为了缓解这种数据集偏见，我们提出了一个非结构化的Visio语言Bert框架，简称为DeVLBert，用于执行基于干预的学习。我们借鉴因果关系研究领域的后门调整思想，提出了几种基于神经网络的Bert式域外预训练体系结构。对图像检索（IR）、零镜头检索（Zero-shot IR）和视觉问答三个下游任务的定量结果表明，DeVLBert通过提高泛化能力而有效。