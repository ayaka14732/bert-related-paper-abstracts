现有的语言模型压缩方法大多使用一个简单的L2损失来将大型BERT模型的中间表示形式中的知识提取到较小的BERT模型。尽管被广泛使用，但这一设计目标假设隐藏表示的所有维度都是独立的，无法捕获教师网络中间层中的重要结构知识。为了获得更好的蒸馏效果，我们提出了基于中间表征的对比蒸馏（CoDIR），这是一个原则性的知识蒸馏框架，在该框架中，学生通过对比目标通过教师的中间层进行知识蒸馏。CoDIR通过学习区分大量正样本和负样本，有助于学生利用教师隐藏层中的丰富信息。CoDIR可以很容易地应用于在预训练和微调阶段压缩大规模语言模型，并在GLUE基准上实现了卓越的性能，优于最先进的压缩方法。