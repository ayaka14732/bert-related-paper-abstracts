Transformer模型在许多自然语言处理任务中获得了广泛的成功。然而，自我注意的二次复杂性限制了它在长文本中的应用。在本文中，通过二进制划分（BP）在多尺度跨度上采用从细到粗的注意机制，我们提出了BP转换器（简称BPT）。BPT产生$O（k\cdot n\log（n/k））$连接，其中$k$是控制注意力密度的超参数。BPT在计算复杂度和模型容量之间有很好的平衡。在文本分类、机器翻译和语言建模方面的一系列实验表明，与以往的自我注意模型相比，BPT对长文本具有更好的性能。我们的代码、hyperparameters和CUDA内核可在PyTorch中获得。