变压器是强大的序列模型，但需要的时间和内存随序列长度二次增长。在本文中，我们引入了注意矩阵的稀疏分解，它将注意矩阵分解为$O（n\sqrt{n}）$。我们还介绍了a）结构和初始化的变化以训练更深层次的网络，b）重新计算注意矩阵以节省内存，以及c）用于训练的快速注意核。我们称这些变化为稀疏变压器的网络，并表明它们可以使用数百层对数万时间步长的序列进行建模。我们使用相同的体系结构对原始字节中的图像、音频和文本进行建模，为Enwik8、CIFAR-10和ImageNet-64的密度建模设定了新的技术水平。我们生成了无条件的样本，证明了全球一致性和巨大的多样性，并表明在原则上可以对长度为一百万或更多的模型序列使用自我注意。