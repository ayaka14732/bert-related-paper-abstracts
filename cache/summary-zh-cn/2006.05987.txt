本文是一项微调伯特上下文表示的研究，重点是在少数样本场景中观察到的常见不稳定性。我们确定了导致这种不稳定性的几个因素：通常使用带有偏差梯度估计的非标准优化方法；BERT网络的重要部分对下游任务的适用性有限；以及使用预先确定的、少量训练迭代的流行实践。我们对这些因素的影响进行了实证测试，并确定了解决过程中常见的不稳定性的替代做法。鉴于这些观察结果，我们再次访问最近提出的方法，以改进使用BERT的少数样本微调，并重新评估其有效性。一般来说，我们观察到这些方法的影响随着我们改进的过程而显著降低。