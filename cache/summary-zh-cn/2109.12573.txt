使用单语和双语纯文本语料库进行跨语言预培训取得了巨大成功。然而，现有的预训练模型忽略了多语言知识，多语言知识是语言不可知的，但包含丰富的跨语言结构对齐。在这篇文章中，我们提出了XLM-K，一个在预训练中融入多种语言知识的跨语言语言模型。XLM-K将现有的多语言预训练扩展为两个知识任务，即掩蔽实体预测任务和对象蕴涵任务。我们在MLQA、NER和XNLI上评估XLM-K。实验结果清楚地表明，与现有的多语言语言模型相比，有了显著的改进。MLQA和NER的结果显示了XLM-K在知识相关任务中的优越性。XNLI的成功表明，XLM-K具有更好的跨语言迁移能力。此外，我们提供了详细的探索性分析，以确认在我们的训练前方案中获得的所需知识。