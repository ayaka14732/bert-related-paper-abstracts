神经模型的最新发展通过自我注意机制将编码器和解码器连接起来。特别是，完全基于自我注意的Transformer在自然语言处理（NLP）任务方面取得了突破。然而，作为Transformer关键组件的多头注意机制将模型的有效部署限制在资源有限的环境中。本文基于张量分解和参数共享的思想，提出了一种新的基于块项张量分解的自我注意模型（即多重线性注意）。我们测试和验证提出的注意方法在三个语言建模任务（即，PTB，WiKeTeXT-103和十亿）和神经机器翻译任务（即，WMT-2016英语德语）。与Transformer、Transformer XL和Transformer with tensor train decomposition等多种语言建模方法相比，多重线性注意不仅可以大大压缩模型参数，而且可以获得性能改进。