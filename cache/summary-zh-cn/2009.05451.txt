NLP领域的最新进展表明，迁移学习通过调整预先训练的模型而不是从头开始，有助于实现新任务的最新结果。Transformers在为许多NLP任务（包括但不限于文本分类、文本生成和序列标记）创建新的最新结果方面取得了重大改进。这些成功案例大多基于大型数据集。在本文中，我们关注学术界和工业界科学家经常面临的一个现实场景：给定一个小数据集，我们是否可以使用像BERT这样的大型预训练模型，并获得比简单模型更好的结果？为了回答这个问题，我们使用为构建聊天机器人而收集的用于意图分类的小数据集，并比较简单的双向LSTM模型与预先训练的BERT模型的性能。我们的实验结果表明，对于小数据集，双向LSTM模型可以获得比BERT模型更高的结果，并且这些简单模型的训练时间比调整预训练的模型要短得多。我们得出结论，模型的性能取决于任务和数据，因此在选择模型之前，应考虑这些因素，而不是直接选择最流行的模型。