已有的关于预训练语言模型（LMs）的研究主要集中在句子层面的句法任务上。在本文中，我们引入文档级话语探测来评估预训练LMs捕捉文档级关系的能力。我们对7个预训练的LMs、4种语言和7个语篇探测任务进行了实验，发现BART总体上是捕获语篇的最佳模型——但仅在其编码器中，与基线模型相比，BERT的表现出奇地好。在不同的模型中，哪一层最能捕获话语信息存在着巨大的差异，模型之间也存在着巨大的差异。