来自Transformers的双向编码器表示（BERT）代表了预训练语言模型的最新体现，这些模型最近促进了广泛的自然语言处理任务。在本文中，我们展示了如何将BERT有效地应用于文本摘要，并为抽取模型和抽象模型提出了一个通用框架。我们介绍了一种新的基于BERT的文档级编码器，它能够表达文档的语义并获得其句子的表示。我们的提取模型是建立在这个编码器之上的，通过堆叠几个句子间转换层。对于抽象总结，我们提出了一种新的微调调度方案，该方案对编码器和解码器采用不同的优化器，以缓解两者之间的不匹配（前者预训练，而后者不训练）。我们还证明了两阶段微调方法可以进一步提高生成摘要的质量。在三个数据集上的实验表明，我们的模型在提取和抽象环境中都获得了最先进的结果。我们的代码可在https://github.com/nlpyang/PreSumm