大多数机器翻译系统从左到右自动回归生成文本。相反，我们使用掩蔽语言建模目标来训练模型，以预测目标词的任何子集，条件是输入文本和部分掩蔽的目标翻译。这种方法允许高效的迭代解码，我们首先以非自回归的方式预测所有目标单词，然后重复屏蔽并重新生成模型最不确定的单词子集。通过将此策略应用于恒定的迭代次数，我们的模型将非自回归和并行解码翻译模型的最新性能水平平均提高了4 BLEU以上。它还能够达到一个典型的从左到右变压器模型的大约1 BLEU点，同时解码速度明显加快。