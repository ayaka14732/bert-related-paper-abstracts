我们通过扩展增量聚类算法，利用上下文编码器和神经组件，研究了在固定内存约束下的共指消解建模。给定一个新句子，我们的端到端算法根据从早期文档上下文（如果有）创建的显式实体表示提出并评分每个提及范围。然后使用这些跨距更新实体的表示，以免忘记；我们在整个文档中只保留一组固定的突出实体。在这项工作中，我们成功地转换了一个高性能模型（Joshi et al.，2020），渐进地将其内存使用量减少到恒定空间，在OntoNotes 5.0上F1的相对损失仅为0.3%。