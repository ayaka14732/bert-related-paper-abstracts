Lample等人（2019）提出的产品密钥存储器（PKM）能够通过有效地增加模型容量来提高预测精度，而计算开销很小。然而，他们的实证应用仅限于因果语言建模。受预训练语言模型（PLM）最近取得的成功的推动，我们研究了如何将大型PKM整合到PLM中，以便对广泛的下游NLP任务进行微调。我们定义了一个新的内存使用度量，使用该度量进行仔细观察发现，在PKM增强模型的训练期间，大多数内存插槽仍然过时。为了通过解决这个问题来训练更好的PLM，我们提出了简单但有效的解决方案：（1）从无记忆预训练的模型权重初始化；（2）通过添加而不是替换前馈网络来增强PKM。我们验证了它们对于PKM增强PLM的预训练、提高内存利用率和下游性能至关重要。代码和预训练重量可在https://github.com/clovaai/pkm-transformers.