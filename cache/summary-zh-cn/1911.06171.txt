近年来，无监督预训练在计算语言学领域越来越流行，这得益于它在促进自然语言理解（NLU）方面的惊人成功以及有效利用大规模未标记语料库的潜力。然而，尽管NLU取得了成功，但在自然语言生成（NLG）方面，无监督预训练的力量仅得到了部分挖掘。主要障碍源于NLG的一种特殊性质：文本通常是基于特定的上下文生成的，这可能因目标应用程序而异。因此，为NLU场景中的预培训设计通用体系结构是一件棘手的事情。此外，在学习目标任务时保留从预培训中学习到的知识也是一个非常重要的问题。这篇综述总结了最近通过无监督的预训练增强NLG系统的努力，特别侧重于促进将预训练模型集成到下游任务中的方法。根据它们处理上述障碍的方式，它们被分为基于架构的方法和基于策略的方法。还提供了讨论，以进一步深入了解这两条工作线之间的关系，一些信息丰富的经验现象，以及未来工作可以致力于的一些可能方向。