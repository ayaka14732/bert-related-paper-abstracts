由最先进的基于转换器的语言模型（transformer-based language models，TLM，如BERT、GPT、T5等）学习的情境化实体表示，利用注意机制从训练数据语料库中学习数据上下文。但是，这些模型不使用知识上下文。知识上下文可以理解为知识图中实体及其与相邻实体关系的语义。我们提出了一种新颖有效的技术，在微调过程中，将概念和模糊实体的多个知识图中的知识上下文注入TLM。它将知识图嵌入到齐次向量空间中，为实体引入新的标记类型，对齐实体位置ID，以及选择性注意机制。我们将BERT作为基线模型，并通过注入来自ConceptNet和WordNet的知识上下文来实现“知识注入的BERT”，这在GLUE benchmark的八个不同子任务上显著优于BERT和其他最近的知识感知BERT变体，如ERNIE、SenseBERT和BERT_CS。KI-BERT基本模型甚至在特定领域任务（如QQP、QNLI和MNLI的SciTail和学术子集）方面显著优于BERT大型模型。