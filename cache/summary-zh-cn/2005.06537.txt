多头专注神经结构在各种自然语言处理任务上取得了最新成果。有证据表明，它们被过度参数化了；注意力集中可以在没有显著性能损失的情况下进行修剪。在这项工作中，我们取而代之的是“重新分配”它们——模型学习在不同的输入上激活不同的头部。借鉴多头注意与专家混合的关系，提出了注意专家混合模型（MAE）。MAE使用块坐标下降算法进行训练，该算法在更新（1）专家职责和（2）专家参数之间交替进行。机器翻译和语言建模实验表明，MAE在这两项任务上都优于强基线。特别是，在WMT14英语到德语翻译数据集上，MAE比“transformer base”提高了0.8 BLEU，参数数量相当。我们的分析表明，我们的模型学习将不同的专家专门化到不同的输入。