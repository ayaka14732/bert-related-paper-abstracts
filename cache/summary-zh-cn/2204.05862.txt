我们将偏好建模和来自人类反馈的强化学习（RLHF）应用于微调语言模型，以充当有益和无害的助手。我们发现这种对齐训练提高了几乎所有NLP评估的性能，并且与python编码和摘要等专门技能的训练完全兼容。我们探索了一种迭代的在线培训模式，在这种模式下，偏好模型和RL策略每周以新鲜的人类反馈数据进行更新，从而有效地改进了我们的数据集和模型。最后，我们研究了RLHF训练的鲁棒性，并确定了RL奖励和策略及其初始化之间KL偏差的平方根之间的大致线性关系。除了我们的主要结果，我们还对校准、竞争目标和OOD检测的使用进行了外围分析，将我们的模型与人类作家进行了比较，并使用最近相关工作中出现的提示提供了我们模型的样本。