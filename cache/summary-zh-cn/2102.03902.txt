Transformers已经成为一个强大的工具，用于广泛的自然语言处理任务。驱动Transformers令人印象深刻的性能的一个关键组件是自我注意机制，它编码其他令牌对每个特定令牌的影响或依赖。虽然有益，但自我关注输入序列长度的二次复杂性限制了其应用于更长的序列——社区正在积极研究这一主题。为了解决这一局限性，我们提出了Nystr\“{o}mformer——一种具有良好可扩展性的序列长度函数模型。我们的想法是基于调整Nystr\“{o}m方法，使其以$o（n）$复杂度近似标准自我注意。Nystr\“{o}mformer的可伸缩性使应用程序能够使用数千个令牌执行更长的序列。我们在GLUE基准上对多个下游任务进行评估，并使用标准序列长度对IMDB进行审查，发现我们的Nystr\“{o}mformer的性能相当，或者在少数情况下甚至稍好一些，而不是标准的自我关注。在远程竞技场（LRA）基准测试中的长序列任务中，Nystr\“{o}mformer相对于其他有效的自我注意方法表现良好。我们的代码可在https://github.com/mlpen/Nystromformer.