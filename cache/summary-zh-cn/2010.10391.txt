背景词嵌入模型，如BioBERT和Bio_-Albert，通过将预训练过程集中于特定领域的语料库，在生物医学自然语言处理任务中取得了最新成果。然而，这种模型没有考虑专家领域知识。在这项工作中，我们引入了UmlsBERT，这是一种上下文嵌入模型，通过一种新的知识扩充策略在预训练过程中集成了领域知识。更具体地说，使用统一医学语言系统（UMLS）元叙词表对UmlsBERT进行了两种方式的扩充：i）连接UMLS中具有相同基本“概念”的单词，以及ii）利用UMLS中的语义组知识创建具有临床意义的输入嵌入。通过应用这两种策略，UmlsBERT可以将临床领域知识编码到单词嵌入中，并在常见命名实体识别（NER）和临床自然语言推理临床NLP任务上优于现有的领域特定模型。