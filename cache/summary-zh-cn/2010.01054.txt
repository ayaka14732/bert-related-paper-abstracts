我们提出了Masker，一种用于风格转换的无监督文本编辑方法。为了解决没有并行源目标对的情况，我们训练源域和目标域的屏蔽语言模型（MLM）。然后我们发现两个模型在可能性方面分歧最大的文本范围。这允许我们识别要删除的源标记，以转换源文本以匹配目标域的样式。删除的令牌被目标传销所取代，通过使用填充的传销变体，我们避免了必须预先确定插入令牌的数量。我们对句子融合和情感转移的实验表明，掩蔽者在完全无监督的环境中表现出竞争性。此外，在低资源环境下，在蒙版机生成的银色训练数据上对监督方法进行预训练时，可将其准确性提高10个百分点以上。