最近语言表征学习的成功背后是一个计算量大且记忆密集的神经网络。知识提取是在资源匮乏的环境中部署如此庞大的语言模型的一项主要技术，它可以不受限制地将所学的单个单词表示的知识转移。在本文中，受最近观察到的语言表征相对定位且整体上具有更多语义知识的启发，我们提出了一个新的语言表征学习知识提取目标，该目标通过两种类型的跨表征关系传递上下文知识：单词关系和层转换关系。与语言模型的其他最新蒸馏技术不同，我们的上下文蒸馏对教师和学生之间的架构更改没有任何限制。我们验证了我们的方法在具有挑战性的语言理解任务基准测试中的有效性，不仅在各种规模的体系结构中，而且结合最近提出的自适应规模修剪方法DynaBERT。