BERT及其变体在各种NLP任务中取得了最先进的性能。从那时起，各种各样的工作被提出来分析BERT中捕获的语言信息。然而，目前的工作并没有提供一个关于伯特如何能够在基于阅读理解的问答任务上达到接近人类水平的表现的洞察。在这项工作中，我们试图为RCQA解释BERT。由于BERT层没有预定义的角色，我们使用集成的渐变来定义层的角色或功能。根据定义的角色，我们对所有层进行初步分析。我们观察到，最初的层关注于查询通道交互，而后面的层更关注于上下文理解和增强答案预测。特别是对于量词问题（多少/多少），我们注意到BERT在后面的几层中重点关注容易混淆的单词（即文章中的其他数字量），但仍然能够正确预测答案。微调和分析脚本将在https://github.com/iitmnlp/BERT-Analysis-RCQA .