视觉和语言预培训（VLP）提高了各种联合视觉和语言下游任务的绩效。当前的VLP方法严重依赖于图像特征提取过程，其中大多数涉及区域监控（例如，目标检测）和卷积结构（例如，ResNet）。尽管在文献中被忽略，但我们发现它在（1）效率/速度方面存在问题，简单地提取输入特征需要比多模态交互步骤更多的计算；（2）表达能力，因为它是视觉嵌入器及其预定义视觉词汇表的表达能力的上限。在本文中，我们提出了一个最小的VLP模型，即视觉和语言转换器（ViLT），在这个意义上，视觉输入的处理被大大简化为与我们处理文本输入相同的无卷积方式。我们表明，ViLT比以前的VLP模型快几十倍，但具有竞争力或更好的下游任务性能。我们的代码和预先训练的重量可在https://github.com/dandelin/vilt.