虽然传统微调的GPT在自然语言理解（NLU）方面无法取得很好的效果，但我们通过一种新的方法P-调优（采用可训练的连续提示嵌入）证明了GPT在NLU任务上优于或可与类似大小的BERT相媲美。在知识探索（LAMA）基准测试中，最佳GPT恢复率为64\%(P@1)在测试期间，无需提供任何额外的文本即可获得世界知识，这大大提高了20%以上的先前最佳成绩。在SuperGlue基准上，GPT在监督学习中取得了与类似规模的BERT相当甚至更好的性能。重要的是，我们发现P-调整还提高了BERTs在少镜头和监督设置下的性能，同时大大减少了对即时工程的需求。因此，P-tuning在少镜头的SuperGlue基准测试中优于最先进的方法。