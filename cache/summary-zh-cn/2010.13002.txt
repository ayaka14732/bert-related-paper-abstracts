最近最先进的总结方法利用大型预训练变压器模型。将这些模型提炼成更小的学生模型对于实际应用至关重要；然而，NLP文献中提出了许多不同的蒸馏方法。最近关于分类和回归任务的提取伯特的工作显示了使用直接知识提取的强大性能。或者，机器翻译从业者使用伪标记提取，在伪标记中，小模型在大模型的翻译上进行训练。第三种更简单的方法是“收缩和微调”（SFT），它通过将参数复制到较小的学生模型中，然后进行微调，从而避免任何显式蒸馏。我们比较了Pegasus和BART的这三种提取方法、当前和以前的最新技术、预先训练的摘要模型，发现SFT在CNN/DailyMail数据集上优于知识提取和伪标记，但在更抽象的XSUM数据集上执行伪标记的能力不足。PyTorch代码和不同大小的检查点可通过拥抱面部变压器在这里获得http://tiny.cc/4iy0tz.