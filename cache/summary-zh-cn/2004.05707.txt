近年来，基于神经网络的文本分类方法取得了很大的进展。特别是，使用注意机制的模型，如BERT，已经证明能够捕获句子或文档中的上下文信息。然而，他们获取语言词汇全局信息的能力更为有限。后者是图卷积网络（GCN）的强度。在本文中，我们提出了VGCN-BERT模型，该模型将BERT的能力与词汇表图卷积网络（VGCN）相结合。局部信息和全局信息通过BERT的不同层进行交互，允许它们相互影响并共同构建分类的最终表示。在我们对多个文本分类数据集的实验中，我们的方法优于单独的BERT和GCN，并且比以前的研究中报告的更有效。