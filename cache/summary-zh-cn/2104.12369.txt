大规模预训练语言模型（PLM）已成为自然语言处理（NLP）的新范式。具有数千亿参数（如GPT-3）的PLM在自然语言理解和生成方面表现出了强大的性能，并且使用了\textit{上下文中的少量镜头}学习。在这项工作中，我们介绍了我们训练大型自回归语言模型PanGu-$\alpha$的实践，该模型具有多达2000亿个参数。PanGu-$\alpha$由MindSpore开发，并在2048 Ascend 910 AI处理器集群上进行训练。基于MindSpore Auto parallel实现了训练并行策略，该策略由五个并行维度组成，可将训练任务有效扩展到2048个处理器，包括数据并行、操作级模型并行、流水线模型并行、优化器模型并行和重物质化。为了增强PanGu-$\alpha$的泛化能力，我们从广泛的领域收集了1.1TB的高质量中文数据，对模型进行预训练。我们实证测试了PanGu-$\alpha$在各种场景下的生成能力，包括文本摘要、问答、对话生成等。此外，我们还研究了模型规模对中国NLP任务中的少数镜头性能的影响。实验结果表明，PanGu-$\alpha$在很少放炮或零放炮设置下执行各种任务的能力优越。