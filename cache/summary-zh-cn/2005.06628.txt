Transformers\citep{vaswani2017attention}已逐渐成为许多最先进的自然语言表示模型的关键组件。最近一个基于转换器的模型-BERT\citep{devlin2018bert}在各种自然语言处理任务上取得了最新的成果，包括GLUE、第1.1版和第2.0版。然而，这个模型在计算上是禁止的，并且有大量的参数。在这项工作中，我们重温了BERT的架构选择，以获得更轻的模型。我们专注于减少参数的数量，但我们的方法可以应用于其他目标，如触发器或延迟。我们表明，通过减少算法选择的正确架构设计维度，而不是减少变压器-编码器层的数量，可以获得更有效的光伯特模型。特别是，我们的舒伯特在GLUE和SQuAD数据集上的平均准确度比具有三个编码器层且参数数量相同的伯特高6.6\%$。