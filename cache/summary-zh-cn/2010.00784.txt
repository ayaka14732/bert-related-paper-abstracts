预训练大型语言模型已经成为自然语言处理社区的标准。这类模型是在一般数据（如图书语料库和英文维基百科）上预先训练的，并且经常在同一领域的任务上进行微调。然而，为了在领域外任务（如临床命名实体识别和关系提取）上实现最先进的性能，需要额外的领域内预培训。在实践中，分阶段的多域预训练在使用通用基准（如GLUE）进行评估时，表现为灾难性遗忘（CF）。在本文中，我们对缓解CF的已知方法进行了实证研究。我们发现，弹性权重整合提供了最好的总体得分，在七项一般任务中，绩效仅下降0.33%，同时在生物医学任务中保持竞争力。此外，我们还探索了基于梯度和潜在聚类的数据选择技术，以提高使用弹性权重合并和经验重放方法时的覆盖率。