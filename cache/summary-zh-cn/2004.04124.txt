BERT是一种由大量语料库预先训练的尖端语言表示模型，在各种自然语言理解任务中取得了优异的性能。然而，将BERT应用于在线服务的一个主要障碍问题是，它占用大量内存，导致用户请求的延迟不能令人满意，从而增加了模型压缩的必要性。现有的解决方案利用知识提取框架来学习模仿BERT行为的较小模型。然而，知识提炼的训练过程本身是昂贵的，因为它需要足够的训练数据来模拟教师模型。在本文中，我们提出了一种混合解决方案LadaBERT（通过混合模型压缩实现BERT的轻量级自适应），它结合了不同模型压缩方法的优点，包括权重修剪、矩阵分解和知识提取。LadaBERT在各种公共数据集上实现了最先进的准确性，同时培训开销可以减少一个数量级。