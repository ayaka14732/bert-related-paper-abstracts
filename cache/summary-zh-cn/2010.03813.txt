语言建模的最新进展导致了计算密集型和资源需求型的最新模型。为了实现可持续的实践，我们研究了训练前数据量对紧凑语言模型的影响。根据逐渐增加的法语文本量训练多个基于BERT的模型。通过对法语问答数据集（FQuAD）进行微调，我们观察到只需100 MB的文本即可获得性能良好的模型。此外，我们还表明，过去极少量的训练前数据，在特定任务语料库上进行中间的训练前步骤不会产生实质性的改善。