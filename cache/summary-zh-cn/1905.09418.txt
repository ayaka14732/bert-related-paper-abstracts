多头自我关注是变压器的一个重要组成部分，是神经机器翻译的最新体系结构。在这项工作中，我们评估了编码器中个体注意头对模型整体性能的贡献，并分析了它们所扮演的角色。我们发现，最重要、最自信的头脑扮演着始终如一的角色，而且往往在语言上可以解释。当使用基于随机门和L0惩罚可微松弛的方法修剪头部时，我们观察到专门化的头部最后被修剪。我们新颖的修剪方法在不严重影响性能的情况下去除了绝大多数头部。例如，在英俄WMT数据集上，删减48个编码器头中的38个只会导致0.15 BLEU的下降。