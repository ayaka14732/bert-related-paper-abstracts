在过去的几年里，像伯特和罗伯塔这样的蒙面语言模型（MLM）已经彻底改变了自然语言理解领域。然而，现有的预训练MLM通常输出令牌表示的各向异性分布，该分布占据整个表示空间的一个狭窄子集。这种令牌表示并不理想，特别是对于需要不同令牌的区别语义的任务。在这项工作中，我们提出了TaCL（令牌感知对比学习），这是一种新的连续预训练方法，鼓励BERT学习令牌表示的各向同性和判别分布。TaCL完全无监督，不需要额外数据。我们在广泛的英语和汉语基准上广泛测试我们的方法。结果表明，与原始BERT模型相比，TaCL带来了一致且显著的改进。此外，我们还进行了详细的分析，以揭示我们方法的优点和内部工作原理。