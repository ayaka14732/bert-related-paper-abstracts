知识提炼通常是通过训练一个小模型（学生）来模仿一个大而笨重的模型（老师）。其思想是通过使用输出概率作为软标签来优化学生，从而压缩来自教师的知识。然而，当教师人数相当大时，无法保证教师的内部知识会转移到学生身上；即使学生与软标签非常匹配，其内部表示也可能有很大不同。这种内部不匹配会破坏原本打算从教师转移到学生身上的泛化能力。在本文中，我们建议将一个大型模型（如BERT）的内部表示提取为它的简化版本。我们制定了两种提取这种表示的方法和各种算法来进行提取。我们使用GLUE基准测试中的数据集进行了实验，结果一致表明，从内部表示添加知识提取比仅使用软标签提取更有效。