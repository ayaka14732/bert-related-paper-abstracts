虽然自动度量通常用于评估NLG系统，但它们通常与人类的判断关联性较差。较新的指标（如BERTScore）解决了以前的指标（如BLEU和ROUGE）中的许多弱点，这些指标依赖于n-gram匹配。然而，这些新的方法仍然是有限的，因为它们不考虑生成上下文，所以它们不能正确地奖励生成的文本是正确的，但偏离给定的引用。在本文中，我们提出了语言模型增强相关性评分（MARS），一种新的上下文感知NLG评估指标。火星利用现成的语言模型，在强化学习指导下，创建既考虑生成上下文又考虑可用的人类参考的扩充引用，然后将其用作附加得分的生成文本的引用。与三个常见NLG任务中的七个现有指标相比，MARS不仅实现了与人类参考判断的更高相关性，而且在更大程度上区分了形成良好的候选对象和对抗性样本。