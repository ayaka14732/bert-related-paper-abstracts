从大型未标记语料库中学习的能力使得神经语言模型能够推进自然语言理解的前沿。然而，现有的自我监督技术是在词的形式层面上运作的，作为底层语义内容的替代物。本文提出了一种直接在词义层面采用弱监督的方法。我们的模型名为SenseBERT，经过预先训练，不仅可以预测隐藏的单词，还可以预测它们的WordNet超级意义。因此，我们获得了一个词汇语义层次的语言模型，而不使用人工注释。SenseBERT显著提高了词汇理解，正如我们在SemEval词义消歧实验中所证明的那样，通过在语境任务中实现单词的最新结果，SenseBERT取得了显著的词汇理解。