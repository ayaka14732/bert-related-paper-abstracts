本文提出了一种新的统一预训练语言模型（UniLM），该模型可以对自然语言理解和生成任务进行微调。该模型使用三种类型的语言建模任务进行预训练：单向、双向和序列到序列预测。统一建模是通过使用共享变压器网络和使用特定的自我注意掩码来控制预测条件的上下文来实现的。UniLM在GLUE基准测试、2.0班和CoQA答疑任务方面均优于BERT。此外，UniLM在五个自然语言生成数据集上取得了最新成果，包括将CNN/DailyMail摘要ROUGE-L提高到40.51（绝对提高2.04），将Gigaword摘要ROUGE-L提高到35.75（绝对提高0.86），CoQA生成性问题回答F1得分为82.5（绝对提高37.1），团队问题生成BLEU-4得分为22.12（绝对提高3.75），DSTC7文档固定对话响应生成NIST-4得分为2.67（人因绩效为2.65）。代码和预先培训的模型可在https://github.com/microsoft/unilm.