预先训练的通用特征抽取器，如用于自然语言处理的BERT和用于计算机视觉的VGG，已经成为改进深度学习模型的有效方法，而不需要更多的标记数据。虽然有效，但对于某些部署场景，像BERT这样的功能提取器可能会非常大。我们探讨了BERT的权重修剪，并问：训练前的压缩如何影响迁移学习？我们发现剪枝在三个广泛的领域影响迁移学习。低水平的修剪（30-40%）根本不会影响培训前的损失或转移到下游任务。中等水平的修剪会增加训练前的损失，并防止有用的训练前信息转移到下游任务。高水平的修剪还防止模型拟合下游数据集，从而导致进一步退化。最后，我们观察到对特定任务进行微调不会提高其可裁剪性。我们的结论是，在训练前，可以对BERT进行一次删减，而不是在不影响性能的情况下对每个任务单独进行删减。