微调实际上是利用大型预训练语言模型来执行下游任务的方法。但是，它会修改所有语言模型参数，因此需要为每个任务存储完整副本。在本文中，我们提出了前缀调优，这是一种轻量级的自然语言生成任务微调替代方案，它使语言模型参数保持不变，但优化了一个小的连续任务特定向量（称为前缀）。前缀调优从提示中获得灵感，允许后续令牌关注此前缀，就好像它是“虚拟令牌”。我们将前缀调整应用于GPT-2以生成表到文本，并将前缀调整应用于BART以进行摘要。我们发现，通过只学习0.1\%的参数，前缀调整在完整数据设置中获得了可比的性能，在低数据设置中优于微调，并且可以更好地推断训练期间未看到主题的示例。