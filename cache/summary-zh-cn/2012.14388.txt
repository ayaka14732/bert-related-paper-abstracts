本文提出了一种新的训练方法，即条件掩蔽语言建模（CMLM），以有效地学习大规模未标记语料库中的句子表示。CMLM通过对相邻句子的编码向量进行条件反射，将句子表征学习集成到传销训练中。我们的英文CMLM模型在SentEval上实现了最先进的性能，甚至优于使用监督信号学习的模型。作为一种完全无监督的学习方法，CMLM可以方便地扩展到广泛的语言和领域。我们发现，与双文本检索（BR）和自然语言推理（NLI）任务共同训练的多语言CMLM模型比以前最先进的多语言模型有很大的优势，例如跨语言语义搜索比基线模型提高了10%。我们探讨了学习表征的相同语言偏见，并提出了一种简单的、训练后和模型不可知的方法，以去除表征中的语言识别信息，同时保留句子语义。