众所周知，大型预训练语言模型（LMs）编码了大量的语言信息。然而，高级推理技能，如数值推理，很难仅从语言建模目标学习。因此，现有的数值推理模型使用了灵活性有限的专用体系结构。在这项工作中，我们证明了数值推理适合于自动数据生成，因此可以通过生成大量数据并在多任务设置中进行训练，将此技能注入预先训练的LMs。我们表明，在此数据上对我们的模型GenBERT进行预训练，可以显著提高下降时的性能（49.3$\rightarrow$72.3 F1），达到与同等大小的最先进模型相匹配的性能，同时使用简单和通用的编码器-解码器架构。此外，GenBERT很好地概括了数学单词问题数据集，同时在标准RC任务上保持了高性能。我们的方法提供了一个将技能注入大型预训练LMs的通用方法，只要该技能适合自动数据扩充。