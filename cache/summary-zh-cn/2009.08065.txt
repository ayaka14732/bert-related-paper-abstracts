预先训练的大规模语言模型在许多自然语言处理（NLP）任务中越来越显示出高精度。然而，硬件平台上有限的重量存储和计算速度阻碍了预训练模型的普及，尤其是在边缘计算时代。在这项工作中，我们提出了一种高效的基于转换器的大规模语言表示方法，使用硬件友好的块结构修剪。我们将重新加权的组套索合并到块结构剪枝中进行优化。除了显著减少重量存储和计算外，该方法还实现了较高的压缩率。在通用语言理解评估（GLUE）基准任务的不同模型（BERT、RoBERTa和DistilBERT）上的实验结果表明，在某些任务上，我们实现了高达5.0x的准确度，并且零或轻微降低了准确度。我们提出的方法也与现有的紧凑型预训练语言模型（如使用知识提取的DistilBERT）正交，因为在DistilBERT的基础上可以进一步获得1.79倍的平均压缩率，而精度降低为零或很小。它适用于在资源受限的边缘设备上部署最终的压缩模型。