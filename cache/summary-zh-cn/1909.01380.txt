我们试图了解在不同的学习目标下，深层神经网络中各个标记的表示和学习特征空间的结构是如何在层之间演化的。我们将重点放在变压器上进行分析，因为它们在各种任务中都被证明是有效的，包括机器翻译（MT）、标准从左到右语言模型（LM）和屏蔽语言建模（MLM）。以前的工作使用黑盒探测任务来显示变压器学习到的表示因目标的不同而显著不同。在这项工作中，我们使用典型相关分析和互信息估计器来研究信息如何在变压器层之间流动，以及这个过程如何依赖于学习目标的选择。例如，当您从底层到顶层时，从左到右的语言模型中关于过去的信息会消失，关于未来的预测也会形成。相比之下，对于传销，表示最初获取关于令牌周围的上下文的信息，部分忘记令牌标识并产生更广义的令牌表示。然后在顶级传销层重新创建令牌标识。