上下文语言模型（CLM）将NLP基准推向了一个新的高度。在文本分类等下游任务中利用CLM提供的单词嵌入已经成为一种新的规范。然而，除非得到解决，否则CLM容易在数据集中学习到内在的性别偏见。因此，下游NLP模型的预测可能因性别词的不同而显著不同，例如将“他”替换为“她”，甚至性别中性词。在本文中，我们将重点分析一种流行的CLM，即BERT。我们分析了在与情绪和情绪强度预测相关的五个下游任务中，它导致的性别偏见。对于每项任务，我们利用BERT的单词嵌入训练一个简单的回归器。然后，我们使用公平评估语料库评估回归者的性别偏见。理想情况下，从具体的设计来看，模型应该摒弃输入中的性别信息特征。然而，研究结果表明，该系统的预测对性别特定的单词和短语有很大的依赖性。我们声称，这种偏见可以通过从单词嵌入中去除性别特征来减少。因此，对于BERT中的每一层，我们确定主要编码性别信息的方向。在词语嵌入的语义空间中，由这些方向形成的空间称为性别子空间。我们提出了一种算法，可以找到细粒度的性别方向，即每个层有一个主方向。这避免了在多个维度中实现性别子空间的需要，并防止遗漏其他重要信息。实验表明，去除这些方向上的嵌入成分在减少下游任务中的BERT诱导偏差方面取得了巨大成功。