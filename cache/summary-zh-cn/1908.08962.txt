自然语言表示法的最新发展伴随着大量昂贵的模型，这些模型通过自我监督的预训练来利用大量的通用领域文本。由于将此类模型应用于下游任务的成本，已经提出了几种预训练语言表示的模型压缩技术（Sun等人，2019年；Sanh，2019年）。然而，令人惊讶的是，仅仅预先训练和微调紧凑模型的简单基线被忽略了。在本文中，我们首先表明，在较小的体系结构中，预训练仍然很重要，并且微调预训练的紧凑模型可以与并行工作中提出的更复杂的方法相竞争。从预先训练的紧凑模型开始，我们接着探索通过标准知识提取从大型微调模型转移任务知识。由此产生的简单但有效的通用算法，即预训练蒸馏，带来了进一步的改进。通过大量实验，我们更普遍地探讨了在两个变量（模型大小和未标记任务数据的属性）下预训练和蒸馏之间的相互作用。一个令人惊讶的观察结果是，即使顺序应用于相同的数据，它们也会产生复合效应。为了加速未来的研究，我们将公开我们的24个预先训练的微型BERT模型。