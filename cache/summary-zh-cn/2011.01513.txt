大多数预先训练的语言模型（PLM）在子词级别上使用字节对编码（BPE）或其变体构造词表示，通过这种编码，几乎可以避免OOV（词汇量外）词。然而，这些方法将一个单词分割成子单词单元，使得表示不完整和脆弱。在本文中，我们提出了一个字符感知的预训练语言模型，名为CharBERT，它改进了以前的方法（如BERT、RoBERTa）来解决这些问题。我们首先从序列字符表示中构造每个标记的上下文单词嵌入，然后通过一个新的异构交互模块融合字符表示和子单词表示。我们还提出了一个新的训练前任务NLM（Noised LM），用于无监督的字符表示学习。我们在原始数据集和对抗性拼写错误测试集上评估了我们在问答、序列标记和文本分类任务上的方法。实验结果表明，该方法能显著提高PLMs的性能和鲁棒性。预训练模型、评估集和代码可在https://github.com/wtma/CharBERT