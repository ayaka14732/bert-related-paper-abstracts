脚本——描述典型日常活动的标准化事件序列——通过提供期望、解决歧义和填写未说明的信息来帮助理解叙述。然而，到目前为止，他们已经证明很难作者或摘录文本。在这项工作中，我们首次证明，预训练的神经语言模型（LMs）可以进行微调，以在不同粒度级别生成适用于各种日常场景（例如烘焙蛋糕）的高质量脚本。为此，我们收集了大量（6.4k）的众包偏序脚本（名为proScript），该脚本比以前的数据集大得多，并开发了结合语言生成和结构预测生成脚本的模型。我们定义了两个互补任务：（i）边缘预测：给定场景和无序事件，将事件组织成有效（可能是偏序）脚本；（ii）脚本生成：仅给定场景，生成事件并将其组织成（可能是偏序）脚本。我们的实验表明，我们的模型表现良好（例如，任务（i）中的F1=75.7），说明了一种克服以前脚本收集障碍的新方法。我们还表明，在人的水平绩效方面仍有很大的改进空间。总之，我们的任务、数据集和模型为学习脚本知识提供了一个新的研究方向。