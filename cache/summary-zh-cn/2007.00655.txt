预先训练的语言模型能掌握多少知识？最近的研究发现，经过预训练的变形金刚擅长于建模语义，但尚不清楚它们在多大程度上掌握了人类知识，或者如何确保它们掌握了人类知识。在本文中，我们在不改变转换器结构、插入显式知识层或添加语义信息的外部存储的情况下，将知识意识纳入语言模型预训练中。相反，我们只是在预训练中用实体扩展标记器向转换器的输入发送实体存在的信号；在输出端，执行额外的实体预测任务。我们的实验表明，仅通过在预训练中添加这些实体信号，变压器参数中就包含了更多的知识：我们观察到语言建模的准确性、LAMA知识探测任务中的事实正确性、，我们还表明，我们的知识感知语言模型（KALM）可以作为GPT-2模型的替代品，在不进行任务相关培训的情况下显著改善下游任务，如零炮问答。