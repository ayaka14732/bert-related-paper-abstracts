视觉语言预训练（VLP）在各种跨模态下游任务中取得了令人印象深刻的成绩。然而，大多数现有方法只能从对齐的图像字幕数据中学习，并且严重依赖昂贵的区域特征，这极大地限制了它们的可扩展性和性能。在本文中，我们提出了一个端到端的统一模式预训练框架，即UNIMO-2，用于对齐图像字幕数据和未对齐的纯图像和纯文本语料库的联合学习。我们建立了一个统一的转换器模型，共同学习图像和文本之间的视觉表示、文本表示和语义对齐。特别是，我们建议通过共享扎根空间对图像和文本进行扎根学习，这有助于连接未对齐的图像和文本，并在不同类型的语料库上对齐视觉和文本语义空间。实验表明，我们的扎根学习方法可以改善文本和视觉语义对齐，从而提高在各种跨模态任务中的表现。此外，得益于对不同类型语料库的有效联合建模，我们的模型在单模态视觉和文本任务上也取得了令人印象深刻的性能。我们的代码和模型在UNIMO项目页面上公开https://unimo-ptm.github.io/.