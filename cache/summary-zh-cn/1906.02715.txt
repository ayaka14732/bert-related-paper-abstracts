Transformer架构为自然语言处理带来了巨大的希望。考虑到一个单一的预训练模型可以被微调以在许多不同的任务上表现良好，这些网络似乎可以提取出普遍有用的语言特征。一个自然的问题是这些网络如何在内部表示这些信息。本文描述了一个特别有效的模型，伯特的定性和定量研究。在高层次上，语言特征似乎表现在单独的语义和句法子空间中。我们发现了词义的精细几何表示的证据。我们还提供了注意矩阵和单个单词嵌入中句法表征的经验描述，以及解释这些表征几何结构的数学论证。