大型变压器模型通常在许多任务上实现最先进的结果，但培训这些模型的成本可能过高，特别是在长序列上。我们介绍了两种提高变压器效率的技术。首先，我们将点积注意力替换为使用位置敏感哈希的点积注意力，将其复杂性从O（$L^2$）更改为O（$L\log L$），其中$L$是序列的长度。此外，我们使用可逆剩余层而不是标准剩余层，这允许在训练过程中只存储一次激活，而不是$N$次，其中$N$是层数。所得到的模型，重整器，与变压器模型一致，同时在存储器上效率更高，并且在长序列上快得多。