多语种预训练语言模型显示了显著的零射跨语言迁移能力。这种迁移是通过对一种语言感兴趣的任务进行微调，并对一种不同的语言进行评估而产生的，而在微调过程中没有看到这种情况。尽管取得了可喜的结果，但我们仍然缺乏对这种转移来源的正确理解。使用一种新的层消融技术和对模型内部表示的分析，我们表明，流行的多语言语言模型多语言BERT可以看作是两个子网络的叠加：一个多语言编码器和一个任务特定的语言不可知预测器。虽然编码器对于跨语言传输至关重要，并且在微调过程中基本保持不变，但任务预测器对传输几乎不重要，可以在微调过程中重新初始化。我们通过三个不同的任务、十七种不同类型的语言和多个领域的广泛实验来支持我们的假设。