本文提出了加利西亚语的单语伯特模型。我们遵循最近的趋势，即即使对于资源相对较低的语言，也可以建立健壮的单语BERT模型，同时比著名的官方多语言BERT（mBERT）性能更好。更具体地说，我们发布了两个单语加利西亚伯特模型，分别使用6层和12层变压器构建；使用有限的资源进行培训（单个24GB GPU上约4500万个令牌）。然后，我们对一些任务进行了详尽的评估，如词性标记、依赖项解析和命名实体识别。为此，所有这些任务都在纯序列标签设置中转换，以便运行BERT，而无需在其上包含任何附加层（我们仅使用输出分类层将上下文表示映射到预测标签）。实验表明，我们的模型，特别是12层模型，在大多数任务中都优于mBERT的结果。