像BERT这样预先训练好的上下文表示在自然语言处理中取得了巨大的成功。然而，人们发现，来自预先训练的语言模型的句子嵌入在没有微调的情况下很难捕捉到句子的语义。在本文中，我们认为在伯特嵌入中的语义信息没有被充分利用。我们首先从理论上揭示了蒙面语言模型预训练目标与语义相似性任务之间的理论联系，然后对BERT语句嵌入进行了实证分析。我们发现，BERT总是导致句子语义空间的非光滑各向异性，这损害了它的语义相似性。为了解决这个问题，我们建议通过规范化使用无监督目标学习的流，将各向异性句子嵌入分布转化为平滑的各向同性高斯分布。实验结果表明，我们提出的BERT-flow方法在各种语义-文本相似性任务上比最先进的句子嵌入方法获得了显著的性能提升。该守则可于https://github.com/bohanli/BERT-flow.