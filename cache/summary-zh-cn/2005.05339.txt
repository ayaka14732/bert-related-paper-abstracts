我们提出了一种简单的文本填充方法，即预测文档中任何位置的文本缺失跨度。虽然填充可以实现丰富的功能，特别是对于写作辅助工具，但更多的注意力集中在语言建模上——一种填充的特例，其中文本在文档末尾被预测。在本文中，我们的目标是将语言模型（LMs）的功能扩展到更一般的填充任务。为此，我们在包含人工屏蔽文本和被屏蔽文本的串联的序列上训练（或微调）现成的LMs。我们证明了这种方法，我们称之为通过语言建模填充，可以使LMs在三个不同的领域有效地填充整个句子：短篇小说、科学摘要和歌词。此外，我们还表明，人类很难将我们的方法填充的句子识别为短篇小说领域中机器生成的句子。