在本文中，我们深入研究了Transformer架构的两个核心组件：自我关注和上下文嵌入。特别地，我们研究了注意权重和标记嵌入的可识别性，以及上下文到隐藏标记的聚合。我们发现，对于长度超过注意头维度的序列，注意权重是不可识别的。我们建议将有效注意作为一种补充工具，用于改进基于注意的解释性解释。此外，我们还表明，输入标记在很大程度上保留了它们在整个模型中的身份。我们还发现证据表明，身份信息主要编码在嵌入的角度，并随着深度的增加而逐渐减少。最后，我们通过一种新的基于梯度属性的量化方法证明了在生成上下文嵌入时输入信息的强烈混合。总的来说，我们证明了自我注意分布是不可直接解释的，并且提供了更好地理解和进一步研究变压器模型的工具。