虽然以前的大多数工作都集中在不同的预训练目标和迁移学习体系结构上，但我们问如何使预训练模型最好地适应给定的目标任务。我们关注两种最常见的自适应形式：特征提取（预训练权重被冻结）和直接微调预训练模型。我们使用两种最先进的模型对不同NLP任务的实证结果表明，微调与特征提取的相对性能取决于训练前任务和目标任务的相似性。我们探索这一发现的可能解释，并为NLP从业者提供一套适应指南。