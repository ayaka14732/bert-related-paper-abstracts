在本文中，我们将ELECTRA风格的任务引入到跨语言模型预训练中。具体来说，我们提出了两个预训练任务，即多语言替换标记检测和翻译替换标记检测。此外，我们在多语言语料库和平行语料库上对模型XLM-E进行了预训练。我们的模型在各种跨语言理解任务上都优于基线模型，并且计算量小得多。此外，分析表明，XLM-E具有更好的跨语言迁移能力。