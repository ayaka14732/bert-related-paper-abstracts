我们介绍了SpanBERT，这是一种预训练方法，旨在更好地表示和预测文本的跨度。我们的方法通过（1）屏蔽连续的随机跨度，而不是随机令牌，以及（2）训练跨度边界表示来预测屏蔽跨度的整个内容，而不依赖其中的单个令牌表示，从而扩展了BERT。SpanBERT始终优于BERT和我们更好的调整基线，在范围选择任务（如问题回答和共指消解）方面有很大的收益。特别是，在与BERT large相同的训练数据和模型大小的情况下，我们的单一模型在1.1班和2.0班分别获得94.6%和88.7%的F1。我们还实现了OntoNotes共指消解任务（79.6\%F1）的最新发展，在TACRED关系提取基准测试中表现出色，甚至在GLUE方面也取得了进步。