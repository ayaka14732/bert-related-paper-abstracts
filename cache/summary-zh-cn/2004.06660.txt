最近，NLP在使用大型预训练模型方面出现了激增。用户下载在大型数据集上预先训练的模型权重，然后根据自己选择的任务微调权重。这就提出了一个问题：下载不可信的预训练权重是否会构成安全威胁。在本文中，我们证明了有可能构造“权重中毒”攻击，在这种攻击中，预先训练的权重被注入漏洞，这些漏洞在微调后暴露“后门”，使得攻击者能够通过简单地注入任意关键字来操纵模型预测。我们证明，通过应用正则化方法（我们称之为RIPPLe）和初始化过程（我们称之为嵌入手术），即使数据集和微调过程的知识有限，也可能发生此类攻击。我们在情感分类、毒性检测和垃圾邮件检测方面的实验表明，这种攻击具有广泛的适用性，并构成了严重的威胁。最后，我们概述了针对此类攻击的实际防御措施。复制我们实验的代码可在https://github.com/neulab/RIPPLe.