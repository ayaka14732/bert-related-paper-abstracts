如何使无监督的语言预训练更高效、资源更少，是自然语言处理的一个重要研究方向。在本文中，我们致力于通过提供更好的数据利用率来提高语言预训练方法的效率。众所周知，在语言数据语料库中，词的分布是重尾分布。大部分单词只出现很少几次，稀有单词的嵌入通常没有得到很好的优化。我们认为，这种嵌入携带的语义信号不足，这可能会导致数据利用效率低下，并减缓整个模型的预训练。为了缓解这一问题，我们建议动态记录（TNF），它在预训练期间动态记录罕见单词，以帮助模型在下次出现时理解它们。具体地说，TNF维护一个注释字典，当稀有词出现在句子中时，它会将稀有词的上下文信息保存为注释。当同一个稀有词在训练过程中再次出现时，可以利用事先保存的注释信息来增强当前句子的语义。通过这样做，TNF提供了更好的数据利用率，因为跨句子信息被用来覆盖句子中罕见的单词所造成的语义不足。我们在BERT和ELECTRA上实现TNF以检查其效率和有效性。实验结果表明，在达到相同性能时，TNF的训练时间比其主干预训练模型少60\%$。当使用相同的迭代次数进行训练时，TNF在大多数下游任务和平均GLUE分数上都优于其主干方法。源代码附在补充材料中。