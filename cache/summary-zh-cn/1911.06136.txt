预先训练的语言表示模型（PLM）不能很好地从文本中获取事实知识。相比之下，知识嵌入（KE）方法可以通过信息实体嵌入有效地表示知识图（KG）中的关系事实，但传统的KE模型不能充分利用丰富的文本信息。在本文中，我们提出了一个统一的知识嵌入和预训练语言表示模型（KEPLER），该模型不仅可以更好地将事实知识集成到PLM中，而且可以生成具有强PLM的有效文本增强KE。在开普勒中，我们使用PLM作为嵌入对文本实体描述进行编码，然后联合优化KE和语言建模目标。实验结果表明，开普勒在各种NLP任务上都取得了最新的性能，并且在KG链路预测方面也可以作为一个归纳的KE模型。此外，为了对开普勒进行预训练和评估，我们构建了Wikidata5M，这是一个具有对齐实体描述的大规模KG数据集，并在此基础上对最先进的KE方法进行了基准测试。它将作为一个新的KE基准，促进对大型KG、归纳式KE和带文本KG的研究。源代码可以从https://github.com/THU-KEG/KEPLER.