近年来，预训练语言模型在自然语言处理领域占据主导地位，并为各种复杂的自然语言任务带来了显著的性能提升。最突出的预训练语言模型之一是BERT，它以英语和多语言版本发布。尽管多语种的BERT在许多任务中表现良好，但最近的研究表明，在单一语言上训练的BERT模型明显优于多语种版本。因此，训练荷兰伯特模型对于荷兰NLP任务的广泛应用具有很大的潜力。以前的方法使用了早期的BERT实现来训练荷兰语版本的BERT，而我们使用RoBERTa（一种经过稳健优化的BERT方法）来训练一个名为robert的荷兰语模型。我们测量了它在各种任务上的性能以及微调数据集大小的重要性。我们还评估了特定语言标记器的重要性和模型的公平性。我们发现RobBERT改进了各种任务的最新结果，尤其是在处理较小数据集时，它的性能明显优于其他模型。这些结果表明，它是一个强大的预训练模型，适用于各种各样的荷兰语任务。预先训练和微调的模型公开提供，以支持进一步的荷兰NLP下游应用。