我们开发了一种新的方法，称为功率伯特（PoWER-BERT），以提高流行的伯特模型的推理时间，同时保持准确性。它的工作原理是：a）利用与字向量（中间编码器输出）相关的冗余并消除冗余向量。b） 基于自我注意机制，通过制定一种策略来衡量哪些词向量的重要性，从而确定要消除哪些词向量。c） 通过增加BERT模型和损失函数，学习要消除多少词向量。在标准GLUE基准测试上的实验表明，与BERT相比，PoWER BERT在推理时间上减少了4.5倍，准确度损失小于1%。我们表明，与以前的方法相比，幂BERT在准确性和推理时间之间提供了更好的折衷。我们证明，我们的方法在应用于高度压缩的BERT版本ALBERT时，推理时间减少了6.8倍，准确度损失小于1%。PoWER BERT的代码可在https://github.com/IBM/PoWER-BERT.