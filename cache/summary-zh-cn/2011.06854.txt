随着自然语言处理任务模型的激增，理解模型之间的差异及其相对优点变得更加困难。简单地看一下整体度量（如精度、BLEU或F1）之间的差异，并不能告诉我们为什么或如何使用特定方法，以及不同的数据集如何影响模型设计选择。在本文中，我们提出了一种通用的方法，用于命名实体识别（NER）任务的可解释性评估。建议的评估方法使我们能够解释模型和数据集之间的差异，以及它们之间的相互作用，确定当前系统的优缺点。通过提供我们的分析工具，我们使未来的研究人员能够轻松地进行类似的分析，并推动这一领域的进展：https://github.com/neulab/InterpretEval.