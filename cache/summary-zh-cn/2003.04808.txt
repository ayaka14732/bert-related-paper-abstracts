当前的阅读理解模型很好地推广到分布测试集，但在逆向选择的输入上表现不佳。以前大多数关于对抗性输入的工作都研究过敏感：语义不变的文本扰动，导致模型的预测在不应该改变时发生变化。在这项工作中，我们将重点放在互补问题上：过度预测不敏感，输入文本有意义地改变，但模型的预测没有改变，即使它应该改变。我们提出了一种嘈杂的对抗性攻击，该攻击在问题的语义变化中搜索，模型错误地预测了相同的答案，并且概率更高。尽管包含无法回答的问题，但SQuAD2.0和NewsQA模型都容易受到这种攻击。这表明，虽然准确，模型往往依赖于虚假模式，并没有充分考虑在一个问题中指定的信息。我们将数据增强和对抗性训练作为防御手段进行了实验，发现两者都能显著降低对隐藏数据和隐藏攻击空间的攻击脆弱性。解决不敏感问题还可以改善AddSent和AddOneSent的结果，并且当面临训练/评估分布不匹配时，模型更具普遍性：它们不太容易过度依赖仅存在于训练集中的预测线索，并且比传统模型高出10.9%。