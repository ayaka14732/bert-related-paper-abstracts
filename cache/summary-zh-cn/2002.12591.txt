最近关于开放领域问答的研究已经通过使用预先训练的语言模型（如BERT）实现了显著的性能改进。最先进的方法通常遵循“检索和读取”管道，并在将检索到的文档馈送到阅读器模块之前，使用基于BERT的重新排序器过滤检索到的文档。BERT检索器将问题和每个检索到的文档的连接作为输入。尽管这些方法在QA准确性方面取得了成功，但由于连接，它们几乎无法处理高吞吐量的传入问题，每个问题都有大量检索到的文档。为了解决效率问题，我们提出了DC-BERT，这是一种解耦的上下文编码框架，具有双BERT模型：在线BERT只对问题进行一次编码，离线BERT对所有文档进行预编码并缓存其编码。在团队开放式和自然问题开放式数据集上，DC-BERT的文档检索速度提高了10倍，同时与最先进的开放式领域问题回答方法相比，保留了大部分（约98%）的QA性能。