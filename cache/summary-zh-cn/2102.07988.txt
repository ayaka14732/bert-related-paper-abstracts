模型并行已成为训练现代大规模深层语言模型的必要条件。在这项工作中，我们从现有的模型并行方法中确定了一个新的正交维度：由于其自回归特性，可以在基于转换器的语言模型的单个训练序列中执行管道并行。与以前的工作相比，这可以实现更细粒度的管道。基于这一关键思想，我们设计了一种高性能令牌级流水线并行算法TeraPipe，用于基于变压器的语言模型的同步模型并行训练。我们开发了一种新的基于动态规划的算法来计算给定特定模型和集群配置的最佳流水线执行方案。我们表明，与最先进的模型并行方法相比，TeraPipe可以将AWS集群上具有1750亿个参数的最大GPT-3模型的训练速度提高5.0x，该集群具有48个p3.16xlarge实例。复制代码可在以下网址找到：https://github.com/zhuohan123/terapipe