中国预先训练的语言模型通常将文本处理为一系列字符，而忽略了更粗的粒度，例如单词。在这项工作中，我们提出了一种新的汉语预训练范式——格子BERT，它明确地结合了单词表示和字符，从而可以以多粒度的方式对句子进行建模。具体来说，我们从一个句子中的字符和单词构造一个格图，并将所有这些文本单元输入到转换器中。我们设计了一种晶格位置注意机制来利用自注意层中的晶格结构。我们进一步提出了一个掩蔽段预测任务，以推动模型从晶格中固有的丰富但冗余的信息中学习，同时避免学习意外的技巧。在11个中文自然语言理解任务上的实验表明，在12层设置下，我们的模型平均提高了1.5%，在线索基准上实现了基础尺寸模型的新水平。进一步的分析表明，latticebert可以利用晶格结构，其改进来自于对冗余信息和多粒度表示的探索。我们的代码将在https://github.com/alibaba/pretrained-language-models/LatticeBERT.