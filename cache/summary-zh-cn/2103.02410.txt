用领域知识丰富语言模型是关键但困难的。基于世界上最大的公共学术图开放学术图（OAG），我们预先训练了一个学术语言模型，即OAG-BERT，该模型集成了大量异构实体，包括论文、作者、概念、地点和从属关系。为了更好地赋予OAG-BERT捕捉实体信息的能力，我们开发了新的预训练策略，包括异构实体类型嵌入、实体感知2D位置编码和跨域感知实体掩蔽。对于零炮推断，我们设计了一种特殊的解码策略，允许OAG-BERT从头开始生成实体名称。我们评估了各种下游学术任务的OAG-BERT，包括NLP基准测试、零镜头实体推理、异构图链接预测和作者姓名消歧。结果表明，所提出的预培训方法对于理解学术文本和从异构实体建模知识都是有效的。OAG-BERT已被部署到多个实际应用程序中，如AMiner系统中的审阅者推荐和论文标记。它也可以通过CogDL包向公众提供。