基于Transformer的语言模型受益于对数百到数千个先前标记的上下文进行条件化。这些上下文的哪些方面有助于准确的模型预测？我们描述了一系列实验，这些实验通过选择性地去除在英语维基百科上训练的transformer语言模型中的词汇和结构信息来测量可用信息。在中长期语境中，我们发现一些极具破坏性的语境操作——包括改变句子中的词序和删除除名词以外的所有单词——删除了不到15%的可用信息。我们的结果表明，长上下文，而不是其详细的句法和命题内容，对于电流互感器语言模型的低复杂度是重要的。