大型预训练模型在许多自然语言处理任务中取得了巨大成功。然而，当它们应用于特定领域时，这些模型会受到领域转移的影响，并在延迟和容量限制的微调和在线服务方面带来挑战。在本文中，我们提出了一种为特定领域开发小型、快速和有效的预训练模型的通用方法。这是通过调整现成的通用预训练模型并在目标域中执行任务无关知识提取来实现的。具体来说，我们在适应阶段提出了特定领域的词汇扩展，并利用语料库级别的出现概率来自动选择增量词汇的大小。然后，我们系统地探索不同的策略来压缩特定领域的大型预训练模型。我们在生物医学和计算机科学领域进行实验。实验结果表明，在特定领域的任务中，我们的方法比BERT-BASE模型取得了更好的性能，比BERT-BASE模型小3.3倍，快5.1倍。代码和预先培训的模型可在https://aka.ms/adalm.