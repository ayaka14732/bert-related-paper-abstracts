人类可以利用在学习之前的任务时获得的知识，通过很少的例子有效地学习新的语言任务。在本文中，我们探讨了这种跨任务泛化能力是否可以获得，以及如何获得，并进一步应用于在不同NLP任务中构建更好的少镜头学习者。我们介绍了CrossFit，一个用于研究跨任务泛化能力的问题设置，它标准化了可见/不可见的任务划分、不同学习阶段的数据访问以及评估协议。为了在CrossFit中实例化不同的可见/不可见任务分区并促进深入分析，我们介绍了NLP少数镜头健身房，它是一个由160个不同的少数镜头NLP任务组成的存储库，这些任务是从开放存取NLP数据集创建的，并转换为统一的文本到文本格式。我们的分析表明，通过使用一组可见任务的上游学习阶段，可以提高对不可见任务的少量快照学习能力。我们还观察到，上游学习任务的选择会显著影响不可见任务的少数镜头表现，要求进一步分析任务相似性和可转移性。