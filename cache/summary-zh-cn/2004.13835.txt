最近，大型预训练语言模型（如BERT和GPT-2）的成功表明，在下游对话生成任务中加入语言先验知识是有效的。但是，预先训练的模型在对话任务上的性能并不像预期的那样最佳。在本文中，我们提出了一个预先训练的角色交替语言模型（PRAL），专门为面向任务的会话系统设计。我们采用（Wu等人，2019年）分别为两个扬声器建模。我们还设计了一些技术，如起始位置随机化、知识提取和历史折扣，以提高训练前的性能。通过清理13个现有数据集，我们引入了一个面向任务的对话预训练数据集。我们在三个不同的下游任务上测试PRAL。结果表明，PRAR表现更好或等同于最先进的方法。