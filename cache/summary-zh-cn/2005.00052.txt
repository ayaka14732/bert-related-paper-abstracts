最先进的预先训练的多语言模型（如多语言BERT和XLM-R）背后的主要目标是通过零触发或少触发跨语言传输在低资源语言中启用和引导NLP应用程序。然而，由于模型容量有限，在这种低资源语言和训练前未见过的语言上，它们的迁移性能最差。我们提出了MAD-X，这是一个基于适配器的框架，通过学习模块化语言和任务表示，它可以实现对任意任务和语言的高可移植性和参数有效传输。此外，我们还介绍了一种新的可逆适配器体系结构和一种强基线方法，用于将预先训练好的多语言模型适应新语言。在命名实体识别和因果常识推理方面，MAD-X在跨一组具有代表性的类型多样语言的跨语言迁移方面优于最新技术，并在问答方面取得了有竞争力的结果。我们的代码和适配器可从AdapterHub.ml获得