在自然语言处理（NLP）中，流行变压器模型的多头自注意被广泛使用，包括用于抽取摘要任务。为了分析和修剪参数重的自我注意机制，有多种方法提出了更多参数轻的自我注意替代方案。在本文中，我们提出了一种新的参数精益自我注意机制使用话语先验。我们的新树自我关注基于文档级的话语信息，用另一个轻量级的替代方案扩展了最近提出的“合成器”框架。我们的实证结果表明，我们的树自我注意方法在抽取摘要任务上取得了有竞争力的胭脂分数。与原始的单头变压器模型相比，尽管注意成分的参数显著减少，但树注意方法在EDU和句子水平上都达到了相似的性能。当应用更平衡的超参数设置时，我们在句子层面上的表现明显优于8头变压器模型，需要的参数数量级更少。