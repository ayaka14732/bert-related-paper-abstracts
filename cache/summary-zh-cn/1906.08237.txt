基于自回归语言建模的预训练方法与基于自回归语言建模的预训练方法相比，基于自编码的预训练方法具有更好的性能。然而，依赖于使用掩码破坏输入，BERT忽略了掩码位置之间的依赖关系，并遭受了预训练微调差异。鉴于这些优点和缺点，我们提出了XLNet，这是一种广义自回归预训练方法，它（1）通过最大化因子分解顺序的所有排列的期望可能性来学习双向上下文，（2）由于其自回归公式，克服了BERT的局限性。此外，XLNet将Transformer XL（最先进的自回归模型）的思想集成到预训练中。从经验上看，在可比较的实验设置下，XLNet在20项任务上都优于BERT，通常相差很大，包括问答、自然语言推理、情绪分析和文档排名。