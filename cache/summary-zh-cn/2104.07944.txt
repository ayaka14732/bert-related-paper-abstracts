在在线领域特定的客户服务应用程序中，由于数据集的可用性和噪音有限，许多公司难以成功部署高级NLP模型。虽然先前的研究表明，为特定领域的任务迁移大型开放领域预培训模型具有潜力，但在此类社交媒体客户服务环境中，特别是在多语言条件下，尚未严格评估适当的（预）培训策略。我们通过收集包含客户服务对话（865k推特）的多语言社交媒体语料库，比较各种预培训和微调方法，将其应用于5项不同的最终任务，来解决这一差距。我们表明，在对特定的最终任务进行微调之前，在域内数据集上预先训练通用多语言转换器模型，可以持续提高性能，特别是在非英语环境中。