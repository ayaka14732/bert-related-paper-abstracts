学习能够准确建模语义的表示是自然语言处理研究的一个重要目标。许多语义现象依赖于句法结构。最近的工作考察了训练前表征的最新模型（如BERT）在多大程度上捕捉了这种依赖结构的现象，但主要局限于英语中的一种现象：主语和动词之间的数字一致性。我们在一个新的跨26种语言的半自动管理数据集中评估了BERT对四种类型的结构相关协议关系的敏感性。我们表明，单语言和多语言的BERT模型总体上都很好地捕获了语法敏感的协议模式，但我们也强调了它们性能下降的特定语言环境。