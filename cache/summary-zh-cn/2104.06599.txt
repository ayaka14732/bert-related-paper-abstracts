自然语言提示最近被用于引导预训练的语言模型执行其他人工智能任务，使用填充空白范式（Petroni等人，2019年）或少数镜头外推范式（Brown等人，2020年）。例如，语言模型保留训练语料库中的事实知识，这些知识可以通过要求他们在句子提示中“填空”来提取。但是，这个提示来自哪里？我们探索了通过梯度下降学习提示的想法——或者是从以前的工作中微调提示，或者是从随机初始化开始。我们的提示由“软词”组成，即不一定是语言模型中嵌入的词类型的连续向量。此外，对于每个任务，我们优化了提示的混合，学习哪些提示最有效，以及如何集成它们。在多个英语LMs和任务中，我们的方法大大优于以前的方法，表明语言模型中隐含的事实知识以前被低估了。此外，获取这些知识的成本很低：随机初始化几乎和知情初始化一样好。