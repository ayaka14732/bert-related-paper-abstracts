在自然语言处理中，预训练语言模型是普遍存在的。尽管取得了成功，但大多数可用的模型要么接受了关于英语数据的培训，要么接受了关于以多种语言连接数据的培训。这使得在除英语以外的所有语言中实际使用此类模型非常有限。本文以法语为例，探讨了基于单语变换器的语言模型在其他语言中训练的可行性，并对我们的语言模型进行了词性标注、依存分析、命名实体识别和自然语言推理任务的评估。我们表明，使用网络爬网数据比使用维基百科数据更可取。更令人惊讶的是，我们发现一个相对较小的网络爬网数据集（4GB）所得到的结果与使用较大数据集（130+GB）得到的结果一样好。我们的最佳表现模型CamemBERT在所有四项下游任务中都达到或改进了最先进的水平。