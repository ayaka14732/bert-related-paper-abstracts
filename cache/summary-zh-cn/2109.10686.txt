关于变压器架构的伸缩行为，仍然存在许多悬而未决的问题。这些扩展决策和发现可能非常关键，因为培训运行通常伴随着相关的计算成本，这会对财务和/或环境产生影响。本文的目标是展示从预训练和微调变压器中获得的缩放见解。虽然Kaplan等人对变压器语言模型的缩放行为进行了全面研究，但范围仅限于上游（预培训）损耗。因此，目前尚不清楚这些发现是否会转移到pretrain finetune范式下的下游任务中。本文的主要发现如下：（1）我们表明，除了模型大小外，下游微调的模型形状也很重要，（2）缩放协议在不同的计算区域运行不同，（3）广泛采用的T5基和T5大尺寸是帕累托无效的。为此，我们提出了改进的缩放协议，与广泛采用的T5基础模型相比，我们重新设计的模型实现了类似的下游微调质量，同时参数减少了50%，训练速度加快了40%。我们公开发布了100多个不同T5配置的预训练检查点，以促进未来的研究和分析。