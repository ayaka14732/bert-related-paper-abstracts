基于注意的序列到序列（seq2seq）模型在自动语音识别（ASR）中取得了很好的效果。然而，当这些模型以从左到右的方式解码时，它们无法访问右侧的上下文。我们通过知识提炼，将BERT作为外部语言模型应用于seq2seq ASR，从而利用左右上下文。在我们提出的方法中，BERT生成软标签来指导seq2seq ASR的训练。此外，我们利用当前话语之外的语境作为输入。实验评估表明，我们的方法显著提高了自发日语语料库（CSJ）中seq2seq基线的ASR性能。从BERT中提取的知识要比从只关注左上下文的transformer LM中提取的知识好。我们还展示了在当前话语之外利用语境的有效性。我们的方法优于其他LM应用方法，如n-最佳重新扫描和浅层融合，但不需要额外的推理成本。