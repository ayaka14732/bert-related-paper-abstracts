对于多模式机器学习研究而言，仅使用少数注释示例构建能够快速适应众多任务的模型是一个开放的挑战。我们介绍了Flamingo，一系列具有这种能力的视觉语言模型（VLM）。Flamingo模型包括以下关键架构创新：（i）桥接强大的预训练视觉模型和语言模型，（ii）处理任意交错的视觉和文本数据序列，以及（iii）无缝接收图像或视频作为输入。由于其灵活性，Flamingo模型可以在包含任意交错文本和图像的大规模多模式web语料库上进行训练，这是赋予它们在上下文中少量快照学习能力的关键。我们对提出的Flamingo模型进行了全面评估，探索并测量了其快速适应各种图像和视频理解基准的能力。这些任务包括开放式任务，如视觉问答，其中模型被提示必须回答的问题，字幕任务，评估描述场景或事件的能力，以及封闭式任务，如多项选择视觉问答。对于位于该频谱上任何位置的任务，我们证明了单个Flamingo模型可以通过使用特定于任务的示例来提示模型，从而实现少数镜头学习的最新状态。在许多基准测试中，Flamingo实际上超过了根据数千倍于特定任务的数据进行微调的模型的性能。