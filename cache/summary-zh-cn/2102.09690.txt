当提供包含一些训练示例的自然语言提示时，GPT-3可以执行许多任务。我们发现，这种类型的少数镜头学习可能是不稳定的：提示格式、训练示例的选择，甚至训练示例的顺序都可能导致准确性从近乎偶然到近乎先进的变化。我们证明，这种不稳定性源于语言模型对预测某些答案的偏见，例如，那些放在提示末尾附近或在训练前数据中常见的答案。为了缓解这种情况，我们首先通过在给定训练提示和无内容测试输入（如“N/a”）的情况下询问模型的预测来估计模型对每个答案的偏差。然后，我们拟合校准参数，使该输入的预测在所有答案中保持一致。在一组不同的任务中，此上下文校准程序大大提高了GPT-3和GPT-2的平均精度（高达30.0%绝对精度），并减少了不同提示选择之间的差异。