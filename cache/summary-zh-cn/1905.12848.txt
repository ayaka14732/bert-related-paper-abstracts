会话机器理解（CMC）要求理解多轮对话的上下文。使用训练前语言模型BERT已经成功地进行了单轮机器理解，而由于BERT对输入序列的数量和长度有限制，因此还没有建立用BERT对多轮问答的建模。在本文中，我们提出了一个简单而有效的方法，与伯特为CMC。我们的方法使用伯特编码一段独立的条件与每个问题和每个答案在一个多回合的背景。然后，该方法根据用BERT编码的段落表示预测答案。对具有代表性的CMC数据集QuAC和CoQA的实验表明，我们的方法优于最近发表的方法（+0.8 F1关于QuAC和+2.1 F1关于CoQA）。此外，我们对对话历史的数量和类型对CMC准确性的影响进行了详细分析，我们发现黄金回答历史（在实际对话中可能没有给出）对两个数据集的模型性能贡献最大。