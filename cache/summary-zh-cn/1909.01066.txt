在大型文本语料库上预训练语言模型的最新进展导致了对下游NLP任务的大量改进。在学习语言知识的同时，这些模型还可以存储训练数据中存在的关系知识，并且可以回答“填空”完形填空语句结构的查询。与结构化知识库相比，语言模型有许多优点：它们不需要模式工程，允许实践者查询一个开放的关系类，易于扩展到更多数据，并且不需要人工监督培训。我们对大量最先进的预训练语言模型中已经存在的关系知识（无需微调）进行了深入分析。我们发现（i）在没有微调的情况下，BERT包含与传统NLP方法竞争的关系知识，传统NLP方法具有一定的oracle知识访问权限，（ii）BERT在开放领域问题回答方面也表现出色，并且（iii）在有监督的基线下通过标准的语言模型预训练方法，某些类型的事实知识比其他类型的更容易学习。这些模型在不进行任何微调的情况下回忆事实知识的能力惊人地强大，证明了它们作为无监督的开放领域QA系统的潜力。重现我们分析的代码可在https://github.com/facebookresearch/LAMA.