我们提出了一种利用预训练语言模型的有效方法，其中我们学习用于预训练权重的选择性二进制掩码，而不是通过微调来修改它们。对一系列NLP任务的掩蔽BERT和RoBERTa的广泛评估表明，我们的掩蔽方案产生的性能与微调相当，但当需要同时推断多个任务时，其内存占用要小得多。通过内在评估，我们证明了由屏蔽语言模型计算的表示编码了解决下游任务所需的信息。通过分析损失情况，我们发现掩蔽和微调产生的模型位于极小值，可以通过线段连接，几乎保持恒定的测试精度。这证实了掩蔽可以作为精细调谐的有效替代品。