预先训练的情境表征模型（Peters等人，2018；Devlin等人，2018）推动了许多NLP任务的最新进展。BERT的最新版本（Devlin，2018）包括一个同时对104种语言进行预训练的模型，该模型在自然语言推理任务中具有令人印象深刻的零镜头跨语言迁移性能。本文探讨了mBERT（Multilanguage）作为一种零镜头语言迁移模型在5项NLP任务中更广泛的跨语言潜力，这些任务涉及来自不同语系的39种语言：NLI、文档分类、NER、词性标记和依存分析。我们比较了mBERT和最佳的零射跨语言迁移方法，发现mBERT在每项任务上都具有竞争力。此外，我们还研究了以这种方式利用mBERT的最有效策略，确定mBERT在多大程度上概括了语言特有的特征，并测量了影响跨语言迁移的因素。