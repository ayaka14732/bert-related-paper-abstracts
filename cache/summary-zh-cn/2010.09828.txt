跨语言实体将多种语言中的背景信息链接到单一语言知识库。我们为这项任务提出了一个神经排序架构，它使用神经网络中提及和上下文的多语言表示。我们发现，在单语和多语环境下，BERT的多语言能力导致了稳健的性能。此外，我们探索了零镜头语言迁移，发现了令人惊讶的鲁棒性能。我们研究了零镜头退化，发现它可以通过建议的辅助训练目标得到部分缓解，但剩余的错误最好归因于域转移，而不是语言迁移。