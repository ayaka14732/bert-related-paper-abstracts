在本文中，我们提出了一种多任务深层神经网络（MT-DNN），用于跨多个自然语言理解（NLU）任务学习表征。MT-DNN不仅利用了大量跨任务数据，而且还受益于正则化效应，该效应导致更通用的表示，以适应新的任务和域。MT-DNN扩展了Liu等人（2015年）提出的模型，加入了预先训练的双向变压器语言模型，称为BERT（Devlin等人，2018年）。MT-DNN在十项NLU任务（包括SNLI、SciTail和九项GLUE任务中的八项）上获得了最新的结果，将GLUE基准推至82.7%（绝对改善2.2%）。我们还使用SNLI和SciTail数据集证明，MT-DNN学习的表示允许域自适应，域内标签明显少于预先训练的BERT表示。代码和预先培训的模型可在https://github.com/namisan/mt-dnn.