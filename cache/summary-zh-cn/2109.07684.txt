通用语言模型已经显示出令人印象深刻的能力，在一系列的下游自然语言处理（NLP）任务和基准时，与最先进的方法相媲美，当从非常少的示例推断指令时。在此，我们评估了GPT和T5模型在无任何参数更新的情况下对非英语语言进行多类别分类的多语言技能。我们发现，在给定一些英语例子作为上下文的情况下，预先训练的语言模型不仅可以预测英语测试样本，而且可以预测非英语测试样本。最后，我们发现语言模型的上下文少镜头跨语言预测结果明显优于随机预测，并且与现有的最新跨语言模型相比具有竞争力。