最近通过自我监督对大量文本进行预训练的巨大模型在各种自然语言处理任务中取得了最新的成果。然而，这些庞大且昂贵的模型很难在实践中用于下游任务。最近的一些工作使用知识提炼来压缩这些模型。然而，我们看到，与大教师相比，小学生模型的表现存在差距。在这项工作中，除了有限的标记训练实例之外，我们还利用了大量域内未标记的传输数据来弥合提取BET的差距。我们表明，简单的RNN为基础的学生模型，即使具有硬蒸馏，可以执行与给定的巨大的教师传递集。通过软蒸馏和利用教师的中间表现，学生的表现可以进一步提高。我们的研究表明，我们的学生模型可以将庞大的教师压缩高达26倍，同时在低资源环境下，使用少量的标记数据，仍然可以匹配甚至略微超过教师的表现。此外，对于XtremeDistil（慕克吉和Hassan Awadallah，2020年）的多语言扩展工作，我们展示了对多语言类BERT教师模型的大量提炼，在参数压缩方面高达35倍，在批量推理延迟加速方面高达51倍，同时在41种语言中保留95%的F1分数。