在海量数据集上训练大型深层神经网络在计算上非常具有挑战性。最近，使用大批量随机优化方法来解决这个问题的兴趣激增。这一研究领域最突出的算法是LARS，它通过采用分层自适应学习速率，在几分钟内在ImageNet上训练ResNet。然而，对于像伯特这样的注意力模型，LAR的表现很差，这表明它在不同任务中的表现并不一致。在本文中，我们首先研究了一种原则性的分层自适应策略，以加速使用大批量和小批量的深层神经网络的训练。利用该策略，我们开发了一种新的分层自适应大批量优化技术LAMB；然后，我们提供了LAMB和LARS的收敛性分析，显示了在一般非凸设置下收敛到一个稳定点。我们的实证结果表明，LAMB在各种任务（如BERT和ResNet-50训练）中表现优异，超参数调整很少。特别是对于BERT培训，我们的优化器支持使用非常大的批量32868，而不会降低性能。通过将批量大小增加到TPUv3 Pod的内存限制，BERT训练时间可以从3天减少到76分钟（表1）。LAMB实现可在https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py