随着移动设备变得无处不在，与各种用户界面（UI）定期交互是许多人日常生活的一个常见方面。为了提高这些设备的可访问性并使其能够在各种设置中使用，构建能够帮助用户并通过UI完成任务的模型至关重要。然而，要实现这一目标还面临着一些挑战。首先，外观相似的UI组件可以具有不同的功能，因此理解它们的功能比分析它们的外观更重要。其次，特定于领域的功能，如网页中的文档对象模型（DOM）和移动应用程序中的视图层次结构（VH），提供了有关UI元素语义的重要信号，但这些功能不是自然语言格式。第三，由于用户界面的巨大多样性以及缺乏标准的DOM或VH表示，构建具有高覆盖率的用户界面理解模型需要大量的培训数据。受NLP中基于预训练的方法以数据高效方式解决各种问题的成功启发，我们引入了一种新的预训练UI表示模型ActionBert。我们的方法旨在利用用户交互跟踪中的视觉、语言和领域特定功能，预先训练UI及其组件的通用功能表示。我们的关键直觉是，用户操作（例如，对不同UI组件的一系列单击）揭示了有关其功能的重要信息。我们对所提出的模型进行了广泛的下游任务评估，从图标分类到基于其自然语言描述的UI组件检索。实验表明，所提出的ActionBert模型在所有下游任务中的性能比多模式基线高达15.5%。