注释方法的创新已经成为阅读理解（RC）数据集和模型的催化剂。挑战当前RC模型的一个最新趋势是在注释过程中加入一个模型：人类以敌对的方式创建问题，以至于模型无法正确回答问题。在这项工作中，我们研究了这种注释方法，并将其应用于三种不同的设置中，总共收集了36000个样本，注释循环中的模型越来越强。这使我们能够探索诸如对抗效应的再现性、从使用不同模型在回路中强度收集的数据转移，以及推广到不使用模型收集的数据等问题。我们发现，对敌对收集的样本进行训练会导致对非敌对收集的数据集的强泛化，但随着循环中的模型越来越强，性能会逐渐恶化。此外，我们发现更强的模型仍然可以从收集的数据集中学习，这些数据集中的模型在循环中非常弱。当使用循环中的BiDAF模型收集数据进行训练时，RoBERTa在无法回答的问题上达到39.9F1——仅略低于使用RoBERTa自身收集的数据进行训练时（41.0F1）。