与单语模型相比，跨语言模型通常需要更具表现力的词汇表来充分表示所有语言。我们发现，由于词汇量有限，许多语言在最近的跨语言语言模型中的代表性不足。为此，我们提出了一种VoCap算法来确定每种语言所需的词汇量。然而，增加词汇量会显著降低训练前的速度。为了解决这个问题，我们提出了基于k-NN的目标采样来加速昂贵的softmax。我们的实验表明，使用VoCap学习的多语种词汇有利于跨语种语言模型的预训练。此外，基于k-NN的目标抽样减少了增加词汇量的副作用，同时实现了可比的性能和更快的训练前速度。代码和经过培训的多语言词汇表可在https://github.com/bozheng-hit/VoCapXLM.