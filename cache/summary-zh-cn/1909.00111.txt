递归神经网络平均可以很好地预测即将出现的单词；然而，在句法复杂的语境中，他们往往会意外地赋予不合语法的单词很高的概率。我们调查通过增加网络规模和训练语料库在多大程度上可以缓解这些缺点。我们发现，增加网络规模所带来的收益在一定程度上是最小的。同样，扩大训练语料库会产生递减的回报；我们估计，为了使模型与人的表现相匹配，训练语料库需要非常大。通过与GPT和BERT（基于变换器的基于数十亿单词的训练模型）的比较，可以发现这些模型在某些结构中的性能甚至比我们的LSTM差。我们的结果为更高效的数据架构提供了依据。