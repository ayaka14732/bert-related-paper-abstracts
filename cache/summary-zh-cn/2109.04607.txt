我们介绍了IndoBERTweet，这是第一个印尼Twitter的大规模预训练模型，它是通过扩展一个单语训练的印尼BERT模型和附加的特定领域词汇来训练的。我们特别关注词汇失配下的有效模型调整，并对新单词类型初始化BERT嵌入层的不同方法进行基准测试。我们发现，在七个基于Twitter的数据集上，使用平均BERT子词嵌入进行初始化使预训练速度提高了五倍，并且在外部评估方面比提出的词汇适应方法更有效。