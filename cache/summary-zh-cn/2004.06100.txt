虽然像BERT这样的预训练变压器在分布示例中实现了高精度，但它们是否可以推广到新的分布？我们通过构建一个新的具有真实分布变化的鲁棒性基准，系统地度量了七个NLP数据集的分布外（OOD）泛化。我们测量了先前模型的泛化，包括单词袋模型、ConvNets和LSTMs，并且我们表明，预训练变压器的性能下降要小得多。预训练变压器在检测异常或OOD示例方面也更有效，而以前的许多模型往往比偶然性更差。我们研究了影响稳健性的因素，发现较大的模型不一定更稳健性，蒸馏可能有害，更多样化的预训练数据可以增强稳健性。最后，我们展示了未来的工作可以提高OOD健壮性的地方。