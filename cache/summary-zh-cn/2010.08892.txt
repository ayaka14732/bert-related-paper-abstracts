跨语言摘要（CLS）旨在以目标语言为源语言的文章生成摘要。传统的解决方案采用两步法，即翻译后总结或总结后翻译。近年来，端到端模型取得了较好的效果，但这些方法主要受到对大规模标记数据依赖性的限制。我们提出了一种基于混合语言预训练的解决方案，该方案利用了跨语言任务（如翻译）和单语任务（如蒙面语言模型）。因此，我们的模型可以利用大量的单语数据来增强对语言的建模。此外，该体系结构没有特定于任务的组件，这节省了内存并提高了优化效率。实验表明，这种预训练方案可以有效地提高跨语言摘要的性能。在神经跨语言摘要（NCLS）数据集中，我们的模型实现了2.82（英文对中文）和1.15（中文对英文）的ROUGE-1分数比最先进的结果提高。