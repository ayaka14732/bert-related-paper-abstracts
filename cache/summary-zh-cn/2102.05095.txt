我们提出了一种无卷积的视频分类方法，该方法完全基于空间和时间上的自我注意。我们的方法称为“时间转换器”，通过直接从一系列帧级补丁中学习时空特征，使标准转换器结构适应视频。我们的实验研究比较了不同的自我注意方案，并表明“分散注意”（时间注意和空间注意分别应用于每个块）在所考虑的设计选择中可以获得最佳的视频分类精度。尽管采用了全新的设计，TimeSformer在多个动作识别基准上取得了最先进的结果，包括Kinetics-400和Kinetics-600的最佳报告准确性。最后，与3D卷积网络相比，我们的模型训练速度更快，测试效率显著提高（精度略有下降），并且还可以应用于更长的视频剪辑（超过一分钟）。有关代码和型号，请访问：https://github.com/facebookresearch/TimeSformer.