最近，在最先进的文本摘要模型中，BERT被用于文档编码。然而，基于句子的抽取模型通常会导致抽取的摘要中出现冗余或无信息的短语。此外，整个文档中的长期依赖关系不能很好地由BERT捕获，它是在句子对而不是文档上进行预训练的。为了解决这些问题，我们提出了一个语篇感知的神经摘要模型——DiscoBert。DiscoBert提取次句子的话语单位（而不是句子）作为更细粒度提取选择的候选。为了捕获语篇单元之间的长期依赖关系，基于RST树和共指提及构建结构语篇图，并用图卷积网络编码。实验表明，与其他基于BERT的模型相比，该模型的性能要优于现有的方法，在流行的摘要基准上有显著的优势。