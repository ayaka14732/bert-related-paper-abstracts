在预训练和微调范式下，预训练语言模型（PLM）在各种自然语言处理（NLP）任务中取得了巨大成功。plm具有大量的参数，计算量大，资源消耗大。因此，模型修剪被引入到大规模PLM的压缩中。然而，大多数先验方法只考虑任务特定知识对下游任务的影响，而忽略修剪过程中不必要的任务无关知识，这可能导致灾难性遗忘问题，并导致泛化能力差。为了在我们的剪枝模型中保持任务不可知和任务特定的知识，我们在预训练和微调的范式下提出了对比剪枝（CAP）。它被设计为一个通用框架，兼容结构化和非结构化修剪。CAP统一于对比学习，使剪枝模型能够从预先训练的任务不可知知识模型和微调的任务特定知识模型中学习。此外，为了更好地保持修剪模型的性能，快照（即每次修剪迭代中的中间模型）也可以作为修剪的有效监督。我们的大量实验表明，采用CAP始终会产生显著的改进，特别是在非常高的稀疏性场景中。仅保留3%的模型参数（即97%的稀疏性），CAP在QQP和MNLI任务中成功实现了原始BERT性能的99.2%和96.3%。此外，我们的探索性实验表明，CAP修剪后的模型具有更好的泛化能力。