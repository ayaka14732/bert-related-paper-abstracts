在本文中，我们研究了预训练数据的内在性质如何影响微调的下游性能。为此，我们在几个具有特定特征的语料库上预先训练不同的基于转换器的蒙面语言模型，并在GLUE基准上微调这些语言模型。我们发现，在非结构化数据上预先训练的模型比在下游任务上直接从头训练的模型要好。我们的结果还表明，对结构化数据的预训练并不总是能够使模型获得可以转移到自然语言下游任务的能力。令我们大吃一惊的是，我们发现，对某些非人类语言数据的预训练使GLUE的表现接近于对另一种非英语语言的预训练。