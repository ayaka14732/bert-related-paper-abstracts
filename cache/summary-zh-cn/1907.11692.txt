语言模型预训练带来了显著的性能提升，但仔细比较不同方法是一项挑战。训练在计算上是昂贵的，通常在不同大小的私有数据集上进行，并且，正如我们将要展示的，超参数的选择对最终结果有重大影响。我们提出了一项伯特预训练的复制研究（Devlin等人，2019年），该研究仔细测量了许多关键超参数和训练数据大小的影响。我们发现，BERT的训练明显不足，可以匹配或超过发布后的每个模型的性能。我们最好的车型在胶水、比赛和阵容方面都达到了最先进的水平。这些结果突出了以前被忽视的设计选择的重要性，并对最近报告的改进来源提出了疑问。我们发布了我们的模型和代码。