预训练语言模型（PLM）如何从训练集中学习事实知识？我们研究了两个最重要的机制：推理和记忆。先前的工作试图量化PLM学习到的事实数量，但我们使用合成数据提出了第一项研究，调查培训中出现的事实与PLM学习到的事实之间的因果关系。对于推理，我们表明，PLM似乎学会了正确应用一些符号推理规则，但与其他规则（包括两跳推理）存在冲突。进一步的分析表明，即使是学习推理规则的应用也存在缺陷。对于记忆，我们确定图式一致性（事实系统地由其他事实支持）和频率是其成功的关键因素。