学习高质量的句子表征有利于广泛的自然语言处理任务。尽管基于BERT的预训练语言模型在许多下游任务上都取得了很高的性能，但是本地派生的句子表示被证明是折叠的，因此在语义-文本相似性（STS）任务上的性能较差。在本文中，我们提出了一个用于自我监督句子表征迁移的对比框架ConSERT，该框架采用对比学习以无监督和有效的方式对BERT进行微调。通过使用未标记文本，ConSERT解决了BERT派生语句表示的崩溃问题，使其更适用于下游任务。在STS数据集上的实验表明，ConSERT比之前的最新技术水平相对提高了8%，甚至可以与受监督的SBERT-NLI相媲美。当进一步纳入NLI监管时，我们在STS任务上实现了新的最先进的表现。此外，ConSERT仅在1000个可用样本的情况下获得了可比结果，显示了其在数据稀缺场景中的稳健性。