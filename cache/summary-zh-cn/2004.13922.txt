来自Transformers的双向编码器表示（BERT）在各种NLP任务中表现出了惊人的改进，并且已经提出了连续变体来进一步改进预先训练的语言模型的性能。在本文中，我们的目标是重新审视汉语预训练语言模型，以检验其在非英语语言中的有效性，并向社区发布汉语预训练语言模型系列。我们还提出了一个简单但有效的模型，称为MacBERT，它在几个方面改进了RoBERTa，特别是采用传销作为校正（Mac）的掩蔽策略。我们对八个中文NLP任务进行了广泛的实验，以重新审视现有的预训练语言模型以及提出的MacBERT模型。实验结果表明，MacBERT可以在许多NLP任务中获得最先进的性能，并且我们还利用一些可能有助于未来研究的发现消除细节。现有资源：https://github.com/ymcui/MacBERT