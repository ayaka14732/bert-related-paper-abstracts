大规模预先训练的语言模型，如BERT，在语言理解任务中取得了巨大的成功。然而，如何利用BERT生成语言仍然是一个悬而未决的问题。在本文中，我们提出了一种新的方法，条件掩蔽语言建模（C-MLM），以实现对目标生成任务的BERT微调。微调的BERT（教师）被用作额外的监督，以改进传统的Seq2Seq模型（学生），从而获得更好的文本生成性能。通过利用BERT独特的双向性，提取在BERT中学习到的知识可以鼓励自回归Seq2Seq模型提前计划，对连贯文本生成实施全局序列级监督。实验表明，该方法在机器翻译和文本摘要等多语言生成任务上的性能明显优于强转换基线。我们提出的模型在IWSLT德语-英语和英语-越南语机器翻译数据集上也达到了新的水平。代码可在https://github.com/ChenRocks/Distill-BERT-Textgen.