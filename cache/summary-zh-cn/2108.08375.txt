本文研究了注意头在基于变压器的模型中的相对重要性，以帮助它们在跨语言和多语言任务中的解释能力。先前的研究发现，在每项单语言自然语言处理（NLP）任务中，只有少数注意头是重要的，修剪其余的注意头可以使模型的性能相当或得到改进。然而，在跨语言和多语言任务中，修剪注意头的影响尚不清楚。通过大量的实验，我们发现：（1）在基于多语言转换的模型中剪除大量的注意头通常对其在跨语言和多语言任务中的表现有积极的影响；（2）可以使用梯度对要剪除的注意头进行排序，并通过一些试验进行识别。我们的实验集中在序列标记任务上，可能适用于其他跨语言和多语言任务。为了全面性，我们研究了两个预先训练的多语言模型，即多语言BERT（mBERT）和XLM-R，它们分别涉及9种语言的三个任务。我们还讨论了我们的发现的有效性及其对真正资源稀缺的语言和其他任务设置的可扩展性。