在原始文本上预训练大规模神经语言模型对于改进自然语言处理（NLP）中的迁移学习做出了重要贡献。随着基于转换器的语言模型的引入，例如来自转换器的双向编码器表示（BERT），NLP从自由文本中提取信息的性能在普通领域和医学领域都有了显著提高；然而，对于很少有高质量和大型公共可用数据库的领域，很难训练出性能良好的特定BERT模型。我们假设这个问题可以通过对特定领域的语料库进行上采样并以平衡的方式使用它对更大的语料库进行预训练来解决。我们提出的方法包括一个单一的干预和一个选项：在上采样和扩大词汇量后同时进行预训练。我们进行了三次实验，并对结果进行了评估。我们确认，我们的日语医学BERT在医学文档分类任务方面优于传统基线和其他BERT模型，并且我们的英语BERT使用通用和医学领域语料库进行预训练，在生物医学语言理解方面表现良好，可供实际使用评估（蓝色）基准。此外，我们的增强生物医学BERT模型（在训练前未使用临床笔记）表明，BLUE基准的临床和生物医学得分均比未使用我们提出的方法训练的消融模型高0.3分。通过从适合于目标任务的语料库中提取上采样实例进行均衡的预训练，我们可以构建一个高性能的BERT模型。