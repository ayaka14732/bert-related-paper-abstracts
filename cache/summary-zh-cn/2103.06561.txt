近年来，人们对多模态预训练模型进行了深入的探索，以架起视觉和语言之间的桥梁。然而，大多数模型通过假设文本和图像模态之间存在很强的语义相关性，明确地模拟了图像-文本对之间的跨模态交互。由于这种强假设在现实场景中往往无效，我们选择隐式建模大规模多模态预训练的跨模态相关性，这是我们团队领导的中国项目“文兰”的重点。具体来说，基于图像-文本对的弱相关性假设，我们在跨模态对比学习框架下提出了一个双塔预训练模型BriVL。与采用简单对比学习方法的OpenAI CLIP不同，我们通过将最新的方法MoCo应用到跨模态场景中，设计了更高级的算法。通过构建一个大型的基于队列的字典，我们的BriVL可以在有限的GPU资源中包含更多的负样本。我们进一步构建了一个名为RUC CAS WenLan的大型中文多源图像文本数据集，用于对BriVL模型进行预训练。大量实验表明，预先训练的BriVL模型在各种下游任务上都优于UNITER和OpenAI CLIP。