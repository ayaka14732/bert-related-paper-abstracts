预先训练的语言模型已成功用于开放域对话的响应生成。提出了四种主要框架：（1）对源语句和目标语句分别使用变压器编码器和解码器进行变压器编码；（2） Transformer-Dec对源语句和目标语句使用Transformer解码器；（3） Transformer MLM使用Transformer解码器，在源端应用双向注意，在目标端应用从左到右的注意，并使用蒙面语言模型目标；（4）采用自回归目标的变压器AR。在这项研究中，我们在3个数据集上对这些框架进行了比较，我们的比较表明，最好的框架在源端使用双向注意，并且不分离编码器和解码器。我们还检验了模型差异，我们的实验证实了模型的性能直接受到潜在差异的影响。然后，我们提出了两种修正方法，以减少差异，并提高模型性能。这些结果表明，当我们使用预先训练的模型时，差异是一个重要的考虑因素，并且差异的减少可以导致性能的提高。