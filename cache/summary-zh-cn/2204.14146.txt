预先训练的语言模型通常不会以符合我们偏好的方式执行任务，例如生成冒犯性文本或事实上不正确的摘要。最近的工作通过学习一种简单的人类评估形式来解决上述问题：对模型生成的任务输出进行比较。比较反馈传达了关于每个人类评价的人类偏好的有限信息。在这里，我们建议从自然语言反馈中学习，这种反馈可以根据人类的评估传递更多信息。我们使用三步学习算法从模型输出的语言反馈中学习。首先，我们在初始输出和反馈上调整语言模型，以生成许多改进。第二，我们选择与反馈具有最高相似度的细化。第三，我们对语言模型进行微调，以最大化给定输入的所选优化的可能性。在合成实验中，我们首先评估语言模型是否准确地结合反馈以产生改进，发现只有大型语言模型（175B个参数）才能做到这一点。我们的学习算法仅使用100个人类书面反馈样本，将GPT-3模型微调到大致人类水平的概括能力。