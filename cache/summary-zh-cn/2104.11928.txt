任务不可知的知识提取，一个师生框架，已被证明是有效的伯特压缩。尽管在NLP任务上取得了有希望的结果，但它需要大量的计算资源。在本文中，我们提出了一种通用且灵活的策略，即先提取后提取（ETD），以重用教师的参数进行高效且有效的任务无关提取，该策略可应用于任何规模的学生。具体来说，我们介绍了两种ETD变体，ETD Rand和ETD Impt，它们分别以随机方式和遵循重要性度量来提取教师参数。这样，学生在蒸馏过程开始时就已经掌握了一些知识，这使得蒸馏过程收敛得更快。我们在GLUE基准和团队上演示了ETD的有效性。实验结果表明：（1）与没有ETD策略的基线相比，ETD可以节省70%的计算成本。此外，在使用相同的计算资源时，它比基线获得更好的结果。（2） ETD是通用的，已被证明对不同的蒸馏方法（如TinyBERT和MiniLM）和不同规模的学生有效。源代码将在发布后公开。