在本文中，我们提出了文本VQA和文本标题任务的文本感知预训练（TAP）。这两项任务的目的是阅读和理解图像中的场景文本，分别用于问答和图像字幕生成。与无法捕获场景文本及其与视觉和文本模式的关系的传统视觉语言预训练不同，TAP在预训练中明确地将场景文本（由OCR引擎生成）合并。通过三项预训练任务，包括蒙面语言建模（MLM）、图像-文本（对比）匹配（ITM）和相对（空间）位置预测（RPP），TAP有效地帮助模型在三种模式（文本-单词、视觉对象和场景-文本）之间学习更好的对齐表示。由于这种对齐表示学习，即使是在同一下游任务数据集上预先训练，TAP已经将TextVQA数据集的绝对准确度提高了+5.4%，而非TAP基线。为了进一步提高性能，我们基于概念性字幕数据集OCR-CC构建了一个大规模数据集，其中包含140万个场景文本相关的图像-文本对。在这个OCR-CC数据集上预先训练后，我们的方法在多个任务上大幅度优于最新技术，即TextVQA的准确率为+8.3%，ST-VQA的准确率为+8.6%，TextCaps的苹果酒分数为+10.2。