预训练的语境化语言模型（如BERT）在各种自然语言处理基准上取得了令人印象深刻的结果。得益于多个预训练任务和大规模训练语料库，预训练模型可以捕捉复杂的句法词关系。在本文中，我们使用深层上下文化语言模型BERT来完成adhoc表的检索任务。我们研究了如何在考虑BERT的表结构和输入长度限制的情况下对表内容进行编码。我们还提出了一种方法，该方法结合了以前关于表检索的文献中的特征，并与BERT联合训练它们。在公共数据集上的实验中，我们表明，在不同的评估指标下，我们的最佳方法可以在很大程度上优于以前的最新方法和BERT基线。