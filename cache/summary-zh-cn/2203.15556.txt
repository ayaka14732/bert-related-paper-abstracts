我们研究了在给定的计算预算下训练转换语言模型的最佳模型大小和令牌数量。我们发现，当前的大型语言模型训练严重不足，这是由于最近关注缩放语言模型，同时保持训练数据量不变的结果。通过在50亿至5000亿个令牌上训练400多个语言模型，从7000万到160多亿个参数，我们发现对于计算优化训练，模型大小和训练令牌的数量应该相等：模型大小每翻一番，训练令牌的数目也应该翻一番。我们通过训练一个预测的计算最优模型Chinchilla来检验这一假设，该模型使用与Gopher相同的计算预算，但具有70B个参数和更多4$\times$的数据。Chinchilla在一系列下游评估任务上均显著优于Gopher（280B）、GPT-3（175B）、Jurasci-1（178B）和威震天图灵NLG（530B）。这也意味着Chinchilla在微调和推理方面使用的计算量大大减少，极大地促进了下游的使用。作为一个亮点，Chinchilla在MMLU基准上达到了67.5%的最先进的平均精度，比Gopher提高了7%以上。