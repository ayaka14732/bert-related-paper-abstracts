我们提出了同上，一个新的实体匹配系统的基础上预先训练变压器为基础的语言模型。我们将EM作为一个序列对分类问题进行微调和转换，以利用具有简单体系结构的此类模型。我们的实验表明，直接应用在大型文本语料库上预训练的语言模型，如BERT、DistilBERT或RoBERTa，已经显著提高了匹配质量，并优于以前的最新技术（SOTA），在基准数据集上的F1分数高达29%。我们还开发了三种优化技术，以进一步提高同上的匹配能力。同上，通过突出显示在做出匹配决策时可能感兴趣的重要输入信息片段，可以注入领域知识。同上，还总结了过长的字符串，以便仅保留基本信息并用于EM。最后，同上采用了SOTA技术，将文本增强为EM，用（困难的）示例增强训练数据。通过这种方式，同上，为了提高模型的匹配能力，我们不得不“更加努力地”学习。我们开发的优化进一步将同上的性能提高了9.8%。也许更令人惊讶的是，我们发现同上可以用最多一半的标记数据实现之前的SOTA结果。最后，我们在一个真实的大规模EM任务中演示了同上的有效性。在匹配由789K和412K记录组成的两个公司数据集时，同上，F1得分高达96.5%。