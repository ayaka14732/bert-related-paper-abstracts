BERT在几个NLP任务中取得了令人印象深刻的性能。然而，在专门领域对其适应指南的调查有限。在这里，我们关注法律领域，在这里我们探索了几种将伯特模型应用于下游法律任务的方法，在多个数据集上进行评估。我们的研究结果表明，以前的预培训和微调指南往往是盲目遵循的，但在法律领域并不总是一概而论。因此，我们建议对在专业领域应用BERT时可用的策略进行系统调查。这些是：（a）使用开箱即用的原始BET，（b）通过在特定领域语料库上进行额外的预培训来调整BET，以及（c）在特定领域语料库上从头开始对BET进行预培训。在对下游任务进行微调时，我们还提出了更广阔的超参数搜索空间，并发布了LEGAL-BERT，这是一系列旨在协助法律NLP研究、计算法和法律技术应用的BERT模型。