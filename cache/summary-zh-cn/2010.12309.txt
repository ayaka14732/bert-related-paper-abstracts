深层神经网络和庞大的语言模型在自然语言应用中无处不在。众所周知，它们需要大量的培训数据，因此，在低资源环境下，提高性能的工作越来越多。受最近神经模型的基本变化和流行的预训练和微调范式的推动，我们综述了低资源自然语言处理的有希望的方法。在讨论了数据可用性的不同维度之后，我们对在训练数据稀疏时支持学习的方法进行了结构化概述。这包括创建附加标记数据的机制，如数据增强和远程监控，以及减少目标监控需求的转移学习设置。我们调查的目的是解释这些方法在需求方面的差异，因为了解它们对于选择适合特定低资源环境的技术至关重要。这项工作的进一步关键方面是突出未决问题，并概述未来研究的前景。