最近的研究表明，训练数据集多样性的增加提高了大规模语言模型的一般跨领域知识和下游泛化能力。考虑到这一点，我们提出了\textit{the Pile}：一个825 GiB的英语文本语料库，旨在训练大规模的语言模型。该桩由22个不同的高质量子集（包括现有和新建）构成，其中许多来自学术或专业来源。我们对GPT-2和GPT-3在桩上的非协调性能进行的评估表明，这些模型在其许多组成部分上存在困难，例如学术写作。相反，在桩上训练的模型在桩的所有组件上都比原始CC和CC-100显著提高，同时改善了下游评估的性能。通过深入的探索性分析，我们为潜在用户记录了数据的潜在方面。我们公开了其构造中使用的代码。