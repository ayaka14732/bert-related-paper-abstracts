变形金刚的使用已经从学习语言语义发展到形成有意义的语言表征。这些体系结构通常参数化过度，需要大量计算。在这项工作中，我们扩展了自适应方法，以了解更多关于模型可解释性和计算效率的信息。具体来说，我们研究了注意广度、稀疏和结构化的辍学方法，以帮助理解他们的注意力机制是如何扩展到视觉和语言任务的。我们进一步表明，这些方法可以帮助我们更多地了解网络如何感知输入序列的复杂性、不同模式的稀疏偏好以及其他相关现象。