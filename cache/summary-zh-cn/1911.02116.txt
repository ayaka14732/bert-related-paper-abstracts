这篇论文表明，大规模的多语种语言模型预训练可以显著提高跨语种迁移任务的成绩。我们在100种语言上训练一个基于转换器的屏蔽语言模型，使用超过2 TB的过滤公共爬网数据。我们的模型被称为XLM-R，在多种跨语言基准测试中显著优于多语言BERT（mBERT），包括XNLI的平均准确率+14.6%，MLQA的平均F1分数+13%，NER的平均F1分数+2.4%。XLM-R在低资源语言上表现尤其出色，与以前的XLM模型相比，斯瓦希里语的XNLI准确率提高了15.7%，乌尔都语的准确率提高了11.4%。我们还对实现这些收益所需的关键因素进行了详细的实证分析，包括（1）正迁移和容量稀释，以及（2）高资源和低资源语言在规模上的性能之间的权衡。最后，我们首次展示了在不牺牲每种语言性能的情况下进行多语言建模的可能性；XLM-R在GLUE和XNLI基准上与强大的单语模型非常有竞争力。我们将公开我们的代码、数据和模型。