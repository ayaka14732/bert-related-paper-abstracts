多实体之间的关系建模近年来受到越来越多的关注，为了加速文档级关系提取的研究，人们收集了一个称为DocRED的新数据集。此任务的当前基线使用BiLSTM对整个文档进行编码，并从头开始进行培训。我们认为，这种简单的基线不足以模拟实体之间的复杂交互。在本文中，我们进一步应用了预训练语言模型（BERT），为这项任务提供了一个更强的基线。我们还发现，分阶段解决此任务可以进一步提高性能。第一步是预测两个实体是否存在关系，第二步是预测特定关系。