最近的研究表明，语言模型（LM）捕获了关于事实或常识的不同类型的知识。然而，由于没有一个模型是完美的，它们在许多情况下仍然无法提供适当的答案。在本文中，我们提出了这样一个问题：“我们如何知道语言模型何时有信心知道特定查询的答案？”我们从校准的角度来研究这个问题，概率模型的预测概率的性质实际上与正确性的概率有很好的相关性。我们检查了三个强大的生成模型——T5、BART和GPT-2——并研究它们在QA任务中的概率是否得到了很好的校准，发现答案是一个相对强调的否。然后我们检查了校准这些模型的方法，以使它们的置信度得分通过微调与正确性的可能性更好地关联，事后概率修改，或预测输出或输入的调整。在不同数据集上的实验证明了我们方法的有效性。我们还进行了分析，以研究这些方法的优点和局限性，为校准LMs方法的进一步改进提供了依据。我们已经在https://github.com/jzbjyb/lm-calibration.