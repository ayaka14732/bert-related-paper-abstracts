基于深度学习的语言模型在大型非注释文本语料库上进行预训练，已被证明能够为自然语言处理提供有效的迁移学习，最近的方法，如基于转换器的伯特模型，推动了各种任务的最新发展。虽然大多数关于这些模型的工作都集中在高资源语言上，特别是英语，但最近的一些工作引入了多语言模型，可以对这些模型进行微调，以处理大量不同语言的任务。然而，我们仍然缺乏对这些模型功能的透彻理解，尤其是对于资源较低的语言。本文以芬兰语为研究对象，对一系列任务中的多语言BERT模型进行了全面的评估，并将其与从头开始训练的新的芬兰语BERT模型进行了比较。新的特定于语言的模型被证明系统地、明显地优于多语言模型。虽然多语言模型在很大程度上无法达到先前提出的方法的性能，但定制的芬兰BERT模型为所有参考任务的所有语料库建立了新的最新结果：词性标记、命名实体识别和依赖项解析。我们发布了模型以及为本研究创建的所有相关资源，并在https://turkunlp.org/finbert .