变压器在许多NLP任务中被证明是有效的。然而，他们的培训需要在设计尖端优化器和仔细学习速率调度器方面付出大量努力（例如，传统SGD无法有效培训变压器）。我们的目标是从实证和理论的角度理解$\textit{变压器培训的复杂性}$。我们的分析表明，不平衡的梯度不是训练不稳定的根本原因。相反，我们确定了一种放大效应，它会对训练产生重大影响——对于多层变压器模型中的每一层，对其剩余分支的严重依赖会使训练不稳定，因为它会放大小参数扰动（例如，参数更新），并导致模型输出中的显著扰动。然而，我们观察到，轻微的依赖性限制了模型的潜力，并导致训练较差的模型。受我们分析的启发，我们建议使用Admin（$\textbf{Ad}$aptive$\textbf{m}$odel$\textbf{in}$initialization）来稳定早期阶段的训练，并在后期充分发挥其潜力。大量实验表明，Admin更稳定，收敛速度更快，性能更好。具体实施发布于：https://github.com/LiyuanLucasLiu/Transforemr-Clinic.