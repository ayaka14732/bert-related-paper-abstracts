预训练蒙面语言模型（MLM）需要对大多数NLP任务进行微调。相反，我们通过伪对数似然分数（PLL）对MLM进行开箱即用的评估，该分数是通过逐个屏蔽标记计算的。我们发现，PLL在各种任务中的表现都优于自回归语言模型（如GPT-2）的分数。通过重新审视ASR和NMT假设，RoBERTa将端到端LibriSpeech模型的WER相对降低了30%，在低资源翻译对的最新基线上加起来为+1.7 BLEU，并从域适应中进一步获益。我们将这一成功归因于PLL无监督的语言可接受性表达，没有从左到右的偏见，大大提高了GPT-2的分数（+岛效应10分，BLiMP中的NPI许可）。人们可以微调MLM，在不掩蔽的情况下给出分数，从而在单个推理过程中实现计算。总之，锁相环及其相关的伪困惑（PPPL）使得即插即用的使用越来越多的预训练传销；e、 例如，我们使用单一的跨语言模型来重新存储多种语言的翻译。我们发布了我们的语言模型评分库https://github.com/awslabs/mlm-scoring.