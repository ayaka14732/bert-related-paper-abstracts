我们将BERT应用于共指消解，在OntoNotes（+3.9 F1）和GAP（+11.5 F1）基准上实现了显著的改进。对模型预测的定性分析表明，与ELMo和BERT base相比，BERT large在区分相关但不同的实体（如总裁和首席执行官）方面尤其出色。然而，在建模文档级上下文、对话和提及释义方面仍有改进的余地。我们的代码和模型是公开的。