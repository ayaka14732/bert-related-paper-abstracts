预先训练的语言模型（如BERT）在各种自然语言处理（NLP）任务中取得了显著的成功。然而，高存储和计算成本阻碍了预先训练的语言模型在资源受限的设备上的有效部署。在本文中，我们提出了一种新的基于多对多层映射的伯特蒸馏方法，它允许每个中间学生层从任何中间教师层学习。这样，我们的模型可以针对不同的NLP任务自适应地从不同的教师层学习受直觉的驱使，不同的NLP任务需要不同层次的语言知识，这些知识包含在BERT的中间层中。此外，我们利用地球移动器距离（EMD）计算将知识从教师网络转换为学生网络所必须支付的最小累积成本。EMD可实现多对多图层映射的有效匹配。%EMD可以应用于不同规模的网络层，有效地度量教师网络和学生网络之间的语义距离。此外，我们还提出了一种代价注意机制来自动学习EMD中使用的层权重，以进一步提高模型的性能，加快收敛速度。在GLUE benchmark上进行的大量实验表明，我们的模型在准确性和模型压缩方面与强大的竞争对手相比具有竞争力。