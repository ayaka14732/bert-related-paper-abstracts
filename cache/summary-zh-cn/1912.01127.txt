本文介绍了我们对第三届YouTube-8M视频理解比赛的方法，该比赛要求参赛者将视频级别的标签定位到标签实际出现的视频中的精确时间。我们的模型是帧级模型（如GatedNetVLAD和NeXtVLAD）和各种具有测试时间扩展的BERT模型的集合。我们探索了多种方法来将BERT输出作为视频表示，以及多种方法来组合视觉和音频信息。我们建议将测试时间增加为将视频帧移动到一个左或右单位，这增加了预测的多样性，并且事实上显示了评估指标的改进。我们首先在4M训练视频级数据上对模型进行预训练，然后在237K注释视频段级数据上对模型进行微调。我们实现MAP@100K0.7871，在私人测试视频片段数据中，在283个团队中排名第9。