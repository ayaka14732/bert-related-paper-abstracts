在本文中，我们提出了堆叠德伯特，从变压器堆叠去噪双向编码器表示的缩写。与现有系统相比，这种新模型通过在BERT中设计一种新的编码方案，提高了不完整数据中的鲁棒性。BERT是一种完全基于注意机制的强大语言表示模型。自然语言处理中的不完整数据指的是单词缺失或不正确的文本，它的存在可能会妨碍当前模型的性能，这些模型无法承受此类噪声，但即使在胁迫下也必须表现良好。这是因为当前的方法是为干净完整的数据而构建和训练的，因此无法提取能够充分表示不完整数据的特征。我们提出的方法包括通过将嵌入层应用于输入标记，然后再应用转换来获得中间输入表示。这些中间特征作为新型去噪变压器的输入，负责获得更丰富的输入表示。该方法利用多层感知器堆栈通过提取更抽象和有意义的隐藏特征向量来重建缺失词的嵌入，并利用双向变换来改进嵌入表示。我们考虑两个数据集进行训练和评估：聊天机器人自然语言理解评价语料库和Kaggle的推特情感语料库。我们的模型显示，在情绪和意图分类任务中，在推特中出现的非正式/不正确文本以及存在语音-文本错误的文本中，F1分数提高，鲁棒性更好。