近年来，我们看到了一项巨大的努力，即使用多种语言的大规模语料库对多语言文本编码器进行预培训，以促进跨语言迁移学习。然而，由于不同语言的类型差异，跨语言迁移是一个挑战。然而，语言语法，例如句法依赖，可以弥补类型上的差距。以前的工作表明，预先训练的多语言编码器，如mBERT\cite{devlin-etal-2019-bert}，可以捕获语言语法，帮助跨语言迁移。这项工作表明，显式提供语言语法和使用辅助目标对mBERT进行训练来编码通用依赖树结构有助于跨语言迁移。我们在四个NLP任务上进行了严格的实验，包括文本分类、问答、命名实体识别和面向任务的语义分析。实验结果表明，在PAWS-X和MLQA等流行基准测试中，语法增强mBERT在所有语言中的跨语言迁移平均提高了1.4和1.6个百分点。在\emph{generalized}转移设置中，性能显著提高，PAWS-X和MLQA的平均得分分别为3.9分和3.1分。