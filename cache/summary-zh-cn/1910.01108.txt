随着大规模预训练模型的迁移学习在自然语言处理（NLP）中变得越来越普遍，在边缘和/或受限制的计算训练或推理预算中操作这些大型模型仍然具有挑战性。在这项工作中，我们提出了一种方法来预先训练一个较小的通用语言表示模型，称为DistilBERT，然后可以对其进行微调，使其在广泛的任务（如较大的任务）上具有良好的性能。虽然之前的大多数工作都研究了使用蒸馏来构建特定于任务的模型，但我们在训练前阶段利用了知识蒸馏，并表明可以将BERT模型的大小减少40%，同时保留97%的语言理解能力，并加快60%。为了利用大模型在训练前学习到的归纳偏差，我们引入了一种结合语言建模、蒸馏和余弦距离损失的三重损失。我们的更小、更快、更轻的模型比预训练更便宜，我们在概念验证实验和比较性的设备研究中展示了其在设备上计算的能力。