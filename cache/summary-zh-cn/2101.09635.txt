基于Transformer的语言模型，更具体地说是基于BERT的体系结构，在许多下游任务中都实现了最先进的性能。然而，对于资源相对较低的语言（如泰语），模型的选择仅限于基于小得多的数据集训练基于BERT的模型或微调多语言模型，两者都会产生次优的下游性能。此外，大规模的多语种预培训没有考虑到泰语的语言特点。为了克服这些限制，我们在一个大的、消除重复的、干净的培训集（总大小为78GB）上预先培训了一个基于RoBERTa base体系结构的语言模型，该培训集由社交媒体帖子、新闻文章和其他公开可用数据集的不同领域策划。我们应用特定于泰语的文本处理规则，最重要的是保留空格，在子词标记化之前，空格是泰语中重要的块和句子边界。我们还使用较小的数据集对单词级、音节级和句子片段标记化进行了实验，以探索标记化对下游性能的影响。我们的模型wangchanberta base att spm在78.5GB数据集上未经训练，在人类注释的单语言环境中，在序列分类和标记分类任务方面优于强基线（NBSVM、CRF和ULMFit）和多语言模型（XLMR和mBERT）。