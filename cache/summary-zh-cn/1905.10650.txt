注意力是一种强大且普遍存在的机制，它允许神经模型在进行预测时通过加权平均值来关注特定的显著信息。特别是，多头注意力是许多最新的NLP模型背后的驱动力，例如基于变压器的MT模型和BERT。这些模型并行应用多个注意机制，每个注意“头”潜在地集中在输入的不同部分，这使得能够表达简单加权平均之外的复杂功能。在本文中，我们做出了一个令人惊讶的观察，即即使模型已经使用多个头部进行了训练，但在实践中，大部分注意力头部都可以在测试时移除，而不会显著影响性能。事实上，有些层甚至可以简化为一个头部。我们进一步研究了用于修剪模型的贪婪算法，以及从中获得的潜在速度、内存效率和准确性改进。最后，我们分析了模型中哪些部分更依赖于有多个头部的结果，并提供了前兆证据，证明训练动力学在多个头部注意所提供的收益中发挥了作用。