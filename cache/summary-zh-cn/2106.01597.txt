尽管自然语言处理研究最近取得了一些进展，但自然语言生成中的跨语言迁移研究相对较少。在这项工作中，我们将对自然语言生成（NLG）的监督从高资源语言（HRL）转移到多个低资源语言（LRL）。我们考虑了四个NLG任务（文本摘要、问题生成、新闻标题生成和分词生成）和三种句法上不同的语言，即英语、印地语和日语。我们提出了一个无监督的跨语言语言生成框架（称为ZmBART），该框架不使用任何并行或伪并行/反向翻译数据。在这个框架中，我们进一步使用三种语言的单语数据，通过辅助任务对mBART序列到序列去噪自动编码器模型进行预训练。辅助任务的目标函数接近目标任务，丰富了mBART的多语言潜在表示，为目标任务提供了良好的初始化。然后，使用特定任务的有监督英语数据对该模型进行微调，并在零镜头设置下使用低资源语言直接评估该模型。为了克服灾难性遗忘和伪相关问题，我们分别采用了冻结模型组件和数据论证方法。这种简单的建模方法给了我们很好的结果。我们尝试了少量的镜头训练（1000个有监督的数据点），这进一步提高了模型的性能。我们进行了几次消融和跨语言转移性分析，以证明ZmBART的稳健性。