主动学习通过选择最关键的示例进行标注，努力降低注释成本。通常，主动学习策略取决于分类模型。例如，不确定性抽样取决于校准不良的模型置信分数。在冷启动环境中，由于模型不稳定和数据匮乏，主动学习是不切实际的。幸运的是，现代NLP提供了额外的信息来源：预先训练的语言模型。训练前的损失可以找到让模型吃惊的例子，应该标记为有效的微调。因此，我们将语言建模损失视为分类不确定性的代理。利用BERT，我们开发了一种基于蒙蔽语言建模损失的简单策略，该策略可以最小化文本分类的标记成本。与其他基线相比，我们的方法在更少的采样迭代次数和计算时间内达到更高的精度。