大型transformer模型在许多自然语言处理应用中取得了非凡的成功。然而，训练和部署这些模型对于长序列来说可能代价高昂，因为变压器的标准自我注意机制在序列长度方面使用$O（n^2）$时间和空间。在本文中，我们证明了自我注意机制可以用低秩矩阵来近似。我们进一步利用这一发现提出了一种新的自我注意机制，该机制在时间和空间上将整体自我注意复杂性从$O（n^2）$降低到$O（n）$。得到的线性变压器，\TrimtT{LeNeMeX }，与标准变压器模型相匹配，同时具有更大的存储和具有时效性的性能。