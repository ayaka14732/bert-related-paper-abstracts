关于深层神经语言模型可解释性的最新研究表明，自然语言语法的许多属性都编码在它们的表征空间中。然而，这类研究往往局限于单一语言和单一语言形式主义。在这项研究中，我们的目的是调查语言模型所捕获的句法结构的外表在多大程度上依附于表层句法或深层句法分析风格，以及这些模式在不同语言中是否一致。我们对在13种不同语言上训练的BERT和ELMo模型应用了一个提取有向依赖树的探测，探测了两种不同的语法注释样式：通用依赖（UD），优先处理深层语法关系，以及表面语法通用依赖（SUD），重点关注表面结构。我们发现，这两种模型都表现出对UD的偏好，而不是SUD——在不同语言和层次上存在有趣的变化——并且这种偏好的强度与树形状的差异相关。