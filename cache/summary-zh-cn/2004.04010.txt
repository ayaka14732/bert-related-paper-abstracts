基于Transformer的深层NLP模型使用数亿个参数进行训练，限制了它们在计算受限环境中的适用性。在本文中，我们通过定义冗余的概念来研究这些限制的原因，我们将冗余分为两类：一般冗余和任务特定冗余。我们剖析了两种流行的预训练模型，BERT和XLNet，研究它们在表示层和更细粒度的神经元层上表现出多少冗余。我们的分析揭示了有趣的见解，例如：i）网络中85%的神经元是冗余的，ii）当优化下游任务时，至少92%的神经元可以被移除。基于我们的分析，我们提出了一个有效的基于特征的迁移学习过程，在使用最多10%的原始神经元的同时，保持了97%的性能。