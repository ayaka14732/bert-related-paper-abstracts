预训练变换器语言模型（LMs）在自然语言处理中的成功导致了广泛的预训练设置。特别是，这些模型采用了多种子词标记化方法，最显著的是字节对编码（BPE）（Sennrich等人，2016；Gage，1994）、词条法（Schuster和Nakajima，2012）和单格语言建模（Kudo，2018）来分割文本。然而，据我们所知，文献中没有直接评估标记化对语言模型预训练的影响。我们分析了BPE和unigram LM标记化之间的差异，发现后一种方法可以恢复与形态学更紧密对齐的子词单元，并避免了因BPE贪婪的构造过程而产生的问题。然后，我们比较使用这些标记化预训练的相同转换器屏蔽语言模型的微调任务性能。在下游任务和两种语言（英语和日语）中，我们发现unigram LM标记化方法匹配或优于BPE。我们希望未来的预训练LMS的开发人员会考虑在更普遍的BPE上采用UNIGRAM LM方法。