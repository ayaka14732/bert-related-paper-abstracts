由于野外标记数据集的可用性有限，情感识别是一项具有挑战性的任务。在语音和自然语言等领域，自监督学习在标记数据集有限的任务上取得了改进。像BERT这样的模型学习在单词嵌入中融入上下文，这可以提高下游任务（如问答）的性能。在这项工作中，我们将自我监督训练扩展到多模式应用。我们学习多模态表示使用变压器培训的蒙面语言建模任务与音频，视频和文本功能。该模型对情感识别的下游任务进行了微调。我们在CMU-MOSEI数据集上的结果表明，与基线相比，这种预训练技术可以将情绪识别性能提高3%。