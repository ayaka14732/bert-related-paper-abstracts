最近的汉语预训练模型忽略了汉语特有的两个重要方面：字形和拼音，它们为语言理解提供了重要的语法和语义信息。在这项工作中，我们提出了ChineseBERT，它将汉字的{\it glyph}和{\it pinyin}信息合并到语言模型预训练中。字形嵌入是基于汉字的不同字体获得的，能够从视觉特征中捕捉字符语义，拼音嵌入是汉字发音的特征，处理了汉语中普遍存在的异名现象（同一个字符的发音不同，含义不同）.在大规模未标记中文语料库上进行预训练后，所提出的ChineseBERT模型比基线模型具有显著的性能提升，只需较少的训练步骤。Porpseed模型在广泛的中文NLP任务上实现了新的SOTA性能，包括机器阅读理解、自然语言推理、文本分类、文本分类等entence对匹配，以及命名实体识别中的竞争性能。代码和预训练模型可在https://github.com/ShannonAI/ChineseBert.