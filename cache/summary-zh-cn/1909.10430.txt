ELMo（Peters et al.，2018）、Flair NLP（Akbik et al.，2018）或BERT（Devlin et al.，2019）提供的语境化单词嵌入（CWE）是NLP的一项重大创新。CWE根据各自的上下文提供单词的语义向量表示。在文本分类、序列标记或机器翻译等任务中，它们比静态单词嵌入更具优势。由于同一单词类型的向量可以根据各自的上下文而变化，因此它们隐式地为词义消歧（WSD）提供了一个模型。我们介绍了一种简单但有效的方法，使用最近邻分类的水务工程。我们比较了不同CWE模型在该任务中的性能，并且可以报告两个标准WSD基准数据集的改进情况，这些改进超过了当前的技术水平。我们进一步表明，预训练的BERT模型能够将多义词放入嵌入空间的不同“意义”区域，而ELMo和Flair NLP似乎不具备这种能力。