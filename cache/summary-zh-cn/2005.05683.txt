我们进行了深入的研究，以诊断预先训练的语言编码员（ELMo、BERT和RoBERTa）在遇到自然语法错误时的行为。具体来说，我们收集非母语人士的真实语法错误，并在干净的文本数据上进行对抗性攻击来模拟这些错误。我们使用这种方法来促进在下游应用程序上调试模型。结果证实，所有测试模型的性能都受到影响，但影响程度各不相同。为了解释模型行为，我们进一步设计了一个语言可接受性任务，以揭示他们识别不合语法句子的能力和错误的位置。我们发现，固定的上下文编码器与一个简单的分类器训练的句子正确性预测能够定位错误的位置。我们还为BERT设计了一个完形填空测试，发现BERT捕获了上下文中错误和特定标记之间的交互。我们的结果有助于理解语言编码器对语法错误的鲁棒性和行为。