我们研究了交叉熵损失下语言模型性能的经验标度律。损失随着模型大小、数据集大小和用于训练的计算量呈幂律变化，有些趋势跨越七个数量级以上。其他架构细节（如网络宽度或深度）在大范围内的影响最小。简单方程控制过拟合对模型/数据集大小的依赖性以及训练速度对模型大小的依赖性。这些关系允许我们确定固定预算的最佳分配。较大模型的样本效率明显更高，因此最佳计算效率训练包括在相对较少的数据量上训练非常大的模型，并在收敛之前显著停止。