Transformer架构彻底改变了自然语言处理（NLP）领域。基于Transformers的模型（例如，BERT）为许多重要的Web服务提供了动力，如搜索、翻译、问答等。虽然大量研究关注这些模型的训练，但在提高其推理性能方面所做的努力相对较少。本文通过对基于CPU的变压器模型的可扩展性和性能的实证分析来解决这一差距。重点关注广受欢迎的BERT模型，我们确定了变压器体系结构中的关键组件，在这些组件中进行了大量计算，并提出了三种优化方法来加快计算速度。使用HuggingFace的推理基准对优化进行评估，结果表明，优化的加速比高达x2.37。所考虑的优化不需要对模型的实现进行任何更改，也不会影响其准确性。