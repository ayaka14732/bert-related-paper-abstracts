在计算机视觉和自然语言处理中，大规模的预训练和特定任务的微调现在是许多任务的标准方法。最近，人们提出了多种方法来训练视觉和语言BERT，以应对人工智能这两个关键领域交叉点的挑战。这些模型可以分为单流或双流编码器。我们研究了这两个范畴之间的差异，并展示了如何在单一的理论框架下将它们统一起来。然后，我们进行对照实验，以识别五个V&L Bert之间的经验差异。我们的实验表明，训练数据和超参数导致了报告结果之间的大部分差异，但它们也揭示了嵌入层在这些大规模模型中起着关键作用。