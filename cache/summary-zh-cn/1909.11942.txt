在对自然语言表示进行预训练时，增加模型大小通常会提高下游任务的性能。然而，由于GPU/TPU内存限制和更长的训练时间，在某些情况下，进一步增加模型变得更加困难。为了解决这些问题，我们提出了两种参数缩减技术来降低内存消耗和提高训练速度。综合的经验证据表明，我们提出的方法导致模型的规模比原始的BERT好得多。我们还使用了一种自我监督损失模型，该模型侧重于句子间连贯性的建模，并表明它对多句子输入的下游任务具有一致的帮助。因此，我们的最佳模型在GLUE、RACE和\squad基准测试上建立了新的最先进的结果，同时与BERT相比，它的参数更少。代码和预训练模型可在https://github.com/google-research/ALBERT.