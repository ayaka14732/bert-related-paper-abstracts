pretrain-finetune范式的一个众所周知的局限性在于其由“一刀切”词汇表造成的不灵活。这可能会削弱将预训练模型应用于自然语言生成（NLG）任务时的效果，特别是对于存在显著差异的上游和下游任务之间的子词分布。为了解决这个问题，我们扩展了vanilla-pretrain-finetune管道，增加了一个嵌入传输步骤。具体地说，引入了一个即插即用嵌入生成器，根据其形态相似的预训练嵌入生成任意输入标记的表示。因此，也可以有效地初始化下游任务中不匹配令牌的嵌入。我们在pretrain finetune模式下对各种NLG任务进行了实验。实验结果和广泛的分析表明，所提出的策略为我们提供了自由转移词汇的机会，从而使下游NLG模型更高效、性能更好。