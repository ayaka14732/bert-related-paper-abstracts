我们研究了自然语言处理中的模型提取问题，其中只有对受害者模型具有查询访问权限的对手试图重建该模型的本地副本。假设对手和受害者模型都微调了大型预训练语言模型，如BERT（Devlin et al.2019），我们表明对手不需要任何真实的训练数据来成功发起攻击。事实上，攻击者甚至不需要使用语法或语义上有意义的查询：我们表明，随机单词序列加上特定于任务的启发式方法，可以有效地查询各种NLP任务的模型提取，包括自然语言推理和问答。因此，我们的工作强调了一个只有在NLP社区内转向转移学习方法才能实现的漏洞：对于几百美元的查询预算，攻击者可以提取一个性能仅略低于受害者模型的模型。最后，我们研究了两种针对模型提取的防御策略——隶属度分类和API水印——它们虽然能够成功地抵御幼稚的对手，但对于更复杂的对手却无能为力。