基于变压器的大型模型被证明可以简化为较少数量的自我关注头和层。我们从彩票假说的角度考虑这一现象，使用结构和数量修剪。对于微调的BERT，我们表明：（a）可以找到实现与完整模型相当的性能的子网络，以及（b）从模型其余部分取样的类似大小的子网络性能较差。引人注目的是，通过结构化剪枝，即使是最差的子网络也仍然具有高度的可训练性，这表明大多数预先训练的伯特权重都是潜在有用的。我们还研究了“好”的子网络，看看它们的成功是否可以归因于优秀的语言知识，但发现它们不稳定，并且不能用有意义的自我注意模式来解释。