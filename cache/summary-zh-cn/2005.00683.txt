最近的研究表明，预训练语言模型（PTLM），如伯特，具有一定的常识和事实知识。他们认为，通过预测掩蔽词，将PTLM用作“神经知识库”是有希望的。令人惊讶的是，我们发现这可能不适用于数字常识知识（例如，一只鸟通常有两条腿）。在本文中，我们研究是否以及在多大程度上我们可以从PTLMs中归纳出数字常识知识，以及这个过程的稳健性。为了研究这一点，我们引入了一个新的探测任务，该任务包含一个诊断数据集NumerSense，其中包含13.6k掩蔽词预测探测（10.5k用于微调，3.1k用于测试）。我们的分析表明：（1）在任何微调之前，BERT及其强变异体RoBERTa在诊断数据集上表现不佳；（2） 远程监控的微调带来了一些改进；（3） 与人的绩效相比，最佳监督模型的绩效仍然很差（准确率分别为54.06%和96.3%）。