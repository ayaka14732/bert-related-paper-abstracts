我们提出了一种通过将输入文本与动态检索的文本百科全书背景知识结合起来表示输入文本的方法。我们将我们的方法应用到阅读理解任务中，将问题和段落以及它们提到的实体的背景句子编码在一起。我们表明，从文本中整合背景知识对于注重事实推理的任务是有效的，并允许直接重用强大的预训练伯特式编码器。此外，通过对背景增强输入文本中单词的自监督掩蔽语言模型目标进行适当的预训练，可以进一步提高知识集成。在TriviaQA上，我们的方法与不动态集成背景知识的可比RoBERTa模型相比，F1提高了1.6到3.1。在MRQA这一大量不同的QA数据集上，我们看到了领域内的一致收益，以及BioASQ（2.1到4.2 F1）、TextbookQA（1.6到2.0 F1）和DuoRC（1.1到2.0 F1）领域外的巨大改进。