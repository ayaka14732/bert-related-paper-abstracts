变形金刚具有学习长期依赖性的潜力，但在语言建模环境中受到固定长度上下文的限制。我们提出了一种新的神经结构Transformer XL，它能够在不破坏时间一致性的情况下，实现超过固定长度的学习依赖性。它由一个段级递归机制和一个新的位置编码方案组成。我们的方法不仅能够捕获长期依赖，而且还解决了上下文碎片问题。因此，Transformer XL学习的依赖性比RNN长80%，比vanilla Transformers长450%，在短序列和长序列上都实现了更好的性能，并且在评估期间比vanilla Transformers快1800多倍。值得注意的是，我们将bpc/Implexity的最新结果在enwiki8上提高到0.99，在text8上提高到1.08，在WikiText-103上提高到18.3，在十亿字上提高到21.8，在Penn Treebank上提高到54.5（无微调）。当仅在WikiText-103上接受培训时，Transformer XL能够生成具有数千个标记的合理连贯、新颖的文本文章。我们的代码、预训练模型和超参数在Tensorflow和PyTorch中都可用。