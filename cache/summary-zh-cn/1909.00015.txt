注意机制在NLP中已变得无处不在。最近的体系结构，尤其是Transformer，通过分层、多头关注学习强大的上下文感知单词表示。多头学习不同类型的单词关系。然而，使用标准softmax注意，所有注意头都是密集的，为所有上下文单词分配了非零权重。在这项工作中，我们引入了自适应稀疏变换，其中注意头具有灵活的、上下文相关的稀疏模式。这种稀疏性是通过将softmax替换为$\alpha$-entmax来实现的：softmax是一种可微的泛化，它允许低得分单词精确地获得零权重。此外，我们还推导了一种自动学习$\alpha$参数的方法，该参数控制$\alpha$-entmax的形状和稀疏性，从而允许注意头在集中或分散行为之间进行选择。与机器翻译数据集上的softmax转换器相比，我们的自适应稀疏转换器提高了可解释性和头部多样性。我们的方法的定量和定性分析结果包括，不同层次的头部学习不同的稀疏性偏好，并且在注意力分布上比softmax变形金刚更加多样化。此外，在不牺牲准确性的情况下，注意力头部的稀疏性有助于揭示不同的头部特化。