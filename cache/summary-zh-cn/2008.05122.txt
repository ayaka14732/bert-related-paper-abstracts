我们介绍了语言可解释性工具（LIT），一个用于可视化和理解NLP模型的开源平台。我们关注关于模型行为的核心问题：为什么我的模型做出这样的预测？什么时候表现不佳？在输入的受控变化下会发生什么？LIT将本地解释、聚合分析和反事实生成集成到一个基于浏览器的简化界面中，以实现快速探索和错误分析。我们包括一系列不同工作流程的案例研究，包括探索情绪分析的反事实，测量共指系统中的性别偏见，以及探索文本生成中的局部行为。LIT支持广泛的模型——包括分类、seq2seq和结构化预测——并通过声明性、框架无关的API进行高度扩展。LIT正在积极开发中，代码和完整文档可在https://github.com/pair-code/lit.