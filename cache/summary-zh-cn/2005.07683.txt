在纯监督学习中，幅度剪枝是一种广泛使用的减小模型大小的策略；然而，在已经成为最先进的自然语言处理应用标准的迁移学习机制中，它的效率较低。我们建议使用运动剪枝，这是一种简单、确定的一阶权重剪枝方法，更适合于预训练模型的微调。我们给出了该方法的数学基础，并将其与现有的零阶和一阶修剪方法进行了比较。实验表明，在剪枝大型预训练语言模型时，运动剪枝在高稀疏区域表现出显著的改进。当与蒸馏相结合时，该方法只需3%的模型参数即可实现最小的精度损失。