在自然语言处理（natural language processing，NLP）中，像BERT这样的大量预训练模型已经成为一系列下游任务训练的标准起点，在深度学习的其他领域也出现了类似的趋势。同时，关于彩票假设的研究表明，NLP和计算机视觉的模型包含更小的匹配子网络，这些子网络能够孤立地进行完全精确的训练并转移到其他任务。在这项工作中，我们结合这些观察来评估在预先训练的伯特模型中是否存在这样的可训练、可转移的子网络。对于一系列下游任务，我们确实发现匹配子网络的稀疏度为40%到90%。我们发现这些子网络是在（预先训练的）初始化阶段出现的，这与之前的NLP研究有所不同，在NLP研究中，这些子网络是在经过一定训练后才出现的。蒙面语言建模任务（用于预训练模型的相同任务）上发现的子网络普遍传输；那些在其他任务中发现的，如果有的话，会以有限的方式转移。随着大规模的预培训逐渐成为深度学习的中心范式，我们的研究结果表明，主要的彩票观察结果仍然与此相关。代码可在https://github.com/VITA-Group/BERT-Tickets.