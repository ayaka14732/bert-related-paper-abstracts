预训练的深层语言模型在NLP中带来了巨大的性能提升。尽管取得了成功，Schick和Sch\“utze（2020年）最近的研究表明，这些模型很难理解稀有词。对于静态词嵌入，这个问题已经通过单独学习稀有词的表示来解决。在这项工作中，我们将这个想法转移到预训练语言模型中：我们引入BERTRAM，这是一个基于BERT的强大体系结构，能够推断出高质量的语言适合作为深层语言模型输入表示的稀有词的可嵌入性。这是通过使词的表面形式和上下文在深层体系结构中相互交互来实现的。由于稀有和中频表示的改进，将BERTRAM集成到BERT中可大大提高性能一个罕见单词探测任务和三个下游任务中的单词。