BERT（Devlin et al.，2018）和RoBERTa（Liu et al.，2019）在句子对回归任务（如语义-文本相似性（STS））上建立了新的最先进的性能。然而，这需要将两个句子都输入到网络中，这会造成巨大的计算开销：在10000个句子的集合中找到最相似的句子对需要使用BERT进行大约5000万次推理计算（~65小时）。BERT的构造使得它不适用于语义相似性搜索以及聚类等无监督任务。在本出版物中，我们介绍了句子BERT（SBERT），这是对预训练BERT网络的一种修改，它使用连体和三重网络结构来推导语义上有意义的句子嵌入，可以使用余弦相似性进行比较。这减少了寻找最相似配对的工作量，从使用BERT/RoBERTa的65小时减少到使用SBERT的约5秒，同时保持了BERT的准确性。我们评估了SBERT和SRoBERTa在普通STS任务和迁移学习任务中的表现，其表现优于其他最先进的句子嵌入方法。