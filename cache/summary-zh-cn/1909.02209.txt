语言表征的最新研究仔细地将语境化特征融入到语言模型训练中，这使得一系列的成功尤其是在各种机器阅读理解和自然语言推理任务中。然而，现有的语言表示模型（包括ELMo、GPT和BERT）仅利用简单的上下文敏感特性，如字符或单词嵌入。他们很少考虑结合结构化语义信息，它可以为语言表达提供丰富的语义。为了促进对自然语言的理解，我们建议将预先训练好的语义角色标记中的显式上下文语义结合起来，并引入一种改进的语言表示模型，语义感知的BERT（SemBERT），该模型能够通过BERT主干显式地吸收上下文语义。SemBERT以轻松微调的方式保持其BERT前体的方便可用性，而无需进行实质性的任务特定修改。与BERT相比，语义感知的BERT在概念上同样简单，但功能更强大。它在十项阅读理解和语言推理任务中取得了新的水平或显著提高了结果。