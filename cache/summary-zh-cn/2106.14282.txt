考虑到当今NLP中普遍存在预先培训的情境化表达，人们已经做出了很多努力来理解它们包含的信息，以及为什么它们似乎普遍成功。使用这些表示的最常见方法是为最终任务对其进行微调。然而，微调如何改变底层嵌入空间的研究较少。在这项工作中，我们研究了英国伯特家族，并使用两种探测技术来分析微调是如何改变空间的。我们假设微调通过增加与不同标签相关的示例之间的距离来影响分类性能。我们通过精心设计的五个不同NLP任务的实验来证实这一假设。通过这些实验，我们还发现了一个例外，即“微调总是提高性能”。最后，通过比较微调前后的表示，我们发现微调不会对表示进行任意更改；相反，它会根据下游任务调整表示，同时在很大程度上保留数据点的原始空间结构。