基于大型transformer的语言模型已被证明在许多分类任务中非常有效。然而，它们的计算复杂性妨碍了它们在需要对大量候选对象进行分类的应用中的使用。虽然以前的工作已经研究了减少模型大小的方法，但是在推理过程中提高批量吞吐量的技术却很少受到关注。在本文中，我们介绍了级联变压器，这是一种简单而有效的技术，可以将基于变压器的模型应用到级联变压器中。每个ranker用于在批处理中修剪候选子集，从而显著提高推理时的吞吐量。来自transformer模型的部分编码在重排者之间共享，从而提供进一步的加速。与最先进的变压器模型相比，我们的方法减少了37%的计算量，而对准确性几乎没有影响，这在两个英语问答数据集上进行了测量。