在自然语言处理文献中，神经网络正变得越来越深刻和复杂。这一趋势的最新代表是深层语言表示模型，包括伯特、埃尔莫和GPT。这些发展使人们相信，上一代用于语言理解的较浅的神经网络已经过时。然而，在本文中，我们证明了在不改变体系结构、外部训练数据或附加输入特征的情况下，基本的、轻量级的神经网络仍然具有竞争力。我们建议将BERT（一种最先进的语言表示模型）中的知识提取到单层BiLSTM中，以及用于句子对任务的暹罗对应模型中。在释义、自然语言推理和情感分类的多个数据集中，我们使用ELMo获得了可比的结果，同时使用的参数大约减少了100倍，推理时间减少了15倍。