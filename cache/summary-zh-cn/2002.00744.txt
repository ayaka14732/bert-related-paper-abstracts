诸如BERT之类的预训练模型广泛应用于自然语言处理任务中，并经过微调以持续改进各种自然语言处理任务的性能。然而，在我们的协议语料库上训练的经过微调的BERT模型在实体链接（EL）任务上仍然表现不佳。在本文中，我们提出了一个模型，该模型将微调语言模型与RFC域模型相结合。首先，我们设计了一个协议知识库作为协议EL的指南。其次，我们提出了一种新的模型PEL-BERT，将协议中的命名实体链接到协议知识库中的类别。最后，我们对预先训练的语言模型在描述性文本和抽象概念上的表现进行了全面的研究。实验结果表明，我们的模型在带注释的数据集上实现了最先进的EL性能，优于所有基线。