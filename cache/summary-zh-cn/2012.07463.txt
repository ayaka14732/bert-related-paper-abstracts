虽然预训练网络的任务特定微调在NLP方面带来了显著的经验进步，但网络的大尺寸使得微调难以在多任务、内存受限的环境中部署。我们提出差异剪枝作为一种简单的方法，在pretrain-finetune框架内实现参数有效的迁移学习。该方法将精细调整视为学习特定于任务的差异向量，该差异向量应用于预训练参数向量之上，该参数向量保持不变，并在不同任务之间共享。在训练过程中，使用L0范数惩罚的可微近似自适应修剪微分向量，以鼓励稀疏性。随着任务数量的增加，差异修剪变得更加有效，因为它只需要存储每个任务的差异向量的非零位置和权重，而存储共享预训练模型的成本保持不变。此外，它不需要在培训期间访问所有任务，这使得它在任务以流形式到达或任务集未知的环境中具有吸引力。我们发现，使用差异修剪进行微调的模型可以匹配GLUE基准上完全微调基线的性能，而每个任务只修改0.5%的预训练模型参数。