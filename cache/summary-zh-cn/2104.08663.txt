现有的神经信息检索（IR）模型通常是在同质和狭窄的环境中进行研究的，这使得对其非分布（OOD）泛化能力的了解非常有限。为了解决这一问题，并便于研究人员广泛评估其模型的有效性，我们引入了Benchmarking IR（BEIR），这是一种用于信息检索的健壮且异构的评估基准。我们从不同的文本检索任务和领域中仔细挑选了18个公开可用的数据集，并在BEIR基准上评估了10个最先进的检索系统，包括词汇、稀疏、密集、后期交互和重新排序体系结构。我们的结果表明，BM25是一个稳健的基线，基于重新排序和后期交互的模型平均实现了最佳的零炮性能，但计算成本较高。相比之下，密集和稀疏检索模型的计算效率更高，但其性能往往低于其他方法，这突出了它们在泛化能力方面的巨大改进空间。我们希望这个框架能让我们更好地评估和理解现有的检索系统，并有助于在未来加速向更好的健壮性和通用性系统的发展。BEIR可在以下网址公开获取：https://github.com/UKPLab/beir.