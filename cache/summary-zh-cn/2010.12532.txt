大型预先训练的语言模型（如BERT）是许多NLP任务最近改进的驱动力。然而，BERT只接受过预测缺失单词的训练，不管是在面具后面还是在下一句话中，除了通过无监督的预训练获得的信息外，他对词汇、句法或语义信息一无所知。我们提出了一种新的方法，将语言知识以单词嵌入的形式显式地注入到预训练的BERT的任何层中。我们在注入依赖性嵌入和反拟合嵌入时对多个语义相似性数据集的性能改进表明，这些信息是有益的，并且目前从原始模型中缺失。我们的定性分析表明，反向拟合嵌入注入对涉及同义词对的情况特别有帮助。