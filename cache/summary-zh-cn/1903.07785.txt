我们提出了一种新的预训练双向转换器模型的方法，该模型在各种语言理解问题中提供了显著的性能增益。我们的模型解决了完形填空式的单词重建任务，其中每个单词都被删除，并且必须根据文本的其余部分进行预测。实验表明，GLUE和NER以及选区分析基准测试的最新结果在性能上有很大提高，这与同时引入的BERT模型一致。我们还详细分析了一些有助于有效预训练的因素，包括数据域和大小、模型容量以及完形填空目标的变化。