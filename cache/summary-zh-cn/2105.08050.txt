变压器已成为深度学习中最重要的架构创新之一，并在过去几年中实现了许多突破。在这里，我们提出了一个简单的网络体系结构，gMLP，基于带选通的MLP，并表明它可以在关键的语言和视觉应用程序中执行和转换。我们的比较表明，自我注意对于视觉转换器来说并不重要，因为gMLP可以达到同样的精确度。对于BERT，我们的模型在训练前的复杂度上与Transformers实现了对等，并且在一些下游NLP任务上更好。在gMLP性能较差的微调任务中，使gMLP模型大得多可以缩小与变压器的差距。总的来说，我们的实验表明，随着数据和计算量的增加，gMLP可以扩展和转换。