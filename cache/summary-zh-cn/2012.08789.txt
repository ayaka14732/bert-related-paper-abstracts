在这项工作中，我们建议通过训练前的错误预测来提高语言训练前方法的有效性。忽略输入句子中与错误预测语义冲突的单词可能是在预训练时产生错误预测的原因。因此，我们假设训练前的错误预测可以作为模型病态焦点的检测器。如果我们训练模型更多地关注与错误预测的冲突，而较少关注输入句子中的其余单词，那么错误预测可以更容易地得到纠正，整个模型也可以得到更好的训练。为此，我们将介绍较少关注错误预测上下文（McMisP）。在McMisP中，我们记录单词之间的共现信息，以无监督的方式检测预测错误的冲突单词。然后，当发生错误预测时，McMisP使用这些信息指导注意模块。具体地说，变压器中的几个注意模块经过优化，以便更多地关注输入句子中很少与错误预测同时出现的单词，反之亦然。结果表明，McMisP显著加速了BERT和ELECTRA，提高了它们在下游任务中的性能。