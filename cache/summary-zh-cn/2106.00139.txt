经过预先训练的文本编码器，如BERT及其变体，最近在许多NLP任务中取得了最先进的性能。这些预训练方法虽然有效，但通常需要大量计算资源。为了加速预训练，ELECTRA训练了一个鉴别器，该鉴别器预测每个输入令牌是否被生成器替换。然而，这个新任务作为一个二元分类，语义信息较少。在这项研究中，我们提出了一种新的文本编码器预训练方法，改进了基于多任务学习的ELECTRA。具体来说，我们训练鉴别器同时检测替换的令牌并从候选集中选择原始令牌。我们进一步开发了两种技术来有效地结合所有训练前任务：（1）使用基于注意的网络来处理特定任务的头部；（2）共享生成器和鉴别器的底层。在GLUE和SQuAD数据集上的大量实验证明了我们提出的方法的有效性和效率。