预先训练的文本编码器已经在许多NLP任务上迅速提升了最新水平。我们关注一个这样的模型，BERT，目的是量化语言信息在网络中的捕获位置。我们发现，该模型以可解释和本地化的方式表示传统NLP管道的步骤，并且负责每个步骤的区域按预期顺序出现：词性标记、解析、NER、语义角色，然后是共同引用。定性分析表明，该模型可以而且经常会动态调整该管道，在消除高层表示信息歧义的基础上修改下层决策。