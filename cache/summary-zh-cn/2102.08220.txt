非自回归生成（NAG）由于其快速的推理速度，近年来受到了广泛的关注。然而，现有NAG模型的生成质量仍然落后于自回归模型。在这项工作中，我们证明了BERT可以作为NAG模型的主干，从而大大提高性能。此外，我们还设计了一些机制来缓解vanilla NAG模型的两个常见问题：前缀输出长度的不灵活性和单个令牌预测的条件独立性。最后，为了进一步提高该模型的速度优势，我们提出了一种新的解码策略，即比率优先，用于可以预先近似估计输出长度的应用。为了综合评估，我们在三个文本生成任务上测试了该模型，包括文本摘要、句子压缩和机器翻译。实验结果表明，我们的模型显著优于现有的非自回归基线，并且与许多强自回归模型相比具有竞争力。此外，我们还进行了广泛的分析实验，以揭示每个拟议组件的影响。