Transformer在自然语言处理（如机器翻译、问答）中已变得无处不在；然而，它需要大量的计算来实现高性能，这使得它不适合于受到硬件资源和电池的严格限制的移动应用。在本文中，我们提出了一种高效的移动NLP体系结构Lite Transformer，以便于在边缘设备上部署移动NLP应用程序。关键原语是长-短距离注意（LSRA），其中一组头部专门进行局部上下文建模（通过卷积），而另一组则专门进行长距离关系建模（通过注意）。这种专业化在三个成熟的语言任务上带来了比vanilla transformer一致的改进：机器翻译、抽象摘要和语言建模。在资源有限的情况下（500M/100M MAC），Lite Transformer在WMT'14英法版上的性能分别比Transformer高1.2/1.7 BLEU。Lite Transformer将变压器基础模型的计算量减少了2.5倍，BLEU分数降低了0.3。结合剪枝和量化，我们进一步将Lite Transformer的模型大小压缩了18.2x。对于语言建模，Lite Transformer的复杂度比500米左右的Transformer低1.8。值得注意的是，Lite Transformer在移动NLP设置上比基于AutoML的Evolutiond Transformer高出0.5个BLEU，而无需花费超过250 GPU年的昂贵架构搜索。守则已于https://github.com/mit-han-lab/lite-transformer.