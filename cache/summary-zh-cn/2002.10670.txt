在本文中，我们探讨了BERT arXiv:1810.04805在斯坦福问答数据集（SQuAD2.0）2.0版上的参数效率。在冻结不同数量的最终变压器层以及包括arXiv:1902.00751中提出的适配器层的同时，我们评估了BERT的参数效率。此外，我们还尝试使用上下文感知卷积（CACNN）过滤器（如arXiv:1709.08294v3中所述）作为SQuAD2.0任务的最终增强层。这一探索的部分动机是arXiv:1907.10597，它为扩大人工智能模型的评估标准以包括各种资源效率度量提供了一个令人信服的理由。虽然我们没有根据arXiv:1907.10597中提出的浮点运算效率来评估这些模型，但我们检查了与训练时间、推理时间和模型参数总数相关的效率。我们的结果在很大程度上证实了适配模块的arXiv:1902.00751的结果，同时也表明，由于训练和推理时间的增加，通过添加上下文感知卷积滤波器而获得的F1分数是不实际的。