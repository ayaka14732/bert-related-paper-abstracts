最近，当对下游任务（包括信息检索（IR））进行微调时，预训练的语言表示模型（如BERT）已显示出巨大的成功。然而，为特别检索定制的训练前目标尚未得到很好的探索。在本文中，我们提出了使用代表词预测（PROP）进行预训练的方法来进行ad-hoc检索。PROP的灵感来源于经典的IR统计语言模型，特别是查询可能性模型，该模型假设查询是作为代表“理想”文档的文本片段生成的。基于这一思想，我们构建了用于预训练的代表词预测（ROP）任务。给定一个输入文档，我们根据文档语言模型对一对词集进行采样，其中可能性较高的词集被认为更能代表文档。然后，我们结合蒙面语言模型（MLM）的目标，预先训练Transformer模型来预测两个词集之间的成对偏好。通过对各种具有代表性的下游即席检索任务进行进一步微调，PROP在没有预训练或使用其他预训练方法的情况下实现了基线的显著改进。我们还表明，在零和低资源红外设置下，PROP都可以实现令人兴奋的性能。代码和预先培训的模型可在https://github.com/Albert-Ma/PROP.