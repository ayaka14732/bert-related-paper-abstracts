最近，基于变换器的预训练语言模型（如BERT和GPT）在许多自然语言处理（NLP）任务中都有了很大的改进。然而，这些模型包含大量的参数。GPT2和威震天等更大更精确的模型的出现，表明了大型预训练变压器模型的趋势。然而，在生产环境中使用这些大型模型是一项复杂的任务，需要大量的计算、内存和电源资源。在这项工作中，我们展示了如何在BERT的微调阶段执行量化感知训练，以便以最小的精度损失将BERT压缩$4\倍。此外，如果针对支持8位整数的硬件进行优化，生成的量化模型可以加快推理速度。