深度预训练变压器网络在各种排序任务中都是有效的，例如问答和即席文档排序。然而，他们的计算费用认为他们的成本在实践中令人望而却步。我们提出的方法称为PreTTR（预计算转换器术语表示法），它大大减少了深度转换器网络的查询时间延迟（web文档排名的加速比高达42倍），使这些网络在实时排名场景中更实用。具体来说，我们在索引时预计算部分文档术语表示（无需查询），并在查询时将它们与查询表示合并以计算最终排名分数。由于令牌表示的规模很大，我们还提出了一种有效的方法，通过训练压缩层以匹配注意分数来减少存储需求。我们的压缩技术将所需的存储空间减少了95%，并且可以在不大幅降低排名性能的情况下进行应用。