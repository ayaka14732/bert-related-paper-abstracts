我们提出了UniT，一个统一的转换器模型，用于同时学习不同领域中最突出的任务，从目标检测到自然语言理解和多模态推理。基于transformer编码器-解码器架构，我们的单元模型使用编码器对每个输入模态进行编码，并使用编码输入表示的共享解码器对每个任务进行预测，然后是特定于任务的输出头。整个模型是端到端联合训练的，每个任务都有损失。与以前使用transformers进行多任务学习的工作相比，我们在所有任务中共享相同的模型参数，而不是单独微调特定于任务的模型，并在不同领域处理更多种类的任务。在我们的实验中，我们在8个数据集上共同学习了7项任务，在每个任务上都取得了很好的性能，而且参数明显较少。我们的代码以MMF格式提供，网址为https://mmf.sh.