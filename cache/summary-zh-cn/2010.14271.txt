跨语言机器阅读理解（CLMRC）仍然是一个具有挑战性的问题，因为缺乏大规模的低源语言注释数据集，如阿拉伯语、印地语和越南语。以前的许多方法使用翻译数据，将丰富的源语言（如英语）翻译为低源语言，作为辅助监督。然而，如何有效地利用翻译数据并减少翻译带来的噪音的影响仍然是一项艰巨的任务。在本文中，我们通过一种称为语言分支机器阅读理解（LBMRC）的新的增强方法来应对这一挑战并提高跨语言迁移性能。语言分支是一种语言的一组段落，与所有目标语言的问题成对出现。在LBMRC的基础上，我们训练了精通个人语言的多机器阅读理解（MRC）模型。然后，我们设计了一种多语言提取方法，将多语言分支模型中的知识融合到所有目标语言的单一模型中。将LBMRC与多语言蒸馏相结合，可以更好地抵抗数据噪声，从而提高模型的跨语言能力。同时，生成的单个多语言模型适用于所有目标语言，从而节省了多个模型的训练、推理和维护成本。在两个CLMRC基准上的大量实验清楚地表明了我们提出的方法的有效性。