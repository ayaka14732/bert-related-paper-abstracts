本文提出了一个统一的视觉语言预训练（VLP）模型。该模型的统一之处在于：（1）它可以针对视觉语言生成（例如，图像字幕）或理解（例如，视觉问答）任务进行微调；（2）它使用共享的多层转换网络进行编码和解码，这与许多现有方法不同，其中编码器和解码器使用单独的模型实现。统一的VLP模型使用两个任务的无监督学习目标在大量图像-文本对上进行预训练：双向和序列到序列（seq2seq）掩蔽视觉语言预测。这两项任务仅在预测条件的上下文中有所不同。这是通过为共享变压器网络使用特定的自我注意面具来控制的。据我们所知，VLP是第一个报告的模型，它在视觉语言生成和理解任务方面实现了最先进的结果，与图像字幕和视觉问答一样，跨越三个具有挑战性的基准数据集：COCO字幕、Flickr30k字幕和VQA 2.0。代码和预先培训的模型可在https://github.com/LuoweiZhou/VLP.