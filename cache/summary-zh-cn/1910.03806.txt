多语言BERT模型在104种语言上进行训练，旨在作为通用语言模型和句子编码工具。我们探讨了该模型在多个任务中对几种语言的表现：一个诊断分类探测特定语法属性的嵌入，一个完形填空任务测试语言建模能力以填补句子中的空白，以及自然语言生成任务，测试生成符合给定上下文的连贯文本的能力。我们发现，目前可用的多语言BERT模型明显不如单语模型，并且在许多情况下不能替代训练有素的单语模型。我们发现，英语和德语模型在生成时表现良好，而多语言模型尤其缺乏北欧语言。