视觉和语言推理要求理解视觉概念、语言语义，最重要的是，理解这两种模式之间的对齐和关系。因此，我们提出LXMERT（从变压器学习跨模态编码器表示）框架来学习这些视觉和语言连接。在LXMERT中，我们构建了一个大型转换器模型，该模型由三个编码器组成：对象关系编码器、语言编码器和跨模态编码器。接下来，为了使我们的模型具有连接视觉和语言语义的能力，我们通过五个不同的代表性预训练任务对模型进行了大量的图像和句子对预训练：掩蔽语言建模、掩蔽对象预测（特征回归和标签分类）、跨模态匹配、，形象问答。这些任务有助于学习模态内和模态间的关系。在对预先训练的参数进行微调后，我们的模型在两个可视化问答数据集（即VQA和GQA）上实现了最先进的结果。我们还通过将预先训练的跨模态模型应用于具有挑战性的视觉推理任务NLVR2，并将之前的最佳结果绝对值提高了22%（54%至76%），从而证明了该模型的通用性。最后，我们展示了详细的消融研究，以证明我们的新模型组件和预训练策略对我们的强大结果有显著贡献；并给出了不同编码器的几种注意可视化。代码和预先培训的模型可在以下网站公开获取：https://github.com/airsplay/lxmert