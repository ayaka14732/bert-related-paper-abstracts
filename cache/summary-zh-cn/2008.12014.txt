基于转换器的语言模型，如BERT及其变体，在通用基准数据集（如GLUE、SQUAD、RACE）的几个下游自然语言处理（NLP）任务中取得了最先进的性能。然而，这些模型大多应用于资源丰富的英语。在本文中，我们介绍了希腊文-BERT，一种基于单语BERT的现代希腊文语言模型。我们评估了它在三个NLP任务中的性能，即词性标注、命名实体识别和自然语言推理，获得了最先进的性能。有趣的是，在两个基准测试中，希腊文-BERT比两个基于多语言转换器的模型（M-BERT、XLM-R）以及在预先训练的单词嵌入上运行的较浅的神经基线表现出色（5%-10%）。最重要的是，我们公开了GREEK-BERT和我们的训练代码，以及说明如何为下游NLP任务微调GREEK-BERT的代码。我们期望这些资源能够促进现代希腊语的NLP研究和应用。