自我注意网络（SAN）可以通过BERT和XLNet等无监督的预训练范式从双向表征学习中获益匪浅。在本文中，我们提出了一种类似XLNet的预训练方案“语音XLNet”，用于无监督声学模型预训练，以使用SAN学习语音表示。预训练的SAN在混合SAN/HMM框架下进行了微调。我们推测，通过洗牌语音帧顺序，语音XLNet中的排列可以作为一个强大的正则化器，鼓励SAN通过其注意力权重关注全局结构进行推断。此外，Speech XLNet还允许该模型探索双向上下文，以便进行有效的语音表示学习。在TIMIT和WSJ上的实验表明，与随机初始化权值训练相比，语音XLNet在收敛速度和识别精度方面都大大提高了SAN/HMM的性能。我们的最佳系统在TIMIT和WSJ任务上分别实现了11.9%和8.3%的相对改善。特别是，最佳系统在TIMIT测试集上实现了13.3%的电话错误率（PER），据我们所知，这是从单个系统获得的最低PER。