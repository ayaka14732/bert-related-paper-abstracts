大型语言模型通常要经过数十万个计算日的训练，在零次和少量学习方面表现出了卓越的能力。考虑到计算成本，这些模型很难在没有大量资本的情况下复制。对于通过API提供的少数数据，不允许访问完整的模型权重，这使得它们难以研究。我们介绍了开放式预训练变换器（OPT），这是一套仅针对解码器的预训练变换，参数范围从125M到175B，我们旨在与感兴趣的研究人员充分、负责任地共享。我们表明，OPT-175B与GPT-3相当，但只需要1/7的碳足迹即可开发。我们还发布了我们的日志，详细介绍了我们面临的基础设施挑战，以及对所有发布的模型进行实验的代码。