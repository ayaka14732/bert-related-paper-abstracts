现代预先训练的语言模型大多建立在主干上，以交错顺序堆叠自我注意和前馈层。在本文中，除了这种刻板的层模式之外，我们还从层类型集和层顺序两个方面利用层的多样性来改进预先训练的模型。具体来说，除了原始的自我注意和前馈层外，我们还将卷积引入层类型集中，实验发现这对预训练模型是有益的。此外，除了原来的交错顺序，我们还探索了更多的层顺序，以发现更强大的体系结构。然而，引入的层种类导致了超过数十亿个候选的大型体系结构空间，而从头开始训练单个候选模型已经需要巨大的计算成本，这使得通过直接训练大量候选模型来搜索这样的空间是不可承受的。为了解决这个问题，我们首先预先训练一个超网，从中可以继承所有候选模型的权重，然后采用一种基于预训练精度的进化算法来寻找最优结构。大量实验表明，该方法得到的LV-BERT模型在各种下游任务上都优于BERT及其变体。例如，LV BERT small在胶水测试集上达到79.8，比强基线ELECTRA small高1.8。