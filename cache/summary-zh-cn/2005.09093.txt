在104种语言上训练的多语言BERT（mBERT）在几个NLP任务上表现出令人惊讶的良好跨语言性能，即使没有明确的跨语言信号。然而，这些评估侧重于高资源语言的跨语言迁移，仅涵盖mBERT所涵盖语言的三分之一。我们将探讨mBERT如何在更广泛的语言集上执行，重点关注低资源语言的表示质量，通过语言内性能来衡量。我们考虑了三个任务：命名实体识别（99种语言）、词性标注和依存句法分析（每个语言54种）。mBERT在高资源语言上的性能优于基线，或与基线相当，但在低资源语言上的性能更差。此外，这些语言的单语BERT模型做得更糟。与类似的语言搭配，单语BERT和mBERT之间的性能差距可以缩小。我们发现，低资源语言的更好模型需要更有效的预训练技术或更多的数据。