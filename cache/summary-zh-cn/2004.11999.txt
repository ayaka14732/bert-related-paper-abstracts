预训练的神经模型，如BERT，当微调以执行自然语言推理（NLI）时，通常在标准数据集上显示出高精度，但在受控挑战集上显示出对词序的惊人缺乏敏感性。我们假设，这一问题主要不是由预训练模型的局限性引起的，而是由于在微调阶段可能传达句法结构重要性的众包NLI示例的缺乏。我们探索了几种方法，通过对MNLI语料库中的句子进行句法转换来生成语法信息示例，从而增强标准训练集。表现最好的增强方法，主语/宾语倒置，将BERT对受控示例的准确度从0.28提高到0.73，而不影响MNLI测试集的性能。这种改进超越了用于数据扩充的特定构造，表明扩充导致BERT招募抽象语法表示。