我们提出了VideoCLIP，这是一种对比方法，用于预训练零镜头视频和文本理解的统一模型，而不在下游任务上使用任何标签。VideoCLIP通过对比时间重叠的正视频文本对和最近邻检索的硬负片，训练视频和文本的转换器。我们对一系列下游任务（包括序列级文本视频检索、视频QA、令牌级动作定位和动作分割）的实验显示了最先进的性能，超过了先前的工作，在某些情况下甚至优于监督方法。该守则可于https://github.com/pytorch/fairseq/tree/main/examples/MMPT.