用上下文化的单词表示代替静态单词嵌入在许多NLP任务中取得了显著的改进。然而，像ELMo和BERT这样的模型所产生的语境化表示究竟有多上下文？每个单词是否有无限多个特定于上下文的表示，或者单词本质上被指定为有限数量的词义表示之一？首先，我们发现所有单词的语境化表示在语境化模型的任何层中都不是各向同性的。虽然同一个词在不同语境中的表现形式仍然比两个不同词的表现形式具有更大的余弦相似性，但这种自相似性在上层要低得多。这表明上下文化模型的上层产生了更多上下文特定的表示，就像LSTM的上层产生了更多任务特定的表示一样。在ELMo、BERT和GPT-2的所有层中，平均而言，一个单词的上下文化表示中只有不到5%的方差可以通过该单词的静态嵌入来解释，这为上下文化表示的成功提供了一些理由。