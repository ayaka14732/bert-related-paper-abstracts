多语言BERT（mBERT）提供了104种语言的句子表示，这对许多多语言任务都很有用。以前的工作在形态学和句法任务中使用零镜头迁移学习来探索mBERT的跨语言性。我们转而关注mBERT的语义属性。我们证明了mBERT表示可以分为语言特定组件和语言中性组件，语言中性组件在建模语义方面具有足够的通用性，可以实现高精度的单词对齐和句子检索，但对于更困难的机器翻译质量评估任务来说还不够好。我们的工作提出了一些有趣的挑战，必须解决这些挑战才能构建更好的语言中立表示，特别是对于需要语义语言转换的任务。