预训练语言模型（PLM）能生成派生复杂词吗？我们提出了第一项研究调查这个问题，以伯特为例PLM。我们考察了BERT在不同环境下的推导能力，从使用未修改的预训练模型到完全微调。我们最好的模型，DagoBERT（衍生和生成优化的BERT），明显优于先前最先进的衍生生成（DG）。此外，我们的实验表明，输入分割对BERT的派生知识有着至关重要的影响，这表明，如果使用形态信息的单位词汇表，PLM的性能可以进一步提高。