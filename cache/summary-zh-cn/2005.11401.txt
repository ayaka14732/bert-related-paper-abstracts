大型预先训练的语言模型已被证明在其参数中存储事实知识，并在对下游NLP任务进行微调时实现最先进的结果。然而，它们访问和精确操作知识的能力仍然有限，因此在知识密集型任务中，它们的性能落后于特定于任务的体系结构。此外，为他们的决策提供出处和更新他们的世界知识仍然是一个开放的研究问题。具有显式非参数记忆可微访问机制的预训练模型可以克服这一问题，但迄今为止仅针对提取下游任务进行了研究。我们探索了检索增强生成（RAG）的通用微调方法——将预先训练的参数和非参数记忆结合起来用于语言生成的模型。我们介绍了RAG模型，其中参数记忆是预先训练的seq2seq模型，非参数记忆是Wikipedia的稠密向量索引，通过预先训练的神经检索器访问。我们比较了两种RAG公式，一种是在整个生成序列的相同检索通道上设置条件，另一种是每个标记可以使用不同的通道。我们在广泛的知识密集型NLP任务上对我们的模型进行了微调和评估，并在三个开放域QA任务上设置了最新技术，优于参数化seq2seq模型和特定于任务的检索和提取架构。对于语言生成任务，我们发现RAG模型生成的语言比最先进的参数化seq2seq基线更具体、多样和真实。