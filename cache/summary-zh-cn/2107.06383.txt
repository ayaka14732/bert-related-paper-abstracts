大多数现有的视觉和语言（V&L）模型依赖预先训练的视觉编码器，使用相对较小的人工标注数据集（与网络爬网数据相比）来感知视觉世界。然而，据观察，大规模的预训练通常可以产生更好的泛化性能，例如，在大量图像标题对上训练的CLIP（对比语言图像预训练）在各种视觉任务中表现出很强的零镜头能力。为了进一步研究CLIP带来的优势，我们建议在两种典型场景中使用CLIP作为各种V&L模型的视觉编码器：1）将CLIP插入特定于任务的微调；2） 将CLIP与V&L预培训相结合，并转移到下游任务。我们发现，CLIP显著优于使用域内注释数据（如自下而上自上而下）训练的广泛使用的视觉编码器。我们在各种V&L任务上取得了具有竞争力或更好的结果，同时在视觉问答、视觉蕴涵和V&L导航任务上取得了最新的成果。我们在https://github.com/clip-vil/CLIP-ViL.