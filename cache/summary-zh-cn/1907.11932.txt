机器学习算法通常容易受到敌对示例的攻击，这些示例与原始示例相比有着不可察觉的变化，但可以愚弄最先进的模型。通过公开恶意制作的对抗性示例，有助于评估甚至提高这些模型的健壮性。在这篇文章中，我们提出了textwooler，一个简单但强大的基线来生成自然的对抗性文本。通过将其应用于两个基本的自然语言任务，文本分类和文本蕴涵，我们成功地攻击了三个目标模型，包括强大的预训练BERT和广泛使用的卷积和递归神经网络。我们从三个方面展示了该框架的优势：（1）有效——它在成功率和干扰率方面优于最先进的攻击；（2）实用性保护——它保留语义内容和语法性，并保持人类的正确分类；（3）高效---它生成对抗性文本，计算复杂度与文本长度成线性关系*代码、预先训练的目标模型和测试示例可在https://github.com/jind11/TextFooler.