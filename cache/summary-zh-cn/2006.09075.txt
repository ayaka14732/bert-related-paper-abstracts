基于枢轴的神经表示模型在NLP领域自适应方面取得了重大进展。然而，以前采用这种方法的工作仅利用来自源域的标记数据和来自源域和目标域的未标记数据，而忽略了合并不一定来自这些域的大量未标记语料库。为了缓解这种情况，我们提出了PERL：一种表示学习模型，它通过基于枢轴的微调扩展了上下文化的单词嵌入模型，如BERT。PERL在22个情绪分类域自适应设置中优于强基线，提高了域内模型性能，生成有效的缩减模型并提高了模型稳定性。