变压器在若干任务中取得了显著的性能，但由于其二次复杂性，相对于输入的长度，它们在很长的序列中速度非常慢。为了解决这个限制，我们将自我关注表示为内核特征映射的线性点积，并利用矩阵积的关联性属性将复杂性从$\mathcal{O}\left（N^2\right）$降低到$\mathcal{O}\left（N\right）$，其中，$N$是序列长度。我们证明了这个公式允许一个迭代实现，极大地加速了自回归变换器，并揭示了它们与递归神经网络的关系。我们的线性变压器实现了与普通变压器类似的性能，并且它们在很长序列的自回归预测上快了4000x。