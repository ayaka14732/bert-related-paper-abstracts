预训练语言模型（PLM）提示通过弥合预训练任务和各种下游任务之间的差距，显示出显著的性能。在这些方法中，prompt tuning冻结PLM，只调整软提示，为使大规模PLM适应下游任务提供了一种高效的解决方案。然而，即时调优尚未得到充分探索。在我们的试点实验中，我们发现，当下游数据足够时，快速调谐的性能与传统的全模型微调相当，而在少数镜头学习设置下，快速调谐的性能要差得多，这可能会阻碍快速调谐在实践中的应用。我们将这种低性能归因于初始化软提示的方式。因此，在这项工作中，我们建议通过在预训练阶段添加软提示来预训练提示，以获得更好的初始化效果。我们将这个经过预先训练的提示调优框架命名为“PPT”。为了保证PPT的通用性，我们将相似的分类任务制定成统一的任务形式，并为该统一任务预训练软提示。大量的实验表明，在全数据和少量镜头设置下，为下游任务调整预先训练的提示可以达到甚至优于全模型微调。我们的方法在实际应用中是有效的。