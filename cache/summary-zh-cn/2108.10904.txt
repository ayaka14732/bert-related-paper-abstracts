随着视觉和文本表征联合建模的最新进展，视觉语言预训练（VLP）在许多多模态下游任务中取得了令人印象深刻的性能。然而，对昂贵注释（包括清晰的图像标题和区域标签）的要求限制了现有方法的可扩展性，并且由于引入了多个特定于数据集的目标，使预训练过程变得复杂。在这项工作中，我们放松了这些限制，并提出了一个最低限度的预训练框架，名为简单视觉语言模型（SimVLM）。与之前的工作不同，SimVLM通过利用大规模弱监督来降低训练复杂性，并且使用单前缀语言建模目标进行端到端训练。在不使用额外数据或特定任务定制的情况下，生成的模型显著优于以前的预训练方法，并在一系列区分性和生成性视觉语言基准上实现了最新的结果，包括VQA（+3.74%VQA分数）、NLVR2（+1.17%准确度）、SNLI-VE（+1.37%准确度）和图像字幕任务（+10.1%苹果酒平均分数）。此外，我们还证明了SimVLM具有很强的泛化和迁移能力，能够实现零镜头行为，包括开放式视觉问答和跨模态迁移。