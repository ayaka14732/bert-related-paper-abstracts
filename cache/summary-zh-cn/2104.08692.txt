多语言T5（mT5）在大量单语文本上预训练了一个序列到序列的模型，在许多跨语言任务中显示了有希望的结果。在本文中，我们改进了带翻译对的多语言文本到文本传输转换器（mT6）。具体来说，我们探讨了三种跨语言文本到文本的预训练任务，即机器翻译、翻译对跨度损坏和翻译跨度损坏。此外，我们提出了一个文本到文本预训练的部分非自回归目标。我们在八个多语言基准数据集上评估了这些方法，包括句子分类、命名实体识别、问答和摘要。实验结果表明，与mT5相比，mT6提高了跨语言迁移能力。