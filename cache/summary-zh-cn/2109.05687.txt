最近的预训练语言模型从数百万个参数扩展到数十亿个参数。因此，在各种下游任务中，需要使用有限的训练语料对一个非常大的预训练模型进行微调。在本文中，我们提出了一种简单而有效的微调技术，即子调优，它通过在向后过程中策略性地屏蔽非子网络的梯度来更新大型预训练模型的参数子集（称为子网络）。在GLUE benchmark中对各种下游任务的实验表明，在四种不同的预训练模型中，子调优始终比香草微调的平均分高1.5~8.6分，比先前的微调技术高0.6~1.3分。此外，领域转移和任务转移的实证结果表明，子调优可以获得更大幅度的泛化性能。