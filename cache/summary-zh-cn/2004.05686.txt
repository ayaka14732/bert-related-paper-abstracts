对于各种自然语言处理任务来说，深度和大型预先训练的语言模型是最先进的。然而，这些模型的巨大规模可能会阻碍它们在实践中的应用。一些近期和并行的工作使用知识提炼将这些巨大的模型压缩为浅层模型。在这项工作中，我们研究了知识提取，重点是多语言命名实体识别（NER）。特别是，我们研究了几种蒸馏策略，并提出了一种利用教师内部表示的阶段优化方案，该方案与教师体系结构无关，并且表明它优于先前工作中采用的策略。此外，我们还研究了几个因素的作用，如未标记数据量、注释资源、模型体系结构和推理延迟等。我们表明，我们的方法导致了MBERT类教师模型的大量压缩，参数压缩高达35倍，批量推理延迟压缩高达51倍，同时保留了41种语言中95%的F1分数。