诸如BERT、GPT等预训练语言模型在语言理解中表现出了极大的有效性。现有预训练方法中的辅助预测任务大多是在标记上定义的，因此可能无法很好地捕捉句子级语义。为了解决这个问题，我们提出了CERT：来自Transformers的对比自监督编码器表示法，它在句子层面上使用对比自监督学习预训练语言表示模型。CERT使用回译创建原始句子的增广。然后，通过预测两个增广句是否来自同一个句子，对预先训练好的语言编码器（如BERT）进行微调。CERT使用简单，可以灵活地插入任何预调试NLP管道。我们在GLUE基准测试中对11项自然语言理解任务的CERT进行了评估，其中CERT在7项任务上优于BERT，在2项任务上达到与BERT相同的性能，在2项任务上的性能不如BERT。在11项任务的平均得分上，CERT优于BERT。有关数据和代码，请访问https://github.com/UCSD-AI4H/CERT