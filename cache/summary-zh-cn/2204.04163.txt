像伯特这样的蒙面语言模型（MLM）是如何学习上下文表示的？在这项工作中，我们分析了传销的学习动态。我们发现，MLM采用抽样嵌入作为锚来估计和向表示注入上下文语义，这限制了MLM的效率和有效性。为了解决这些问题，我们提出了TACO，一种简单而有效的表示学习方法，用于直接建模全局语义。TACO提取并对齐隐藏在上下文化表示中的上下文语义，以鼓励模型在生成上下文化表示时加入全局语义。在GLUE基准上的实验表明，与现有MLM相比，TACO实现了高达5倍的加速，平均提高了1.2个点。代码位于https://github.com/FUZHIYI/TACO.