大型语言模型最近在各种自然语言任务中取得了最先进的性能。与此同时，这些模型的大小和延迟都显著增加，这使得它们的使用成本很高，并提出了一个有趣的问题：语言模型需要大吗？我们从模型压缩的角度来研究这个问题。我们提出了一种通用的结构化剪枝方法，通过使用低秩因子分解对每个权重矩阵进行参数化，并在训练过程中自适应地去除秩-1分量。在语言建模任务上，我们的结构化方法在不同的压缩级别上优于其他非结构化和块结构化修剪基线，同时在训练和推理过程中实现了显著的加速。我们还证明了我们的方法可以应用于修剪大型语言模型中的自适应单词嵌入，以及在几个下游微调分类基准上修剪BERT模型。