基于变压器的预训练语言模型已被证明是学习语境化语言表达的有效方法。然而，当前的方法仅在微调下游任务时利用编码器最后一层的输出。我们认为，仅获取单层的输出限制了预训练表示的能力。因此，我们通过将隐藏表示融合到显式隐藏表示提取器（HIRE）中来深化模型学习的表示，该提取器自动吸收关于最后一层输出的互补表示。利用RoBERTa作为主干编码器，我们提出的对预先训练的模型的改进在多个自然语言理解任务中表现出有效性，并帮助我们的模型在GLUE基准上与最先进的模型竞争。