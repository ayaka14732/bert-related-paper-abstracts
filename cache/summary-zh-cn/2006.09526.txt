最近的研究已经证明了多语言预训练语言模型的跨语言对齐能力。在这项工作中，我们发现跨语言对齐可以通过在使用自己的编码器输出挖掘的句子对上训练seq2seq模型来进一步改进。我们利用这些发现开发了一种新的方法——迭代自监督训练跨语言检索（CRIS），其中挖掘和训练过程被迭代应用，同时提高跨语言对齐和翻译能力。使用这种方法，我们在9种语言方向上获得了最先进的无监督机器翻译结果，平均提高了2.4 BLEU，在16种语言的XTREME基准测试中，在Tatoeba句子检索任务上获得了21.5%的绝对准确率平均提高。此外，当对有监督机器翻译下游任务进行微调时，CRIS与mBART相比，平均带来额外1.8 BLEU改进。