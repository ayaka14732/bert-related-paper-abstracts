Transformer架构依赖于显式位置编码，以保留词序的概念。在本文中，我们认为现有的工作没有充分利用位置信息。例如，正弦波嵌入的初始方案是固定的，不可学习。在本文中，我们首先回顾了绝对位置嵌入和现有的相对位置嵌入方法。然后，我们提出了新的技术，鼓励在自我注意机制中增加查询、关键和相对位置嵌入之间的交互。我们最有希望的方法是绝对位置嵌入的推广，与以前的位置嵌入方法相比，改进了SQuAD1.1的结果。此外，我们还讨论了位置嵌入是否能够足够鲁棒地处理长序列的归纳性质。我们的经验证明，我们的相对位置嵌入方法是合理的推广和鲁棒性从归纳的角度。最后，我们表明，我们提出的方法可以作为一种近似的替代方法，在计算量较小的情况下提高大型模型的精度。