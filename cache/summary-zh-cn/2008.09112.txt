跨语言表示有可能使NLP技术适用于世界上绝大多数语言。然而，它们目前需要大量的训练前语料库或访问类型相似的语言。在这项工作中，我们通过从多语言嵌入中移除语言身份信号来解决这些障碍。为此，我们研究了三种方法：（i）将目标语言的向量空间（全部）重新对齐到枢轴源语言；（ii）去除特定语言的平均值和差异，这将产生更好的识别性嵌入作为副产品；（iii）通过去除词法收缩和句子重新排序，增加语言间的输入相似性。我们评估了19种不同类型语言的XNLI和无参考机器翻译。我们的发现揭示了这些方法的局限性——与向量规范化不同，向量空间重新对齐和文本规范化不能在编码器和语言之间实现一致的增益。然而，由于这些方法的加性效应，它们的组合在所有任务和语言中平均减少了8.9个点（m-BERT）和18.2个点（XLM-R）。我们的代码和模型是公开的。