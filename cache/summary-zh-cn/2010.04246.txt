自然语言理解（NLU）和自然语言生成（NLG）任务具有很强的双重关系，NLU的目标是基于自然语言话语预测语义标签，而NLG的目标恰恰相反。以前的工作主要集中在利用模型训练中的对偶性，以获得性能更好的模型。然而，考虑到当前NLP领域模型规模的快速增长，有时我们可能难以重新培训整个NLU和NLG模型。为了更好地解决这个问题，本文建议在不需要再培训的情况下利用推理阶段的对偶性。在三个基准数据集上的实验证明了该方法在NLU和NLG中的有效性，具有很大的实用潜力。