我们对GPT-3进行了微调，以使用基于文本的web浏览环境来回答长格式问题，该环境允许模型搜索和浏览web。通过将任务设置为可以由人类执行，我们能够使用模仿学习来训练任务模型，然后利用人类反馈来优化答案质量。为了使人类对事实准确性的评估更容易，模型必须在浏览时收集参考资料以支持其答案。我们在ELI5上训练和评估我们的模型，ELI5是Reddit用户提问的数据集。我们的最佳模型是通过使用行为克隆对GPT-3进行微调，然后对训练用于预测人类偏好的奖励模型进行拒绝采样而获得的。这个模型的答案在56%的时间里比我们的人类示威者更喜欢，而在69%的时间里，人们更喜欢Reddit投票最高的答案。