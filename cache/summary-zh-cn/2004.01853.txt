摘要文档摘要通常被建模为序列对序列（Seq2Seq）学习问题。不幸的是，在有限的监督摘要数据上训练基于Seq2Seq的大型摘要模型是一项挑战。本文提出了三个预训练目标，允许我们在未标记文本上预训练基于Seq2Seq的抽象摘要模型。其主要思想是，给定从文档人工构建的输入文本，对模型进行预训练以恢复原始文档。这些目标包括句子重新排序、下一个句子生成和隐藏文档生成，它们与抽象文档摘要任务密切相关。在两个基准摘要数据集（即CNN/DailyMail和纽约时报）上的实验表明，所有三个目标都可以提高基线的性能。与在大规模数据（超过160GB）上预训练的模型相比，我们的方法只需19GB的预训练文本，就可以获得类似的结果，这证明了它的有效性。