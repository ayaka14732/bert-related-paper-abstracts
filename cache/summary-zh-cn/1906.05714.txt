Transformer是一种序列模型，它放弃了传统的循环架构，转而采用完全基于注意力的方法。除了提高性能外，使用注意力的一个优点是，它还可以通过显示模型如何为不同的输入元素分配权重来帮助解释模型。然而，变压器模型中的多层、多头注意机制可能难以解读。为了使模型更易于访问，我们引入了一个开源工具，它可以在多个尺度上可视化注意，每个尺度都提供了关于注意机制的独特视角。我们在BERT和OpenAI GPT-2上演示了该工具，并给出了三个示例用例：检测模型偏差、定位相关注意头以及将神经元与模型行为联系起来。