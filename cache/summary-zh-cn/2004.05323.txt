目前，使用上下文化单词嵌入（如ELMo或BERT）的自我关注神经句法分析器在语音转录本的联合句法分析和不流利检测方面产生了最先进的结果。由于上下文化的单词嵌入是在大量未标记数据上预先训练的，因此使用额外的未标记数据来训练神经模型似乎是多余的。然而，我们证明了自训练（一种用于合并未标记数据的半监督技术）为自关注解析器在不流畅检测方面提供了一种新的技术，证明了自训练提供了与预先训练的上下文化单词表示正交的好处。我们还表明，置乱自训练解析器为不流畅检测提供了进一步的增益。