经过预训练的文本编码器在自然语言处理（NLP）中引起了人们的持续关注，并在不同的任务中表现出了获得有希望的结果的能力。最近的研究表明，外部自监督信号（或通过无监督学习提取的知识，如n-grams）有助于为理解语言（如汉语）提供有用的语义证据，从而相应地提高各种下游任务的性能。为了进一步增强编码器，在本文中，我们建议使用大量数据和先进的训练技术对n-gram增强编码器进行预训练。此外，我们尝试将编码器扩展到不同的语言以及不同的领域，在这些领域中，我们确认相同的体系结构适用于这些不同的环境，并且从跨语言和领域的NLP任务的长列表中观察到新的最先进性能。