近年来，Transformer模型通过在一系列自然语言处理（NLP）任务中提供最先进的性能而引起了人们的极大兴趣。然而，这些模型可能有超过1000亿个参数，对计算和内存的要求非常高。我们通过近似计算解决这一挑战，特别针对NLP任务中变压器的使用。变压器通常经过预先培训，然后通过转移学习专门用于特定任务。基于预先训练的变压器在几个下游NLP任务中经常被过度参数化的观察，我们提出了一个框架来创建更小、更快、在某些情况下更精确的模型。该框架的关键基石是显著性分析（SA）方法，该方法识别预先培训的变压器中对给定任务不太重要的部件，以及近似不太重要部件的技术。我们的近似方法包括对块、注意头和权重组进行修剪，对不太重要的权重进行量化，以及基于低复杂度符号匹配的注意机制。我们的框架可以根据用户的约束条件进行调整，以生成更快、更小和/或更精确的模型。我们将我们的框架应用于七个变压器模型，包括优化模型，如DistilBERT和Q8BERT，以及三个下游任务。我们证明，我们的框架生成的模型速度快4倍，小14倍（相对精度降低不到0.5%），或者精度高5.5%，同时模型大小提高9.83倍，速度提高2.94倍。