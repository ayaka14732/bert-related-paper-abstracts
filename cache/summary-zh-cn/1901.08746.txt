随着生物医学文档数量的快速增长，生物医学文本挖掘变得越来越重要。随着自然语言处理（NLP）技术的发展，从生物医学文献中提取有价值的信息越来越受到研究者的欢迎，而深度学习促进了有效生物医学文本挖掘模型的发展。然而，将自然语言处理的进展直接应用于生物医学文本挖掘往往会产生不令人满意的结果，因为单词分布会从一般领域语料库转移到生物医学语料库。在本文中，我们研究了最近引入的预训练语言模型BERT如何适用于生物医学语料库。我们介绍了BioBERT（来自生物医学文本挖掘变压器的双向编码器表示），这是一种在大规模生物医学语料库上预先训练的领域特定语言表示模型。使用几乎相同的任务体系结构，当对生物医学语料库进行预训练时，BioBERT在各种生物医学文本挖掘任务中的表现大大优于BERT和以前的最新模型。虽然BERT的性能与以前最先进的模型相当，但在以下三个具有代表性的生物医学文本挖掘任务中，BioBERT的性能显著优于它们：生物医学命名实体识别（F1分数提高0.62%）、生物医学关系提取（F1分数提高2.80%）生物医学问答（MRR提高12.24%）。我们的分析结果表明，对生物医学语料库进行预训练有助于理解复杂的生物医学文本。我们免费提供BioBERT的预训练重量https://github.com/naver/biobert-pretrained，以及微调BioBERT的源代码，可在https://github.com/dmis-lab/biobert.