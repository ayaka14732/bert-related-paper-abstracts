学习高级多模态表征的多模态预训练是向深度学习和人工智能迈进的又一步。在这项工作中，我们提出了一个新的模型，即InterBERT（用于交互的BERT），这是我们系列多模态预训练方法M6（多模态到多模态多任务巨型变压器）的第一个模型。该模型对不同形态的信息流之间的交互具有很强的建模能力。单流交互模块能够有效地处理多模态的信息，顶部的两流模块保持了各模态的独立性，避免了单模态任务的性能下降。我们使用三个预训练任务对模型进行预训练，包括遮罩分段建模（MSM）、遮罩区域建模（MRM）和图像文本匹配（ITM）；并在一系列视觉和语言下游任务上对模型进行微调。实验结果表明，InterBERT优于一系列强基线，包括最新的多模态预训练方法，分析表明MSM和MRM对预训练是有效的，我们的方法在单模态任务中可以达到与BERT相当的性能。此外，我们提出了一个大规模的中文多模态预训练数据集，并开发了中文InterBERT，这是第一个中文多模态预训练模型。我们从中国最大的电子商务平台——移动淘宝网（mobile Taobao）提出的310万对图文数据集上预训练中文InterBERT。我们对基于文本的图像检索模型进行了微调，最近我们在线部署了基于主题的推荐模型。