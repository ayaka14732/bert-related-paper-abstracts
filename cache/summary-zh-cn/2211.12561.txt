最近的多模态模型如DALL-E和CM3在文本到图像和图像到文本生成方面取得了显著进展。然而，这些模型将所有学习到的知识（例如埃菲尔铁塔的外观）存储在模型参数中，需要越来越大的模型和训练数据来获取更多的知识。为了以更可扩展和模块化的方式集成知识，我们提出了一种检索增强的多模态模型，该模型使基础多模态模型（生成器）能够引用检索器从外部存储器（例如，网络上的多模态文档）中获取的相关知识。具体来说，我们使用预训练的CLIP模型实现检索器，使用CM3 Transformer架构实现生成器，并使用LAION数据集训练该模型。我们得到的模型名为检索增强CM3（RA-CM3），是第一个可以检索和生成文本和图像混合的多模态模型。我们表明，RA-CM3在图像和字幕生成任务上显著优于基线多模态模型，如DALL-E和CM3（MS-COCO上的12个FID和17个CIDEr改进），同时需要更少的训练计算（DALL-E的30%以下）。此外，我们表明，RA-CM3具有新的能力，如知识密集型图像生成和多模态上下文学习。