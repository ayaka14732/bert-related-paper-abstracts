在Transformer模型中，“自我注意”将有注意嵌入的信息结合到下一层的焦点嵌入表示中。因此，在转换器的各个层中，来自不同令牌的信息变得越来越混合。这使得注意力权重在解释时不可靠。在本文中，我们考虑的问题，通过自我关注量化这一信息流。当我们使用注意权重作为输入标记的相对相关性时，我们提出了两种方法来近似给定注意权重的输入标记的注意，即注意卷展和注意流。我们发现，这些方法在信息流方面提供了互补的观点，并且与原始注意相比，两者都与使用消融方法和输入梯度获得的输入标记的重要性得分具有更高的相关性。