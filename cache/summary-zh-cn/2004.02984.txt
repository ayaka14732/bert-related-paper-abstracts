自然语言处理（NLP）最近通过使用具有数亿个参数的庞大预训练模型取得了巨大成功。但是，这些型号的型号尺寸大，延迟时间长，因此无法部署到资源有限的移动设备上。在本文中，我们提出MobileBERT来压缩和加速流行的BERT模型。与原始的BERT一样，MobileBERT是任务不可知的，也就是说，它可以通过简单的微调一般地应用于各种下游NLP任务。基本上，MobileBERT是BERT_LARGE的精简版，同时配备了瓶颈结构和精心设计的自我关注和前馈网络之间的平衡。为了训练MobileBERT，我们首先训练了一个专门设计的教师模型，一个结合了BERT_大模型的倒瓶颈模型。然后，我们从这位老师向MobileBERT进行知识转移。实证研究表明，MobileBERT比BERT_BASE小4.3倍，速度快5.5倍，同时在知名基准上取得了竞争性成果。在GLUE的自然语言推理任务中，MobileBERT在Pixel 4手机上的GLUEscore为77.7（比BERT_BASE低0.6），延迟为62毫秒。在团队v1.1/v2.0问答任务中，MobileBERT的开发F1得分为90.0/79.2（比BERT_BASE高1.5/2.1）。