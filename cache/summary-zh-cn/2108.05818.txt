预训练模型（PTM）正在革新人工智能（AI）技术。它可以在海量数据上学习通用语言特性，然后在特定于任务的数据上进行微调。不幸的是，PTM培训的计算硬件要求非常昂贵，这使得它成为AI社区中一小部分人的游戏。因此，我们提出了一个名为PatrickStar的系统，以降低PTM的硬件要求，并使每个人都可以访问它们。PatrickStar使用CPU-GPU异构内存空间存储模型数据。与现有的工作不同，我们首先以细粒度的方式管理模型数据，将它们组织在内存块中，并在异构内存空间中动态分布。在预热迭代中收集的运行时内存统计数据的指导下，块在异构内存中得到了有效的编排，并生成了较低的CPU-GPU数据传输量。PatrickStar与零冗余优化器共生，使用数据并行性扩展到多个GPU，具有更低的通信带宽要求和更高效的带宽利用率。该系统可以在现有工作无法完成的更大模型和更大批量上训练任务。实验结果表明，PatrickStar在8xV100和240GB CPU内存节点上训练了一个120亿参数的GPT模型，该模型是SOTA工作的模型规模限制的1.5倍，并实现了比SOTA更高的计算效率。即使在一台700美元的个人电脑上，它也能训练出7亿个参数的GPT模型。我们的代码是公开的。