最近的研究表明，多语言BERT（mBERT）学习丰富的跨语言表达，允许跨语言的迁移。我们研究了嵌入在mBERT中的单词级翻译信息，并提出了两种简单的方法，无需微调即可显示出色的翻译能力。结果表明，大部分信息是以非线性方式编码的，而其中一些信息也可以用纯线性工具恢复。作为我们分析的一部分，我们检验了这样一个假设，即mBERT学习包含语言编码成分和抽象的跨语言成分的表示，并在mBERT表示中明确识别经验语言身份子空间。