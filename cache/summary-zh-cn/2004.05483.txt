自然语言理解包括字里行间的阅读和隐含的背景知识。当前的系统要么依赖预先训练的语言模型作为世界知识的唯一隐式来源，要么借助外部知识库（KBs）来整合其他相关知识。我们提出了一个基于自我对话的无监督框架，作为多项选择常识任务的新替代方案。受基于探究的发现学习（Bruner，1961）的启发，我们的方法通过一些信息查询问题（如“$\textit{what is definition of…}$”）来查询语言模型，以发现更多的背景知识。实证结果表明，自对话程序在六分之四的常识基准上显著提高了零镜头语言模型基线的性能，并与从外部知识库获取知识的模型相竞争。虽然我们的方法在几个基准上提高了性能，但人类法官并不总是认为自言自语诱导的知识有用，这就提出了关于常识推理预先训练的语言模型的内部工作原理的有趣问题。