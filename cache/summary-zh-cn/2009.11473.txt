古代汉语是中国文化的精髓。古汉语领域有几个自然语言处理任务，如古今汉语翻译、诗歌生成和对联生成。以往的研究通常使用监督模型，这些模型严重依赖于并行数据。然而，要获得大规模的古汉语平行数据是很困难的。为了充分利用更容易获得的单语古汉语语料库，我们发布了AnchiBERT，这是一个基于BERT体系结构的预训练语言模型，在大规模古汉语语料库上进行训练。我们从语言理解和生成任务两个方面对AnchiBERT进行评估，包括诗歌分类、古今汉语翻译、诗歌生成和对联生成。实验结果表明，AnchiBERT模型优于BERT模型和非预训练模型，并在所有情况下都达到了最先进的结果。