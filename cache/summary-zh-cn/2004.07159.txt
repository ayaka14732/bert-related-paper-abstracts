自监督预训练，如BERT、MASS和BART，已成为自然语言理解和生成的有力技术。现有的预训练技术采用自动编码和/或自回归目标来训练基于转换器的模型，方法是使用一些屏蔽标记从损坏的文本中恢复原始单词标记。现有技术的训练目标通常与许多语言生成任务的目标不一致，例如生成性问题回答和会话反应生成，以便在给定的上下文中生成新的文本。这项工作为PALM提出了一种新方案，该方案在大型未标记语料库上联合预训练自动编码和自回归语言模型，专门用于生成基于上下文的新文本。新方案缓解了现有去噪方案在预训练和微调之间引入的不匹配，其中生成比重建原始文本更多。大量实验表明，PALM在各种语言生成基准上取得了最新的成果，包括生成性问答（在官方MARCO排行榜上排名第一）、CNN/DailyMail上的摘要以及Gigaword、团队上的问题生成、，以及康奈尔大学电影对白中的对话反应生成。