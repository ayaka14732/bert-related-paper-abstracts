信息提取是NLP中的一项重要任务，它可以自动提取数据以填充关系数据库。从历史上看，研究和数据是为英文文本而产生的，随后几年又产生了阿拉伯文、中文（ACE/OntoNotes）、荷兰语、西班牙语、德语（CoNLL评估）和许多其他语言的数据集。自然趋势是将每种语言视为不同的数据集，并为每种语言建立优化的模型。在本文中，我们研究了一种基于多语言BERT的单一命名实体识别模型，该模型同时在多种语言上进行联合训练，并且能够比仅在一种语言上训练的模型更准确地解码这些语言。为了改进初始模型，我们研究了正则化策略的使用，如多任务学习和部分梯度更新。除了作为一个可以处理多种语言（包括代码转换）的单一模型外，该模型还可以用于对新语言进行开箱即用的零炮预测，即使是那些没有训练数据的语言。结果表明，该模型不仅具有与单语模型相媲美的性能，而且在CoNLL02荷兰和西班牙数据集、OntoNotes阿拉伯语和汉语数据集上也取得了最新的结果。此外，它在看不见的语言上表现相当好，在三种CoNLL语言上实现了最先进的零拍。