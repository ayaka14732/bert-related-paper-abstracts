蒙面语言建模（MLM）预训练模型（如BERT）通过使用[MASK]替换某些标记来破坏输入，然后训练模型以重建原始标记。这是一种有效的技术，在所有NLP基准测试中都取得了良好的结果。我们建议通过掩蔽一些令牌的位置以及掩蔽的输入令牌ID来扩展这个想法。我们遵循与BERT相同的标准方法，掩蔽标记位置的百分比，然后使用额外的完全连接分类器阶段预测其原始值。这种方法显示了良好的性能提升（.3\%的改进），从而在收敛时间上获得了额外的改进。对于Graphcore IPU，具有位置掩蔽的BERT基的收敛只需要原始BERT论文中50%的标记。