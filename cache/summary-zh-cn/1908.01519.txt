最近，阅读理解模型在大规模数据集上取得了接近人类的表现，如SHAND、CoQA、MS Macro、RACE等。这在很大程度上是由于发布了预训练的情境化表示，如BERT和ELMo，可以针对目标任务进行微调。尽管取得了这些进步，并且创建了更具挑战性的数据集，但大部分工作仍然是针对英语的。在这里，我们研究了在大规模英语数据集上进行微调的多语种BERT对阅读理解（例如种族）的有效性，并将其应用于保加利亚多项选择阅读理解。我们提出了一个新的数据集，其中包含2221个来自十二年级各种科目（历史、生物、地理和哲学）预科考试的问题，以及412个来自历史在线测验的额外问题。虽然测验作者没有给出相关的上下文，但我们结合了维基百科的知识，检索与问题+每个答案选项组合相匹配的文档。此外，我们还尝试了不同的索引和预训练策略。评估结果显示准确率为42.23%，远高于基线24.89%。