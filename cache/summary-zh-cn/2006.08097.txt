背景预训练语言模型，如BERT（Devlin et al.，2019），通过对大量未标记文本资源的培训，在各种NLP任务中取得了重大突破。金融部门也积累了大量的金融沟通文本。但是，目前还没有预训练的金融特定语言模型可用。在这项工作中，我们通过使用大规模的金融通信语料库对金融领域特定的伯特模型FinBERT进行预训练来解决这一需求。在三个金融情绪分类任务上的实验证实了FinBERT模型相对于一般领域BERT模型的优势。代码和预训练模型可在https://github.com/yya518/FinBERT. 我们希望这将对从事金融NLP任务的从业者和研究人员有用。