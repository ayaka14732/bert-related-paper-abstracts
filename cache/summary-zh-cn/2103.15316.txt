诸如BERT等预训练模型在许多自然语言处理任务中取得了巨大的成功。然而，如何通过这些预训练模型获得更好的句子表示仍然值得探索。以往的研究表明，各向异性问题是基于BERT的句子表示的一个关键瓶颈，它阻碍了模型充分利用底层语义特征。因此，一些增强句子分布各向同性的尝试，如基于流的模型，已被应用于句子表征，并取得了一些改进。在本文中，我们发现传统机器学习中的白化操作同样可以增强句子表示的各向同性并获得竞争性结果。此外，白化技术还能够降低句子表征的维数。实验结果表明，该算法不仅具有良好的性能，而且大大降低了存储成本，加快了模型检索速度。