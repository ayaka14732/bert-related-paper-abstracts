对于许多（少数）语言来说，培训大型模型所需的资源是不可用的。我们研究了在尽可能少的数据情况下零镜头迁移学习的性能，以及语言相似性在这一过程中的影响。我们使用来自两种低资源目标语言变体的数据对四个基于BERT的模型的词汇层进行重新训练，而转换层则在模型源语言的词性标记任务上进行独立的微调。通过结合新的词汇层和微调转换层，我们实现了两种目标语言的高任务性能。在语言相似性较高的情况下，10MB的数据似乎足以实现实质性的单语迁移性能。基于单语的BERT模型在重新训练词法层后通常比基于多语的BERT模型获得更高的下游任务性能，即使目标语言包括在多语模型中。