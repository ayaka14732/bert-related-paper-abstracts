多语BERT（M-BERT）在有监督和零次跨语言迁移学习中都取得了巨大的成功。然而，这一成功只集中在维基百科培训的前104种语言上。在本文中，我们提出了一种简单而有效的方法来扩展M-BERT（E-BERT），从而使它能够对任何新的语言都有好处，并且表明我们的方法也有利于已经存在于M-BERT中的语言。我们在27种语言上进行了大量的命名实体识别（NER）实验，其中只有16种是M-BERT语言，并且在已经使用M-BERT语言的语言上，F1平均增长约6%，在新语言上，F1平均增长23%。