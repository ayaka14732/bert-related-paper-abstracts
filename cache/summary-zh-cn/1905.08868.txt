在现有的共指消解者中发现了性别偏见。为了消除性别偏见，发布了一个性别平衡的数据集性别模糊代词（GAP），最佳基线模型仅达到66.9%F1。来自Transformers的双向编码器表示（BERT）打破了几个NLP任务记录，可用于GAP数据集。然而，在特定任务上进行微调在计算上是昂贵的。在本文中，我们提出了一种端到端解析器，它将预训练的BERT与关系图卷积网络（R-GCN）相结合。R-GCN用于消化结构句法信息和学习更好的任务特定嵌入。实证结果表明，在明确的句法监督下，在不需要微调BERT的情况下，R-GCN的嵌入在共指任务上优于原始BERT嵌入。我们的工作将GAP数据集上的代码片段上下文基线F1分数从66.9%显著提高到80.3%。我们参与了2019年GAP共同参考共享任务，我们的代码可在线获取。