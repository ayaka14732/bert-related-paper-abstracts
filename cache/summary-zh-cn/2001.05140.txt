主导的图神经网络（GNN）过度依赖于图链接，已经出现了一些严重的性能问题，如暂停动画问题和过度平滑问题。此外，由于内存限制限制了节点间的批处理，固有的互连特性排除了图内的并行化，这对于大型图来说变得至关重要。在本文中，我们将介绍一种新的图神经网络，即graph-BERT（graph-based BERT），它完全基于注意机制，没有任何图卷积或聚合算子。我们建议在局部上下文中用采样的无链接子图训练GRAPH-BERT，而不是用完整的大输入图来训练GRAPH-BERT。GRAPH-BERT可以在独立模式下有效地学习。同时，如果有任何有监督的标签信息或特定的面向应用的目标可用，则预先训练好的GRAPH-BERT也可以直接或通过必要的微调转移到其他应用任务。我们在几个图形基准数据集上测试了GRAPH-BERT的有效性。基于预先训练好的GRAPH-BERT和节点属性重建和结构恢复任务，我们进一步对GRAPH-BERT在节点分类和图聚类任务上进行了微调。实验结果表明，GRAPH-BERT在学习效果和效率上都优于现有的GNNs。