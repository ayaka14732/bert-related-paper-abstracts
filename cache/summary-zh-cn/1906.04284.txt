Transformer是一种完全基于注意力的网络替代品，它已经在一系列NLP任务中实现了最先进的结果。在本文中，我们分析了一个转换语言模型GPT-2小预训练模型中的注意结构。我们对单个实例的注意进行可视化，并在大型语料库上分析注意和语法之间的交互作用。我们发现，关注目标的不同部分的语音在不同的层深度模型中，注意对齐与依赖关系最强的中间层。我们还发现，模型的最深层捕捉了最遥远的关系。最后，我们提取了典型的句子，这些句子揭示了特定注意头所针对的高度特定的模式。