计算机视觉通过（a）将图像表示为均匀排列的像素阵列和（b）卷积高度局部化的特征而取得了显著的成功。然而，卷积对所有图像像素都一视同仁，不管其重要性如何；明确建模所有图像中的所有概念，而不考虑内容；并努力将空间上遥远的概念联系起来。在这项工作中，我们通过（a）将图像表示为语义视觉标记和（b）运行转换器来密集地建模标记关系来挑战这种范式。关键的是，我们的视觉转换器在语义标记空间中运行，根据上下文明智地处理不同的图像部分。这与像素空间变换形成鲜明对比，像素空间变换需要更多数量级的计算。使用先进的训练方法，我们的VTs显著优于卷积测试，在使用更少的触发器和参数的同时，将ImageNet top-1上的ResNet精度提高了4.6到7个点。对于LIP和COCO内容的语义分割，基于VT的特征金字塔网络（FPN）实现了高0.35点的mIoU，同时将FPN模块的失败次数减少了6.5倍。