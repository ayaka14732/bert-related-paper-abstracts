我们提出了一种新的自我注意机制，可以学习其最佳注意广度。这允许我们显著扩展Transformer中使用的最大上下文大小，同时保持对内存占用和计算时间的控制。我们在字符级语言建模任务中展示了我们的方法的有效性，通过使用8k字符的最大上下文，我们在text8和enwiki8上实现了最先进的性能。