存在各种类型的预训练架构，包括自回归模型（例如，GPT）、自动编码模型（例如，BERT）和编解码器模型（例如，T5）。另一方面，NLP任务的性质不同，主要有三类：分类、无条件生成和条件生成。然而，没有一个预训练框架对所有任务都表现得最好，这给模型开发和选择带来了不便。我们提出了一个新的预培训框架GLM（通用语言模型）来应对这一挑战。与以前的工作相比，我们的体系结构有三个主要优点：（1）它在分类、无条件生成和条件生成任务上表现良好，只使用一个预训练模型；（2） 由于改进的预训练细调一致性，它在分类上优于类BERT模型；（3） 它自然地处理可变长度的空白填充，这对于许多下游任务至关重要。从经验上看，在相同数量的预训练数据下，GLM在SuperGLUE自然语言理解基准上显著优于BERT。此外，具有1.25x BERT-Large参数的GLM在NLU、条件生成和无条件生成中同时达到最佳性能，这表明了其对不同下游任务的可推广性。