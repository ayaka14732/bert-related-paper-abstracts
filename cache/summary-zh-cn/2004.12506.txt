NLP的最新进展归因于大规模预训练语言模型的出现。特别是GPT-2，由于其从左到右的语言建模目标，它适合于生成任务，但其生成的文本的语言质量在很大程度上尚未探索。我们的工作在理解GPT-2在语篇连贯方面的输出方面迈出了一步。我们对GPT-2输出中的显性话语关系在有机生成和微调情景下的有效性进行了全面研究。结果表明，GPT-2并不总是生成包含有效语篇关系的文本；然而，它的文本更符合人类对微调场景的期望。我们提出了一种解耦策略来缓解这些问题，并强调了显式建模话语信息的重要性。