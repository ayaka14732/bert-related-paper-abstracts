在模型尺寸不断增大的大型数据集上进行预培训，现在已被证明是提高几乎所有NLP任务性能的有效方法。一个值得注意的例外是信息检索，到目前为止，额外的预培训未能产生令人信服的结果。我们表明，通过正确的训练前设置，可以克服这一障碍。我们通过对大型bi编码器模型进行预培训来证明这一点：1）最近发布的一组6500万个综合生成的问题，2）从pushshift.io提供的现有Reddit对话数据集中获得的2亿个评论后对。我们在一组信息检索和对话检索基准上进行了评估，结果表明，与受监督的基线相比有了实质性的改进。