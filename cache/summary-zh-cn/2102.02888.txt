大型模型（如BERT和GPT-3）的可扩展培训需要基于模型设计、体系结构和系统功能的仔细优化。从系统的角度来看，通信已经成为一个主要的瓶颈，特别是在使用标准TCP互连的商品系统上，这种互连提供有限的网络带宽。通信压缩是减少此类系统训练时间的重要技术。其中一种最有效的方法是误差补偿压缩，它即使在1位压缩下也能提供稳健的收敛速度。然而，最先进的误差补偿技术仅适用于基本的优化器，如SGD和动量SGD，它们与梯度呈线性关系。它们不能与Adam等基于非线性梯度的优化器配合使用，Adam为BERT等模型提供了最先进的收敛效率和精度。在本文中，我们提出了1位Adam，它将通信量减少了$5倍，提供了更好的可扩展性，并提供了与未压缩Adam相同的收敛速度。我们的主要发现是Adam的方差（非线性项）变得稳定（在热身阶段之后），并且可以用作剩余训练（压缩阶段）的固定前提条件。在多达256个GPU上进行的实验表明，1位Adam使BERT大型预训练的吞吐量提高了3.3倍，使小队微调的吞吐量提高了2.9倍。此外，我们还为我们提出的工作提供了理论分析。