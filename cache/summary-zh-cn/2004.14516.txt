提出了一种基于跨语言广度预测的有监督词对齐方法。我们首先将单词对齐问题形式化为从源句子中的标记到目标句子中的跨度的独立预测集合。由于这相当于一个班v2.0风格的问答任务，因此我们使用多语言BERT来解决这个问题，该BERT在手动创建的gold word对齐数据上进行了微调。通过在问题中添加标记的上下文，我们极大地提高了单词对齐的准确性。在使用汉语、日语、德语、罗马尼亚语、法语和英语的五个单词对齐数据集进行的实验中，我们表明，在不使用任何比特文本进行预训练的情况下，所提出的方法明显优于以前的有监督和无监督单词对齐方法。例如，我们的汉英数据F1得分为86.7，比之前最先进的监督方法高出13.3分。