为下游NLP任务从转换器（BERT）预训练双向编码器表示是一项非繁琐的任务。我们对5个伯特模型进行了预训练，这些模型在训练集的大小、正式和非正式阿拉伯语的混合以及语言预处理方面有所不同。所有这些都旨在支持阿拉伯语方言和社交媒体。这些实验强调了数据多样性的中心性和语言感知分割的有效性。他们还强调，更多的数据或更多的训练步骤并不需要更好的模型。我们的新模型在几个下游任务上取得了最新的成果。生成的模型以QARiB的名义发布给社区。