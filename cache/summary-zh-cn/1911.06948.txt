尽管机器阅读理解（RC）有了很大的发展，但现有的RC模型仍然容易受到攻击，并且对不同类型的对抗性示例不具有鲁棒性。神经模型过度自信地预测语义不同对抗性示例的错误答案，而过度敏感地预测语义等价对抗性示例的错误答案。现有的提高此类神经模型鲁棒性的方法仅仅缓解了这两个问题中的一个，而忽略了另一个问题。在本文中，我们借助于外部语言知识，同时解决了现有RC模型中存在的过度自信问题和过度敏感问题。我们首先结合外部知识施加不同的语言约束（实体约束、词汇约束和谓词约束），然后通过后验正则化对RC模型进行正则化。语言约束对语义不同和语义等价的对抗性例子都有更合理的预测，后验正则化提供了一种有效的机制来整合这些约束。我们的方法可以应用于任何现有的神经RC模型，包括最先进的BERT模型。大量实验表明，该方法显著提高了基本RC模型的鲁棒性，并能更好地同时处理这两个问题。