最近的研究发现，多语言伯特（mBERT）是一种基于变换器的多语言掩蔽语言模型，能够进行零次跨语言迁移，这表明其表示的某些方面是跨语言共享的。为了更好地理解这种重叠，我们将最近在神经网络内部表示中寻找句法树的工作扩展到多语言环境。我们证明了mBERT表示的子空间可以恢复除英语以外的其他语言的句法树距离，并且这些子空间在不同语言之间近似共享。基于这些结果，我们提出了一种无监督的分析方法，该方法提供了mBERT学习句法依赖标签表示的证据，其形式为聚类，这在很大程度上符合通用依赖分类法。这一证据表明，即使没有明确的监督，多语言蒙面语言模型也能学习某些语言共性。