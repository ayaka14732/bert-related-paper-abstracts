迁移学习从根本上改变了自然语言处理（NLP）研究的格局。许多现有的最先进的模型首先在大型文本语料库上进行预训练，然后在下游任务上进行微调。然而，由于下游任务的数据资源有限，且预训练模型的容量非常大，主动微调往往会导致自适应模型过度拟合下游任务的数据，并忘记预训练模型的知识。为了以更具原则性的方式解决上述问题，我们提出了一种新的计算框架，用于对预先训练的语言模型进行鲁棒和高效的微调。具体而言，我们提出的框架包含两个重要组成部分：1。平滑诱导正则化，有效管理模型的容量；2.Bregman近邻点优化是一类信赖域方法，可以防止知识遗忘。我们的实验表明，我们提出的方法在多个NLP基准上达到了最先进的性能。