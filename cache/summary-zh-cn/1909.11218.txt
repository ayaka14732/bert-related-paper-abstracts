神经网络模型中的注意层提供了对模型预测背后的推理的洞察，这些推理通常被批评为不透明。最近，关于注意力权重的可解释性出现了看似矛盾的观点（Jain&Wallace，2019；Vig&Belinkov，2019）。在这种混乱中，需要更系统地理解注意机制。在这项工作中，我们试图通过给出一个全面的解释来填补这一空白，该解释证明了这两种观察结果的合理性（即，注意何时可解释，何时不可解释）。通过对不同NLP任务的一系列实验，我们验证了我们的观察结果，并通过手动评估强化了我们关于注意力可解释性的主张。