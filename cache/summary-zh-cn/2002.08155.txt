我们提出了CodeBERT，一种编程语言（PL）和自然语言（NL）的双模预训练模型。CodeBERT学习支持下游NL-PL应用的通用表示法，如自然语言代码搜索、代码文档生成等。我们使用基于转换器的神经架构开发CodeBERT，并使用混合目标函数对其进行训练，该混合目标函数包含替换标记检测的预训练任务，这是为了检测从发电机取样的可能的替代品。这使我们能够利用NL-PL对的双峰数据和单峰数据，前者为模型训练提供输入标记，而后者有助于学习更好的生成器。我们通过微调模型参数在两个NL-PL应用程序上评估CodeBERT。结果表明，CodeBERT在自然语言代码搜索和代码文档生成任务上都达到了最先进的性能。此外，为了研究在CodeBERT中学习到什么类型的知识，我们构建了一个NL-PL探测数据集，并在零炮设置中进行评估，其中预先训练的模型的参数是固定的。结果表明，CodeBERT在NL-PL探测上的性能优于以前的预训练模型。