先前的研究指出，BERT的计算成本随序列长度呈二次增长，从而导致更长的训练时间、更高的GPU内存限制和碳排放。虽然最近的工作试图在培训前解决这些可伸缩性问题，但这些问题在微调中也很突出，特别是对于文档分类等长序列任务。因此，我们的工作重点是优化文档分类微调的计算成本。我们通过在一个名为TopicBERT的统一框架中互补学习主题和语言模型来实现这一点。这大大减少了自我关注操作的数量——这是一个主要的性能瓶颈。因此，我们的模型实现了1.4倍（$\sim40\%$）的加速，减少了$CO_2$排放，同时在5个数据集上保持了$99.9\%$的性能。