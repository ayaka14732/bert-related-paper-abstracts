在许多自然语言处理任务中，使用掩模语言模型（如伯特）的双向编码器的成功促使研究人员尝试将这些预先训练的模型合并到神经机器翻译（NMT）系统中。然而，所提出的合并预训练模型的方法并不简单，主要集中在BERT上，这缺乏对其他预训练模型可能对翻译性能产生的影响的比较。在本文中，我们证明了简单地使用定制和合适的双语预训练语言模型（称为BiBERT）的输出（语境化嵌入）作为NMT编码器的输入可以实现最先进的翻译性能。此外，我们还提出了随机层选择方法和双向翻译模型的概念，以确保充分利用语境嵌入。在不使用反向翻译的情况下，我们的最佳模型在IWSLT'14数据集上的En->De和De->En的BLEU分数分别为30.45和38.61，在WMT'14数据集上的En->De和De->En的BLEU分数分别为31.26和34.94，超过了所有公布的数字。