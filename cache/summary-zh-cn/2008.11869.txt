在自然语言理解（NLU）的许多任务中，诸如BERT等预训练语言模型表现出了显著的性能。模型中的标记通常是细粒度的，因为对于像英语这样的语言，它们是单词或子单词，对于像汉语这样的语言，它们是字符。例如，在英语中，存在形成自然词汇单位的多词表达式，因此粗粒度标记化的使用似乎也是合理的。事实上，细粒度和粗粒度标记化在学习预先训练的语言模型方面都有优点和缺点。在本文中，我们在细粒度和粗粒度标记化的基础上提出了一种新的预训练语言模型AMBERT（Multi-grainedbert）。对于英语，AMBERT将单词序列（细粒度标记）和短语序列（粗粒度标记）作为标记化后的输入，使用一个编码器处理单词序列，另一个编码器处理短语序列，利用两个编码器之间的共享参数，最后创建一系列词语的语境化表示和一系列短语的语境化表示。实验已经在中文和英文的基准数据集上进行，包括线索、胶水、队伍和种族。结果表明，AMBERT在所有情况下都优于BERT，特别是对于中国人来说，改进是显著的。我们还开发了一种提高AMBERT推理效率的方法，该方法在与BERT相同的计算量下仍然比BERT具有更好的性能。