大规模跨语言模型（LM）如mBERT、Unicoder和XLM在跨语言表征学习方面取得了巨大成功。然而，当应用于零触发跨语言迁移任务时，大多数现有方法仅使用单一语言输入进行LM微调，而没有利用不同语言之间的固有跨语言对齐，这对于多语言任务来说是必不可少的。在本文中，我们提出了滤波器，一种增强的融合方法，将跨语言数据作为XLM微调的输入。具体地说，FILTER首先在浅层中独立地对源语言中的文本输入及其在目标语言中的翻译进行编码，然后在中间层中执行跨语言融合以提取多语言知识，最后执行进一步的特定于语言的编码。在推理过程中，该模型根据目标语言中的文本输入及其在源语言中的翻译进行预测。对于分类等简单任务，目标语言中的翻译文本与源语言共享相同的标签。但是，对于更复杂的任务，例如问答、NER和词性标记，这种共享标签变得不那么准确，甚至不可用。为了解决这个问题，我们进一步提出了一个额外的KL发散自我教学损失模型训练，基于自动生成软伪标签翻译文本在目标语言。大量实验表明，过滤器在两个具有挑战性的多语言多任务基准测试XTREME和XGLUE上达到了新的水平。