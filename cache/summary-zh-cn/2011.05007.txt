本文讨论了基于BERT的多语言口语理解（SLU）模型在多大程度上可以跨语言传递知识的问题。通过实验，我们将表明，尽管它在远距离语言群体中也能很好地工作，但与理想的多语言性能仍有差距。此外，我们提出了一种新的基于BERT的对抗性模型体系结构，用于学习多语言SLU的语言共享和特定语言表示。我们的实验结果证明，该模型能够缩小差距，达到理想的多语言性能。