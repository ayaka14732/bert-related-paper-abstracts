在大多数NLP任务中，使用基于变换器的大容量预训练语言模型（如BERT）进行迁移学习已成为一种主要方法。简单地对下游任务上的大型语言模型进行微调，或者将其与特定任务的预训练相结合，通常是不可靠的。特别是，随着随机种子的变化或预训练和/或微调迭代次数的变化，性能会有很大的变化，微调模型容易受到对抗性攻击。我们提出了一种简单而有效的基于适配器的方法来缓解这些问题。具体地说，我们在预训练模型的每一层中插入小瓶颈层（即适配器），然后固定预训练层并在下游任务数据上训练适配器层，使用（1）特定于任务的无监督预训练，然后（2）特定于任务的有监督训练（例如，分类、序列标记）。我们的实验表明，这样的训练方案提高了向各种下游任务转移学习的稳定性和对抗鲁棒性。