语言模型在当前的自然语言处理中无处不在，其多语言能力近年来引起了广泛关注。然而，目前的分析几乎完全集中于标准基准（多语种变体），并且依赖于干净的培训前和任务特定语料库作为多语种信号。在本文中，我们介绍了XLM-T，一个用于在Twitter中使用和评估多语言模型的框架。该框架有两大特点：（1）强大的多语言基线，包括XLM-R（Conneau et al.2020）模型，该模型在超过30种语言的数百万条推文上预先训练，以及随后微调目标任务的起始代码；（2）一组统一的情绪分析Twitter数据集，使用八种不同的语言。这是一个模块化的框架，可以轻松地扩展到其他任务，并与最近也旨在使Twitter特定数据集同质化的努力相结合（Barbieri et al.2020）。