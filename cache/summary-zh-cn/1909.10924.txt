端到端口语理解（SLU）是一种直接从语音特征中推断语义而不需要中间文本表示的方法。尽管端到端SLU系统的声学模型组件可以使用自动语音识别（ASR）目标进行预训练，但SLU组件只能从有限的特定任务训练数据中学习语义特征。在本文中，我们首次提出对端到端SLU系统中的SLU组件进行大规模无监督预训练，以便SLU组件能够从大量未标记的音频数据中保留语义特征。由于声学模型组件（即音素后验序列）的输出与文本序列有很大的不同，我们提出了一种新的预训练模型BERT-PLM，该模型通过排列语言建模表示来自变换器的双向编码器表示。BERT-PLM通过相当于部分置换语言建模目标的回归目标对未标记数据上的SLU组件进行训练，同时利用BERT网络的完整双向上下文信息。实验结果表明，我们的方法比最先进的端到端系统的性能要好，误差降低了12.5%以上。