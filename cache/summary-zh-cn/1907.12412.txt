最近，预训练模型在各种语言理解任务中取得了最新的成果，这表明大规模语料库的预训练可能在自然语言处理中起着至关重要的作用。目前的预训练程序通常侧重于训练模型，通过几个简单的任务来掌握单词或句子的共现情况。然而，除了共现之外，训练语料库中还存在其他有价值的词汇、句法和语义信息，如命名实体、语义贴近度和语篇关系。为了最大限度地从训练语料库中提取词汇、句法和语义信息，我们提出了一个名为ERNIE 2.0的持续训练前框架，该框架通过不断的多任务学习逐步构建和学习训练前任务。实验结果表明，ERNIE2.0在16项任务上优于BERT和XLNet，其中包括GLUE基准上的英语任务和汉语中的几个常见任务。源代码和预先培训的模型已在https://github.com/PaddlePaddle/ERNIE.