自BERT出现以来，包括XLNet和RoBERTa在内的最新作品利用了大量语料库和大量参数预先训练的句子嵌入模型。由于此类模型具有大型硬件和大量数据，因此需要很长时间进行预训练。因此，尝试制作性能相对较好的小型模型非常重要。在本文中，我们利用较小的词汇表和数据集训练了一个韩国人特有的模型KR-BERT。由于朝鲜语是形态丰富的语言之一，使用非拉丁字母的资源很差，因此捕捉多语BERT模型遗漏的特定语言现象也很重要。我们测试了几个标记器，包括我们的双向字块标记器，并调整了从子字符级到字符级标记化的最小标记跨度，以便为我们的模型构建更好的词汇表。通过这些调整，我们的KR-BERT模型的表现相当，甚至比使用约1/10语料库的其他现有预训练模型更好。