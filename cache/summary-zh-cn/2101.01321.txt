基于Transformer的模型，如BERT和RoBERTa，在许多自然语言处理任务中取得了最先进的成果。然而，它们的内存占用、推理延迟和功耗在边缘甚至在数据中心都是令人望而却步的高效推理。虽然量化可能是一个可行的解决方案，但之前对基于变压器的模型进行量化的工作在推理过程中使用浮点算法，这无法有效地利用仅整数逻辑单元，如最近的图灵张量核或传统的仅整数ARM处理器。在这项工作中，我们提出了I-BERT，一种新的基于变压器的模型量化方案，该方案使用纯整数算法对整个推理进行量化。基于非线性运算的轻量级纯整数近似方法，例如GELU、Softmax和层规范化，I-BERT执行端到端纯整数的BERT推断，无需任何浮点计算。我们使用RoBERTa Base/Large来评估我们对粘合下游任务的方法。我们表明，在这两种情况下，与全精度基线相比，I-BERT获得了相似（略高）的精度。此外，与FP32推理相比，我们初步实现的I-BERT在T4 GPU系统上的INT8推理的加速比为2.4-4.0x。该框架是在PyTorch中开发的，并且是开源的。