预先训练的语言表示模型，如BERT，从大规模语料库中获取一般的语言表示，但缺乏特定领域的知识。在阅读领域文本时，专家们利用相关知识进行推理。为了使机器能够实现这一功能，我们提出了一种基于知识的语言表示模型（K-BERT）和知识图（KGs），其中三元组作为领域知识注入到句子中。然而，过多的知识整合可能会使句子偏离正确的意思，这就是知识噪音（knowledgenoise，KN）问题。为了克服KN，K-BERT引入了软位置和可见矩阵来限制知识的影响。K-BERT能够从预先训练的BERT中加载模型参数，因此，通过配备KG，K-BERT可以轻松地将领域知识注入到模型中，而无需自己进行预训练。我们的调查显示在12个NLP任务中有很好的结果。特别是在特定领域的任务（包括金融、法律和医学）中，K-BERT显著优于BERT，这表明K-BERT是解决需要专家的知识驱动问题的最佳选择。