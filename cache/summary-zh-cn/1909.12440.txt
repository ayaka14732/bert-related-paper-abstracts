最近，预训练语言模型在广泛的自然语言处理任务中取得了显著的成功。然而，在多语言环境中，在每种语言的大规模语料库上预先训练深层语言模型是非常耗费资源的。一种替代方案是在数百种语言的大规模语料库上预先训练一个强大的多语言深层语言模型，而不是完全独立地预先训练单语语言模型。然而，在这样的模型中，每种语言的词汇量相对较小，尤其是对于低资源语言。这一限制不可避免地阻碍了这些多语言模型在序列标记等任务上的性能，其中深入的标记级或句子级理解至关重要。在本文中，受先前设计用于单语环境的方法的启发，我们研究了两种基于预先训练的多语言模型的方法（即联合映射和混合映射），用于解决各种任务的词汇表外（OOV）问题，包括词性标注、命名实体识别、，机器翻译质量评估和机器阅读理解。实验结果表明，使用混合映射更具前景。据我们所知，这是首次尝试在多语言环境中解决和讨论OOV问题的工作。