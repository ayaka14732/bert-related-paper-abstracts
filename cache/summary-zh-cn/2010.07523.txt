基于方面的情绪分析（ABSA）和目标ASBA（TABSA）允许根据上下文从同一文本中得出更细粒度的情绪推断。例如，给定的文本可以有不同的目标（例如，社区）和不同的方面（例如，价格或安全），与每个目标方面对关联的情绪不同。在本文中，我们研究了在自我注意模型中添加上下文是否可以提高（T）ABSA的性能。我们提出了两种不同的上下文引导的BERT（CG-BERT），它们学习在不同的上下文下分配注意力。我们首先采用上下文感知转换器来生成CG-BERT，它使用上下文引导的softmax注意。接下来，我们提出了一个改进的准注意CG-BERT模型，该模型学习支持减法注意的合成注意。我们在两个（T）ABSA数据集上使用预训练的BERT对两个模型进行训练：SentiHood和SemEval-2014（任务4）。两种模型都取得了最新的成果，我们的QACG-BERT模型具有最好的性能。此外，我们还分析了我们提出的模型中上下文的影响。我们的工作为在基于上下文的自然语言任务中，在预先训练的基于自我注意的语言模型中添加上下文依赖提供了更多的证据。