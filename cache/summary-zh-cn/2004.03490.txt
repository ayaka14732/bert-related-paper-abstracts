虽然模型在流行的问答（QA）数据集（如SQuAD）上已经达到了超人的性能，但它们在问答本身的任务上还没有超越人类。在本文中，我们通过评估五个数据集中基于BERT的模型，研究模型是否在从QA数据集中学习阅读理解。我们评估模型对域外示例的可推广性、对缺失或错误数据的响应以及处理问题变化的能力。我们发现，没有一个数据集对我们的所有实验都具有鲁棒性，并指出了数据集和评估方法中的缺陷。根据我们的分析，我们提出了构建未来QA数据集的建议，这些数据集可以更好地评估通过阅读理解回答问题的任务。我们还发布了将QA数据集转换为共享格式的代码，以便于在https://github.com/amazon-research/qa-dataset-converter.