最近，多语种的BERT在跨语言迁移任务中表现得非常出色，优于静态的非语境化单词嵌入。在这项工作中，我们提供了一个深入的实验研究，以补充现有文献的跨语言能力。我们比较了相同数据下非语境化和语境化表征模式的跨语言能力。我们发现数据大小和上下文窗口大小是影响可转移性的关键因素。我们还观察了多语言文本中特定于语言的信息。通过操纵潜在表示，我们可以控制多语种BERT的输出语言，实现无监督的标记翻译。我们进一步表明，基于这一观察，有一种计算成本低廉但有效的方法可以提高多语种语言学习者的跨语言能力。