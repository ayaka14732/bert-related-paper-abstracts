在本文中，我们提出了潜在关系语言模型（LRLMs），这是一类通过知识图关系参数化文档中单词和其中出现的实体的联合分布的语言模型。该模型具有许多吸引人的特性：它不仅提高了语言建模性能，而且能够通过关系注释给定文本实体跨度的后验概率。实验证明，与基于单词的基线语言模型和以前采用知识图信息的方法相比，经验改进的效果更好。定性分析进一步证明了所提出的模型在上下文中学习预测适当关系的能力。