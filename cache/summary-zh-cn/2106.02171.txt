最近，mT5——T5的大规模多语言版本——利用统一的文本到文本格式，在各种多语言NLP任务上获得最先进的结果。在本文中，我们调查了将并行数据纳入mT5预训练的影响。我们发现，多任务语言建模与目标，如机器翻译在训练前是一个简单的方法，以提高下游的多语言和跨语言任务的性能。然而，随着模型容量的增加，收益开始减少，这表明并行数据对于大型模型可能不那么重要。同时，即使在更大的模型尺寸下，我们发现使用并行数据进行预训练仍然在有限的标记数据区域中提供益处。