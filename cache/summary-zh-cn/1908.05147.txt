对于机器阅读理解来说，有效地从错综复杂、冗长的文章中建模语言知识并克服噪音是提高机器阅读理解能力的关键。传统的注意模型只关注所有的词，没有明确的约束，这导致了对一些不必要的词的不准确的关注。在这项工作中，我们建议使用语法来指导文本建模，将显式语法约束纳入注意机制，以获得更好的语言动机词表示。具体地说，对于自关注网络（SAN）赞助的基于转换器的编码器，我们在SAN中引入了感兴趣的语法依赖（SDOI）设计，以形成具有语法引导的自关注的SDOI-SAN。然后，语法引导网络（SG Net）由这个额外的SDOI-SAN和原始Transformer编码器中的SAN组成，通过一个双上下文体系结构实现更好的语言表示。为了验证其有效性，将所提出的SG网络应用于基于变压器编码器的典型预训练语言模型BERT。在包括2.0队和RACE在内的流行基准上进行的大量实验表明，拟议的SG网络设计有助于在强大的基线上实现实质性的性能改进。