受StyleGAN在不同领域生成高度真实图像的能力的启发，最近的许多工作都集中在理解如何使用StyleGAN的潜在空间来操纵生成的和真实的图像。然而，发现语义上有意义的潜在操作通常涉及人类对多个自由度的仔细检查，或者为每个期望的操作创建一个带注释的图像集合。在这项工作中，我们探索如何利用最近引入的对比语言图像预训练（CLIP）模型的力量，为StyleGAN图像处理开发一个基于文本的界面，而不需要手动操作。我们首先介绍一种优化方案，该方案利用基于片段的丢失来修改输入潜在向量，以响应用户提供的文本提示。接下来，我们描述了一个潜在映射器，该映射器为给定的输入图像推断文本引导的潜在操作步骤，从而允许更快、更稳定的基于文本的操作。最后，我们提出了一种将文本提示映射到StyleGAN样式空间中输入不可知方向的方法，从而实现交互式文本驱动的图像操作。大量的结果和比较证明了我们方法的有效性。