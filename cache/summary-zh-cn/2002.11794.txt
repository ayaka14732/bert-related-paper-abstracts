由于硬件资源有限，训练深度学习模型的目标通常是在训练和推理的时间和内存限制下最大限度地提高准确性。我们研究了这种环境下模型大小的影响，重点研究了NLP任务中受计算限制的转换器模型：自监督预训练和高资源机器翻译。我们首先表明，即使较小的Transformer模型在每次迭代中执行得更快，但较宽和较深的模型收敛的步骤要少得多。此外，这种加速收敛的速度通常超过了使用更大模型的额外计算开销。因此，计算效率最高的训练策略是反直觉地训练非常大的模型，但在少量迭代后停止。这导致了大型变压器模型的训练效率和小型变压器模型的推理效率之间的明显权衡。然而，我们表明，与小模型相比，大模型对量化和修剪等压缩技术更具鲁棒性。因此，我们可以两全其美：高度压缩的大型模型比轻度压缩的小型模型精度更高。