大型预先训练的语言模型（LMs）已经证明了与少数shot学习者一样的卓越能力。然而，他们的成功在很大程度上取决于模型参数的缩放程度，这使得训练和服务具有挑战性。在本文中，我们提出了一种新的方法，称为EFL，它可以将小LMs转化为更好的少镜头学习者。这种方法的关键思想是将潜在的NLP任务重新表述为包含任务，然后用8个示例对模型进行微调。我们进一步证明了我们提出的方法可以：（i）自然地与基于无监督对比学习的数据扩充方法相结合；（ii）易于扩展到多语言的少数镜头学习。对18项标准NLP任务的系统评估表明，该方法将现有的各种SOTA少数镜头学习方法提高了12%，并产生了500倍大的模型（如GPT-3）具有竞争力的少数镜头性能。