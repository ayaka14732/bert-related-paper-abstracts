来自Transformers（BERT）的双向编码器表示在各种自然语言处理任务中达到最先进的结果。然而，对其内部功能的了解仍然不够，令人不满意。为了更好地理解BERT和其他基于变压器的模型，我们对BERT的隐藏状态进行了分层分析。与以前的研究不同，我们认为隐藏状态包含同样有价值的信息，而以前的研究主要是通过注意权重来解释变压器模型。具体而言，我们的分析侧重于对问答任务（QA）进行微调的模型，作为复杂下游任务的一个示例。我们检查QA模型如何转换令牌向量以找到正确答案。为此，我们应用了一组通用的和特定于QA的探测任务，这些任务揭示了存储在每个表示层中的信息。我们对隐藏状态可视化的定性分析为伯特的推理过程提供了更多的见解。我们的结果表明，BERT中的转换经历了与传统管道任务相关的阶段。因此，系统可以隐式地将特定于任务的信息合并到其令牌表示中。此外，我们的分析表明，微调对模型的语义能力几乎没有影响，甚至可以在早期层的向量表示中识别预测错误。