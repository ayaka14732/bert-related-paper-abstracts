经过预训练的多语言上下文表示已经显示出巨大的成功，但由于其预训练数据的限制，它们的好处并不平等地适用于所有语言变体。这对这些模型不熟悉的语言变体提出了挑战，这些模型的标记和未标记数据太有限，无法有效地训练单语模型。我们建议使用额外的特定于语言的预训练和词汇扩充，以使多语言模型适应低资源环境。通过对四种不同的低资源语言变体的依赖性分析作为案例研究，我们证明了这些方法显著提高了基线性能，尤其是在资源最低的情况下，并且证明了这些模型的预训练数据与目标语言变体之间关系的重要性。