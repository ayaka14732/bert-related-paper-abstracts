基于转换器的预训练语言模型（T-PTLM）在几乎所有NLP任务中都取得了巨大的成功。这些模型的发展始于GPT和BERT。这些模型建立在变压器、自监督学习和转移学习的基础上。基于转换的PTLM使用自监督学习从大量文本数据中学习通用语言表示，并将此知识转移到下游任务中。这些模型为下游任务提供了良好的背景知识，从而避免了从头开始训练下游模型。在这篇全面的调查报告中，我们首先简要概述了自我监督学习。接下来，我们解释各种核心概念，如预训练、预训练方法、预训练任务、嵌入和下游适应方法。接下来，我们将介绍一种新的T-PTLM分类法，然后简要概述各种基准，包括内在和外在基准。我们总结了使用T-PTLM的各种有用库。最后，我们强调了将进一步改进这些模型的一些未来研究方向。我们坚信，本综合调查报告将作为学习核心概念以及了解T-PTLMs最新情况的良好参考。