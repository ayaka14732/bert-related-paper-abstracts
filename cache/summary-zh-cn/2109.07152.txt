Transformer体系结构在自然语言处理领域已经变得无处不在。为了解释基于变压器的模型，对它们的注意模式进行了广泛的分析。然而，变压器的结构不仅仅是由多个头部组成的；其他部件也有助于变压器的进步性能。在本研究中，我们将变压器的分析范围从单纯的注意模式扩展到整个注意块，即多头注意、剩余连接和层规范化。我们对基于转换器的掩蔽语言模型的分析表明，通过注意执行的令牌到令牌的交互对中间表示的影响比之前假设的要小。这些结果为现有报告提供了新的直观解释；例如，放弃学习到的注意模式往往不会对表现产生负面影响。我们的实验代码是公开的。