抽象摘要的神经模型往往在高度专业化、摘要特定的建模附加组件（如指针生成器、覆盖率建模和推断时间启发式）的存在下实现最佳性能。我们在这里展示了预训练可以补充这些建模的进步，从而使用两个关键概念：全网络初始化和多阶段预训练，在短形式和长形式的抽象摘要中产生改进的结果。我们的方法允许模型从多个预训练任务中获得过渡利益，从通用语言任务到专门的摘要任务，再到更专门的任务，例如基于项目符号的摘要。使用这种方法，我们证明了与随机初始化的基线相比，Gigaword基准上的1.05胭脂-L点和CNN/DailyMail基准上的1.78胭脂-L点的改进。