迁移学习（Transfer learning）是自然语言处理（NLP）中的一种强大技术，它首先对数据丰富的任务进行预训练，然后再对下游任务进行微调。迁移学习的有效性带来了多种方法、方法和实践。在本文中，我们通过引入一个统一的框架，将所有基于文本的语言问题转换为文本到文本的格式，来探索NLP迁移学习技术的前景。我们的系统研究比较了几十项语言理解任务的训练前目标、体系结构、未标记数据集、迁移方法和其他因素。通过结合我们的探索和scale以及我们新的“庞大干净的爬网语料库”，我们在许多基准上取得了最先进的成果，包括摘要、问答、文本分类等。为了促进NLP迁移学习的未来工作，我们发布了我们的数据集、预先训练的模型和代码。