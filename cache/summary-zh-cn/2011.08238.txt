变压器网络和自我监督预培训一直在自然语言处理（NLP）领域取得最新成果；然而，它们在口语理解（SLU）领域的优点仍需进一步研究。在本文中，我们介绍了一种基于模块化端到端（E2E）SLU变压器网络的架构，该架构允许使用自我监督的预训练声学特征、预训练模型初始化和多任务训练。使用ATIS数据集进行了几个SLU实验，用于预测意图和实体标签/值。这些实验研究了预训练模型初始化和多任务训练与传统滤波器组或自监督预训练声学特征的交互作用。结果表明，在几乎所有的实验中，自监督预训练声学特征都优于滤波器组特征，而且当这些特征与多任务训练结合使用时，它们几乎消除了预训练模型初始化的必要性。