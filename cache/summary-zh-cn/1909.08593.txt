奖励学习使强化学习（RL）能够应用于奖励由人类判断定义的任务，通过向人类提问来构建奖励模型。大多数关于奖励学习的工作都使用了模拟环境，但有关价值观的复杂信息通常用自然语言表达，我们认为语言奖励学习是使RL在现实任务中实用和安全的关键。在本文中，我们基于语言模型的生成预训练的进展，将奖励学习应用于四种自然语言任务：用积极情绪或物理描述语言继续文本，以及在TL上执行摘要任务；DR和CNN/Daily Mail数据集。对于风格的延续，我们仅通过5000次人类比较就获得了很好的结果。为了总结，经过60000次比较训练的模型从输入中复制整句，但跳过不相关的前导；根据我们的人类贴标机，这会导致合理的ROUGE分数和非常好的表现，但可能是利用了贴标机依赖简单启发式的事实。