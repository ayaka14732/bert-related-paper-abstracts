最近，预训练的语言模型BERT（及其经过稳健优化的版本RoBERTa）在自然语言理解（NLU）中引起了广泛关注，并在各种自然语言理解任务（如情感分类、自然语言推理、语义-文本相似性和问答）中实现了最先进的准确性。受Elman[8]线性化探索工作的启发，我们通过将语言结构纳入预训练，将BERT扩展到一个新模型StructBERT。具体来说，我们使用两个辅助任务对StructBERT进行预训练，以充分利用单词和句子的顺序，这分别在单词和句子级别利用语言结构。因此，新模型适用于下游任务所需的不同语言理解水平。StructBERT和结构性预训练在各种下游任务上提供了令人惊讶的良好实证结果，包括将胶水基准的最新水平提高到89.0（优于所有已发布的模型），F1队v1.1问题回答的分数达到93.0，SNLI的准确度达到91.7。