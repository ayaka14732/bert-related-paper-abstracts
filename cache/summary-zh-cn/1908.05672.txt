GPT-2和BERT证明了在各种自然语言处理任务中使用预训练语言模型（LMs）的有效性。然而，LM微调在应用于资源丰富的任务时，往往会遭遇灾难性遗忘。在这项工作中，我们引入了协调训练框架（\方法），这是将预先训练的LMS集成到神经机器翻译（NMT）的关键。我们提出的Cnmt包括三种技术：a）渐进蒸馏，以确保NMT模型能够保留先前预先训练的知识；b） 动态切换门，避免预训练知识的灾难性遗忘；c）根据预定策略调整学习节奏的策略。我们在机器翻译方面的实验表明，WMT14英语-德语语言对的BLEU分数高达3，甚至比之前最先进的训练前辅助NMT的BLEU分数高出1.4。对于包含4000万个句子对的大型WMT14英语-法语任务，我们的基本模型仍比最先进的Transformer大模型显著提高了1个BLEU分数以上。