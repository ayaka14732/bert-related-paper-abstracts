上下文单词表示法通常是在非结构化、未标记的文本上训练的，它不包含任何与真实世界实体相关的明确基础，并且通常无法记住关于这些实体的事实。我们提出了一种将多个知识库（KBs）嵌入到大规模模型中的通用方法，从而用结构化的、人工管理的知识增强其表示。对于每个KB，我们首先使用一个集成的实体链接器来检索相关的实体嵌入，然后通过一种词到实体注意的形式更新上下文单词表示。与以前的方法不同，实体链接器和自监督语言建模目标是在多任务设置中端到端联合训练的，该设置将少量实体链接监督与大量原始文本相结合。在将WordNet和Wikipedia的一个子集集成到BERT中后，知识增强型BERT（KnowBert）显示出更好的困惑度、在探测任务中测量的事实回忆能力以及在关系提取、实体类型和词义消歧方面的下游性能。KnowBert的运行时与BERT的运行时相当，并且可以扩展到大KBs。