用更多的数据、计算和参数扩展语言模型，推动了自然语言处理的重大进展。例如，由于规模化，GPT-3能够在情境学习任务中取得良好的效果。然而，训练这些大型密集模型需要大量的计算资源。在本文中，我们提出并开发了一系列名为GLaM（通才语言模型）的语言模型，它使用稀疏激活的混合专家体系结构来扩展模型容量，同时与密集变体相比，所需的培训成本也大大降低。最大的GLaM有1.2万亿个参数，大约是GPT-3的7倍。它只消耗训练GPT-3所用能量的1/3，推理需要一半的计算次数，同时仍然在29个NLP任务中实现更好的整体零触发和一触发性能。