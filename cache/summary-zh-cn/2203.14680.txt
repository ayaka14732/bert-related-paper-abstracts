基于变换器的语言模型（LMs）是现代NLP的核心，但其内部预测构建过程不透明，在很大程度上无法理解。在这项工作中，我们通过对前馈网络（FFN）层（变压器模型的构建块之一）的操作进行反向工程，朝着揭示这一潜在预测过程迈出了实质性的一步。我们将令牌表示视为词汇表上不断变化的分布，并将每个FFN层的输出视为该分布的附加更新。然后，我们分析了词汇空间中的FFN更新，表明每个更新都可以分解为对应于单个FFN参数向量的子更新，每个更新都促进了通常人类可以解释的概念。然后，我们利用这些发现控制LM预测，将GPT2的毒性降低近50%，并通过简单的早期退出规则提高计算效率，平均节省20%的计算量。