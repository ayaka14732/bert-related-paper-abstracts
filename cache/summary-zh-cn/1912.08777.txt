最近的工作预训练变压器与自我监督的目标在大型文本语料库已显示出巨大的成功时，微调下游NLP任务，包括文本摘要。然而，为抽象文本摘要量身定制的训练前目标尚未探索。此外，缺乏跨不同领域的系统评估。在这项工作中，我们提出了一个新的自我监督目标，在海量文本语料库上预训练基于变压器的大型编解码器模型。在PEGASUS中，重要的句子从输入文档中删除/隐藏，并作为一个输出序列从剩余的句子中一起生成，类似于提取摘要。我们在12个下游摘要任务中评估了我们最好的PEGASUS模型，这些任务涵盖新闻、科学、故事、说明、电子邮件、专利和立法法案。实验表明，它在所有12个下游数据集上都达到了最先进的性能，这些数据集是通过ROUGE分数测量的。我们的模型在低资源汇总方面也显示出令人惊讶的性能，超过了之前在6个数据集上的最新结果，只有1000个示例。最后，我们使用人工评估验证了我们的结果，并表明我们的模型总结在多个数据集上实现了人工绩效。