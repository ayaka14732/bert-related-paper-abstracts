近年来，预训练蒙面语言模型（MLM）彻底改变了NLP。然而，以前的工作表明，如果没有对NLI、句子相似性或使用带注释的任务数据对任务进行进一步的特定任务微调，现成的MLM就不能有效地作为通用词汇或句子编码器。在这项工作中，我们证明了即使没有任何额外的数据和监督，MLM也有可能成为有效的通用词汇和句子编码器。我们提出了一种非常简单、快速和有效的对比学习技术，称为镜像BERT，它可以在20-30秒内将MLM（如BERT和RoBERTa）转换为此类编码器，而无需任何额外的外部知识。Mirror BERT依赖于完全相同或稍微修改的字符串对作为正向（即同义）微调示例，并旨在在身份微调期间最大化它们的相似性。我们报告了在词汇级和句子级任务中，在不同领域和不同语言中，使用镜像BERT比现成的MLM获得了巨大的收益。值得注意的是，在标准句子语义相似性（STS）任务中，我们的自监督镜像伯特模型甚至与先前工作中的任务调整句子伯特模型的性能相匹配。最后，我们深入研究了MLM的内部工作机制，并提出了一些证据，说明为什么这种简单的方法可以产生有效的通用词汇和句子编码器。