像BERT这样预先训练好的语言模型，虽然在许多自然语言处理任务中功能强大，但计算和内存都很昂贵。为了缓解这个问题，一种方法是在部署之前针对特定任务压缩它们。然而，最近关于BERT压缩的工作通常将大型BERT模型压缩为固定的较小尺寸。它们不能完全满足不同硬件性能的边缘器件的要求。在本文中，我们提出了一种新的动态BERT模型（简称DynaBERT），它可以通过选择自适应的宽度和深度来灵活地调整大小和延迟。DynaBERT的训练过程包括首先训练一个宽度自适应的BERT，然后通过从全尺寸模型中提取知识到小的子网络，同时允许宽度和深度自适应。网络重新布线也被用来让更多的子网络共享更重要的注意力头和神经元。在各种效率约束下的综合实验表明，我们提出的动态BERT（或RoBERTa）在其最大尺寸下具有与BERT基（或RoBERTa基）相当的性能，而在较小的宽度和深度下，其性能始终优于现有的BERT压缩方法。代码可在https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/DynaBERT.