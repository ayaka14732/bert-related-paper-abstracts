我们提出了一种新的标记句数据增强方法，称为条件伯特上下文增强。数据扩充方法通常用于防止过度拟合和提高深层神经网络模型的泛化能力。最近提出的上下文增广通过随机替换语言模型预测的更多样的替换词来增广标记句。BERT证明了深层双向语言模型比单向语言模型或前向和后向模型的浅层连接更强大。通过引入一个新的条件掩蔽语言模型，我们将BERT改进为条件BERT。脚注{原BERT论文中出现过一次术语“条件掩蔽语言模型”，表明上下文条件相当于术语“掩蔽语言模型”。在我们的论文中，“条件掩蔽语言模型”指示我们对“蒙版语言模型”}任务应用额外的标签条件约束。经过良好训练的条件BERT可以用于增强上下文增强。在六种不同的文本分类任务上的实验表明，我们的方法可以很容易地应用于卷积或递归神经网络分类器，从而获得明显的改进。