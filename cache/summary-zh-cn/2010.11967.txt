本文展示了如何从预先训练好的语言模型（如BERT、GPT-2/3）构建知识图（KGs），而无需人工监督。流行的KG（如Wikidata、NELL）是以监督或半监督的方式构建的，需要人类创造知识。最近的深层语言模型通过预训练自动从大规模语料库中获取知识。存储的知识使语言模型能够改进下游NLP任务，例如回答问题、编写代码和文章。在本文中，我们提出了一种无监督的方法来将语言模型中包含的知识转换为KGs。我们证明了KGs是由预先训练好的语言模型（无需微调）在语料库上向前传递一次而构建的。我们通过与人类创建的两个KG（Wikidata，TAC KBP）进行比较来证明构建KG的质量。我们的KG还提供现有KG中新的开放事实知识。我们的代码和KG将公开提供。