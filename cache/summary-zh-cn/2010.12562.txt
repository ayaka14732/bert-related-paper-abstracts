由于大规模语言模型预训练的成本过高，已经做出了相当大的努力来逐步训练BERT——从一个较低但成本较低的模型开始，逐步扩展模型以增加计算复杂性。我们的目标是促进对变压器增长的理解，并发现指导渐进式培训的原则。首先，我们发现，与网络架构搜索类似，转换器的增长也有利于复合扩展。具体而言，虽然现有方法仅在单个维度上进行网络增长，但我们观察到，使用复合增长算子并平衡多个维度（例如，模型的深度、宽度和输入长度）是有益的。此外，我们还通过受控比较探索了各个维度的替代增长算子，为算子选择提供了实际指导。根据我们的分析，所提出的方法将基本模型和大型模型的BERT预训练速度分别提高了73.6%和82.2%，同时实现了相当的性能