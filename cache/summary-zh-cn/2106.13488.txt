视觉语言预训练（VLP）旨在从图像-文本对中学习多模态表示，并以微调方式为下游视觉语言任务服务。主流VLP模型采用CNN转换器架构，该架构将图像嵌入CNN，然后将图像和文本与转换器对齐。视觉内容之间的视觉关系在图像理解中起着重要作用，是跨模态对齐学习的基础。然而，CNN在视觉关系学习方面存在局限性，因为局部感受野在建模长期依赖性方面的弱点。因此，学习视觉关系和跨模态对齐这两个目标被封装在同一个变压器网络中。这样的设计可能会忽略每个目标的特殊特性，从而限制变压器中的跨模态对准学习。为了解决这个问题，我们提出了一种用于VLP的完全变换视觉嵌入，以更好地学习视觉关系，并进一步促进跨模态对齐。具体来说，我们提出了一个称为跨模态流（IMF）的指标来衡量视觉和语言模态（即跨模态）之间的相互作用。我们还在Transformer中设计了一种新的掩蔽优化机制，名为掩蔽特征回归（MFR），以进一步促进模态间学习。据我们所知，这是第一项探索变形金刚对VLP视觉特征学习的益处的研究。我们在广泛的视觉语言任务中验证了我们的方法，包括图像文本检索、视觉问答（VQA）、视觉蕴涵和视觉推理。我们的方法不仅优于最先进的VLP性能，而且在IMF指标上也显示出优势。