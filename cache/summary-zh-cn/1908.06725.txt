最先进的预先训练的语言表示模型，例如变压器的双向编码器表示（BERT），很少明确地包含常识知识或其他知识。我们提出了一种将常识知识融入语言表示模型的预训练方法。我们构建了一个与常识相关的多选问答数据集，用于预训练神经语言表示模型。数据集是由我们提出的“对齐、屏蔽和选择”（AMS）方法自动创建的。我们还调查了不同的培训前任务。实验结果表明，在两个与常识相关的基准测试（包括commonsense QA和Winograd Schema Challenge）上，使用所提出的方法并进行微调的预训练模型比以前的最新模型取得了显著的改进。我们还观察到，与原始的BERT模型相比，在提出的预训练方法之后的微调模型在其他自然语言处理任务（如句子分类和自然语言推理任务）上保持了相当的性能。这些结果验证了所提出的方法在显著改善与常识相关的NLP任务的同时，不会降低通用语言表示能力。