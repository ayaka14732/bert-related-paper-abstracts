经过预训练的语言模型的激增已经开启了自然语言处理（NLP）领域的一个新时代，它允许我们构建强大的语言模型。在这些模型中，基于变压器的模型（如BERT）由于其最先进的性能而越来越受欢迎。然而，这些模型通常集中在英语上，将其他语言留给资源有限的多语言模型。本文提出了一种用于波斯语的单语BERT（ParsBERT），与其他体系结构和多语言模型相比，它显示了其最先进的性能。此外，由于波斯语中NLP任务可用的数据量非常有限，因此为不同的NLP任务以及模型的预训练组成了大量数据集。ParsBERT在所有数据集（包括现有数据集和合成数据集）中获得更高的分数，并通过在情感分析、文本分类和命名实体识别任务方面优于多语言BERT和其他先前的工作，提高了最先进的性能。