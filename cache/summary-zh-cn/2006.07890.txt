大型预训练蒙面语言模型已经成为许多NLP问题的最新解决方案。不过，这项研究主要集中在英语方面。虽然存在大量多语言模型，但研究表明，单语模型产生的结果要好得多。我们训练两个三语类伯特模型，一个是芬兰语、爱沙尼亚语和英语，另一个是克罗地亚语、斯洛文尼亚语和英语。我们使用多语言BERT和XLM-R作为基线，评估它们在几个下游任务（NER、词性标记和依赖项解析）上的性能。新创建的BERT和CROSLONGUAL BERT在大多数单语和跨语言情况下改善了所有任务的结果