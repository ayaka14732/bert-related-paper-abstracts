从文本分类到文本生成的自然语言处理（NLP）任务已经被预先训练好的语言模型（如BERT）彻底改变。这使得公司能够通过封装用于下游任务的经过微调的BERT模型，轻松构建强大的API。然而，当一个经过微调的BERT模型作为一个服务部署时，它可能会遭受恶意用户发起的不同攻击。在这项工作中，我们首先介绍对手如何在有限的先验知识和查询的情况下，在多个基准数据集上窃取基于BERT的API服务（受害者/目标模型）。我们进一步表明，提取的模型可以导致针对受害者模型的高度可转移的对抗性攻击。我们的研究表明，基于BERT的API服务的潜在漏洞仍然存在，即使受害者模型和攻击模型之间存在架构不匹配。最后，我们研究了两种保护受害者模型的防御策略，发现除非牺牲受害者模型的性能，否则模型的扩展性和对抗性转移性都可以有效地损害目标模型