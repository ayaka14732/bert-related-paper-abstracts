我们采用多语言BERT为109种语言生成语言不可知的句子嵌入许多单语和多语NLP任务的最新技术是掩蔽语言模型（MLM）预训练，然后是任务特定的微调。虽然英语句子嵌入是通过微调一个预训练的BERT模型得到的，但这种模型还没有应用于多语言句子嵌入。我们的模型将蒙面语言模型（MLM）和翻译语言模型（TLM）预训练与使用双向双编码器的翻译排序任务相结合。由此产生的多语言句子嵌入将超过112种语言的平均双文本检索准确率提高到83.7%，远远高于先前最先进的Tatoeba上的65.5%。我们的句子嵌入还建立了关于BUCC和UN bi文本检索的最新成果。