预先训练的语境化语言模型（如BERT）在广泛的下游自然语言处理（NLP）任务中表现出了极大的有效性。然而，模型提供的有效表示针对序列中的每个标记，而不是每个序列，微调步骤涉及同时输入两个序列，导致具有不同粒度的各种序列的不满意表示。特别是，在这些模型中，句子级表征作为完整的训练上下文，在较低级别的语言单位（短语和单词）上表现较差。在这项工作中，我们提出了BURT（受伯特启发的孪生结构通用表示法），它能够使用大量自然语言推理和复述数据，以多个训练目标，为任何粒度的输入序列（即单词、短语和句子）生成通用的、固定大小的表示法。我们提出的BURT采用暹罗网络，分别从自然语言推理数据集中学习句子级表示，从释义数据集中学习单词/短语级表示。我们评估了不同粒度的文本相似性任务，包括STS任务、SemEval2013任务5（a）和一些常用的单词相似性任务，其中BURT在句子级数据集上显著优于其他表示模型，并在单词/短语级表示方面取得了显著改进。