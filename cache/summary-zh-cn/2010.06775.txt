人类通过听、说、写、读以及与多模态现实世界的互动来学习语言。现有的语言预训练框架显示了纯文本自我监督的有效性，同时本文探讨了视觉监督语言模型的概念。我们发现阻碍这一探索的主要原因是视觉基础语言数据集和纯语言语料库在数量和分布上的巨大差异。因此，我们开发了一种名为“vokenization”的技术，通过上下文将语言标记映射到它们的相关图像（我们称之为“vokens”），将多模态对齐外推到纯语言数据。“vokenizer”在相对较小的图像字幕数据集上进行训练，然后我们将其应用于为大型语言语料库生成vokens。通过使用这些上下文生成的Voken进行训练，我们的视觉监督语言模型在多个纯语言任务（如GLUE、SQuAD和SWAG）上显示出与自我监督备选方案相比的一致改进。代码和预先培训的模型可在https://github.com/airsplay/vokenization