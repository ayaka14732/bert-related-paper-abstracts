知识提取被认为是一种训练和压缩策略，其中教师和学生两个神经网络在训练过程中耦合在一起。教师网络被认为是一个值得信赖的预测者，学生试图模仿其预测。通常，我们会选择一个结构更轻的学生，这样我们就可以实现压缩，同时提供高质量的结果。在这种情况下，蒸馏只发生在最终预测中，而学生也可以从老师对内部组件的监督中获益。基于此，我们研究了中间层的精馏问题。由于学生层和教师层之间可能没有一对一的对齐，现有技术跳过了一些教师层，只从其中的一个子集提取。这一缺点直接影响质量，因此我们提出了一种依赖注意的组合技术。我们的模型融合了教师方面的信息，并考虑了每一层的重要性，然后在组合的教师层和学生层之间进行提炼。使用我们的技术，我们将一个12层的BERT（Devlin等人，2019年）蒸馏成6层、4层和2层的对应物，并在粘合任务中对其进行评估（Wang等人，2018年）。实验结果表明，我们的组合方法能够优于其他现有技术。