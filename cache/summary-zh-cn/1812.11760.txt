我们表明，选区分析得益于跨多种语言和一系列预训练条件的无监督预训练。我们首先比较了无预培训、快速文本、ELMo和BERT对英语的好处，发现BERT优于ELMo，这在很大程度上是由于模型容量的增加，而ELMo反过来又优于非上下文快速文本嵌入。我们还发现，在所有11种测试语言中，预培训都是有益的；然而，大型模型（超过1亿个参数）使得为每种语言训练单独的模型在计算上非常昂贵。为了解决这个缺点，我们展示了联合多语言预训练和微调允许在最终模型中的十种语言之间共享除少量参数以外的所有参数。与微调每种语言的一个模型相比，模型大小减少了10倍，只会导致总体相对误差增加3.2%。我们进一步探讨了联合微调的思想，并表明它为低资源语言提供了一种从其他语言的较大数据集中获益的方法。最后，我们展示了11种语言的最新结果，包括英语（95.8 F1）和汉语（91.8 F1）。