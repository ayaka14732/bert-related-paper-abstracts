大规模预训练模型（PTM）如BERT和GPT最近取得了巨大成功，成为人工智能（AI）领域的一个里程碑。由于复杂的预训练目标和巨大的模型参数，大规模PTM可以有效地从大量标记和未标记的数据中获取知识。通过将知识存储到大参数中并对特定任务进行微调，隐含在大参数中的丰富知识可以使各种下游任务受益，这已通过实验验证和实证分析得到广泛证明。现在AI社区的共识是采用PTM作为下游任务的主干，而不是从头开始学习模型。在本文中，我们深入研究了预训练的历史，特别是它与迁移学习和自我监督学习的特殊关系，以揭示PTM在AI发展谱中的关键地位。此外，我们全面回顾了PTMs的最新突破。这些突破是由计算能力的激增和数据可用性的增加推动的，朝着四个重要方向发展：设计有效的体系结构、利用丰富的上下文、提高计算效率以及进行解释和理论分析。最后，我们讨论了PTMs存在的一系列问题和研究方向，希望我们的观点能够对PTMs的未来研究起到启发和推动作用。