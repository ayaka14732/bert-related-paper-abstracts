随着将事实知识融入到诸如伯特的预训练语言模型中的新兴分支，大多数现有模型考虑浅、静态和单独预训练的实体嵌入，这限制了这些模型的性能增益。很少有作品在注入知识时探索深层情境化知识表示的潜力。在本文中，我们提出了语境化语言和知识嵌入（CoLAKE），它与扩展的MLM目标共同学习语言和知识的语境化表示。CoLAKE不是只注入实体嵌入，而是从大规模知识库中提取实体的知识上下文。为了处理知识上下文和语言上下文的异构性，我们将它们集成到一个统一的数据结构中，即单词知识图（WK-graph）。CoLAKE使用改进的Transformer编码器对大规模WK图进行预训练。我们在知识驱动任务、知识探索任务和语言理解任务上进行实验。实验结果表明，CoLAKE在大多数任务上都优于以前的同类。此外，CoLAKE在我们称为单词知识图完成的合成任务中取得了令人惊讶的高性能，这显示了同时上下文化语言和知识表示的优越性。