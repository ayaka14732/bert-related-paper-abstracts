预训练语言模型（PTLM）在一系列自然语言理解（NLU）和生成（NLG）任务中取得了令人印象深刻的成果。然而，当前的预培训目标，如蒙面代币预测（对于伯特式PTLM）和蒙面跨度填充（对于T5式PTLM），并未明确建模关于日常概念的关系常识知识，这对于许多需要常识来理解或生成的下游任务至关重要。为了用以概念为中心的常识知识增强PTLM，在本文中，我们提出了从文本中学习常识的生成性和对比性目标，并将其用作增量预训练PTLM的中间自我监督学习任务（在对下游数据集进行特定任务微调之前）。此外，我们还开发了一个联合培训前框架，以统一生成目标和对比目标，从而使它们能够相互加强。大量的实验结果表明，我们的方法，概念感知语言模型（CALM），可以在不依赖外部知识图的情况下，将更多的常识知识封装到预先训练好的文本到文本转换器的参数中，从而在NLU和NLG任务中获得更好的性能。我们表明，尽管仅在相对较小的语料库上进行了几个步骤的增量预训练，但CALM的表现优于基线方法，具有一致的优势，甚至与一些较大的PTLM相当，这表明CALM可以作为一种通用的即插即用方法，用于提高PTLM的常识推理能力。