最近，预先训练的语言模型推动了自然语言处理（NLP）领域的发展。变压器双向编码器（BERT）及其优化版RoBERTa的引入产生了重大影响，并增加了预训练模型的相关性。首先，该领域的研究主要是从英语数据开始的，其次是用多语种文本语料库训练的模型。然而，目前的研究表明，多语言模型不如单语模型。目前，还没有出版德语单语言RoBERTa模型，这是我们在这项工作中介绍的（GottBERT）。奥斯卡数据集中的德语部分被用作文本语料库。在评估中，我们将其在两个命名实体识别（NER）任务Conll 2003和GermEval 2014以及文本分类任务GermEval 2018（精细和粗糙）和GNAD上的性能与现有的德语单语言BERT模型和两个多语言BERT模型进行了比较。使用fairseq对GottBERT进行了与原始RoBERTa模型相关的预培训。所有下游任务都使用德国BERT基准中的超参数预设进行训练。实验是利用农场进行的。绩效以$F{1}$分数衡量。GottBERT使用RoBERTa基地架构成功地在256核TPU吊舱上进行了预训练。即使没有广泛的超参数优化，在所有的NER和一个文本分类任务中，GottBERT已经超过了所有其他经过测试的德语和多语言模型。为了支持德国NLP字段，我们在AGPLv3许可下发布了GottBERT。