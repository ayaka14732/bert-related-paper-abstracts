最近，BERT在自然语言理解（NLU）领域引起了广泛关注，并在各种NLU任务中取得了最新成果。然而，它的成功需要大量的深层神经网络和大量的数据，这导致训练时间长，阻碍了开发进程。利用随机梯度方法进行小批量训练是减少训练时间的有效手段。在这一研究路线上，LAMB是一个突出的例子，它将伯特在TPUv3吊舱上的训练时间从3天减少到76分钟。在本文中，我们提出了一种称为LAN的加速梯度方法，以提高使用大批量小批量进行训练的效率。由于理论上学习率的上界是函数的Lipschitz常数的倒数，因此不能总是通过选择更大的学习率来减少优化迭代次数。为了在不损失精度的情况下使用更大的小批量，我们开发了一种新的学习率调度器，克服了使用大学习率的困难。使用所提出的局域网方法和学习速率方案，我们在BERT预训练的第一阶段和第二阶段分别将小批量规模扩大到96K和33K。在192个AWS EC2 P3dn.24xlarge实例上需要54分钟，才能在1.1版班次上达到90.5或更高的F1目标分数，实现云中最快的BERT训练时间。