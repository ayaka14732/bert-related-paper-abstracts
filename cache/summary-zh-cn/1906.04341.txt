大型预训练神经网络（如BERT）最近在NLP方面取得了巨大成功，促使越来越多的研究机构调查他们能够从未标记数据中学习语言的哪些方面。最近的分析主要集中在模型输出（例如，语言模型超越）或内部向量表示（例如，探测分类器）。作为对这些工作的补充，我们提出了分析预训练模型注意机制的方法，并将其应用于BERT。伯特的注意头表现出诸如注意定界符标记、特定位置偏移或广泛注意整个句子等模式，同一层的注意头通常表现出类似的行为。我们进一步表明，某些注意头与句法和共指的语言学概念很好地对应。例如，我们发现大脑能够非常准确地处理动词的直接宾语、名词的限定词、介词的宾语以及相关的提及。最后，我们提出了一个基于注意的探测分类器，并用它进一步证明了大量的句法信息被捕获在伯特的注意中。