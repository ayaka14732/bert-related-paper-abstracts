像BERT这样的预训练语言模型（plm）在自然语言处理方面取得了巨大的进步。新闻文章通常包含丰富的文本信息，PLMs有潜力为各种智能新闻应用（如新闻推荐和检索）增强新闻文本建模。然而，大多数现有的PLM都是具有数亿个参数的大型PLM。许多在线新闻应用程序需要以低延迟容忍度为数百万用户提供服务，这给在这些场景中整合PLM带来了巨大挑战。知识提取技术可以将一个较大的PLM压缩为一个较小的PLM，同时保持良好的性能。然而，现有的语言模型是在像维基百科这样的通用语料库上预先训练和提炼的，维基百科与新闻领域有一些差距，对于新闻智能来说可能是次优的。在本文中，我们提出了NewsBERT，它可以提取PLM以实现高效的新闻智能。在我们的方法中，我们设计了一个师生联合学习和提炼框架，以协作学习教师和学生模型，其中学生模型可以从教师模型的学习经验中学习。此外，我们还提出了一种动量蒸馏方法，将教师模型的梯度加入到学生模型的更新中，以更好地传递教师模型所学的有用知识。在两个具有三个任务的真实数据集上进行的大量实验表明，NewsBERT可以在各种智能新闻应用程序中有效地提高模型性能，而模型要小得多。