使用一个语言模型（LM）预训练两种语言与大单语数据，以初始化一个无监督的神经机器翻译（UNMT）系统产生最先进的结果。然而，当一种语言的可用数据有限时，这种方法会导致较差的翻译。我们提出了一种有效的方法，重用仅在高资源语言上预训练的LM。单语LM在两种语言上都进行了微调，然后用于初始化UNMT模型。为了重用经过预训练的LM，我们必须修改其预定义的词汇表，以适应新的语言。因此，我们提出了一种新的词汇扩展方法。我们的方法RE-LM在英语马其顿语（En-Mk）和英语阿尔巴尼亚语（En-Sq）方面优于竞争性跨语言预训练模型（XLM），在所有四个翻译方向上都获得了超过+8.3个BLEU分数。