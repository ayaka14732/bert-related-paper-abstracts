无监督的跨语言预训练在神经机器翻译（NMT）中取得了很好的结果，极大地减少了对大型并行数据的需求。大多数方法通过屏蔽部分输入并在解码器中重建它们，使屏蔽语言建模（MLM）适应于序列到序列的体系结构。在这项工作中，我们系统地比较了掩蔽和替代目标，这些目标通过根据上下文重新排序和替换单词，产生类似真实（完整）句子的输入。我们在英语$\leftrightarrow$德语、英语$\leftrightarrow$尼泊尔语和英语$\leftrightarrow$僧伽罗语单语数据上用不同的方法预训练模型，并在NMT上进行评估。在（半）监督NMT中，改变预训练目标会导致微调性能的微小差异，而非监督NMT对其更为敏感。为了理解这些结果，我们使用一系列探针彻底研究了预训练模型，并验证它们以不同的方式编码和使用信息。我们得出结论，对并行数据的微调对大多数模型共享的少数属性（如强解码器）最为敏感，而无监督NMT也需要具有强跨语言能力的模型。