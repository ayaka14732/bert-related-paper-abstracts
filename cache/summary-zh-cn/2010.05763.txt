虽然BERT被NLP社区广泛使用，但对其内部工作原理知之甚少。人们曾多次试图阐明伯特的某些方面，但往往得出相反的结论。一个备受关注的问题集中在BERT的过度参数化和利用不足问题上。为此，我们提出了一种新的方法来以结构化的方式对BERT进行微调。具体来说，我们关注大规模多标签文本分类（LMTC），其中文档被分配一个或多个来自一组预定义的分层组织标签的标签。我们的方法指导特定的BERT层从特定的层次结构级别预测标签。通过对两个LMTC数据集的实验，我们发现这种结构化的微调方法不仅可以产生更好的分类结果，而且可以提高参数利用率。