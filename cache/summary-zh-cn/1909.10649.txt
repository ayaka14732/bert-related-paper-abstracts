使用神经网络进行语言表示的最新进展使得将训练模型的学习内部状态转移到下游自然语言处理任务（如命名实体识别（NER）和问答）成为可能。研究表明，利用预先训练好的语言模型可以提高许多任务的总体性能，并且在标记数据稀少时非常有益。在这项工作中，我们训练葡萄牙语的BERT模型，并将BERT-CRF体系结构与葡萄牙语的NER任务相结合，将BERT的传输能力与CRF的结构化预测相结合。我们探索基于特征和微调训练策略的伯特模型。我们的微调方法在HAREM I数据集上获得了新的最先进的结果，在选择性场景（5个NE类）上提高了F1分数1分，在总场景（10个NE类）上提高了4分。