知识提取（KD）是一种将知识从大的教师模型转移到小的学生模型的方法，近年来被广泛应用于BERT模型的压缩。除了原始KD中输出的监督外，最近的工作表明，层级监督对student-BERT模型的性能至关重要。然而，以前的工作是启发式地设计层映射策略（例如，统一层或最后一层），这可能导致性能低下。在本文中，我们建议使用遗传算法（GA）自动搜索最佳图层映射。为了加快搜索过程，我们进一步提出了一种代理设置，其中一小部分训练语料被采样进行蒸馏，并选择三个代表性任务进行评估。在获得最佳层映射后，我们对整个语料库执行任务不可知的BERT蒸馏，以构建一个紧凑的学生模型，该模型可以直接对下游任务进行微调。对评估基准的综合实验表明：1）层映射策略对任务不可知性有显著影响，不同的层映射会导致不同的性能；2） 从提出的搜索过程中得到的最优层映射策略始终优于其他启发式策略；3） 通过优化层映射，我们的学生模型在粘合任务上实现了最先进的性能。