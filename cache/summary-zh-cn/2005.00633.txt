具有语言建模目标（如mBERT、XLM-R）的大规模多语言转换器已经成为NLP中零触发跨语言传输的事实默认传输范式，提供了无与伦比的传输性能。然而，当前的下游评估主要在迁移环境中验证其有效性，这些迁移环境涉及到具有足够数量的训练前数据的语言，以及词汇和类型相近的语言。在这项工作中，我们分析了它们的局限性，并表明通过大规模多语言转换进行的跨语言转换，就像通过跨语言单词嵌入进行的转换一样，在资源贫乏的情况下和对于遥远的语言来说，其效率要低得多。我们的实验包括三个较低级别的任务（词性标注、依赖性分析、NER）和两个高级语义任务（NLI、QA），实验表明，迁移性能与源语言和目标语言之间的语言相似性相关，但也与目标语言预训练语料库的大小相关。我们还展示了廉价的少镜头传输（即，在源代码中进行微调后对一些目标语言实例进行微调）的惊人效果。这表明，应投入更多的研究工作，以达到超出极限零射击条件的目的。