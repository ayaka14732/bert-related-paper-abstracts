注意层广泛应用于自然语言处理（NLP），并开始影响计算机视觉体系结构。训练非常大的变压器模型可以在这两个领域取得显著的改进，但一旦训练，这些网络就会表现出过度参数化的症状。例如，众所周知，许多注意力头可以在不影响准确性的情况下被剪掉。这项工作的目的是加强目前对多个头部如何相互作用的理解。基于注意头学习冗余关键/查询投射的观察结果，我们提出了一个协作的多头注意层，使注意头能够学习共享投射。我们的方案减少了注意层中的参数数量，并且可以作为任何变压器架构中的替代品。我们的实验证实，共享键/查询维度可用于语言理解、机器翻译和视觉。我们还表明，可以将预先训练的多头注意层重新参数化为我们的协作注意层。协作式多头注意将密钥和查询投影的大小减少了4，以获得相同的准确性和速度。我们的代码是公开的。