我们提出了一种自动生成给定文本的域和任务自适应掩码的方法，用于自我监督的预训练，这样我们可以有效地使语言模型适应特定的目标任务（例如问答）。具体地说，我们提出了一种新的基于强化学习的框架，该框架学习掩蔽策略，从而使用生成的掩蔽对目标语言模型进行进一步的预训练，有助于提高在看不见文本上的任务性能。我们使用Office策略演员评论家的熵正则化和经验重演的强化学习，并提出了一个基于变压器的政策网络，可以考虑在给定文本中的单词的相对重要性。我们使用BERT和DistilBERT作为语言模型，在几个问答和文本分类数据集上验证了我们的神经掩码生成器（NMG），通过自动学习最佳自适应掩码，它的性能优于基于规则的掩码策略。