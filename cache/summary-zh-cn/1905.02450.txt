预训练和微调，例如BERT，通过将知识从丰富资源的预训练任务转移到低/零资源的下游任务，在语言理解方面取得了巨大成功。受BERT成功的启发，我们提出了基于编解码器的语言生成任务的掩蔽序列到序列预训练（MASS）。MASS采用编码器-解码器框架来重构给定句子剩余部分的句子片段：其编码器以随机屏蔽片段（几个连续标记）作为输入的句子，其解码器尝试预测该屏蔽片段。通过这种方式，MASS可以联合训练编码器和解码器，以发展表示提取和语言建模的能力。通过进一步细化各种零/低资源语言生成任务，包括神经机器翻译、文本摘要和会话响应生成（3个任务和总共8个数据集），在没有预先训练或其他预训练方法的情况下，质量实现了基线的显著改进。特别是，我们在无监督的英法翻译中达到了最先进的准确性（BLEU分数为37.5），甚至超过了早期基于注意的监督模型。