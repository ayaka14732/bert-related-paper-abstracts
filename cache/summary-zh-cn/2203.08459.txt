像BERT这样的预训练语言模型已经成功地处理了许多自然语言处理任务。然而，这些模型中常用的无监督子词标记化方法（例如，字节对编码-BPE）在处理形态丰富的语言时是次优的。即使有一个形态分析器，将语素简单地排序到标准的BERT架构中，在捕获形态组成性和表达单词相对句法规则方面也是效率低下的。我们通过提出一个简单而有效的两层BERT体系结构来解决这些挑战，该体系结构利用形态分析器并明确表示形态组合性。尽管BERT取得了成功，但它的大多数评估都是在高资源语言上进行的，模糊了它在低资源语言上的适用性。我们在低资源形态丰富的基尼亚卢旺达语上评估了我们提出的方法，并将所提出的模型架构命名为KinyaBERT。一组稳健的实验结果表明，KinyaBERT在命名实体识别任务中的F1成绩优于坚实基线2%，在机器翻译的GLUE基准测试中的平均成绩优于坚实基线4.3%。KinyaBERT微调具有更好的收敛性，即使在存在平移噪声的情况下，也能在多个任务上获得更稳健的结果。