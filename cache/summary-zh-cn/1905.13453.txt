最近已经创建了大量的阅读理解（RC）数据集，但很少有人分析它们是否可以相互推广，以及现有数据集在多大程度上可以用于改进新数据集的性能。在本文中，我们对十个RC数据集进行了这样的调查，对一个或多个源RC数据集进行了培训，评估了泛化，并将其转移到目标RC数据集。我们分析了有助于泛化的因素，并表明在源RC数据集上进行训练并转移到目标数据集可以显著提高性能，即使存在来自BERT的强大上下文表示（Devlin et al.，2019）。我们还发现，在多源RC数据集上进行训练可以实现健壮的泛化和传输，并且可以降低新RC数据集的示例收集成本。根据我们的分析，我们提出了MultiQA，一种基于BERT的模型，在多个RC数据集上进行训练，从而在五个RC数据集上获得最先进的性能。为了研究界的利益，我们共享我们的基础设施。