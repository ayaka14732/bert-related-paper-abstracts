像BERT这样的预训练语言模型在NLP任务上取得了很好的效果，但由于内存占用，在资源有限的设备上是不切实际的。这一足迹的很大一部分来自具有大量输入词汇表和嵌入维度的输入嵌入。现有的用于模型压缩的知识提取方法不能直接用于训练词汇量较小的学生模型。为此，我们提出了一种蒸馏方法，通过混合词汇训练来调整教师和学生的嵌入。我们的方法将BERT-LARGE压缩为具有较小词汇和隐藏维度的任务不可知模型，该模型比其他蒸馏的BERT模型小一个数量级，并且在语言理解基准以及实际对话任务上提供了更好的尺寸精度权衡。