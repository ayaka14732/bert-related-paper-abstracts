蒙面语言建模（MLM）是一种自监督的预训练目标，广泛应用于自然语言处理中，用于学习文本表示。MLM训练一个模型来预测在整个词汇表的多类设置中被[MASK]占位符替换的输入标记的随机样本。在预训练时，通常与传销一起使用令牌或序列级别的其他辅助目标，以提高下游性能（例如，下一句预测）。然而，到目前为止，还没有任何工作试图检验其他更简单的语言直观目标是否可以单独用作主要的训练前目标。在本文中，我们探索了五个简单的基于令牌级别分类任务的预训练目标，作为传销的替代。在GLUE和SQuAD上的实验结果表明，我们提出的方法与使用BERT-BASE架构的MLM具有相当或更好的性能。我们使用更小的模型进一步验证了我们的方法，结果表明，对具有41%的BERT-BASE参数的模型进行预训练，BERT-MEDIUM只会使我们的最佳目标的GLUE分数下降1%。