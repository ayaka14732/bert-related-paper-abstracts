大型预先训练的语言模型，如BERT，已经在各种自然语言处理任务中显示了它们的有效性。然而，巨大的参数大小使得它们很难部署在需要使用有限资源进行快速推理的实时应用程序中。现有方法将BERT压缩到小模型中，而这种压缩是任务独立的，即，对于所有不同的下游任务，相同的压缩BERT。基于面向任务的BERT压缩的必要性和优点，我们提出了一种新的压缩方法AdaBERT，该方法利用可微神经结构搜索将BERT自动压缩为特定任务的任务自适应小模型。我们结合了一个面向任务的知识提取损失来提供搜索提示和一个效率感知损失作为搜索约束，这使得任务自适应BERT压缩能够在效率和有效性之间进行很好的权衡。我们在几个NLP任务上评估了AdaBERT，结果表明，这些任务自适应压缩模型在推理时间上比BERT快12.7倍到29.3倍，在参数大小上比BERT小11.5倍到17.0倍，同时保持了可比性能。