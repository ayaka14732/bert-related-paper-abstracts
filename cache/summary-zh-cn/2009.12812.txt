基于转换器的预训练模型（如BERT）在许多自然语言处理任务中取得了显著的性能。然而，这些模型的计算和内存都很昂贵，阻碍了它们部署到资源受限的设备上。在这项工作中，我们提出了三元BERT，它在一个微调的BERT模型中对权重进行三元化。具体而言，我们使用基于近似和损失感知的三元化方法，并实证研究了BERT不同部分的三元化粒度。此外，为了减少低比特容量导致的精度下降，我们在训练过程中利用了知识提取技术。在GLUE基准和SQuAD上的实验表明，我们提出的TernaryBERT量化方法优于其他BERT量化方法，甚至达到了与全精度模型相当的性能，同时比全精度模型小14.9倍。