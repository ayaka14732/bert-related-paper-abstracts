基于BERT的体系结构目前在许多NLP任务上提供了最先进的性能，但对其成功的确切机制知之甚少。在目前的工作中，我们着重于对自我注意的解释，这是BERT的基本组成部分之一。使用胶水任务的子集和一组手工制作的感兴趣的特征，我们提出了该方法，并对单个BERT头部编码的信息进行了定性和定量分析。我们的研究结果表明，有一组有限的注意力模式在不同的大脑中重复，这表明整体模型过度参数化。虽然不同的大脑始终使用相同的注意力模式，但它们对不同任务的表现有不同的影响。我们表明，在某些大脑中手动禁用注意比常规微调的BERT模型有更好的性能。