在自然语言处理（NLP）任务中，推理速度慢和GPU占用空间大仍然是在生产中应用预先训练好的深度模型的瓶颈。作为一种流行的模型压缩方法，知识提取将知识从一个或多个大型（教师）模型转移到一个小型（学生）模型。当蒸馏中有多个教师模型可用时，最先进的方法会在整个蒸馏过程中为教师模型指定一个固定的权重。此外，现有的大多数方法对每一个教师模型都给予同等的权重。在本文中，我们观察到，由于训练示例的复杂性和学生模型能力的差异，从教师模型中进行差异学习可以提高学生模型的性能。我们系统地开发了一种增强方法，针对不同的训练实例动态分配教师模型的权重，并优化学生模型的性能。我们在几个NLP任务上的大量实验结果清楚地验证了我们方法的可行性和有效性。