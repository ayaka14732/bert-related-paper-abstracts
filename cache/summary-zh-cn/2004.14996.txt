转换器对于序列建模非常强大。几乎所有最先进的语言模型和预先训练的语言模型都基于Transformer体系结构。但是，它仅使用令牌位置索引来区分顺序令牌。我们假设，更丰富的位置信息可以从转换器生成更好的上下文表示。为了验证这一点，我们提出了一种段感知转换器（Segatron），将原始标记位置编码替换为段落、句子和标记的组合位置编码。我们首先将段感知机制引入Transformer XL，它是一种流行的基于Transformer的语言模型，具有内存扩展和相对位置编码。我们发现，我们的方法可以进一步改进Transformer XL基本模型和大型模型，在WikiText-103数据集上实现17.1困惑。我们进一步研究了Segatron的训练前掩蔽语言建模任务。实验结果表明，使用Segatron（SegaBERT）预训练的BERT在各种自然语言处理任务上都优于使用vanilla Transformer的BERT，在零炮句子表征学习上也优于RoBERTa。