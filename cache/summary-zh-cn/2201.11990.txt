经过预训练的通用语言模型可以通过零触发、少触发和微调技术来适应下游任务，从而在各种自然语言处理领域实现最先进的精度。由于它们的成功，这些模型的规模迅速增加，需要高性能的硬件、软件和算法技术来训练如此大的模型。作为微软和NVIDIA共同努力的结果，我们介绍了最大的基于单片变压器的语言模型威震天图灵NLG 530B（MT-NLG）的培训细节，该模型具有5300亿个参数。在本文中，我们首先关注使用DeepSpeed和威震天来训练该模型的基础设施以及3D并行方法。接下来，我们详细介绍了训练过程、训练语料库的设计和数据整理技术，我们认为这是该模型成功的关键因素。最后，我们讨论了各种评估结果，以及MT-NLG显示的其他有趣的观察结果和新特性。我们证明了MT-NLG在几个NLP基准上实现了优异的零、一和少镜头学习精度，并建立了新的最先进的结果。我们相信，我们的贡献将有助于进一步发展大规模培训基础设施、大规模语言模型和自然语言世代。