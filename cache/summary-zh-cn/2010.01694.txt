伯特通过两项任务的预训练，在不同的NLU基准上取得了许多最新成果：蒙面语言建模（MLM）和下一句预测（NSP），后者受到了高度批评。在本文中，我们1）阐明NSP对BERT预训练的影响，2）探索14项可能的辅助预训练任务，其中7项是现代语言模型中的新任务，3）研究将多个任务纳入预训练的不同方式。我们发现，NSP由于其上下文分裂和浅层语义信号，不利于训练。我们还确定了六项辅助训练前任务——句子排序、相邻句子预测、TF预测、TF-IDF预测、FastSent变体和Quick Thinks变体——它们的表现优于纯粹的传销基线。最后，我们证明了在多任务预训练框架中使用多个任务比使用任何单个辅助任务提供更好的结果。使用这些方法，我们在使用不到四分之一的训练标记的GLUE基准测试上优于BERT Base。