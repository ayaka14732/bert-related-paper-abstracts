NLP目前主要由诸如RoBERTa这样的通用预训练语言模型控制，这些模型通过对数十亿个单词的预训练，在NLU任务中取得了优异的成绩。但Transformer LMs从大规模预培训中学到了哪些他们无法从较少数据中学到的确切知识或技能？我们采用四种探测方法——分类器探测、信息论探测、无监督的相对可接受性判断和NLU任务微调——并使用MiniBERTas绘制学习曲线，跟踪这些不同语言能力指标相对于预训练数据量的增长，一组罗伯塔模特预先训练了1M、10M、100M和1B单词。我们发现LMs只需要大约10万或100万个单词就可以学习能够可靠编码我们测试的大多数语法和语义特征的表示。为了获得足够的常识知识和掌握典型下游NLU任务所需的其他技能，需要大量的数据。结果表明，虽然编码语言特征的能力几乎肯定是语言理解所必需的，但在大型预训练模型中，其他形式的知识可能是最近语言理解改进的主要驱动力。