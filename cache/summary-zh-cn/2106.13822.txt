当代关于抽象文本摘要的工作主要集中在高资源语言（如英语）上，这主要是由于低/中资源语言的数据集可用性有限。在这项工作中，我们介绍了XL Sum，这是一个综合性和多样性的数据集，由来自BBC的100万对专业注释文章摘要组成，使用一组精心设计的启发式方法提取。该数据集涵盖从低到高资源的44种语言，其中许多语言目前没有可用的公共数据集。XL Sum是高度抽象、简洁和高质量的，这一点可以从人的评价和内在评价中看出。我们使用XL Sum微调了mT5，这是一种最先进的预训练多语言模型，并对多语言和低资源摘要任务进行了实验。与使用类似单语数据集获得的结果相比，XL Sum得出的结果具有竞争性：我们在10种语言上的得分高于11分，其中一些超过15分，这是通过多语言培训获得的。此外，针对低资源语言的单独培训也提供了有竞争力的表现。就我们所知，就从单一来源收集的样本数量和涵盖的语言数量而言，XL Sum是最大的抽象摘要数据集。我们正在发布数据集和模型，以鼓励未来对多语言抽象摘要的研究。可以在\url中找到这些资源{https://github.com/csebuetnlp/xl-sum}.