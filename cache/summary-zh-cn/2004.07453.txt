随着NLP模型变得越来越大，执行一个经过训练的模型需要大量的计算资源，从而产生货币和环境成本。为了更好地尊重给定的推理预算，我们建议修改上下文表示微调，在推理过程中，允许对简单实例提前（和快速）“退出”神经网络计算，对硬实例晚（和准确）退出。为了实现这一点，我们将分类器添加到不同的BERT层，并使用其校准的置信度分数来做出早期退出决策。我们在两个任务中对五个不同的数据集进行了测试：三个文本分类数据集和两个自然语言推理基准。我们的方法在几乎所有情况下都提供了一个良好的速度/精度权衡，生成的模型比最新技术快五倍，同时保持了它们的精度。与基线BERT模型相比，我们的方法几乎不需要额外的训练资源（时间或参数）。最后，我们的方法减轻了在不同效率水平下对多个模型进行昂贵的再培训的需要；我们允许用户通过在推理时设置单个变量，使用单个训练模型控制推理速度/精度权衡。我们公开发布我们的代码。