虽然大型语言模型在NLP中被广泛使用，但对它们进行预培训被认为是一种奢侈品，只有少数资金雄厚的行业实验室才能负担得起。如何以更为适度的预算来培训这样的模型？我们提出了一种使用单一低端深度学习服务器在24小时内预训练蒙面语言模型的方法。我们证明，通过软件优化、设计选择和超参数调整的组合，可以以原始预培训成本的一小部分生产出与BERT竞争的基于粘合任务的模型。