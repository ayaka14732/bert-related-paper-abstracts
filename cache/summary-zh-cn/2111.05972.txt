随着深度学习模型规模的快速增长，需要用于大型模型培训的系统级解决方案。我们介绍了Amazon SageMaker model parallelism，这是一个与Pyrotch集成的软件库，可以使用model parallelism和其他节省内存的功能轻松训练大型模型。与现有解决方案相比，SageMaker库的实现更具通用性和灵活性，因为它可以在任意模型体系结构上自动划分和运行管道并行，而代码更改最少，并且还为tensor并行提供了通用和可扩展的框架，它支持更广泛的用例，模块化程度足以轻松应用于新的培训脚本。该库还在更大程度上保留了原生PyTorch用户体验，支持模块重用和动态图形，同时让用户完全控制培训步骤的细节。我们评估了GPT-3、RoBERTa、BERT和神经协同过滤的性能，并展示了与现有解决方案相比的竞争力。