当扩展到数千亿个参数时，GPT-3（Brown等人，2020）等预训练语言模型实现了显著的少镜头性能。然而，训练和应用这样大的模型需要大量的计算，这导致了巨大的碳足迹，使得研究人员和实践者很难使用它们。我们表明，与GPT-3类似的性能可以通过更“绿色”的语言模型获得，因为它们的参数计数要小几个数量级。这是通过将文本输入转换成包含任务描述的完形填空问题，并结合基于梯度的优化来实现的；利用未标记的数据提供了进一步的改进。我们通过小型语言模型确定了成功理解自然语言所需的关键因素。