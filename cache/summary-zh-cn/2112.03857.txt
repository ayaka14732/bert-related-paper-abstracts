本文提出了一个用于学习对象级、语言感知和语义丰富的视觉表征的扎根语言图像预训练（GLIP）模型。GLIP将目标检测和短语基础统一起来用于预训练。这种统一带来了两个好处：1）它允许GLIP从检测和接地数据中学习，以改进这两项任务，并引导一个良好的接地模型；2） GLIP可以通过以自我训练的方式生成基础框来利用大量的图像-文本对，从而使学习到的表示语义丰富。在我们的实验中，我们在27M基础数据上预训练GLIP，包括3M人类注释和24M网络爬网图像-文本对。学习到的表示法显示出很强的零拍和少量镜头可转移到各种对象级识别任务。1） 当直接在COCO和LVIS上进行评估时（在训练前没有看到COCO中的任何图像），GLIP分别达到49.8 AP和26.9 AP，超过了许多监督基线。2） 在COCO上进行微调后，GLIP在val上达到60.8 AP，在测试开发上达到61.5 AP，超过了之前的SoTA。3） 当转移到13个下游目标检测任务时，一个单发的GLIP与一个完全受监督的动态头部相匹敌。代码将在https://github.com/microsoft/GLIP.