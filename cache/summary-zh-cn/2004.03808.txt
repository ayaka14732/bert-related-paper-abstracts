应用大型预训练NLP模型（如BERT）的最流行范例之一是在较小的数据集上对其进行微调。然而，一个挑战仍然存在，因为微调后的模型往往过于适合较小的数据集。这种现象的一个症状是，句子中不相关的词，即使它们对人类来说是显而易见的，也会严重降低这些经过微调的伯特模型的性能。在本文中，我们提出了一种新的技术，称为自我监督注意（SSA），以帮助促进这一推广挑战。具体来说，SSA通过“探测”上一次迭代中经过微调的模型，自动迭代生成弱的标记级注意标签。我们研究了将SSA集成到BERT中的两种不同方法，并提出了一种混合方法来结合它们的优点。根据经验，在各种公共数据集上，我们使用SSA增强的BERT模型说明了显著的性能改进。