预训练神经语言模型的最新进展显著提高了许多自然语言处理（NLP）任务的性能。在本文中，我们提出了一种新的模型体系结构DeBERTa（解码增强型BERT与分散注意），它使用两种新技术改进了BERT和RoBERTa模型。第一种是解纠缠注意机制，其中每个单词分别使用两个向量表示，这两个向量分别编码其内容和位置，单词之间的注意权重分别使用其内容和相对位置上的解纠缠矩阵计算。第二，使用增强的掩码解码器在解码层中合并绝对位置，以在模型预训练中预测掩码令牌。此外，采用一种新的虚拟对抗训练方法进行微调，以提高模型的泛化能力。我们发现，这些技术显著提高了模型预训练的效率以及自然语言理解（NLU）和自然语言生成（NLG）下游任务的性能。与RoBERTa Large相比，基于一半训练数据训练的DeBERTa模型在广泛的NLP任务中表现始终更好，MNLI提高了+0.9%（90.2%对91.1%），球队v2.0提高了+2.3%（88.4%对90.7%），比赛提高了+3.6%（83.2%对86.8%）。值得注意的是，我们通过训练一个更大的版本来扩展DeBERTa，该版本由48个具有15亿个参数的变换层组成。在宏观平均得分方面（89.9分对89.8分），显著的性能提升使单德贝塔模型在SuperGLUE基准（Wang et al.，2019a）上首次超过了人类性能，而整体德贝塔模型在2021年1月6日的SuperGLUE排行榜上名列前茅，以相当大的幅度超过人类基线（90.3比89.8）。