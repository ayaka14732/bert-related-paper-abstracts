我们研究将知识注入大型预训练模型（如BERT和RoBERTa）的问题。现有方法通常在注入知识时更新预训练模型的原始参数。然而，当多种知识被注入时，历史注入的知识就会被冲走。为了解决这个问题，我们提出了K-Adapter，这是一个框架，它保留了预先训练好的模型的原始参数，并支持多功能知识注入模型的开发。以RoBERTa为主干模型，K-Adapter为每种注入的知识都提供了一个神经适配器，就像连接到RoBERTa的插件一样。不同适配器之间没有信息流，因此可以以分布式方式有效地训练多个适配器。作为一个案例研究，我们在这项工作中注入了两种知识，包括（1）从Wikipedia和Wikidata上自动对齐的文本三元组获得的事实知识和（2）通过依赖解析获得的语言知识。在三个知识驱动任务（包括关系分类、实体类型和问答）上的结果表明，每个适配器都提高了性能，并且两个适配器的组合带来了进一步的改进。进一步的分析表明，K-Adapter比RoBERTa掌握了更多的知识。