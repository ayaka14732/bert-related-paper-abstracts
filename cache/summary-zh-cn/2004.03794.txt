训练大型语言表示模型已经成为自然语言处理领域的标准。这允许对任意数量的特定任务进行微调，但是，这些大型高容量模型可以继续对特定于域的未标记数据进行训练，从而使初始化对于受监督的任务更加健壮。我们证明，在实践中，这些预先训练的模型在对一般领域（如GLUE）的任务进行评估时，表现出灾难性遗忘的形式。在这项工作中，我们为语言建模提出了冷静、持续的自适应学习：呈现跨多个领域保留知识的模型的技术。通过这些方法，我们能够减少由任务特定模型引入的监督任务之间的性能差距，我们在生物医学和临床领域使用持续学习设置进行演示。