上下文嵌入，如ELMo和BERT，超越了Word2Vec等全局单词表示，并在广泛的自然语言处理任务中实现了突破性的性能。上下文嵌入根据每个单词的上下文为其指定一个表示，从而捕获不同上下文中单词的用法，并编码跨语言传输的知识。在这项调查中，我们回顾了现有的上下文嵌入模型、跨语言多语言预训练、上下文嵌入在下游任务中的应用、模型压缩和模型分析。