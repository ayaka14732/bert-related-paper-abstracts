我们通过基于与前面标记的局部相似性，对从大型语料库检索到的文档块进行条件化处理，来增强自回归语言模型。凭借价值2万亿美元的令牌数据库，我们的检索增强型Transformer（RETRO）获得了与GPT-3和Jurassic-1相当的性能，尽管使用的参数少了25$\倍。经过微调后，复古性能转化为下游知识密集型任务，如问答。RETRO结合了一个冻结的伯特检索器、一个可微编码器和一个分块交叉注意机制，根据比训练期间通常消耗的数据多一个数量级的数据来预测令牌。我们通常从头开始训练RETRO，但也可以通过检索快速改装预先训练过的变压器，并且仍能获得良好的性能。我们的工作为通过前所未有的规模的外显记忆改进语言模型开辟了新途径。