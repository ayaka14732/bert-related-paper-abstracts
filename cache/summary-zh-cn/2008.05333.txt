自我监督学习，又称预训练，在自然语言处理中非常重要。大多数预训练方法首先随机屏蔽句子中的某些位置，然后训练一个模型来恢复被屏蔽位置的标记。通过这种方式，模型可以在没有人为标记的情况下进行训练，并且海量数据可以使用数十亿个参数。因此，优化效率变得至关重要。本文从梯度方差约简的角度来解决这一问题。特别是，我们首先提出了一个有原则的梯度方差分解定理，它表明语言训练前的随机梯度方差可以自然地分解为两项：批量数据样本产生的方差和掩码采样产生的方差。第二个术语是自我监督学习和监督学习之间的关键区别，这使得预训练速度变慢。为了减少第二部分的方差，我们采用了重要性抽样策略，其目的是根据建议分布而不是均匀分布对掩码进行抽样。可以证明，如果建议分布与梯度范数成正比，则抽样方差减小。为了提高效率，我们引入了一个掩码建议网络（MAPNet），该网络近似于最佳掩码建议分布，并与模型一起进行端到端的训练。实验结果表明，该模型收敛速度快，性能优于基准的BERT模型。