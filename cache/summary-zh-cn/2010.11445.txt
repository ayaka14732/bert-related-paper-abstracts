端到端语音到文本翻译（E2E-ST）是一种将源语言语音直接翻译成目标语言文本的翻译方法，在实际应用中有着广泛的应用，但传统的级联翻译方法（ASR+MT）往往会遇到错误传播的问题。另一方面，现有的端到端解决方案在很大程度上依赖于源语言转录进行预训练或具有自动语音识别（ASR）的多任务训练。相反，我们提出了一种简单的技术，仅在语音端以自监督方式学习鲁棒语音编码器，它可以利用语音数据而无需转录。这种被称为掩蔽声学建模（MAM）的技术不仅为改进E2E-ST提供了一种替代解决方案，而且还可以对任何声学信号（包括非语音信号）进行无注释的预训练。我们在8个不同的翻译方向上进行实验。在不使用任何转录的情况下，我们的技术实现了+1.1 BLEU和+2.3 BLEU的平均改善（MAM预训练）。使用任意声学信号对MAM进行预训练，这些语言的平均改善率为+1.6 BLEU。与ASR多任务学习解决方案相比，我们的预训练MAM模型不使用转录，在训练过程中响应转录，达到了相似的精度。