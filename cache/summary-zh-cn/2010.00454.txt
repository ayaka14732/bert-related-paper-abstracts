最近，大型预先训练的语言模型，如BERT，在许多自然语言处理任务中已经达到了最先进的性能，但是对于许多语言，包括爱沙尼亚语，BERT模型还不可用。然而，存在几种可以同时处理多种语言的多语言BERT模型，并且这些模型也经过爱沙尼亚数据的训练。在本文中，我们评估了四种多语言模型——多语言BERT、多语言蒸馏BERT、XLM和XLM RoBERTa——在几个NLP任务上的性能，包括词性和词法标记、NER和文本分类。我们的目标是在这些多语言的BERT模型和这些任务的现有基线神经模型之间建立一个比较。我们的研究结果表明，多语言BERT模型在不同的爱沙尼亚NLP任务上都能很好地推广，在词性和词形标记以及文本分类方面优于所有基线模型，并且达到了与NER最佳基线相当的水平，与其他多语言模型相比，XLM RoBERTa取得了最高的结果。