多语言预训练表示通常依赖于子词切分算法来创建共享的多语言词汇表。然而，标准的启发式算法往往导致次优分割，特别是对于数据量有限的语言。在本文中，我们采取了两个主要步骤来缓解这个问题。首先，我们从经验上证明，在对预先训练的多语言表征进行微调的过程中，应用现有的子词正则化方法（Kudo，2018；Provilkov et al.，2020）可以提高跨语言迁移的有效性。其次，为了充分利用不同的可能输入分割，我们提出了多视图子词正则化（MVR），这是一种加强使用标准分割和概率分割标记的输入预测之间一致性的方法。XTREME多语言基准测试（Hu等人，2020年）的结果表明，MVR比使用标准分割算法带来了高达2.5个点的一致改进。