在预先训练语言模型的时代，变形金刚实际上是模型架构的选择。虽然最近的研究显示了完全卷积（CNN）架构的前景，但尚未使用预训练微调范式对其进行探索。在语言模型的上下文中，当预先训练时，卷积模型是否与转换器竞争？本文调查了这个研究问题，并提出了一些有趣的发现。通过对8个数据集/任务的大量实验，我们发现基于CNN的预训练模型具有竞争力，在某些情况下优于变压器模型，尽管有一些警告。总的来说，本文中概述的研究结果表明，将预培训和体系结构进步混为一谈是错误的，应该单独考虑这两种进步。我们相信，我们的研究为替代架构中的健康乐观铺平了道路。