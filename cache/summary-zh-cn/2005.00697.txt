基于Transformer的QA模型在所有层都使用输入范围内的自我关注——即跨问题和输入通道——导致它们速度慢且内存密集。事实证明，我们可以在所有层面上，特别是在较低层面上，不需要投入广泛的自我关注。我们引入了变形器，一个分解的变换器，它在底层用问题范围和通道范围的自我关注来代替完全的自我关注。这允许对输入文本表示进行独立于问题的处理，从而支持预计算段落表示，从而大大减少运行时计算量。此外，由于变形器与原始模型非常相似，我们可以使用标准变换器的预训练权重初始化变形器，并直接在目标QA数据集上进行微调。我们展示了变形器版本的BERT和XLNet可以将QA的速度提高4.3倍以上，并且使用简单的基于蒸馏的损失，它们只会导致精度下降1%。我们在https://github.com/StonyBrookNLP/deformer.