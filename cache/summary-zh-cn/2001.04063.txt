提出了一种新的序列到序列预训练模型ProphetNet，该模型引入了一种新的自监督目标future n-gram prediction和提出的n流自注意机制。在传统的序列到序列模型中，ProphetNet不是优化一步超前预测，而是通过n步超前预测进行优化，n步超前预测基于每个时间步的先前上下文令牌同时预测下一个n令牌。未来n-gram预测明确地鼓励模型为未来令牌进行规划，并防止对强局部相关性的过度拟合。我们分别使用基本规模数据集（16GB）和大规模数据集（160GB）对ProphetNet进行预训练。然后，我们在CNN/DailyMail、Gigaword和1.1班的抽象摘要和问题生成任务基准上进行实验。实验结果表明，与使用相同规模的预训练语料库的模型相比，ProphetNet在所有这些数据集上都取得了最新的结果。