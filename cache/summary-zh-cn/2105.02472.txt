预训练跨语言模型的引入为多语言NLP任务带来了决定性的改进。然而，由于缺乏标记的任务数据，需要各种方法来缩小与高资源语言的差距。特别是零炮方法，通常使用翻译后的任务数据作为训练信号，以弥合源语言和目标语言之间的性能差距。我们介绍了XeroAlign，这是一种简单的跨语言预训练变形金刚（如XLM-R）任务特定对齐方法。XeroAlign使用翻译的任务数据来鼓励模型为不同语言生成类似的句子嵌入。XeroAligned XLM-R（称为XLM-RA）在三种多语言自然语言理解任务上显示出比基线模型的强大改进，以实现最先进的零射击结果。XLM- RA的文本分类精度超过了用标记数据训练的XLM-R，并在一个跨语言的对抗性复述任务上与最先进的模型相匹配。