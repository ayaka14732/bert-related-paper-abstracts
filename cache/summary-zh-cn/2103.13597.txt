Transformer是一种基于注意的神经网络，它由两个子层组成，即自注意网络（SAN）和前馈网络（FFN）。现有的研究试图分别增强这两个子层，以提高Transformer的文本表示能力。在本文中，我们对SAN和FFN作为掩码注意网络（MAN）提出了一种新的理解，并证明它们是具有静态掩码矩阵的MAN的两种特殊情况。然而，它们的静态掩码矩阵限制了文本表示学习中的局部性建模能力。因此，我们引入了一个新的层称为动态掩码注意网络（DMAN），该层具有可学习的掩码矩阵，能够自适应地建模局部性。为了结合DMAN、SAN和FFN的优点，我们提出了一种顺序分层结构来组合这三种类型的层。对各种任务的广泛实验，包括神经机器翻译和文本摘要表明，我们的模型优于原始变压器。