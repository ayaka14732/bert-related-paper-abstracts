伯特在自然语言理解（NLU）任务上取得了优异的成绩。然而，BERT拥有大量的参数，需要部署一定的资源。在加速方面，最近提出了动态提前退出BERT（DeeBERT），它包含多个退出，并采用动态提前退出机制以确保有效的推理。在获得效率-性能折衷的同时，多出口BERT中早期出口的性能明显低于晚期出口。在本文中，我们利用梯度正则化自蒸馏对多出口BERT（RomeBERT）进行鲁棒训练，可以有效地解决早期和晚期出口之间的性能不平衡问题。此外，所提出的RomeBERT对多出口和BERT主干采用一阶段联合训练策略，而DeeBERT需要两个阶段，需要更多的训练时间。在GLUE数据集上进行了大量实验，以证明我们方法的优越性。我们的代码可在https://github.com/romebert/RomeBERT.