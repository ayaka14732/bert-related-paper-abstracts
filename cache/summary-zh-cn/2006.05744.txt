预先训练的上下文表示（例如，伯特）已经成为实现许多NLP任务的最先进结果的基础。然而，大规模的预训练在计算上是昂贵的。ELECTRA是加速预训练的早期尝试，它训练了一个判别模型，预测每个输入令牌是否被生成器替换。我们的研究表明，ELECTRA的成功主要是因为它降低了预训练任务的复杂性：二元分类（替换标记检测）比生成任务（掩蔽语言建模）学习效率更高。然而，这样一个简化的任务在语义上的信息较少。为了获得更好的效率和效果，我们提出了一种新的元学习框架MC-BERT。训练前任务是一个带有拒绝选项的多项选择完形填空测试，其中元控制器网络提供训练输入和候选人。通过GLUE自然语言理解基准测试的结果表明，我们提出的方法是高效的：在相同的计算预算下，它在GLUE语义任务上的性能优于基线。