基于大型语料库的语言模型预训练在构建丰富的语境表征方面取得了巨大成功，并在各种自然语言理解（NLU）任务中取得了显著的成绩。尽管取得了成功，但大多数当前的预训练语言模型（如BERT）都是基于单粒度标记化进行训练的，通常使用细粒度字符或子单词，这使得它们很难学习粗粒度单词和短语的精确含义。在本文中，我们提出了一种简单而有效的预训练方法LICHEE来有效地融合输入文本的多粒度信息。我们的方法可以应用于各种预先训练的语言模型，提高它们的表示能力。在CLUE和SuperGLUE上进行的大量实验表明，我们的方法在几乎不增加额外推理成本的情况下，对各种中文和英文NLU任务实现了全面改进，并且我们的最佳集成模型在CLUE基准测试中达到了最先进的性能。