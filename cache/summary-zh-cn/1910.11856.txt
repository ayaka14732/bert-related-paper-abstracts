最先进的无监督多语言模型（例如，多语言BERT）已被证明可以在零炮跨语言环境下进行推广。这种泛化能力归因于使用共享的子词词汇和跨多种语言的联合训练，从而产生深刻的多语言抽象。我们通过设计一种替代方法来评估这一假设，该方法将单语模型转移到词汇层面的新语言中。更具体地说，我们首先在一种语言上训练一个基于转换器的掩蔽语言模型，然后通过学习具有相同掩蔽语言建模目标的新嵌入矩阵将其转换为一种新语言，冻结所有其他层的参数。这种方法不依赖于共享词汇或联合培训。然而，我们表明，在标准的跨语言分类基准和一个新的跨语言问答数据集（XQuAD）上，它与多语言BERT具有竞争力。我们的结果与关于多语言模型泛化能力基础的共同信念相矛盾，并表明深层单语模型学习一些跨语言泛化的抽象。我们还发布了XQuAD作为一个更全面的跨语言基准，它包括240个段落和1190个问题-答案对，由专业翻译人员从SQuAD v1.1翻译成十种语言。