基于转换器的模型由于其大容量、固有的并行性和高性能，在自然语言处理中已变得无处不在。Transformer块的上下文化组件是$\textit{pairwise dot product}$注意，它对长度为$L$的序列有很大的$\Omega（L^2）$内存需求，限制了它处理长文档的能力。最近，人们对这一问题产生了极大的兴趣，提出了多种近似方法来减少使用稀疏注意矩阵的二次记忆需求。在这项工作中，我们建议使用长度为$M$（$\ll L$）的密集的基于注意的$\textit{global memory}$来扩充稀疏转换器块，它提供每个位置的整个输入序列的聚合全局视图。我们的扩展具有可管理的$O（M\cdot（L+M））$内存开销，并且可以与以前的稀疏解决方案无缝集成。此外，全局内存还可以用于序列压缩，方法是仅使用内存表示来表示长输入序列。我们的经验表明，我们的方法在一系列任务上取得了实质性的改进，包括（a）需要全局推理的合成任务，（b）掩蔽语言建模，以及（c）阅读理解。