受预训练语言模型（LMs）编码常识知识的证据启发，最近的工作已经应用LMs自动填充常识知识图（CKG）。然而，对于它们对多个CKG、看不见的关系和新实体的泛化缺乏理解。本文从知识能力、可转移性和归纳性三个方面分析了LMs进行广义常识推理的能力。我们在这三个方面的实验表明：（1）LMs能够适应由多个CKG定义的不同模式，但不能重用知识来概括新的关系。（2） 适应的LMs很好地概括了看不见的主题，但对新奇的对象则不太适用。未来的工作应该研究如何提高LMs中常识挖掘的可转移性和归纳性。