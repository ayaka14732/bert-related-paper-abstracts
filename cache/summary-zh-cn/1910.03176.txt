使用预先训练好的模型进行微调在许多语言任务中取得了优异的效果。在这项研究中，我们重点研究了这样一种自我注意网络模型，即BERT，它在不同语言理解基准的层次叠加方面表现良好。然而，在许多下游任务中，层之间的信息被用于微调的BERT忽略。此外，尽管自我注意网络以其捕捉全局依赖性的能力而闻名，但在强调局部环境的重要性方面仍有改进的余地。鉴于这些优点和缺点，本文提出了一种广义微调方法SesameBERT，该方法（1）通过压缩和激励来提取所有层之间的全局信息，（2）通过高斯模糊捕获相邻上下文来丰富局部信息。此外，我们在HANS数据集中证明了我们的方法的有效性，该数据集用于确定模型是否采用了浅层启发式而不是学习潜在的泛化。实验表明，SesameBERT在GLUE基准测试和HANS评估集方面优于BERT。