在这项工作中，我们扩展了Transformers（BERT）的双向编码器表示，重点是定向协同注意，以在SQUAD2.0数据集上获得改进的F1性能。BERT所基于的Transformer体系结构将层次全局注意力放在上下文和查询的连接上。我们在BERT体系结构中添加的内容通过一组改进的Transformer编码器单元，通过更加集中的上下文到查询（C2Q）和查询到上下文（Q2C）的关注，增强了这种关注。此外，我们还探索在共同注意体系结构中添加基于卷积的特征提取，以将局部信息添加到自我注意中。我们发现，共同注意力显著提高了无答案F1，在基本架构中提高了4分，在大型架构中提高了1分。添加跳过连接后，无应答F1进一步改进，而不会导致has应答F1中的额外丢失。局部特征提取的添加增加了关注度，在基础架构中产生了77.03的总体dev F1。我们将我们的发现应用于包含两倍多层的大型BERT模型，并进一步使用了我们自己的通过反向翻译创建的SQUAD 2.0数据集的增强版本，我们将其命名为SQUAD 2.Q。最后，我们进行了超参数调整，并为最终的F1/EM 82.317/79.442（关注类固醇，PCE测试排行榜）整合了我们的最佳模型。