我们将一般语言智能定义为重用先前获得的关于语言词汇、语法、语义和语用惯例的知识以快速适应新任务的能力。利用这一定义，我们分析了最先进的自然语言理解模型，并进行了广泛的实证调查，通过一系列实验评估学习过程中所获得知识的任务独立性，以对照这些标准对其进行评估。除了任务性能之外，我们还提出了一个新的基于测试数据在线编码的评估指标，该指标量化了现有代理（模型）学习新任务的速度。我们的研究结果表明，尽管该领域在推广到许多任务的模型体系结构方面取得了令人印象深刻的进展，但这些模型仍然需要大量的领域内培训示例（例如，微调、培训特定于任务的模块），并且容易发生灾难性遗忘。此外，我们发现，我们的模型远远不能解决一般任务（例如，文档问答），而是过分适合特定数据集（例如，团队）的怪癖。我们讨论了缺失的部分和关于如何向一般语言智能发展的猜测。