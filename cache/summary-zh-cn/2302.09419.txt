预训练基础模型（PFM）被视为具有不同数据模式的各种下游任务的基础。预训练的基础模型，如BERT、GPT-3、MAE、DALLE-E和ChatGPT，在大规模数据上进行训练，为广泛的下游应用程序提供合理的参数初始化。PFM背后的预训练思想在大型模型的应用中发挥着重要作用。与以前应用卷积和递归模块进行特征提取的方法不同，生成预训练（GPT）方法应用Transformer作为特征提取器，并在具有自回归范式的大型数据集上进行训练。类似地，BERT使用transformer在大型数据集上作为上下文语言模型进行训练。最近，ChatGPT在大型语言模型上取得了可喜的成功，它应用了一种零镜头或很少显示提示的自回归语言模型。随着PFM的非凡成功，人工智能在过去几年中在各个领域掀起了波澜。文献中提出了大量的方法、数据集和评估指标，因此需要进行更新的调查。本研究全面回顾了PFM在文本、图像、图表以及其他数据模式中的最新研究进展、当前和未来的挑战以及机遇。我们首先回顾了自然语言处理、计算机视觉和图形学习中的基本组成部分和现有的预训练。然后，我们讨论了其他数据模式的其他高级PFM，以及考虑数据质量和数量的统一PFM。此外，我们还讨论了PFM基本原理的相关研究，包括模型效率和压缩、安全性和隐私性。最后，我们列出了关键的含义、未来的研究方向、挑战和悬而未决的问题。