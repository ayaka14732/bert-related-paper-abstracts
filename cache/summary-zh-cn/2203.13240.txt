基于转换器的模型通常为给定序列中的每个令牌分配相同的计算量。我们开发了一种简单但有效的“令牌丢弃”方法来加速变压器模型（如BERT）的预训练，而不会降低其在下游任务中的性能。简言之，我们从模型的中间层开始丢弃不重要的令牌，以使模型关注重要的令牌；丢弃的令牌稍后由模型的最后一层拾取，以便模型仍然生成全长序列。我们利用已经内置的屏蔽语言建模（MLM）损失来识别不重要的标记，几乎没有计算开销。在我们的实验中，这种简单的方法将BERT的预训练成本降低了25%，同时在标准下游任务上实现了类似的总体微调性能。