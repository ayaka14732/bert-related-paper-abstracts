我们提出了一个统一的视觉语言预训练模型（VLMo），该模型通过模块化变压器网络联合学习双编码器和融合编码器。具体来说，我们介绍了混合模式专家（MoME）转换器，其中每个模块包含一组特定于模式的专家和一个共享的自我注意层。由于MoME的建模灵活性，预训练VLMo可以作为视觉语言分类任务的融合编码器进行微调，或者用作高效图像文本检索的双编码器。此外，我们还提出了一种分段预训练策略，该策略有效地利用了图像-文本对之外的大规模纯图像和纯文本数据。实验结果表明，VLMo在各种视觉语言任务（包括VQA和NLVR2）上取得了最先进的结果。代码和预训练模型可在https://aka.ms/vlmo.