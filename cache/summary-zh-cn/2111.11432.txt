对我们多样而开放的世界的自动化视觉理解要求计算机视觉模型能够很好地进行概括，并对特定任务进行最小程度的定制，类似于人类视觉。计算机视觉基础模型是解决现实世界计算机视觉应用的关键任务，它在不同的大规模数据集上进行训练，可以适应广泛的下游任务。虽然现有的视觉基础模型（如CLIP、ALIGN和Wu Dao 2.0）主要关注将图像和文本表示映射到跨模态共享表示，但我们引入了一个新的计算机视觉基础模型Florence，将表示从粗略（场景）扩展到精细（对象），从静态（图像）扩展到动态（视频），从RGB到多种模式（标题、深度）。通过结合Web级图像文本数据的通用视觉语言表示，我们的Florence模型可以轻松地适应各种计算机视觉任务，例如分类、检索、目标检测、VQA、图像标题、视频检索和动作识别。此外，Florence在许多类型的迁移学习中表现出色：全采样微调、线性探测、少镜头迁移和零镜头迁移，用于新图像和对象。所有这些属性对于我们的vision foundation模型服务于通用vision任务至关重要。Florence在44个具有代表性的基准测试中取得了最新的最新成果，例如ImageNet-1K零炮分类，前1位精度为83.74，前5位精度为97.18，COCO微调的mAP为62.4，VQA的mAP为80.36，Kinetics-600的mAP为87.8。