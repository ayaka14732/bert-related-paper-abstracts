神经机器翻译（NMT）需要大量的并行语料库来提供最先进的翻译质量。低资源NMT通常通过转移学习解决，转移学习利用大型单语或平行语料库进行预培训。单语预训练方法，如MASS（掩蔽序列到序列）对于提高具有小型平行语料库的语言的NMT质量非常有效。然而，它们不考虑使用句法分析器获得的语言信息，而句法分析器对于一些自然语言处理（NLP）任务来说是非常宝贵的。为此，我们提出JASS，日语特定序列到序列，作为一种新的NMT预训练替代方案，将日语作为源语言或目标语言。JASS是联合BMASS（Bunsetsu MASS）和BRSS（Bunsetsu Reordering Sequence to Sequence）预培训，重点是日语语言单位bunsetsus。在我们对ASPEC日语——英语和新闻评论日语——俄语翻译的实验中，我们表明JASS可以给出与MASS相比甚至更好的结果。此外，我们首次表明，联合MASS和JASS预训练的结果显著优于单独的方法，表明了它们的互补性。我们将发布我们的代码、预先训练的模型和bunsetsu注释数据，作为研究人员在自己的NLP任务中使用的资源。