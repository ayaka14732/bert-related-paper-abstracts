具有文本、布局和图像的多模式预培训最近在视觉丰富的文档理解任务中实现了SOTA性能，这表明了跨不同模式联合学习的巨大潜力。在本文中，我们提出了LayoutXLM，一个用于多语言文档理解的多模式预训练模型，它旨在为视觉丰富的文档理解跨越语言障碍。为了准确评估LayoutXLM，我们还引入了一个名为XFUND的多语言表单理解基准数据集，其中包括7种语言（中文、日语、西班牙语、法语、意大利语、德语、葡萄牙语）的表单理解示例，并为每种语言手动标记键值对。实验结果表明，LayoutXLM模型在XFUND数据集上的性能明显优于现有的SOTA跨语言预训练模型。预先培训的LayoutXLM模型和XFUND数据集可在https://aka.ms/layoutxlm.