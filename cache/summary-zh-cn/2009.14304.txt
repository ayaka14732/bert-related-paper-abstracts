当对下游任务进行微调时，多语言BERT（mBERT）已显示出合理的零镜头跨语言传输能力。由于mBERT没有经过明确的跨语言监督的预先训练，因此可以通过将mBERT与跨语言信号对齐来进一步提高迁移性能。先前的工作提出了几种方法来对齐上下文化嵌入。在本文中，我们分析了不同形式的跨语言监督和不同的对齐方法如何影响mBERT在零炮设置下的传输能力。具体来说，我们比较了并行语料库与基于词典的监控以及旋转与基于微调的对齐方法。我们评估了八种语言的不同对齐方法在两个任务上的性能：名称实体识别和语义槽填充。此外，我们还提出了一种新的归一化方法，该方法持续改进了基于旋转的对齐的性能，包括对远距离和类型不同的语言显著提高了3%。重要的是，我们确定了对齐方法对任务类型的偏见以及与迁移语言的接近程度。我们还发现，平行语料库的监督通常优于词典对齐。