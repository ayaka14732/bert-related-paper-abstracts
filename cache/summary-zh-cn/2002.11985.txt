预先训练的基于Transformer的模型已经在各种自然语言处理（NLP）任务中实现了最先进的性能。然而，这些模型通常有数十亿个参数，因此，太过消耗资源和计算密集，不适合具有严格延迟要求的低性能设备或应用程序。一个潜在的补救办法是模型压缩，这已经引起了很多研究关注。在这里，我们总结了压缩变压器的研究，重点是特别流行的伯特模型。特别是，我们调查了BERT压缩的最新技术，阐明了压缩大型变压器模型的当前最佳实践，并深入了解了各种方法的工作原理。我们的分类和分析也为实现轻量级、准确和通用的NLP模型提供了有希望的未来研究方向。