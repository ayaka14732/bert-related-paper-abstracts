我们考虑极端多标签文本分类（XMC）问题：给定一个输入文本，从一个大标签集合返回最相关的标签。例如，输入文本可以是Amazon.com上的产品描述，标签可以是产品类别。XMC是NLP社区中一个重要但具有挑战性的问题。最近，深度预训练的transformer模型在许多NLP任务（包括句子分类）上取得了最先进的性能，尽管标签集很小。然而，由于大输出空间和标签稀疏性问题，天真地将深度转换器模型应用于XMC问题会导致次优性能。在本文中，我们提出了X-Transformer，这是第一个用于微调XMC问题的深度转换器模型的可伸缩方法。该方法在四个XMC基准数据集上获得了最新的结果。特别是，在一个有大约50万个标签的Wiki数据集上prec@1X-Transformer的性能为77.28%，与最先进的XMC方法Parabel（线性）和AttentionXML（神经）相比有了很大的改进，分别达到68.70%和76.95%precision@1分别地我们进一步将X-Transformer应用于来自Amazon的product2query数据集，并获得了10.7%的相对改进prec@1在寓言之上。