大规模的视觉和语言表征学习在各种视觉语言任务上取得了很大的进步。大多数现有方法使用基于转换器的多模式编码器来联合建模视觉标记（基于区域的图像特征）和单词标记。由于视觉标记和单词标记是不对齐的，因此对于多模态编码器来说，学习图像-文本交互是一个挑战。在本文中，我们引入了一种对比损失法，在通过跨模态注意将图像和文本表征融合（ALBEF）之前，将它们对齐，从而实现更基础的视觉和语言表征学习。与大多数现有方法不同，我们的方法不需要边界框注释，也不需要高分辨率图像。为了提高从含噪网络数据中学习的能力，我们提出了动量蒸馏，这是一种从动量模型产生的伪目标中学习的自训练方法。我们从互信息最大化的角度对ALBEF进行了理论分析，表明不同的训练任务可以被解释为生成图像-文本对视图的不同方式。ALBEF在多个下游视觉语言任务上实现了最先进的性能。在图像文本检索方面，ALBEF优于在数量级较大的数据集上预先训练的方法。在VQA和NLVR$^2$上，与最新技术相比，ALBEF实现了2.37%和3.84%的绝对改善，同时具有更快的推理速度。代码和预先培训的模型可在https://github.com/salesforce/ALBEF/.