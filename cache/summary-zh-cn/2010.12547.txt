我们提出了一种简单的方法来对齐多语言上下文嵌入，作为预训练后的一个步骤，以提高预训练模型的零镜头跨语言传输能力。利用并行数据，我们的方法通过最近提出的翻译语言建模目标在单词层面上对齐嵌入，并通过对比学习和随机输入洗牌在句子层面对齐嵌入。在对下游任务进行微调时，我们还使用英语执行句子级代码切换。在XNLI上，我们的最佳模型（从mBERT初始化）在零炮设置下比mBERT提高了4.7%，在使用少于18%的相同并行数据和31%的模型参数的情况下，实现了与XLM的平移序列相当的结果。在MLQA上，我们的模型优于XLM-R_基，其参数比我们的多57%。