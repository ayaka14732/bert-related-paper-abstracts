本文介绍了一种针对特定语言的多语言屏蔽语言模型的自适应方法。预先训练的双向语言模型在阅读理解、自然语言推理和情感分析等广泛任务中表现出最先进的性能。目前有两种不同的方法来训练这样的模型：单语和多语。虽然特定于语言的模型表现出优异的性能，但多语言模型允许执行从一种语言到另一种语言的转换，并同时解决不同语言的任务。这项工作表明，从多语言模式到单语模式的迁移学习可以显著提高阅读理解、释义检测和情绪分析等任务的成绩。此外，单语模型的多语言初始化大大减少了训练时间。俄语的预先培训模型是开源的。