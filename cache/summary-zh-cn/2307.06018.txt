大型语言模型（LLM）表现出非凡的理解、推理和生成以下自然语言指令的能力。然而，LLM的发展主要集中在高资源语言上，如英语，从而限制了其在其他语言中的适用性和研究。因此，我们介绍了PolyLM，一种在6400亿（B）代币上训练的多语言LLM，有两种模型大小：1.7B和13B。为了增强其多语言能力，我们1）将双语数据整合到训练数据中；以及2）在预培训期间，采用课程学习策略，将非英语数据的比例从第一阶段的30%提高到最后阶段的60%。此外，我们提出了一种多语言自指令方法，该方法自动生成132.7K条不同的多语言指令用于模型微调。为了评估模型的性能，我们收集了几个现有的多语言任务，包括多语言理解、问答、生成和翻译。大量实验表明，PolyLM在多语言任务上超越了LLaMA和BLOOM等其他开源模型，同时在英语方面保持了相当的性能。我们的模型，连同指令数据和多语言基准，可在以下网址获得：\url{https://modelscope.cn/models/damo/nlp_polylm_13b_text_generation}。