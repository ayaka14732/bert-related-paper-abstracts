基于Transformer的NLP模型使用数亿甚至数十亿个参数进行训练，这限制了它们在计算受限环境中的适用性。虽然参数的数量通常与性能相关，但不清楚下游任务是否需要整个网络。受最近关于修剪和提取预训练模型的工作的启发，我们探索了在预训练模型中删除层的策略，并观察了修剪对下游粘合任务的影响。我们能够将BERT、RoBERTa和XLNet模型删减高达40%，同时保持高达98%的原始性能。此外，我们表明，我们的修剪模型与使用知识蒸馏构建的，无论是在大小和性能方面。我们的实验产生了有趣的观察结果，如：（i）较低的层次对维持下游任务绩效最为关键，（ii）一些任务，如释义检测和句子相似性，对层次的下降更为稳健，以及（iii）使用不同的目标函数训练的模型表现出不同的学习模式，并且w.r.t层下降。