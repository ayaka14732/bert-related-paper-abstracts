由于自我注意的时间和记忆复杂性在序列长度上是二次型的，因此变换器在长序列上是缓慢的，并且需要记忆。近似注意力方法试图通过权衡模型质量来降低计算复杂度来解决这个问题，但通常无法实现挂钟加速。我们认为，一个缺失的原理是使注意力算法具有IO意识——考虑GPU内存级别之间的读写。我们提出了FlashAttention，这是一种IO感知的精确注意力算法，它使用平铺来减少GPU高带宽存储器（HBM）和GPU片上SRAM之间的存储器读/写次数。我们分析了FlashAttention的IO复杂性，表明它比标准注意力需要更少的HBM访问，并且对于一系列SRAM大小来说是最优的。我们还将FlashAttention扩展到块稀疏注意力，产生了一种比任何现有近似注意力方法都快的近似注意力算法。FlashAttention训练变形金刚的速度比现有的基线快：与MLPerf 1.1训练速度记录相比，BERT大型（序列长度512）上的端到端挂钟加速率为15%，GPT-2（序列长度1K）上的加速率为3$\times$，远程竞技场（序列长度1K-4K）上的加快率为2.4$\times$。FlashAttention和块稀疏FlashAttendance在Transformers中实现更长的上下文，产生了更高质量的模型（GPT-2的困惑度提高了0.7，长文档分类的提升点提高了6.4）和全新的功能：第一个在Path-X挑战（序列长度16K，61.4%准确率）和Path-256（序列长度64K，63.1%准确率）上实现了优于机会性能的变形金刚。