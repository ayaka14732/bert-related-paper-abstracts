最近对领域特定的BERT模型的研究表明，当模型在领域内数据上进行预训练时，可以提高下游任务的有效性。通常，这些模型中使用的预训练数据是根据其主题选择的，例如生物学或计算机科学。考虑到使用社交媒体文本的应用范围及其独特的语言多样性，我们分别对tweet和论坛文本进行了预训练，并实证证明了这两种资源的有效性。此外，我们还研究了如何使用相似性度量来指定域内预训练数据。我们公开在https://bit.ly/35RpTf0.