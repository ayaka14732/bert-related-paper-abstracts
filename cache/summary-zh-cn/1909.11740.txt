联合图像-文本嵌入是大多数视觉和语言（V+L）任务的基础，在这些任务中，多模态输入被同时处理以实现联合视觉和文本理解。在本文中，我们介绍了UNITER，一种通用的图像-文本表示，通过对四个图像-文本数据集（COCO、视觉基因组、概念性字幕和SBU字幕）的大规模预训练学习，它可以通过联合多模式嵌入为异构下游V+L任务提供支持。我们设计了四个预训练任务：蒙蔽语言建模（MLM）、蒙蔽区域建模（MRM，有三个变体）、图像文本匹配（ITM）和词区域对齐（WRA）。与之前将联合随机掩蔽应用于两种模式的工作不同，我们在训练前任务中使用条件掩蔽（即，掩蔽语言/区域建模以图像/文本的完全观察为条件）。除了用于全局图像文本对齐的ITM之外，我们还通过使用最优传输（OT）提出了WRA，以明确鼓励在预训练期间在单词和图像区域之间进行细粒度对齐。综合分析表明，条件掩蔽和基于OT的WRA都有助于更好的预训练。我们还进行了彻底的消融研究，以找到训练前任务的最佳组合。大量实验表明，UNITER在六个V+L任务（超过九个数据集）中实现了最新水平，包括视觉问答、图像文本检索、引用表达理解、视觉常识推理、视觉蕴涵和NLVR$^2$。代码可在https://github.com/ChenRocks/UNITER.