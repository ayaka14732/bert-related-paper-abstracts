本文研究通过师生知识提取压缩预训练的语言模型，如BERT（Devlin et al.，2019）。以前的工作通常迫使学生模型严格模仿老师预测的平滑标签。作为替代方案，我们提出了一种新的BERT提取方法，即要求教师生成平滑的单词id，而不是标签，用于教授知识提取中的学生模型。我们称这种方法为文本平滑。实际上，我们在BERT中使用蒙蔽语言模型（MLM）的softmax预测来生成给定文本的单词分布，并使用预测的软单词ID平滑这些输入文本。我们假设平滑标签和平滑文本都可以隐式地增加输入语料库，而文本平滑直观上更有效，因为它可以在一个神经网络前向步骤中生成更多的实例。GLUE和SQuAD上的实验结果表明，我们的解决方案与现有的BERT蒸馏方法相比，可以获得具有竞争力的结果。