大型语言模型已被证明可以通过少量的快照学习在各种自然语言任务中取得显著的性能，这大大减少了使模型适应特定应用程序所需的特定于任务的训练示例的数量。为了进一步了解规模对少数镜头学习的影响，我们训练了一个5400亿参数、密集激活的变形金刚语言模型，我们称之为路径语言模型PaLM。我们使用Pathways在6144 TPU v4芯片上训练PaLM，Pathways是一种新的ML系统，能够跨多个TPU吊舱进行高效训练。我们通过在数百种语言理解和生成基准上实现最先进的少镜头学习结果，展示了扩展的持续优势。在许多这些任务上，PaLM 540B实现了突破性的性能，在一系列多步骤推理任务上超过了经过微调的最先进水平，在最近发布的大基准上也超过了人类的平均性能。大量大型试验台任务显示出模型规模的不连续改进，这意味着当我们扩展到最大的模型时，性能急剧提高。PaLM在多语言任务和源代码生成方面也具有强大的能力，我们在大量基准测试中对此进行了演示。此外，我们还对偏倚和毒性进行了全面分析，并研究了训练数据记忆在模型尺度上的程度。最后，我们讨论了与大型语言模型相关的伦理考虑，并讨论了潜在的缓解策略。