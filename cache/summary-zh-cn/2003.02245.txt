基于语言模型的预训练模型（如BERT）在不同的NLP任务中提供了显著的收益。在本文中，我们研究了不同类型的基于变压器的预训练模型，如用于条件数据扩充的自回归模型（GPT-2）、自动编码器模型（BERT）和seq2seq模型（BART）。我们表明，将类标签预先添加到文本序列中提供了一种简单而有效的方法来调整预先训练的模型以进行数据扩充。此外，在三个分类基准上，预先训练的Seq2Seq模型在低资源环境下优于其他数据增强方法。此外，我们还探讨了不同的基于预训练模型的数据扩充在数据多样性方面的差异，以及这些方法如何很好地保留类标签信息。