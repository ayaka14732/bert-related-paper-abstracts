准确的词汇蕴涵（LE）和自然语言推理（NLI）通常需要大量昂贵的注释。为了减少对标记数据的需求，我们引入了WikiNLI：一种用于改进NLI和LE任务的模型性能的资源。它包含428899对短语，这些短语是根据维基百科中自然注释的类别层次结构构建的。我们表明，我们可以通过在WikiNLI上预先培训BERT和RoBERTa等强基线，并将模型转移到下游任务上，从而改进它们。我们对从其他知识库（如WordNet和Wikidata）中提取的短语进行了系统的比较，发现在WikiNLI上进行预培训的效果最好。此外，我们还用其他语言构建了WikiNLI，并表明对它们进行预训练可以提高相应语言NLI任务的性能。