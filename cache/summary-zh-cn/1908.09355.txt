经过预训练的语言模型（如BERT）已被证明对自然语言处理（NLP）任务非常有效。然而，在训练这些模型时对计算资源的高需求阻碍了它们在实践中的应用。为了缓解大规模模型训练中的资源短缺问题，我们提出了一种患者知识提取方法，将原始的大型模型（教师）压缩为同样有效的轻量级浅层网络（学生）。与以前的知识提取方法不同，我们的学生模型只使用教师网络最后一层的输出进行提取，我们的学生模型耐心地从教师模型的多个中间层学习增量知识提取，遵循两种策略：（$i$）PKD last：从最后的$k$层学习；和（$ii$）PKD Skip：从每$k$层学习。这两种患者蒸馏方案能够利用教师隐藏层中的丰富信息，并鼓励学生模型通过多层蒸馏过程耐心地向教师学习和模仿。从经验上看，这可以转化为多个NLP任务的改进结果，在不牺牲模型准确性的情况下显著提高训练效率。