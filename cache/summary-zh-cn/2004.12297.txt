许多自然语言处理和信息检索问题可以形式化为语义匹配任务。该领域的现有工作主要集中在短文本之间的匹配（如问答），或短文本和长文本之间的匹配（如即席检索）。长格式文档间的语义匹配在新闻推荐、相关文章推荐、文档聚类等领域有着重要的应用，但目前对长格式文档间语义匹配的研究相对较少，需要更多的研究工作。近年来，基于自我注意的模型，如Transformers和BERT，在文本匹配任务中取得了最先进的性能。然而，由于自我注意相对于输入文本长度的二次计算复杂性，这些模型仍然局限于短文本，如几个句子或一个段落。在本文中，我们通过提出用于长格式文档匹配的基于暹罗多深度转换器的分层（SMITH）编码器来解决这个问题。我们的模型包含一些创新，以适应较长文本输入的自我注意模型。为了更好地捕获文档中的句子级语义关系，除了BERT使用的蒙蔽词语言建模任务外，我们还使用一个新的蒙蔽句块语言建模任务对模型进行预训练。我们在多个长格式文档匹配基准数据集上的实验结果表明，我们提出的SMITH模型优于以前的最新模型，包括分层注意、基于多深度注意的分层递归神经网络和BERT。与基于BERT的基线相比，我们的模型能够将最大输入文本长度从512增加到2048。我们将开源一个基于Wikipedia的基准数据集、代码和一个预先训练的检查点，以加速未来对长格式文档匹配的研究。