中文分词是汉语理解的一项基本任务。最近，基于神经网络的模型在解决领域内CWS任务方面取得了优异的性能。去年，Transformers的双向编码器表示（BERT）作为一种新的语言表示模型被提出作为许多自然语言任务的主干模型，并重新定义了相应的性能。BERT的出色性能促使我们将其应用于解决CWS任务。通过在第二届国际汉语分词大赛的基准数据集中进行深入的实验，我们获得了一些敏锐的观察结果。即使数据集包含标签不一致的问题，BERT也可以稍微提高性能。当应用充分学习的特征时，较简单的分类器Softmax可以获得与更复杂的分类器相同的性能，例如条件随机场（CRF）。BERT的性能通常随着模型大小的增加而增加。BERT提取的特征也可以作为其他神经网络模型的候选特征。