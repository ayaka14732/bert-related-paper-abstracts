大规模预训练语言模型在自然语言理解任务中取得了优异的性能。然而，如何将它们应用于对话生成任务，特别是那些以多种来源为反应条件的任务，仍在研究之中。以前的工作只是将所有输入源连接起来，或者对来自不同输入源的信息进行平均。在这项工作中，我们研究了基于预训练语言模型GPT2的多输入源对话模型。我们探索了各种方法来融合不同来源的多个独立注意信息。我们的实验结果表明，适当的融合方法比简单的融合基线与对话历史具有更高的相关性。