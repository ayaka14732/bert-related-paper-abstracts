视听模式之间的直观交互对于跨模式的自我监督学习很有价值。这一概念已在视频动作识别和声学场景分类等一般视听任务中得到验证。然而，视听讲话的自我监督仍有待探索。我们提出了一种从原始音频波形中学习自监督语音表示的方法。我们通过将纯音频自我监控（通过预测信息音频属性）与视觉自我监控（通过从音频生成对话人脸）相结合来训练原始音频编码器。视觉借口任务驱动音频表示来捕获与嘴唇运动相关的信息。这丰富了音频编码器的视觉信息和编码器可以用于评估没有视觉模态。我们的方法在已建立的孤立词分类基准上与现有的自监督音频特征相比具有竞争力，并且在从较少的标签学习方面显著优于其他方法。值得注意的是，我们的方法也优于完全监督训练，因此为语音相关任务提供了强大的初始化能力。我们的研究结果证明了多模态自我监控在视听讲话中学习良好音频表征的潜力。