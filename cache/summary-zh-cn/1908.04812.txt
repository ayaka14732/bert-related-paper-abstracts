我们主要研究基于检索的对话系统中的多轮应答选择。在本文中，我们将Transformer（BERT）强大的预训练语言模型双向编码器表示用于多回合对话系统，并提出了一种在特定领域语料库上高效的后训练方法。虽然BERT很容易被各种NLP任务采用，并且优于每个任务的先前基线，但如果任务语料库过于关注某个领域，它仍然有局限性。对特定领域语料库（如Ubuntu语料库）的后期培训有助于模型培训一般语料库（如英语维基百科）中未出现的上下文表示和单词。实验结果表明，我们的方法在两个响应选择基准（即Ubuntu语料库V1，Advisory Corpus V1）上实现了新的技术水平，在两个基准上的性能分别提高了5.9%和6%R@1.