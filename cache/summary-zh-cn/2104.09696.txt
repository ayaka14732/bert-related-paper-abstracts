多语言模型，如M-BERT和XLM-R，由于其零次跨语言迁移学习能力而越来越受欢迎。然而，对于不同类型的语言和不同的基准，它们的泛化能力仍然不一致。最近，元学习作为一种在低资源情景下促进迁移学习的有前途的技术受到了关注：特别是自然语言理解中的跨语言迁移。在这项工作中，我们提出了X-METRA-ADA，一种适用于NLU的跨语言元迁移学习适应方法。我们的方法采用MAML，一种基于优化的元学习方法，来学习适应新的语言。我们在两个具有挑战性的跨语言NLU任务上广泛评估了我们的框架：面向多语言任务的对话和类型多样的问题回答。我们表明，我们的方法优于简单的微调，在大多数语言的两个任务上都达到了有竞争力的性能。我们的分析表明，X-METRA-ADA可以利用有限的数据进行更快的适应。