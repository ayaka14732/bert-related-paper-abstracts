在大规模语料库上预训练的神经语言表示模型（如BERT）可以很好地从纯文本中捕获丰富的语义模式，并可以进行微调以持续改进各种自然语言处理任务的性能。然而，现有的预训练语言模型很少考虑结合知识图（KGS），它可以提供丰富的结构化知识事实，以更好地理解语言。我们认为KGs中的信息实体可以通过外部知识增强语言表达。在本文中，我们利用大规模文本语料库和KGs来训练一个增强的语言表示模型（ERNIE），该模型可以同时充分利用词汇、句法和知识信息。实验结果表明，ERNIE在各种知识驱动任务上都取得了显著的改进，同时在其他常见NLP任务上与最先进的模型BERT相当。本文的源代码可以从https://github.com/thunlp/ERNIE.