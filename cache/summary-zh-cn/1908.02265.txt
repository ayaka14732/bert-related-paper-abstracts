我们提出了ViLBERT（Vision and Language BERT的缩写），一个学习图像内容和自然语言的任务不可知联合表示的模型。我们将流行的BERT体系结构扩展到多模态双流模型，在通过共同注意转换层交互的独立流中处理视觉和文本输入。我们在自动收集的大型概念标题数据集上通过两个代理任务对模型进行预训练，然后将其转移到多个已建立的视觉和语言任务——视觉问答、视觉常识推理、引用表达式、，以及基于字幕的图像检索——只需对基础架构进行少量添加。与现有的任务特定模型相比，我们观察到任务之间的显著改进——在所有四项任务上都达到了最先进的水平。我们的工作表明，从仅仅作为任务训练一部分的视觉和语言之间的学习基础转变为将视觉基础视为一种可训练和可转移的能力。