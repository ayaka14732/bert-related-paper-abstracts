近年来，用于多方会话（MPC）的各种神经模型在收件人识别、说话人识别和反应预测等任务上取得了令人印象深刻的改进。然而，现有的MPC方法通常都是将对话者和话语分别表示出来，而忽略了MPC固有的复杂结构，这种结构可以提供关键的对话者和话语语义，从而增强会话理解过程。为此，我们提出了MPC-BERT，这是一个预训练的MPC理解模型，它考虑了在一个统一的模型中学习谁对谁说什么，以及几个精心设计的自我监督任务。具体地说，这些任务一般可分为：（1）对话者结构建模，包括应答话语识别、同一说话人搜索和指针一致性区分；（2）话语语义建模，包括屏蔽共享话语恢复和共享节点检测。我们评估了MPC-BERT的三个下游任务，包括收件人识别、说话人识别和响应选择。实验结果表明，MPC-BERT在两个基准测试中，在所有三个下游任务上都取得了新的最新性能，大大优于以前的方法。