我们介绍了MARGE，一个预先训练好的序列到序列模型，具有无监督的多语言多文档释义目标。MARGE提供了一种替代主流蒙面语言建模范式的方法，在这种范式中，我们通过检索一组相关文本（多种语言）并对其进行条件化处理来自我监督目标文本的重建，以最大限度地提高生成原始文本的可能性。我们表明，只要给定一个随机初始化，就可以共同学习进行检索和重建。目标噪音捕获了释义、翻译、多文档摘要和信息检索等方面，允许在多个任务上实现强大的零拍性能。例如，在没有额外的特定任务培训的情况下，我们在文档翻译方面的BLEU分数高达35.8。我们进一步表明，微调在许多语言中的一系列辨别性和生成性任务中都有很好的表现，使MARGE成为迄今为止最普遍适用的预训练方法。