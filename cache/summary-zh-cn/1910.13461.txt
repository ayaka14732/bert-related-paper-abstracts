我们提出了BART，一种用于预训练序列到序列模型的去噪自动编码器。BART的训练方法是：（1）使用任意噪声函数破坏文本，（2）学习一个模型来重建原始文本。它使用一个标准的基于转换器的神经机器翻译架构，尽管它很简单，可以被看作是概括伯特（由于双向编码器），GPT（与左到右解码器），以及许多其他更近的预训练方案。我们评估了许多去噪方法，通过随机洗牌原始句子的顺序和使用一种新的填充方案，其中文本的跨度被替换为单个掩码标记，找到了最佳性能。当对文本生成进行微调时，BART特别有效，但也适用于理解任务。它将RoBERTa的表现与GLUE和SQuAD上的可比训练资源相匹配，在一系列抽象对话、问答和总结任务上取得了最新成果，最多可获得6个胭脂。BART还为机器翻译提供了比反向翻译系统1.1 BLEU的增加，只有目标语言预培训。我们还报告了在BART框架内复制其他预训练方案的消融实验，以更好地衡量哪些因素对最终任务绩效影响最大。