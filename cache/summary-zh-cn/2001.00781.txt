无监督表征学习的最新发展成功地确立了NLP中迁移学习的概念。主要有三种力量推动了这一研究领域的改进：更精细的体系结构更好地利用了上下文信息。它们不是简单地插入静态的预先训练的表示法，而是基于端到端可训练模型中的环境学习，具有更智能化的语言建模目标。除此之外，更大的语料库被用作资源，用于以自我监督的方式对大型语言模型进行预训练，然后对监督任务进行微调。并行计算和云计算的进步，使得在相同甚至更短的时间内对这些模型进行训练成为可能。这三项发展凝聚在新的最新技术（SOTA）结果中，并以越来越高的频率显示出来。这些改进的来源并不总是显而易见的，因为不可能完全理清三种驱动力的作用。我们致力于提供几个大型预培训语言模型的清晰、简明概述，这些模型在过去两年中在使用新架构和资源方面取得了SOTA成果。我们希望向读者澄清模型之间的差异，并试图进一步深入了解词法/计算改进以及架构更改的单一贡献。我们明确不打算量化这些贡献，而是将我们的工作视为一个概述，以确定基准比较的潜在起点。此外，我们暂时希望指出在开源和可复制研究领域改进的潜在可能性。