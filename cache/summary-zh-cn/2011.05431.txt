在过去的十年中，神经语言建模领域发生了巨大的变化，通过使用变压器结构开发了新的模型。然而，由于内存限制和计算复杂性的增加，即使这些模型也难以对长序列建模。训练数据上的共指注释可以提供远远超出此类语言模型建模限制的上下文。在本文中，我们对神经语言模型中使用的Transformer块结构进行了扩展，特别是在GPT2中，以便在训练期间合并实体注释。我们的模型GPT2E将GPT2的Transformer层架构扩展到实体Transformer，实体Transformer是一种设计用于处理共引用信息的架构。为此，我们实现了实体提及的更丰富表示，而培训成本很低。我们展示了GPT2和GPT2E在CoNLL 2012和LAMBADA数据集上的复杂度方面的比较模型性能，以及实体表示的关键差异及其在下游任务（如命名实体识别）中的影响。此外，我们的方法可以被大多数基于Transformer的语言模型采用。