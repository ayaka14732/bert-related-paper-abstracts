实体表示在涉及实体的自然语言任务中很有用。在本文中，我们提出了新的基于双向变换的单词和实体的预训练上下文化表示。该模型将给定文本中的单词和实体视为独立的标记，并输出它们的上下文表示。我们的模型是使用一个新的预训练任务来训练的，该任务基于伯特的蒙面语言模型。这项任务涉及预测从维基百科检索的大型实体标注语料库中随机隐藏的单词和实体。我们还提出了一种实体感知的自我注意机制，它是transformer自我注意机制的扩展，并在计算注意分数时考虑标记（单词或实体）的类型。所提出的模型在广泛的实体相关任务中取得了令人印象深刻的实证性能。特别是，它在五个著名数据集上获得了最先进的结果：开放实体（实体类型）、TACRED（关系分类）、CoNLL-2003（命名实体识别）、ReCoRD（完形填空式问答）和1.1班（抽取式问答）。我们的源代码和预训练表示可在https://github.com/studio-ousia/luke.