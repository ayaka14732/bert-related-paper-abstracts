最近的大规模视觉语言预训练（VLP）的双流体系结构（如CLIP）具有大量的图像-文本对数据，在各种多模式对齐任务中显示了其优越性。尽管取得了成功，但由于文本编码能力弱，生成的模型无法完成多模态生成任务。为了解决这个问题，我们建议通过视觉语言知识提取（VLKD）将双流VLP模型扩展为文本预训练语言模型（PLM），从而实现多模式生成。与从头开始的预训练相比，VLKD具有相当高的数据效率和计算效率。实验结果表明，该模型在开放式视觉问答和图像字幕等多模式生成任务中具有很强的零拍性能。例如，它在VQAv2数据集上实现了44.5%的零炮精度，超过了以前最先进的零炮模型，参数减少了7倍。此外，在VLKD之后，PLM的原始文本语言理解和生成能力得以保持，这使得我们的模型在多模态和单模态任务中都具有通用性。