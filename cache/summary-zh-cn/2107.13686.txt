预训练语言模型（PLM）在自然语言处理中取得了巨大的成功。大多数PLM遵循BERT中架构超参数的默认设置（例如，在前馈子网络中，隐藏维度是中间维度的四分之一）（Devlin等人，2019）。很少有研究探索BERT中架构超参数的设计，特别是对于更有效的小尺寸PLM，这对于在资源受限设备上的实际部署至关重要。在本文中，我们采用一次性神经结构搜索（NAS）来自动搜索结构超参数。具体而言，我们仔细设计了一次性学习技术和搜索空间，为各种延迟约束提供了一种自适应且高效的微型PLM开发方法。我们将我们的方法命名为AutoTinyBERT，并在GLUE和SQuAD基准上评估其有效性。大量实验表明，我们的方法优于基于SOTA搜索的基线（NAS-BERT）和基于SOTA蒸馏的方法（如DistilBERT、TinyBERT、MiniLM和MobileBERT）。此外，基于所获得的体系结构，我们提出了一种比单个PLM的开发速度更快的更高效的开发方法。