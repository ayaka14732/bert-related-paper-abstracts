大型语言模型（LLM）是如何在训练过程中发展和演变的？这些模式是如何随着模型的缩放而变化的？为了回答这些问题，我们引入了\textit｛Pythia｝，这是一套16个LLM，所有LLM都是在以完全相同的顺序看到的公共数据上训练的，大小从70M到12B不等。我们为16个模型中的每一个提供了154个检查点的公共访问权限，以及下载和重建其精确训练数据加载器的工具，以供进一步研究。我们打算\textit｛Pythia｝促进许多领域的研究，我们提出了几个案例研究，包括记忆方面的新结果、术语频率对少镜头表现的影响，以及减少性别偏见。我们证明，这种高度控制的设置可以用来产生对LLM及其训练动力学的新见解。经过训练的模型、分析代码、训练代码和训练数据可以在https://github.com/EleutherAI/pythia.