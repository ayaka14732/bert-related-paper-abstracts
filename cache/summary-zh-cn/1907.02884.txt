意图检测和时隙填充是自然语言口语理解的两大支柱任务。常见的方法是在基于注意力的循环框架中采用联合深度学习架构。在这项工作中，我们的目标是利用成功的“无重复”模型来完成这些任务。我们介绍了Bert-Joint，即一个多语言联合文本分类和序列标记框架。在两个著名的英语基准测试上的实验评估表明，即使只有很少的注释数据可用，该模型也可以获得很好的性能。此外，我们为意大利语注释了一个新的数据集，我们观察到了类似的性能，而无需更改模型。