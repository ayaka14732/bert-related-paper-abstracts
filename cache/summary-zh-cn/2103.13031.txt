本文描述了第一个基于BERT和ALBERT体系结构的捷克单语语言表示模型的训练过程。我们对我们的模型进行了340K多个句子的预训练，这是包含捷克数据的多语言模型的50倍。在11个数据集中，我们有9个优于多语言模型。此外，我们在九个数据集上建立了最新的最新结果。最后，我们根据我们的结果讨论了单语和多语模型的性质。我们为研究社区免费发布所有经过预培训和微调的模型。