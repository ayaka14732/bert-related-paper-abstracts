代词共指消解（PCR）的任务是将代词表达分解为所有提及的事物。与一般的共指消解任务相比，PCR的主要挑战是共指关系预测，而不是提及检测。作为自然语言理解（NLU）的一个重要组成部分，代词解析对于许多下游任务来说至关重要，但对于现有的模型来说仍然具有挑战性，这促使我们审视现有的方法并思考如何做得更好。在这篇综述中，我们首先介绍了普通代词共指消解任务的代表性数据集和模型。然后，我们关注硬代词共指消解问题（如Winograd模式挑战）的最新进展，以分析当前模型对常识的理解程度。我们进行了大量的实验，以表明即使当前的模型在标准评估集上取得了良好的性能，它们仍然没有准备好用于实际应用（例如，所有SOTA模型都难以正确地将代词解析为不常见的对象）。所有实验代码可在https://github.com/HKUST-KnowComp/PCR.