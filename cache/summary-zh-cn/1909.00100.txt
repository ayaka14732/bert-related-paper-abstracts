我们提出了一个实用的方案来训练一个单一的多语言序列标记模型，该模型可以产生最先进的结果，并且小而快，可以在单个CPU上运行。从公共多语言BERT检查点开始，我们的最终模型比最先进的多语言基线小6倍，快27倍，精确度更高。我们证明了我们的模型在低资源语言上的表现尤其出色，并且可以处理codemixed输入文本，而无需对codemixed示例进行显式训练。我们通过对70个树状词库和48种语言的词性标注和形态预测的报告，展示了我们方法的有效性。