阿拉伯语是一种形态丰富的语言，与英语相比，资源相对较少，句法探索较少。鉴于这些局限性，阿拉伯语自然语言处理（NLP）任务，如情感分析（SA）、命名实体识别（NER）和问答（QA），已被证明是非常具有挑战性的。最近，随着基于transformers的模型的激增，基于特定语言的BERT模型已被证明在语言理解方面非常有效，只要它们是在非常大的语料库上预先训练的。这些模型能够为大多数NLP任务设定新的标准并实现最先进的结果。在本文中，我们专门针对阿拉伯语对BERT进行了预训练，以期获得与英语相同的成功。AraBERT的性能与来自Google和其他最先进方法的多语言BERT进行了比较。结果表明，新开发的AraBERT在大多数经过测试的阿拉伯语NLP任务中都达到了最先进的性能。预训练的araBERT模型在https://github.com/aub-mind/arabert 希望鼓励阿拉伯语NLP的研究和应用。