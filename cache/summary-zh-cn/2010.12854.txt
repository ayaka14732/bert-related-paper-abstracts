我们介绍ReadOnce Transformers，一种将基于transformer的模型转换为能够构建信息捕获、任务独立和文本压缩表示的模型的方法。结果表示可在不同的示例和任务之间重用，因此需要在多个示例或任务之间共享一个文档，\emph{read once}。这将加快模型的培训和评估。此外，我们将标准文本到文本转换器模型扩展为表示+文本到文本模型，并对多个下游任务进行评估：多跳QA、抽象QA和长文档摘要。与标准文本到文本模型相比，我们的一次性计算表示法的速度提高了2-5倍，而压缩还允许现有语言模型处理更长的文档，而无需设计新的预训练模型。