预先训练的语言模型（例如，BERT（Devlin et al.，2018）及其变体）在各种NLP任务中取得了显著的成功。然而，这些模型通常由数亿个参数组成，由于延迟和容量限制，这给实际应用中的微调和在线服务带来了挑战。在这项工作中，我们提出了一种简单有效的方法来压缩大型变压器（Vaswani et al.，2017），该方法基于预先训练的模型，称为深度自我注意蒸馏。小模型（学生）通过深入模仿大模型（教师）的自我注意模块进行训练，自我注意模块在变压器网络中起着至关重要的作用。具体来说，我们建议提取教师最后一层的自我注意模块，这对学生来说是有效和灵活的。此外，除了已有研究中使用的注意分布（即查询和键的缩放点积）外，我们还引入了自我注意模块中值之间的缩放点积作为新的深度自我注意知识。此外，我们还表明，引入一名教师助理（Mirzadeh等人，2019年）也有助于提取大型预培训变压器模型。实验结果表明，在不同参数大小的学生模型中，我们的单语模型优于最新的基线。特别是，它使用50%的变压器参数和教师模型的计算，在2.0班和几个胶水基准任务上保持了99%以上的准确性。在将深度自我注意蒸馏应用于多语言预训练模型方面，我们也取得了有竞争力的结果。