语言模型预训练已被证明能够获取惊人数量的世界知识，这对于NLP任务（如问答）至关重要。然而，这些知识隐含在神经网络的参数中，需要更大的网络来覆盖更多的事实。为了以更加模块化和可解释的方式获取知识，我们使用潜在知识检索器对语言模型预训练进行了增强，该检索器允许模型从大型语料库（如维基百科）中检索和处理文档，这些语料库在预训练、微调和推理过程中使用。我们首次展示了如何以无监督的方式预先训练这样的知识检索器，使用蒙面语言建模作为学习信号，并通过考虑数百万文档的检索步骤进行反向传播。我们通过对具有挑战性的开放领域问答（openqa）任务进行微调，证明了检索增强语言模型预训练（REALM）的有效性。我们在三个流行的开放式QA基准上对比了显式和隐式知识存储的最新模型，发现我们在提供可解释性和模块化等定性优势的同时，显著优于所有以前的方法（绝对准确率为4-16%）。