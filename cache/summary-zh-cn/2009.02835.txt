诸如BERT之类的预先训练的语言模型在广泛的自然语言处理任务中取得了巨大的成功。然而，由于缺乏两个层次的领域知识，即短语层次和产品层次，BERT不能很好地支持与电子商务相关的任务。一方面，许多电子商务任务需要准确理解领域短语，而此类细粒度短语级知识并没有通过BERT的训练目标显式建模。另一方面，产品级知识（如产品关联）可以增强电子商务的语言建模，但它们不是事实知识，因此不加区别地使用它们可能会引入噪声。为了解决这个问题，我们提出了一个统一的预训练框架，即E-BERT。具体来说，为了保留短语级知识，我们引入了自适应混合掩蔽，该掩蔽允许模型根据两种模式的拟合过程，从学习初始单词知识自适应地切换到学习复杂短语。为了利用产品级知识，我们引入了邻域产品重构，它训练E-BERT通过去噪交叉注意层预测产品的相关邻域。我们的调查显示，在四个下游任务中，即基于评论的问题回答、方面提取、方面情感分类和产品分类，都有很好的结果。