在这篇文章中，我们讨论了服装行业跨模式检索中的文本和图像匹配问题。与一般领域的匹配不同，服装匹配需要更多地关注服装图像和文本中的细粒度信息。先锋方法从图像中检测感兴趣区域（即RoI），并使用RoI嵌入作为图像表示。一般来说，ROI倾向于表示时尚图像中的“对象级别”信息，而时尚文本倾向于描述更详细的信息，例如样式、属性。因此，对于时尚文本和图像匹配而言，ROI不够细粒度。为此，我们提出FashionBERT，它利用补丁作为图像特征。FashionBERT将预先训练好的BERT模型作为主干网络，学习文本和图像的高级表示。同时，在FashionBERT模型中，我们提出了一种自适应损失来平衡多任务学习。两个任务（即，文本和图像匹配和跨模态检索）被合并来评估FashionBERT。在公共数据集上，实验表明FashionBERT在性能上比基线和最先进的方法有显著的改进。在实践中，FashionBERT应用于一个具体的跨模态检索应用程序。我们提供了详细的匹配性能和推理效率分析。