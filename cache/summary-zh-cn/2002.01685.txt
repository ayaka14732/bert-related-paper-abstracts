最近的分析表明，为语言建模而预先训练的编码器捕获了某种形态句法结构。然而，单词向量的探测框架仍然不会报告标准设置（如成分和依赖项解析）的结果。本文解决了这个问题，并仅依靠预训练架构（而非解码）进行完整解析（英语）。我们首先将成分和依赖项解析转换为序列标记。然后，我们使用一个前馈层直接将字向量映射到编码线性化树的标签。这是用来：（i）看看我们可以在语法建模方面只使用预训练编码器走多远，（ii）阐明不同词向量的语法敏感性（通过在训练期间冻结预训练网络的权重）。为了进行评估，我们使用括号中的F1分数和LAS，并分析跨度长度和相关性位移表示的深度差异。总体结果超过了PTB（93.5%）和端到端EN-EWT UD（78.8%）上现有的序列标签解析器。