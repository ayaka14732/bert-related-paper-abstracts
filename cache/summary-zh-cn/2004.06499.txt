深入了解BERT的内部工作过程表明，它的层类似于经典的NLP管道，越来越复杂的任务集中在后面的层中。为了研究这些结果在多大程度上也适用于英语以外的语言，我们探索了一个基于荷兰BERT的模型和荷兰NLP任务的多语言BERT模型。此外，通过对词性标注的深入分析，我们发现，在给定的任务中，信息分布在网络的不同部分，管道可能不像看上去那么整洁。每一层都有不同的专业知识，因此将不同层的信息结合起来可能更有用，而不是根据最佳的总体性能选择单个层。