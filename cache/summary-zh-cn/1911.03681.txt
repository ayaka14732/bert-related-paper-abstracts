我们提出了一种将实体的事实知识注入预训练的BERT模型的新方法（Devlin等人，2019）：我们将Wikipedia2Vec实体向量（Yamada等人，2016）与BERT的原生词条向量空间对齐，并将对齐的实体向量当作词条向量使用。由此产生的实体增强版BERT（称为E-BERT）在精神上与ERNIE（Zhang等人，2019）和KnowBert（Peters等人，2019）类似，但不需要对BERT编码器进行昂贵的进一步预培训。我们在无监督问答（QA）、监督关系分类（RC）和实体链接（EL）三个方面对E-BERT进行了评估。在所有三项任务中，E-BERT都优于BERT和其他基线。我们还定量地表明，原始的伯特模型过度依赖实体名称的表面形式（例如，猜测某人的名字听起来像意大利语），而e-伯特缓解了这个问题。