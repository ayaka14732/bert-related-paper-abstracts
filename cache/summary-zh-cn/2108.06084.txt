最近的工作表明，在大量未标记文本语料库上训练大容量自回归语言模型（GPT，GPT-2，GPT-3）以生成文本是非常成功的。尽管取得了很好的结果，但自回归模型面临着越来越严重的训练不稳定性问题。我们对GPT-2模型（117M和1.5B参数）的研究表明，较大的模型大小、序列长度、批量大小和学习率将导致较低的训练稳定性和增加分歧风险。为了避免发散和获得更好的泛化性能，必须使用较小的批量和学习率进行训练，这会导致训练效率降低和训练时间延长。为了克服这种稳定性-效率困境，我们提出了一种基于课程学习的方法，这有助于提高自回归模型的训练前收敛速度。更重要的是，我们发现课程学习作为一种正则化方法，具有梯度方差缩减效应，能够训练具有更大批量和学习率的自回归模型，而不存在训练不稳定性，进一步提高了训练速度。我们的评估表明，课程学习使GPT-2模型的培训批量增加了8倍，学习率提高了4倍，而基线方法则难以解决培训分歧。为了在预培训期间实现相同的验证困惑目标，课程学习将所需的代币数量和挂钟时间分别减少了61%和49%。为了在预培训结束时获得相同或更好的零拍WikiText-103/LAMBADA评估结果，课程学习将所需的代币数量和挂钟时间分别减少54%和70%。