经过预先训练的语言模型已经证明了它们在捕获隐式语言特征方面的独特能力。然而，大多数预训练方法侧重于单词级训练目标，而很少研究句子级目标。在本文中，我们提出了句子表征对比学习（CLEAR），它采用多种句子级增强策略来学习噪声不变的句子表征。这些增强包括单词和跨度的删除、重新排序和替换。此外，我们还通过大量实验探讨了对比学习有效性的关键原因。我们观察到，在训练前不同的句子强化会导致不同下游任务的不同表现改善。我们的方法在SentEval和GLUE基准上都优于现有的多种方法。