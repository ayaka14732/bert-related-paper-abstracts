微调预训练语言模型（PTLM），如伯特及其更好的变体罗伯塔，已成为提高自然语言理解（NLU）任务性能的常见做法。表征学习的最新进展表明，各向同性（即单位方差和不相关）嵌入可以显著提高下游任务的性能，具有更快的收敛速度和更好的泛化能力。然而，PTLMs中预训练嵌入物的各向同性研究相对较少。在本文中，我们用直观的可视化方法分析了预训练的PTLM[CLS]嵌入的各向同性，并指出了两个主要问题：它们的标准偏差的高方差和不同维度之间的高相关性。我们还提出了一种新的网络正则化方法，各向同性批量归一化（IsoBN）来解决这个问题，通过动态惩罚主成分，在微调中学习更多各向同性表示。这种简单而有效的微调方法在七个NLU任务的平均值上产生约1.0的绝对增量。