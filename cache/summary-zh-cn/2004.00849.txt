我们建议像素伯特通过深度多模态变换器将图像像素与文本对齐，这些变换器在统一的端到端框架中共同学习视觉和语言嵌入。我们的目标是直接从图像和句子对建立图像像素和语言语义之间更准确、更彻底的联系，而不是将基于区域的图像特征作为最新的视觉和语言任务。我们提出的像素级的语义连接解决了视觉和语言任务中任务特定视觉表示的局限性。它还减轻了包围盒标注的成本，克服了视觉任务中语义标签与语言语义之间的不平衡。为了更好地表示下游任务，我们预先训练了一个通用的端到端模型，该模型包含来自可视基因组数据集和MS-COCO数据集的图像和句子对。我们建议使用随机像素采样机制来增强视觉表示的鲁棒性，并将蒙面语言模型和图像文本匹配作为预训练任务。利用我们预先训练好的模型对下游任务进行的大量实验表明，我们的方法在下游任务中发挥了最大的作用，包括视觉问答（VQA）、图像文本检索、真实视觉推理自然语言（NLVR）。特别是，在公平比较的情况下，与SOTA相比，我们在VQA任务中单个模型的性能提高了2.17个点。