在这项工作中，我们研究了语言预训练中使用的位置编码方法（例如，BERT），并确定了现有公式中的几个问题。首先，我们证明了在绝对位置编码中，对位置嵌入和单词嵌入的加法运算在两种异构信息资源之间带来了混合相关性。它可能会在注意中带来不必要的随机性，并进一步限制模型的表达能力。第二，考虑到符号在下游任务中的特殊作用（整个句子的表示），我们质疑将符号的位置与其他单词一样对待是否是合理的设计。基于上述分析，我们提出了一种新的位置编码方法，称为\textbf{T}变压器，具有\textbf{U}位\textbf{P}位\textbf{E}编码（TUPE）。在自我注意模块中，TUPE使用不同的参数化分别计算单词上下文相关性和位置相关性，然后将它们相加。该设计消除了异构嵌入中的混合和噪声相关性，并通过使用不同的投影矩阵提供了更高的表达能力。此外，TUPE将\texttt{[CLS]}符号从其他位置解开，从而更容易从所有位置捕获信息。大量实验和胶基准烧蚀研究证明了该方法的有效性。代码和型号发布于https://github.com/guolinke/TUPE.