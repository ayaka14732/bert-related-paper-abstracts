大型预先训练的多语言模型，如mBERT、XLM-R，可以在语言理解任务中获得最先进的结果。但是，它们不太适合服务器和边缘设备上的延迟关键型应用程序。减少这些模型所需的内存和计算资源非常重要。为此，我们提出了pQRNN，这是一种基于投影的无嵌入神经编码器，它对于自然语言处理任务来说非常微小和有效。在没有预训练的情况下，尽管PQRNN比LSTM模型小140倍，但PQRNN的性能明显优于预训练嵌入的LSTM模型。在参数数量相同的情况下，它们的性能优于变压器基线，从而显示了它们的参数效率。此外，我们还表明PQRNN是提取大型预训练语言模型的有效学生体系结构。我们进行仔细的烧蚀，研究pQRNN参数、数据增加和蒸馏设置的影响。在具有挑战性的多语言语义分析数据集MTOP上，pQRNN学生的成绩达到mBERT教师的95.9\%，但比mBERT教师小350倍。在mATIS（一种流行的语法分析任务）上，pQRNN的学生平均能够接触到97.1%的老师，同时又小了350倍。我们强有力的结果表明，我们的方法非常适合于对延迟敏感的应用程序，同时能够利用类似mBERT的大型模型。