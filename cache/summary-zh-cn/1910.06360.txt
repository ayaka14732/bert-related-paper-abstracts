行业背景自然语言处理（NLP）研究的最新趋势是在严格的计算限制下操作大规模预训练语言模型，如BERT。虽然大多数模型压缩工作都集中在使用昂贵的预训练蒸馏来“蒸馏”通用语言表示，但较少关注创建更小的任务特定语言表示，可以说，这些语言表示在行业环境中更有用。在本文中，我们研究了通过对底层变压器模型的参数进行结构化剪枝来压缩基于BERT和RoBERTa的问答系统。我们发现，任务特定结构化剪枝和任务特定蒸馏的廉价组合，不需要预先训练蒸馏的费用，可以在一系列速度/精度折衷操作点上产生高性能的模型。我们从为2.0班或自然问题培训的现有全尺寸模型开始，介绍允许单独消除变压器选定部件的闸门。具体而言，我们研究（1）结构化修剪以减少每个变压器层中的参数数量，（2）适用于基于BERT和RoBERTa的模型，（3）适用于2.0班和自然问题，以及（4）将结构化修剪与蒸馏相结合。我们实现了推理速度的近两倍，在自然问题的简短回答准确度上损失不到0.5分。