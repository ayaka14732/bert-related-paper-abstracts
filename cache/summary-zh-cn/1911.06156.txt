在一些NLP任务中，基于注意的模型比传统算法有显著的改进。例如，Transformer是一个说明性示例，它根据输入到编码器的令牌与序列中所有令牌的关系生成其抽象表示。最近的研究表明，尽管这些模型能够通过查看示例来学习句法特征，但将这些信息显式地反馈给深度学习模型可以显著提高其性能。在复杂模型（如Transformer）的有限训练数据设置中，利用诸如词性（POS）之类的句法信息可能特别有益。我们表明，当在完整的WMT 14英译德翻译数据集上进行训练时，具有多种功能的语法注入转换器实现了0.7 BLEU的改进，当在数据集的一小部分上进行训练时，最大提高了1.99 BLEU点。此外，我们发现，将语法合并到BERT微调中，在GLUE基准的许多下游任务上都优于基线。