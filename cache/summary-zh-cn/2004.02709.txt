有监督学习的标准测试集评估分布泛化。不幸的是，当一个数据集存在系统性缺口（例如注释工件）时，这些评估是误导性的：模型可以学习简单的决策规则，这些规则在测试集上表现良好，但不能捕获数据集的预期功能。我们提出了一种新的NLP注释范式，有助于填补测试数据中的系统性缺口。特别是，在构建数据集之后，我们建议数据集作者以小而有意义的方式手动扰动测试实例，从而（通常）更改金标签，创建对比度集。对比集提供了模型决策边界的局部视图，可用于更准确地评估模型的真实语言能力。我们通过为10个不同的NLP数据集（例如，DROP阅读理解、UD解析、IMDb情绪分析）创建对比集来证明对比集的有效性。虽然我们的对比集没有明显的对抗性，但它们的模型性能明显低于原始测试集——在某些情况下高达25%。我们将对比集作为新的评估基准发布，并鼓励未来的数据集构建工作遵循类似的注释过程。