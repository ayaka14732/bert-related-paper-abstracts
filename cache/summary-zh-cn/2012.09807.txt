通过~\textit{prod2vec}，单词嵌入（例如word2vec）已成功应用于电子商务产品。受上下文化嵌入带来的几个NLP任务的最近性能改进的启发，我们建议将类似于BERT的体系结构转移到电子商务：我们的模型--~\textit{Prod2BERT}经过训练，可以通过屏蔽会话建模生成产品的表示。通过在多个车间、不同任务和一系列设计选择上的广泛实验，我们系统地比较了~\textit{Prod2BERT}和~\textit{prod2vec}嵌入的准确性：虽然~\textit{Prod2BERT}在几种情况下都具有优势，我们强调资源和超参数在最佳性能模型中的重要性。最后，我们为实践者提供了在各种计算和数据约束下培训嵌入的指南。