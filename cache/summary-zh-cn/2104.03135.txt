我们研究了卷积神经网络（CNN）和Transformer for vision language pre training（VLPT）的联合学习，旨在从数百万图像-文本对中学习跨模式对齐。最先进的方法提取显著图像区域，并逐步将区域与单词对齐。由于基于区域的视觉特征通常代表图像的一部分，现有的视觉语言模型很难完全理解成对自然语言的语义。在本文中，我们建议SOHO“开箱即看”，以整个图像作为输入，并以端到端的方式学习视觉语言表示。SOHO不需要边界框注释，其推理速度比基于区域的方法快10倍。特别是，SOHO学习通过视觉词典（VD）提取全面而紧凑的图像特征，这有助于跨模态理解。VD被设计用来表示语义相似的一致的视觉抽象。它是动态更新的，并用于我们提出的训练前任务掩蔽视觉建模（MVM）。我们按照标准的VLPT设置，在四个成熟的视觉语言任务上进行实验。特别是，SOHO实现了2.0%的绝对收益R@1MSCOCO文本检索5k测试分割得分，NLVR$^2$test-P分割准确率为1.5%，SNLI-VE测试分割准确率为6.7%。