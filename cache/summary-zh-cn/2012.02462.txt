最近，在下游利用预先训练好的基于转换器的语言模型，任务特定模型在自然语言理解任务方面取得了先进的成果。然而，只有很少的研究探讨了这种方法在资源不足、训练数据点少于1000个的环境中的适用性。在这项工作中，我们探索了BERT（一种预训练的基于转换器的语言模型）的微调方法，通过使用基于池的主动学习来加速训练，同时保持标记新数据的成本不变。我们在GLUE数据集上的实验结果表明，当从未标记数据池中查询时，通过最大化模型的近似知识增益，在模型性能方面具有优势。最后，我们演示并分析了在微调过程中冻结语言模型层的好处，以减少可训练参数的数量，使其更适合低资源设置。