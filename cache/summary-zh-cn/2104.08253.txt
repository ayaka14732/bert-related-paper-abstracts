预先训练的Transformer语言模型（LM）已经成为文本表示编码器。先前的研究对深层LMs进行了微调，以将文本序列（如句子和段落）编码为单个密集向量表示，从而实现高效的文本比较和检索。然而，密集编码器需要大量的数据和复杂的技术，才能在低数据情况下有效地进行训练。本文发现一个关键原因是标准LMs的内部注意结构不适合密集编码器，需要将文本信息聚合到密集表示中。我们建议使用一种新的变压器结构——电容器对密集编码器进行预训练，其中LM预测条件基于密集表示。我们的实验表明，Concerter在各种文本检索和相似性任务上比标准LM有很大的提高。