Lite-BERT（ALBERT）被引入到自然语言的深度双向表征学习中。由于缺乏针对韩国语的预训练的ALBERT模型，最好的可用实践是多语言模型或求助于任何其他基于BERT的模型。在本文中，我们开发并预训练KoreALBERT，一个专门用于韩语理解的单语ALBERT模型。我们引入了一个新的训练目标，即语序预测（WOP），并将其与现有的MLM和SOP标准一起用于相同的体系结构和模型参数。尽管模型参数明显较少（因此训练速度更快），但我们的预训练KoreALBERT在6种不同NLU任务上的表现优于其对应的BERT。与Lan等人在英语中的实证结果一致，KoreALBERT似乎改善了涉及韩语多句编码的下游任务绩效。预先培训的KoreALBERT可公开获取，以鼓励韩国NLP的研究和应用开发。