在这项工作中，我们重点关注自然语言生成（NLG）任务的监督信号在多种语言之间的传递。我们建议在单语言和跨语言设置下预训练序列到序列模型的编码器和解码器。训练前目标鼓励模型在共享空间中表示不同的语言，这样我们就可以进行零镜头跨语言迁移。在预训练过程之后，我们使用单语数据对下游NLG任务的预训练模型进行微调。然后，用单一语言训练的序列到序列模型可以直接在该语言之外进行评估（即，接受多语言输入并产生多语言输出）。在问题生成和抽象摘要方面的实验结果表明，该模型在零镜头跨语言生成方面优于基于机器翻译的流水线方法。此外，跨语言迁移通过利用丰富的资源语言数据提高了低资源语言的自然语言转换性能。我们的实施和数据可在https://github.com/CZWin32768/xnlg.