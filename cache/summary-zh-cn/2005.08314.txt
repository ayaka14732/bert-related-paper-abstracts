近年来，基于文本的自然语言（NL）理解任务的预训练语言模型（LMs）蓬勃发展。此类模型通常针对自由形式的NL文本进行训练，因此可能不适用于对结构化数据进行语义解析等任务，这些任务需要对自由形式的NL问题和结构化表格数据（如数据库表）进行推理。在本文中，我们提出了TaBERT，一种预训练的LM，它可以联合学习NL语句和（半）结构化表的表示。塔伯特在2600万张表格及其英语上下文的大型语料库上接受培训。在实验中，使用TaBERT作为特征表示层的神经语义解析器在具有挑战性的弱监督语义解析基准WikiTableQuestions上获得了新的最佳结果，同时在文本到SQL数据集Spider上具有竞争力。该模型的实施将在http://fburl.com/TaBERT .