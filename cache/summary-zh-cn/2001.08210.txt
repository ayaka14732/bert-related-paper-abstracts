本文证明了多语言去噪预训练可以在各种机器翻译（MT）任务中产生显著的性能增益。我们提出了mBART——一种使用BART目标在多种语言的大规模单语语料库上预先训练的序列对序列去噪自动编码器。mBART是最早通过去除多语言全文中的噪声来预训练完整序列到序列模型的方法之一，而以前的方法只关注编码器、解码器或重建部分文本。预训练一个完整的模型可以直接对其进行微调，以实现有监督（句子级和文档级）和无监督的机器翻译，而无需进行特定任务的修改。我们证明，添加mBART初始化在除最高资源设置外的所有设置中都会产生性能提升，包括低资源MT高达12个BLEU点，以及许多文档级别和无监督模型超过5个BLEU点。我们还表明，它还可以实现无双文本或不在训练前语料库中的新类型的语言对迁移，并对哪些因素对训练前的有效性贡献最大进行了广泛的分析。