大规模预训练语言模型的快速发展极大地增加了对模型压缩技术的需求，其中量化是一种流行的解决方案。在本文中，我们提出了二值化，它通过加权二值化将伯特量化推向极限。我们发现，由于二元BERT的复杂和不规则的损失情况，二元BERT比三元BERT更难直接训练。因此，我们提出了三元权重拆分，它通过从一个半尺寸的三元网络中等效拆分来初始化二进制伯特。因此，二元模型继承了三元模型的良好性能，并且可以在拆分后通过微调新的体系结构来进一步增强。实验结果表明，与全精度模型相比，我们的BinaryBERT的性能仅略有下降，但比全精度模型小24倍，在GLUE和SQuAD基准上实现了最先进的压缩结果。