目前自然语言生成的预训练工作很少关注下游任务的暴露偏差问题。为了解决这个问题，我们提出了一个增强的多流序列到序列预训练和微调框架ERNIE-GEN，该框架使用填充生成机制和噪声感知生成方法来弥补训练和推理之间的差异。为了使生成更接近人类的书写模式，该框架引入了一个逐跨生成流程，该流程训练模型连续预测语义完整的跨，而不是逐字预测。与现有的预训练方法不同，ERNIE-GEN采用多粒度目标采样来构造预训练数据，增强了编码器和解码器之间的相关性。实验结果表明，ERNIE-GEN在一系列语言生成任务中，包括抽象摘要（Gigaword和CNN/DailyMail）、问题生成（团队）、对话生成（人物角色聊天），只需少量的预训练数据和参数，即可获得最先进的结果和生成性问答（CoQA）。