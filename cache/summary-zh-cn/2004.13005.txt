最近发展了多种神经语言模型，如BERT和XLNet，并在各种自然语言处理任务（包括句子分类、问答和文档排序）中取得了令人印象深刻的结果。在本文中，我们探讨了在跨语言信息检索任务中使用流行的双向语言模型BERT来建模和学习英语查询与外语文档之间的相关性。介绍了一种基于BERT的深度相关匹配模型，并利用来自平行语料库的CLIR训练数据，对一个具有弱监督的预训练多语言BERT模型进行了微调。立陶宛文文档检索的实验结果表明，我们的模型是有效的，优于竞争性基线方法。