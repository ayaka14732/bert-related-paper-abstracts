对比学习在计算机视觉中被用来学习图像的高质量表示。然而，由于缺乏一种通用的文本数据扩充方法，对比学习在自然语言处理中的应用并不广泛。在这项工作中，我们探索了使用对比学习的方法来改进关系抽取的BERT模型中的文本表示。该框架的关键在于通过将语言知识无缝集成到数据扩充中，为关系抽取任务定制了独特的对比预训练步骤。此外，我们还研究了从外部知识库构建的大规模数据如何增强BERT对比预训练的通用性。在三个关系提取基准数据集上的实验结果表明，我们的方法可以改进BERT模型的表示，并实现最先进的性能。此外，我们还通过对比预训练的BERT更依赖于预测的基本原理来探索模型的可解释性。我们的代码和数据可在以下网站公开获取：https://github.com/udel-biotm-lab/BERT-CLRE.