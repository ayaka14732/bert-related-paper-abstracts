虽然人们已经对预训练语言模型（LMs）的行为进行了彻底的研究，但很少对预训练期间发生的事情进行研究。因此，我们研究了从一组随机初始化的参数到全能语言模型的发展过程，我们称之为预训练语言模型的胚胎学。我们的研究结果表明，ALBERT在训练前以不同的学习速度学习重建和预测不同词性（POS）的标记。我们还发现，语言知识和世界知识通常不会随着训练前的进行而提高，下游任务的绩效也不会提高。这些发现表明，在预训练过程中，对预训练模型的了解会有所不同，并且有更多的预训练步骤并不一定能为模型提供更全面的知识。我们将提供源代码和预训练模型，以在https://github.com/d223302/albert-embryology.