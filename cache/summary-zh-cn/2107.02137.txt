预先训练的模型在各种自然语言处理（NLP）任务中取得了最先进的结果。最近的工作，如T5和GPT-3表明，扩大预先训练的语言模型可以提高它们的泛化能力。特别是，具有1750亿个参数的GPT-3模型显示了其强大的任务不可知的零炮/少炮学习能力。尽管取得了成功，但这些大规模模型都是在纯文本上训练的，没有引入语言知识和世界知识等知识。此外，大多数大型模型都是以自回归的方式进行训练的。因此，这种传统的微调方法在解决下游语言理解任务时表现出相对较弱的性能。为了解决上述问题，我们提出了一个统一的框架ernie3.0，用于大规模知识增强模型的预训练。它融合了自回归网络和自编码网络，使得训练后的模型可以很容易地适应自然语言理解和生成任务，具有零镜头学习、少量镜头学习或微调功能。我们在一个由纯文本和大规模知识图组成的4TB语料库上用100亿个参数训练了该模型。实证结果表明，该模型在54项中文NLP任务上优于最先进的模型，其英文版在SuperGLUE基准（2021年7月3日）上获得第一名，超过人类绩效+0.8%（90.6%对89.8%）。