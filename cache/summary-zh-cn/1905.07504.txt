最近的一些进展，如GPT和BERT，已经表明成功地结合了预先训练的transformer语言模型和微调操作，以改进下游NLP系统。然而，该框架在有效整合来自其他相关任务的监督知识方面仍然存在一些基本问题。在本研究中，我们研究了一个可转移的BERT（TransBERT）训练框架，该框架不仅可以从大规模未标记数据中转移一般语言知识，还可以从各种语义相关的监督任务中转移特定类型的知识，用于目标任务。特别地，我们建议利用三种迁移任务，包括自然语言推理、情感分类和下一个动作预测，在预训练模型的基础上进一步训练BERT。这使模型能够更好地初始化目标任务。我们以故事结尾预测为目标任务进行实验。最终结果的准确率为91.8%，大大优于以前最先进的基线方法。几个对比实验为如何选择迁移任务提供了一些有益的建议。错误分析显示了基于伯特的故事结局预测模型的优缺点。