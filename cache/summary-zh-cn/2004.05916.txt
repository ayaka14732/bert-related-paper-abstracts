我们深入研究了transformer体系结构中自我注意头的行为。鉴于最近的工作不鼓励使用注意分布来解释模型的行为，我们表明注意分布仍然可以提供对注意头的局部行为的洞察。通过这种方式，我们提出了注意所揭示的局部模式和参照输入的全局模式之间的区别，并从这两个角度分析了伯特。我们使用梯度属性来分析注意头的输出如何依赖于输入标记，有效地扩展了基于局部注意的分析，以解释整个转换层中的信息混合。我们发现，注意和归因分布之间存在着显著的差异，这是由模型内部的语境混合造成的。我们量化了这一差异，并观察到有趣的是，尽管存在混合，但在所有层中仍存在一些模式。