语言模型预训练在学习通用语言表征方面已被证明是有用的。作为一种最先进的语言模型预训练模型，BERT（来自变压器的双向编码器表示）在许多语言理解任务中取得了惊人的成果。在本文中，我们进行了详尽的实验来研究文本分类任务中不同的BERT微调方法，并为BERT微调提供了一个通用的解决方案。最后，提出的解决方案在八个广泛研究的文本分类数据集上获得了最新的结果。