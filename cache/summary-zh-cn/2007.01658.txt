本文介绍了瑞典国家图书馆（KB）数据驱动研究KBLab开发的瑞典BERT（“KB-BERT”）。基于最近为英语以外的语言创建基于转换器的BERT模型的努力，我们将解释如何使用KB的集合为瑞典语创建和训练新的特定于语言的BERT模型。我们还展示了我们的模型与现有模型（主要由瑞典公共就业服务局、Arbetsf \“ormedlingen和Google的多语言M-BERT生成）的比较结果，其中我们证明KB-BERT在从命名实体识别（NER）到词性标记（POS）的一系列NLP任务中都优于这些模型。我们的讨论强调了由于缺乏针对小型语言（如瑞典语）的培训数据和测试平台，仍然存在的困难。我们发布了我们的模型，以供进一步探索和研究：https://github.com/Kungbib/swedish-bert-models .