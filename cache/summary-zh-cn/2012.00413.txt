预先训练的语言模型（PLM）已被证明对各种下游NLP任务是有益的。最近，具有1750亿个参数和570GB训练数据的GPT-3由于具有少量（甚至零次）学习的能力而引起了广泛关注。然而，由于GPT-3的训练语料库主要是英语，而且参数尚未公开，因此应用GPT-3来处理中文NLP任务仍然具有挑战性。在本技术报告中，我们发布了中文预训练语言模型（CPM），该模型对大规模中文训练数据进行生成性预训练。据我们所知，拥有26亿个参数和100GB中文训练数据的CPM是最大的中文预训练语言模型，它可以促进几个后续中文NLP任务，如对话、论文生成、完形填空测试和语言理解。大量的实验表明，CPM在少数镜头（甚至是零镜头）学习的情况下，在许多NLP任务上取得了很好的性能。有关代码和参数，请访问https://github.com/TsinghuaAI/CPM-Generate.