深度注意模型促进了跨多个领域的序列数据建模。特别是在语言建模方面，Transformer XL——一种使用过去激活的远程内存进行扩充的转换器——已被证明在各种经过充分研究的基准测试中是最先进的。Transformer XL在网络的每一层都集成了一个远程内存，这使得它的状态比RNN的前身大数千倍。然而，目前尚不清楚这是否必要。我们进行了一系列的干预，以表明在长程记忆减少6倍的情况下可以获得类似的表现，并且通过限制网络较低层的注意力范围可以获得更好的表现。