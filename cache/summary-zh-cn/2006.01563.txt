命名实体识别（NER）通常被称为序列分类任务，其中每个输入由一句文本组成。然而，很明显，对于这项任务来说，有用的信息往往可以在单句语境的范围之外找到。最近提出的自我注意模型（如BERT）既能有效地捕捉输入中的远距离关系，又能表示由多个句子组成的输入，为在自然语言处理任务中加入跨句子信息的方法创造了新的机会。在这篇论文中，我们提出了一个系统的研究，探讨了在五种语言中，使用BERT模型，使用跨句信息进行NER。我们发现，在BERT输入中添加额外句子形式的上下文系统地提高了所有测试语言和模型的NER性能。在每种输入中包含多个句子也可以让我们在不同的上下文中研究相同句子的预测。我们提出了一种简单的方法，上下文多数投票（CMV），将不同的句子预测结合起来，并证明了这一点，以进一步提高BERT的NER性能。我们的方法不需要对基础的BERT体系结构进行任何更改，而是依赖于用于训练和预测的重构示例。对已建立的数据集（包括CoNLL'02和CoNLL'03 NER基准）的评估表明，我们提出的方法可以改进英语、荷兰语和芬兰语的最新NER结果，实现基于德语的最佳报告BERT结果，并且与西班牙语中其他基于伯特的方法报告的性能相当。我们在开放许可证下发布本工作中实现的所有方法。