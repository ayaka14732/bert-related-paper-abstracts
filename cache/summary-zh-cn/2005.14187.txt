转换器在自然语言处理（NLP）任务中无处不在，但由于计算量大，很难在硬件上部署。为了在资源受限的硬件平台上实现低延迟推断，我们建议使用神经架构搜索设计硬件感知转换器（HAT）。我们首先用$\textit{arbitral encoder decoder attention}$和$\textit{heterogeneous layers}$构建一个大的设计空间。然后，我们训练一个$\textit{SuperTransformer}$，它覆盖设计空间中的所有候选对象，并通过权重共享有效地生成许多$\textit{SubTransformer}$。最后，我们使用硬件延迟约束执行演化搜索，以找到专用于在目标硬件上快速运行的$\textit{SubTransformer}$。对四个机器翻译任务的大量实验表明，HAT可以发现不同硬件（CPU、GPU、物联网设备）的有效模型。在Raspberry Pi-4上运行WMT'14翻译任务时，HAT可以实现$\textbf{3}\times$加速，比基线转换器小$\textbf{3.7}\times$$\textbf{2.7}\times$加速比，$\textbf{3.6}\times$比演进的Transformer更小，搜索成本减少$\textbf{12041}\times$，并且没有性能损失。HAT代码是https://github.com/mit-han-lab/hardware-aware-transformers.git