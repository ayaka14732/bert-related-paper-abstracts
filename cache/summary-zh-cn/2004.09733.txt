最近，预训练语言模型大多遵循先训练后微调的范式，并在各种下游任务中取得了优异的性能。然而，由于预训练阶段通常是任务不可知的，而微调阶段通常受到监督数据不足的影响，因此模型不能很好地捕获特定于领域和特定于任务的模式。在本文中，我们提出了一个三阶段的框架，通过在一般预训练和微调之间添加一个具有选择性掩蔽的任务引导预训练阶段。在这一阶段，通过对域内无监督数据进行蒙蔽语言建模来训练模型，以学习特定于域的模式，我们提出了一种新的选择性蒙蔽策略来学习特定于任务的模式。具体来说，我们设计了一种方法来测量序列中每个令牌的重要性，并选择性地屏蔽重要令牌。在两个情绪分析任务上的实验结果表明，我们的方法在计算量不到50%的情况下可以达到相当甚至更好的性能，这表明我们的方法是有效和高效的。本文的源代码可以从https://github.com/thunlp/SelectiveMasking.