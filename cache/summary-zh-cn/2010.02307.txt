数据到文本的生成由于其广泛的应用，最近吸引了大量的兴趣。现有方法在一系列任务上表现出了令人印象深刻的性能。然而，它们依赖于每个任务的大量标记数据，获取成本很高，因此限制了它们在新任务和域中的应用。在本文中，我们建议利用预培训和迁移学习来解决这个问题。我们提出了一种基于知识的预训练（KGPT），它由两部分组成：1）生成知识丰富文本的基于一般知识的生成模型。2） 一个基于网络的海量知识文本语料库的预训练范例。预先训练的模型可以在各种数据到文本生成任务上进行微调，以生成特定于任务的文本。我们采用三种设置，即完全监督、零炮、少炮来评估其有效性。在完全监督的情况下，我们的模型可以在已知的基线上获得显著的增益。在零炮设置下，我们的模型在没有看到任何示例的情况下，在WebNLG上实现了超过30个胭脂-L，而所有其他基线都失败了。在少数镜头设置下，我们的模型只需要大约十五分之一的标记示例就可以达到与基线模型相同的性能水平。这些实验一致证明了我们提出的框架具有很强的泛化能力https://github.com/wenhuchen/KGPT.