现有的工作，包括ELMO和BERT，揭示了NLP任务预培训的重要性。虽然不存在一个在所有情况下都能发挥最佳效果的单一培训前模型，但有必要开发一个能够有效部署各种培训前模型的框架。为此，我们提出了一个按需组装预培训工具包，即通用编码器表示（UER）。UER是松散耦合的，并用丰富的模块封装。通过按需组装模块，用户可以复制最先进的培训前模型，也可以开发尚未开发的培训前模型。利用UER，我们建立了一个模型动物园，其中包含基于不同语料库、编码器和目标（目标）的预训练模型。通过适当的预训练模型，我们可以在一系列下游数据集上获得最新的结果。