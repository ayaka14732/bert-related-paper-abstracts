在自然语言处理中，对预先训练好的上下文单词嵌入模型进行微调，以使其适用于有监督的下游任务已经变得很普遍。然而，这个过程通常是脆弱的：即使具有相同的超参数值，不同的随机种子也可能导致截然不同的结果。为了更好地理解这一现象，我们对GLUE基准测试中的四个数据集进行了实验，对每个数据集进行数百次微调，同时只改变随机种子。我们发现，与以前报告的结果相比，性能有了显著提高，并且我们量化了最佳模型的性能如何随微调试验数量的变化而变化。此外，我们还考察了随机种子选择的两个影响因素：权重初始化和训练数据顺序。我们发现，这两种方法对样本外性能的差异都有相当大的贡献，并且一些权重初始化在所研究的所有任务中都表现良好。在小数据集上，我们观察到许多微调试验在训练过程中出现了部分分歧，我们为从业者提供了最佳实践，让他们尽早停止训练前景不佳的跑步。我们公开发布所有实验数据，包括2100次试验的训练和验证分数，以鼓励在微调过程中进一步分析训练动态。