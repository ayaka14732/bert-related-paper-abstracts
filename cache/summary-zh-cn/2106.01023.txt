预训练语言模型（PLM）在NLP中取得了巨大的成功。然而，它们庞大的模型尺寸阻碍了它们在许多实际系统中的应用。知识提取是一种流行的PLM压缩技术，它从大型教师PLM中学习一个小的学生模型。然而，从一位教师那里学到的知识可能是有限的，甚至是有偏见的，从而导致低质量的学生模式。在本文中，我们提出了一个多教师知识提取框架MT-BERT，用于预训练的语言模型压缩，它可以从多教师PLM中训练出高质量的学生模型。在MT-BERT中，我们设计了一种多教师协同微调方法，通过共享池和预测层对下游任务中的多教师PLM进行联合微调，以调整其输出空间，实现更好的协同教学。此外，我们还提出了一种多教师隐式损失和多教师蒸馏损失，将多教师PLM中隐藏状态和软标签的有用知识转移到学生模型中。在三个基准数据集上的实验验证了MT-BERT在PLM压缩中的有效性。