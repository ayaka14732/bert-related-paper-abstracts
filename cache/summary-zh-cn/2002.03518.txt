我们提出了评估和加强上下文嵌入对齐的程序，并表明它们在分析和改进多语言BERT方面是有用的。特别是，在我们提出的对齐程序之后，与基础模型相比，BERT在XNLI上表现出显著改善的零炮性能，显著匹配保加利亚语和希腊语的伪全监督翻译训练模型。此外，为了测量对齐程度，我们引入了一种上下文版本的单词检索，并表明它与下游零镜头转移有很好的相关性。使用这个词检索任务，我们还分析了BERT，发现它存在系统性缺陷，例如，使用不同脚本编写的开放类词类和词对对齐较差，这些缺陷通过对齐过程得到纠正。这些结果支持上下文对齐作为理解大型多语言预训练模型的有用概念。