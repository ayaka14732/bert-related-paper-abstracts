我们展示了Sparrow，一种信息寻求对话代理，与提示的语言模型基线相比，它被训练得更有用、更正确、更无害。我们使用来自人类反馈的强化学习来训练我们的模型，并添加了两个新功能，以帮助人类评估者判断代理人的行为。首先，为了使我们的代理人更加有益和无害，我们将良好对话的要求分解为代理人应该遵循的自然语言规则，并分别询问评分者每一条规则。我们证明，这种分解使我们能够收集更具针对性的人类对代理人行为的判断，并允许建立更有效的规则条件奖励模型。第二，我们的代理人在收集对模型陈述的偏好判断时，从支持事实主张的来源提供证据。对于事实问题，Sparrow提供的证据支持78%的抽样答复。麻雀比基线更受欢迎，同时更能抵抗人类的对抗性探测，在探测时只有8%的时间违反了我们的规则。最后，我们进行了广泛的分析，表明尽管我们的模型学会了遵循我们的规则，但它可能表现出分布偏差。