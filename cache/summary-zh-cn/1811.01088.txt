具有语言建模和相关无监督任务的预训练句子编码器最近被证明对于语言理解任务非常有效。通过补充语言模型风格的预训练和对数据丰富的监督任务（如自然语言推理）的进一步训练，我们在GLUE基准上获得了额外的性能改进。通过对BERT进行补充培训（Devlin等人，2018年），我们获得了81.8分——最新水平（截至2019年2月24日），比BERT提高了1.4分。我们还观察到，在这种设置下，随机重启的方差减小。当应用于ELMo（Peters等人，2018a）和Radford等人（2018）的模型时，我们的方法产生了类似的改进。此外，补充训练的好处在数据受限的情况下尤其明显，正如我们在人工限制训练数据的实验中所显示的那样。