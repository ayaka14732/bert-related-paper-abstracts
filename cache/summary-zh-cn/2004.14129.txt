随着网络规模的不断扩大，语言理解任务的最新表现已经实现；目前的记录保持者拥有数十亿个参数。给定在大规模未标记文本语料库上预先训练的语言模型，学习任务只需要非常轻的监督微调：微调步骤的数量通常比总参数计数低五个数量级。这是否意味着微调只会在参数空间中引入与预训练模型的微小差异？如果是这样的话，可以避免为每个任务存储和计算整个模型吗？在这项工作中，我们以变压器的双向编码器表示（BERT）为例来解决这些问题。正如预期的那样，我们发现微调后的模型在参数空间上与预先训练的模型非常接近，其接近程度因层而异。我们表明，仅微调最关键的层就足够了。此外，我们发现，在预训练模型的稀疏版本集中，有许多令人惊讶的好解。因此，只需将预先训练参数的特定层中的一定数量的条目设置为零，就可以实现大型语言模型的微调，从而节省特定任务的参数存储和计算成本。