使语言模型更大并不意味着它们能够更好地遵循用户的意图。例如，大型语言模型可以生成不真实、有毒或对用户毫无帮助的输出。换句话说，这些模型与其用户不一致。在这篇文章中，我们展示了一种通过微调人类反馈来调整语言模型和用户在广泛任务中的意图的方法。从一组贴标机编写的提示和通过OpenAI API提交的提示开始，我们收集了所需模型行为的贴标机演示数据集，我们使用该数据集使用监督学习来微调GPT-3。然后，我们收集了一个模型输出排名的数据集，我们使用该数据集使用来自人类反馈的强化学习来进一步微调这个受监督的模型。我们将生成的模型称为InstructGPT。在对我们的即时分布的人类评估中，1.3B参数InstructGPT模型的输出优于175B GPT-3的输出，尽管其参数少了100倍。此外，InstructionGPT模型显示了真实性的提高和有毒输出生成的减少，同时在公共NLP数据集上具有最小的性能回归。尽管InstructGPT仍然会犯一些简单的错误，但我们的结果表明，对人类反馈进行微调是使语言模型与人类意图保持一致的一个有希望的方向。