多语言预训练语言模型（如多语言BERT）在跨语言迁移方面取得了令人印象深刻的成果。然而，由于模型容量不变，多语种预培训通常落后于单语竞争对手。在这项工作中，我们提出了两种改进零镜头跨语言分类的方法，将知识从单语预训练模型转移到多语模型。在两个跨语言分类基准上的实验结果表明，我们的方法优于香草多语言微调。