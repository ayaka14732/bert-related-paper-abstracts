我们比较了显式量化音频数据或不量化学习表示的自监督表示学习算法。我们发现前者更准确，因为它通过vq-wav2vec[1]建立了良好的数据词汇表，以便在随后的训练中学习有效的表示。与以前的工作不同，我们直接使用连接主义时间分类（CTC）损失对转录语音上预先训练的BERT模型进行微调，而不是将表示输入到特定于任务的模型中。我们还提出了一种直接从连续音频数据学习的伯特式模型，并将原始音频的预训练与频谱特征进行了比较。使用vq-wav2vec词汇表在10小时的标记Librispeech数据上微调BERT模型几乎与在testclean上对100小时标记数据进行训练的最著名报告系统一样好，同时在其他测试上实现25%的功率降低。当仅使用10分钟的标记数据时，其他测试的WER为25.2，清洁测试的WER为16.3。这表明，自我监督可以使语音识别系统在转录数据量接近零的情况下进行训练。