基于知识的视觉问答（VQA）涉及回答需要图像中不存在的外部知识的问题。现有的方法首先从外部资源中检索知识，然后对所选知识、输入图像和问题进行推理以预测答案。然而，这种两步方法可能会导致不匹配，从而可能限制VQA性能。例如，检索到的知识可能有噪声，与问题无关，推理过程中重新嵌入的知识特征可能会偏离其在知识库（KB）中的原始含义。为了应对这一挑战，我们提出了基于知识的VQA的PICa，这是一种简单而有效的方法，通过使用图像字幕提示GPT3。受GPT-3在知识检索和问答方面的强大功能的启发，我们不再像以前那样使用结构化知识库，而是将GPT-3视为一个隐式的非结构化知识库，可以共同获取和处理相关知识。具体地说，我们首先将图像转换为GPT-3可以理解的标题（或标签），然后调整GPT-3，通过提供一些上下文中的VQA示例，以几种简单的方式解决VQA任务。我们通过仔细研究来进一步提高性能：（i）哪些文本格式最能描述图像内容，以及（ii）如何更好地选择和使用上下文中的示例。PICa首次将GPT-3用于多模式任务。通过仅使用16个示例，PICa在OK-VQA数据集上的绝对+8.6分超过了受监督的最新水平。我们还在VQAv2上对PICa进行了基准测试，在VQAv2上，PICa还显示了良好的几次拍摄性能。