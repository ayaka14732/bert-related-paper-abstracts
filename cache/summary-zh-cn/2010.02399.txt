在本文中，我们提出了一种简单有效的技术，允许有效的自监督学习与双向变压器。最近的研究表明，训练模型中的自我注意模式包含了大多数非语言规律，这是我们研究的动机。我们提出了一个计算效率很高的辅助损失函数来引导注意头符合这种模式。我们的方法与实际的预训练目标无关，与基线相比，模型收敛速度更快，下游任务性能更好，在低资源环境下实现最先进的结果。令人惊讶的是，我们还发现注意头的语言特性并不一定与语言建模性能相关。