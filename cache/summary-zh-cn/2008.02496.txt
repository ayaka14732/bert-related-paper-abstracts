像BERT及其变体这样的预先训练的语言模型最近在各种自然语言理解任务中取得了令人印象深刻的性能。然而，BERT严重依赖于全局自我注意块，因此承受着巨大的内存占用和计算成本。虽然它的所有注意头都从全局的角度查询整个输入序列来生成注意图，但是我们观察到一些注意头只需要学习局部依赖，这意味着存在计算冗余。因此，我们提出了一种新的基于广度的动态卷积来代替这些自我注意头来直接建模局部依赖性。新的卷积头与其他自我注意头一起形成了一种新的混合注意块，在全局和局部上下文学习中都更有效。我们用这种混合注意设计装备了伯特，并建立了一个康文伯特模型。实验表明，在各种下游任务中，ConvBERT显著优于BERT及其变体，具有较低的训练成本和较少的模型参数。值得注意的是，ConvBERTbase模型的粘合分数达到了86.4分，比ELECTRAbase高0.7分，而使用的培训成本不到1/4。代码和预先培训的模型将发布。