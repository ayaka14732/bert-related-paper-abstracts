基于Transformer的模型在许多自然语言处理任务中取得了最先进的成果。自我注意体系结构允许transformer将序列中所有元素的信息组合到上下文感知表示中。但是，有关上下文的信息主要存储在相同的元素表示中。这可能会限制处理与整个序列相关的属性的难度。添加可训练内存以选择性地存储序列的局部和全局表示是改进变压器模型的一个有希望的方向。记忆增强神经网络（MANN）扩展了传统的神经结构，具有通用的表示记忆。MANN已经证明了学习简单算法（如复制或反向）的能力，并且可以通过反向传播在不同任务（从问答到语言建模）上成功地进行训练，其性能优于具有类似复杂性的RNN和LSTM。在这项工作中，我们提出并研究了转换器基线的一些扩展（1）通过添加内存令牌来存储非本地表示，（2）为全局信息创建内存瓶颈，（3）使用专用层控制内存更新。我们评估了这些记忆增强变压器，并证明记忆的存在与机器翻译和语言建模任务的模型性能正相关。使用内存标记对预先训练的蒙面语言模型进行扩充，对于GLUE基准测试中的任务，结果喜忧参半。记忆中注意力模式的可视化表明，它提高了模型处理全局上下文的能力。