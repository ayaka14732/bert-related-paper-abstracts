BERT采用蒙面语言建模（MLM）进行预训练，是最成功的预训练模型之一。由于BERT忽略了预测标记之间的依赖关系，XLNet引入了置换语言建模（PLM）进行预训练来解决这个问题。然而，XLNet没有充分利用句子的位置信息，因此在预训练和微调之间存在位置差异。在本文中，我们提出了一种新的预训练方法MPNet，它继承了BERT和XLNet的优点，避免了它们的局限性。MPNet通过置换语言建模（与BERT中的MLM相比）利用预测标记之间的依赖关系，并将辅助位置信息作为输入，使模型看到完整的句子，从而减少位置差异（与XLNet中的PLM相比）。我们在一个大型数据集（超过160GB的文本语料库）上预先训练MPNet，并对各种下行任务（胶水、团队等）进行微调。实验结果表明，在相同的模型设置下，MPNet的性能大大优于MLM和PLM，并在这些任务上取得了比以前最先进的预训练方法（如BERT、XLNet、RoBERTa）更好的结果。代码和预先培训的模型可从以下网址获得：https://github.com/microsoft/MPNet.