多语言上下文嵌入，如多语言BERT和XLM RoBERTa，已被证明对许多多语言任务有用。以前的工作是在形态学和句法任务中，使用零镜头迁移学习间接地探讨表征的跨语言性。相反，我们直接研究多语言上下文嵌入的语言中立性，并与词汇语义相关。我们的研究结果表明，上下文嵌入比对齐的静态单词类型嵌入更具语言中立性，并且总体上比对齐的静态单词类型嵌入更具信息性，而对齐的静态单词类型嵌入经过明确的语言中立性训练。默认情况下，上下文嵌入仍然只是适度的语言中立性，因此我们提出了两种简单的方法来实现更强的语言中立性：第一，通过无监督地集中每种语言的表示，第二，通过在小型并行数据上拟合显式投影。此外，我们还展示了如何在不使用平行数据的情况下，在语言识别方面达到最先进的准确性，并匹配统计方法在平行句子词对齐方面的性能。