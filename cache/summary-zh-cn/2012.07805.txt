发布在私有数据集上经过训练的大型（十亿个参数）语言模型已经变得很普遍。本文演示了在这种情况下，对手可以通过查询语言模型执行训练数据提取攻击来恢复单个训练示例。我们演示了我们对GPT-2的攻击，GPT-2是一种在公共互联网上训练的语言模型，能够从模型的训练数据中提取数百个逐字文本序列。这些提取的示例包括（公共）个人身份信息（姓名、电话号码和电子邮件地址）、IRC对话、代码和128位UUID。我们的攻击是可能的，即使上面的每个序列只包含在训练数据中的一个文档中。我们全面评估我们的提取攻击，以了解导致其成功的因素。令人担忧的是，我们发现大型模型比小型模型更容易受到攻击。最后，我们总结经验教训，并讨论培训大型语言模型的可能保障措施。