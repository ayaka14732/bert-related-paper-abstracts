意图分类和时隙填充是自然语言理解的两个基本任务。它们经常受到小规模的人工标注训练数据的影响，导致泛化能力差，特别是对于稀有词。最近，一种新的语言表示模型BERT（来自变形金刚的双向编码器表示）促进了大规模未标记语料库上深度双向表示的预训练，并在简单微调后为各种自然语言处理任务创建了最先进的模型。然而，还没有太多的努力探索自然语言理解的伯特。在这项工作中，我们提出了一个基于BERT的联合意图分类和时隙填充模型。实验结果表明，与基于注意的递归神经网络模型和时隙选通模型相比，我们提出的模型在几个公共基准数据集上的意图分类精度、时隙填充F1和句子级语义框架精度都有显著提高。