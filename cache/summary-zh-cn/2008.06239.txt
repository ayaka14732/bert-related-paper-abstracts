面向任务的对话系统使用四个相互连接的模块，即自然语言理解（NLU）、对话状态跟踪（DST）、对话策略（DP）和自然语言生成（NLG）。考虑到数据收集的高成本，研究挑战是以最少的样本量（即，几次拍摄）学习每个模块。解决这一问题的最常见和有效的技术是迁移学习，在迁移学习中，对文本或任务特定数据进行预训练的大型语言模型在少数样本上进行微调。这些方法需要微调步骤和每个任务的一组参数。不同的是，语言模型，如GPT-2（Radford et al.，2019）和GPT-3（Brown et al.，2020），通过用很少的例子启动模型，允许很少的镜头学习。在本文中，我们评估了NLU、DST、DP和NLG任务中语言模型的启动少数镜头能力。重要的是，我们强调了这种方法目前的局限性，并讨论了对未来工作的可能影响。