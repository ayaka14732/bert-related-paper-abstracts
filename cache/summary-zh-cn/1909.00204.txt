经过预训练的语言模型在各种自然语言理解（NLU）任务中取得了巨大的成功，因为它能够通过对大规模语料库的预训练捕获文本中的深层语境信息。在本技术报告中，我们介绍了我们在中文语料库上对名为NEZHA（中文理解的神经语境化表示）的语言模型进行预训练的实践，以及对中文NLU任务进行微调的实践。NEZHA的当前版本基于BERT，并有一系列经过验证的改进，包括作为有效位置编码方案的功能性相对位置编码、全词掩蔽策略、混合精度训练和训练模型的LAMB优化器。实验结果表明，NEZHA在完成命名实体识别（人民日报NER）、句子匹配（LCQMC）、中文情感分类（ChnSenti）和自然语言推理（XNLI）等具有代表性的中文任务时，达到了最先进的性能。