语言模型预训练（如BERT）显著提高了许多自然语言处理任务的性能。然而，预先训练的语言模型通常计算量很大，因此很难在资源受限的设备上有效地执行它们。为了加快推理速度，减少模型尺寸，同时保持准确性，我们首先提出了一种新的变压器蒸馏方法，该方法专门用于变压器模型的知识蒸馏（KD）。通过利用这种新的KD方法，可以有效地将大量编码在大型教师BERT中的知识转移到小型学生BERT中。然后，我们为TinyBERT引入了一个新的两阶段学习框架，该框架在训练前和任务特定学习阶段都执行变压器蒸馏。该框架确保TinyBERT能够捕获BERT中的一般领域以及特定于任务的知识。TinyBERT有4层，在经验上是有效的，在GLUE基准测试中，其性能达到其教师BERTBASE的96.8%以上，同时推理速度是其教师Bertbert的7.5倍和9.4倍。4层TinyBERT也明显优于4层最先进的BERT蒸馏基线，只有约28%的参数和约31%的推断时间。此外，TinyBERT具有6层，其性能与其基础相当。