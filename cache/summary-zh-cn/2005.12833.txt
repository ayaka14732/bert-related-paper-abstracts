来自电子健康记录（EHR）的基于深度学习（DL）的预测模型在许多临床任务中提供了令人印象深刻的性能。然而，大型训练群组通常需要达到高精度，这妨碍了在训练数据量有限的场景中采用基于DL的模型。最近，变换器的双向编码器表示（BERT）和相关模型在自然语言处理领域取得了巨大的成功。在一个非常大的训练语料库上对BERT进行预训练，生成情境化嵌入，可以提高在较小数据集上训练的模型的性能。我们提出了Med BERT，它采用BERT框架对来自28490650例EHR患者数据集的结构化诊断数据进行预训练上下文嵌入模型。对两项疾病预测任务进行了微调实验：（1）糖尿病患者心衰预测和（2）从两个临床数据库预测胰腺癌。Med BERT显著提高了预测精度，将接收机工作特性曲线（AUC）下的面积提高了2.02-7.12%。特别是，预先训练的Med BERT通过非常小的微调训练集（300-500个样本）显著提高了任务的性能，将AUC提高了20%以上，或相当于10倍大训练集的AUC。我们相信，Med BERT将有助于使用小型本地训练数据集进行疾病预测研究，减少数据收集费用，并加快人工智能辅助医疗的步伐。