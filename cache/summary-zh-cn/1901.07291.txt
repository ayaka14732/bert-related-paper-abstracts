最近的研究证明了生成性预训练对英语自然语言理解的有效性。在这项工作中，我们将这种方法扩展到多种语言，并展示了跨语言预训练的有效性。我们提出了两种学习跨语言语言模型（XLM）的方法：一种是仅依赖单语数据的无监督学习方法，另一种是利用平行数据的有监督学习方法，该方法具有新的跨语言模型目标。我们在跨语言分类、无监督和有监督机器翻译方面取得了最新的成果。在XNLI上，我们的方法以4.9%的绝对精度提升了最先进的水平。在无监督机器翻译方面，我们在WMT'16德语英语中获得了34.3 BLEU，比以前的技术水平提高了9 BLEU以上。在监督机器翻译方面，我们在WMT'16罗马尼亚语英语中获得了38.5 BLEU的最新水平，比以前的最佳方法高出4 BLEU以上。我们的代码和预训练模型将公开提供。