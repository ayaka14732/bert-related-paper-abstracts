口语理解（SLU）模型的训练经常面临数据匮乏的问题。在本文中，我们提出了一种使用预训练语言模型的数据增强方法，以提高生成话语的可变性和准确性。此外，我们研究并提出了解决SLU中两个以前被忽视的数据稀缺的半监督学习场景的方案：i）丰富的本体：给出了具有大量有效对话行为的本体信息；ii）丰富的话语：大量未标记的话语可用。实验结果表明，我们的方法可以生成综合训练数据，提高语言理解模型在各种场景下的性能。