我们研究了利用文本的句法结构来增强预训练模型（如BERT和RoBERTa）的问题。现有的方法要么在预训练阶段，要么在微调阶段利用文本的语法，因此这两个阶段之间存在差异。这样一个问题将导致需要有人工注释的语法信息，这将现有方法的应用限制在更广泛的场景中。为了解决这个问题，我们提出了一个模型，该模型在预训练和微调阶段都利用了文本的语法。我们的模型基于Transformer，它具有语法感知的注意层，考虑文本的依赖树。我们进一步介绍了一个新的预训练任务，即预测依赖树中标记之间的语法距离。我们在三个下游任务上评估了该模型，包括关系分类、实体类型和问题回答。结果表明，我们的模型在六个公共基准数据集上实现了最先进的性能。我们有两个主要发现。首先，我们证明了注入自动生成的文本语法可以改进预先训练好的模型。其次，与相邻标记之间的局部头部关系相比，标记之间的全局语法距离带来了更大的性能增益。