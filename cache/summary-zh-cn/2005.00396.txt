已经证明，多语言BERT（mBERT）可以产生高质量的多语言表示，并实现有效的零镜头传输。这是令人惊讶的，因为mBERT在训练期间不使用任何跨语言信号。虽然最近的文献研究了这一现象，但多语言现象的原因仍然有些模糊。我们的目标是确定BERT的体系结构特性以及BERT成为多语言所必需的语言特性。为了实现快速实验，我们提出了一种基于合成和自然数据混合训练的小伯特模型的有效设置。总的来说，我们确定了影响多语言性的四个架构元素和两个语言元素。基于我们的见解，我们使用多语言预训练设置进行了实验，该设置使用VecMap修改掩蔽策略，即无监督嵌入对齐。使用三种语言在XNLI上进行的实验表明，我们的发现从小型设置转移到大型设置。