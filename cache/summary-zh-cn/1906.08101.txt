来自Transformers的双向编码器表示（BERT）在各种NLP任务中显示出惊人的改进。最近，一个升级版的BERT发布了全词屏蔽（WWM），它缓解了在预训练BERT时屏蔽部分词条标记的缺点。在本技术报告中，我们采用了中文文本中的整词掩蔽，即掩蔽整词而不是掩蔽汉字，这可能会给蒙面语言模型（MLM）预训练任务带来另一个挑战。所提出的模型在不同的NLP任务上进行了验证，从句子到文档，包括机器阅读理解（CMRC 2018、DRCD、CJRC）、自然语言推理（XNLI）、情感分类（CHNSTICORP）、句子对匹配（LCQMC、BQ语料库）和文档分类（THUCNews）。在这些数据集上的实验结果表明，全词掩蔽可以带来另一个显著的增益。此外，我们还检验了中国预训练模型的有效性：BERT、ERNIE、BERT wwm、BERT wwm ext、RoBERTa wwm ext和RoBERTa wwm ext。我们发布了所有经过预培训的模型：\url{https://github.com/ymcui/Chinese-BERT-wwm