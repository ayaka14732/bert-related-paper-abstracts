在这篇文章中，我们提出了一种语言信息多任务伯特（LIMIT-BERT），用于通过多任务学习（MTL）跨多个语言任务学习语言表示。LIMIT-BERT包括五个关键的语言语法和语义任务：词性（POS）标记、成分和依存句法分析、广度和依存语义角色标记（SRL）。此外，LIMIT-BERT还采用了语言学掩蔽策略：句法和语义短语掩蔽，掩蔽了与句法/语义短语对应的所有标记。与最近的多任务深层神经网络（MT-DNN）（Liu等人，2019）不同，我们的LIMIT-BERT是一种半监督方法，它提供了大量的语言任务数据，与BERT学习语料库一样。因此，LIMIT-BERT不仅提高了语言任务的性能，而且还受益于正则化效应和语言信息，从而产生更通用的表示，以帮助适应新的任务和领域。LIMIT-BERT在Propbank基准上的广度和依赖语义分析以及Penn Treebank上的依赖和成分句法分析方面都获得了最新的或具有竞争力的结果。