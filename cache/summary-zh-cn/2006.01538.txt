诸如BERT之类的深层神经语言模型在许多自然语言处理任务中实现了实质性的最新进展。由于前期培训的工作量和计算成本，语言特定模型通常只针对少量高资源语言（如英语）引入。虽然涵盖大量语言的多语言模型是可用的，但最近的研究表明，单语培训可以产生更好的模型，我们对单语和多语言培训之间权衡的理解是不完整的。在本文中，我们介绍了一个简单、全自动的管道，用于从Wikipedia数据创建特定于语言的BERT模型，并介绍了42个新的此类模型，其中大多数用于迄今为止缺乏专用深层神经语言模型的语言。我们使用最先进的UDify解析器对通用依赖数据评估这些模型的优点，并将性能与使用多语言BERT模型的结果进行对比。我们发现，使用WikiBERT模型的UDify平均优于使用mBERT的解析器，特定于语言的模型在某些语言上显示出显著的性能改进，但在其他语言上的性能改进有限或有所下降。我们还介绍了初步结果，作为理解特定语言模型最有利的条件的第一步。本工作中介绍的所有方法和模型都可以从以下位置获得开放许可证：https://github.com/turkunlp/wikibert.