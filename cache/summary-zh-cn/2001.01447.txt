现有的最先进的神经实体链接模型采用基于注意的词袋上下文模型和从词嵌入中引导的预先训练的实体嵌入来评估主题级上下文兼容性。然而，提及的直接上下文中潜在的实体类型信息被忽略，这导致模型经常将提及链接到具有不正确类型的不正确实体。为了解决这个问题，我们提出了基于预训练的BERT将潜在的实体类型信息注入到实体嵌入中。此外，我们将基于伯特的实体相似性评分集成到最先进模型的局部上下文模型中，以更好地捕获潜在的实体类型信息。我们的模型显著优于标准基准上最先进的实体链接模型（AIDA CoNLL）。详细的实验分析表明，我们的模型修正了大多数由直接基线产生的类型错误。