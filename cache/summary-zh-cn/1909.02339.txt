无监督预训练模型已被证明有助于广泛的下游NLP应用。然而，这些模型保留了传统静态单词嵌入的一些局限性。特别是，它们只编码原始文本语料库中可用的分布知识，通过语言建模目标合并。在这项工作中，我们用外部词汇知识来补充这种分布知识，也就是说，我们将单词级语义相似性的离散知识集成到预训练中。为此，我们将标准的BERT模型推广到多任务学习环境中，将BERT的蒙面语言建模和下一句预测目标与二元词关系分类的辅助任务相结合。我们的实验表明，我们的“词汇知情”伯特（LIBERT），专门用于单词级语义相似性，在一些语言理解任务上比词汇盲的“香草”伯特产生更好的性能。具体地说，在胶粘基准的10个任务中，9个优于伯特，并且在剩下的一个任务中与伯特相媲美。此外，我们在词汇简化的3个基准上取得了一致的成果，在这项任务中，关于单词级语义相似性的知识是至关重要的。