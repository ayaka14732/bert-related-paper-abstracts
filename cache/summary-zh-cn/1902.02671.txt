多任务学习在相关任务之间共享信息，有时减少所需参数的数量。GLUE基准中多个自然语言理解任务的最新成果之前使用了单个大型任务的迁移：使用BERT进行无监督预训练，其中为每个任务微调了单独的BERT模型。我们探讨了多任务方法，这些方法共享一个带有少量额外任务特定参数的伯特模型。使用新的适应模块、PAL或“投射注意层”，我们在GLUE基准上匹配单独微调模型的性能，参数大约减少7倍，并在识别文本蕴涵数据集上获得最先进的结果。