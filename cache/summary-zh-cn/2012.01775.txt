预训练语言模型的最新进展显著改善了神经反应的产生。然而，现有的方法通常将对话上下文视为一个线性的标记序列，并通过标记级的自我注意来学习生成下一个单词。这种标记层面的编码阻碍了语篇层面连贯性的探索。本文介绍了DialogBERT，一种新的会话响应生成模型，它增强了以前基于PLM的对话模型。DialogBERT采用分层变压器架构。为了有效地获取话语间的语篇连贯性，我们提出了两个训练目标，包括掩蔽话语回归和类似于原始BERT训练的分布式话语顺序排序。在三个多回合会话数据集上的实验表明，我们的方法在定量评估方面明显优于基线，如BART和DialoGPT。人类的评估表明，DialogBERT产生的反应比具有显著裕度的基线更连贯、信息更丰富、更人性化。