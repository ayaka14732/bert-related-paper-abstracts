尽管预训练语言模型（PLM）的开发显著提高了各种中文自然语言处理（NLP）任务的性能，但这些中文PLM的词汇仍然是由基于汉字的Google Chinese Bert\cite{devlin2018bert}提供的。第二，蒙面语言模型预训练基于单一词汇，这限制了其下游任务的执行。在这项工作中，我们首先提出了一种新的方法，即emph{seg\\\ tok}，借助汉语分词（CWS）和子词标记形成汉语BERT的词汇。然后，我们提出了三个版本的多词汇预训练（MVP）来提高模型的表达能力。实验表明：（1）与基于字符的词汇相比，emph{seg\\\\\\\\\\\\\\\\\ tok}不仅提高了汉语PLMs在句子级任务上的表现，而且提高了效率；（b） MVP提高了PLM的下游性能，特别是它可以提高序列标记任务的性能。