BERT家族预先训练的语言模型定义了广泛NLP任务的艺术状态。然而，基于BERT的模型的性能主要取决于大量的参数，这阻碍了它们在资源有限的场景中的应用。面对这个问题，最近的研究一直试图将BERT压缩成一个小规模的模型。然而，大多数以前的工作主要集中在一种单一的压缩技术上，很少注意不同方法的组合。当使用集成技术对BERT进行压缩时，一个关键问题是如何设计整个压缩框架以获得最佳性能。针对这个问题，我们集成了三种压缩方法（权重剪枝、低秩因子分解和知识提取（KD）），并探索了一系列关于模型结构、KD策略、剪枝频率和学习速率调度的设计。我们发现，仔细选择设计对压缩模型的性能至关重要。基于实证结果，我们的最佳压缩模型，称为集成技术的改进BERT压缩（ROSITA），比BERT小7.5倍，同时在GLUE基准的五项任务上保持98.5%的性能，优于具有类似参数预算的先前BERT压缩方法。该守则可于https://github.com/llyx97/Rosita.