作为一种预训练的变压器模型，BERT（变压器的双向编码器表示）在多个NLP任务中取得了突破性的性能。另一方面，Boosting是一种流行的集成学习技术，它结合了许多基本分类器，在许多机器学习任务中表现出更好的泛化性能。一些研究表明，BERT的集成可以进一步提高应用性能。然而，目前的集成方法主要集中在装袋或堆叠上，在探索提升方面还没有太多的努力。在这项工作中，我们提出了一种新的Boosting-BERT模型，将多类Boosting集成到BERT中。我们提出的模型使用预训练的转换器作为基础分类器，选择更难的训练集进行微调，并在NLP任务中获得预训练语言知识和增强集成的好处。我们在GLUE数据集和3个流行的中文NLU基准上对所提出的模型进行了评估。实验结果表明，我们提出的模型在所有数据集上都显著优于BERT，并在许多NLP任务中证明了它的有效性。BoostingBERT用RoBERTa作为基分类器取代了BERT基，在几个NLP任务中获得了最新的结果。我们还使用“教师-学生”框架中的知识提取来减少BoostingBERT的计算开销和模型存储，同时保持其实际应用的性能。