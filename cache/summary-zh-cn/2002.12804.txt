我们建议使用一种新的训练过程，即伪掩蔽语言模型（PMLM），为自动编码和部分自回归语言建模任务预先训练一个统一的语言模型。给定一个带有屏蔽标记的输入文本，我们依靠传统的掩码通过自动编码来学习损坏的标记和上下文之间的相互关系，依靠伪掩码通过部分自回归建模来学习屏蔽范围之间的内部关系。通过精心设计的位置嵌入和自我注意掩码，可重用上下文编码以避免冗余计算。此外，用于自动编码的传统掩码提供全局掩码信息，因此在部分自回归语言建模中可以访问所有位置嵌入。此外，这两个任务分别将统一的语言模型预训练为双向编码器和序列到序列解码器。我们的实验表明，使用PMLM预先训练的统一语言模型在多个广泛使用的基准测试中，在广泛的自然语言理解和生成任务上取得了新的最新成果。