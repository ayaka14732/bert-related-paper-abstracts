语言模型预训练在各种下游任务中显示出良好的效果。在此背景下，我们引入了一个跨模态的预训练语言模型，称为语音文本伯特（ST-BERT），用于处理端到端口语理解（E2E SLU）任务。ST-BERT以音素后级和子词级文本为输入，通过我们提出的两个训练前任务：跨模态掩蔽语言建模（CM-MLM）和跨模态条件语言建模（CM-CLM）学习语境化跨模态对齐。在三个基准上的实验结果表明，我们的方法对于各种SLU数据集都是有效的，并且即使在1%的训练数据可用的情况下，性能也会显著下降。此外，我们的方法还通过使用特定于域的语音文本对数据进行域自适应预训练，进一步提高了SLU的性能。