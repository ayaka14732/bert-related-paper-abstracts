我们引入了句子级语言建模，这是一个新的预训练目标，用于以完全自我监督的方式学习语篇语言表征。最近NLP中的预训练方法侧重于学习底层或顶层语言表示：一个极端是从语言模型目标派生的上下文化单词表示，另一个极端是通过对两个给定文本段的顺序分类学习的全序列表示。然而，这些模型并没有被直接鼓励去捕捉自然语言中存在的中等大小结构的表示，例如句子以及它们之间的关系。为此，我们提出了一种新的方法，通过洗牌输入句子序列和训练层次变换模型来重建原始顺序，从而鼓励学习上下文化的句子级表示。通过对GLUE、SQuAD和DiscoEval等下游任务的实验，我们表明我们模型的这一特性大大提高了原始BERT的性能。