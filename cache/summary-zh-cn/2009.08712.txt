大规模预训练语言模型已成为自然语言处理中的普遍现象。然而，这些模型中的大多数要么以高资源语言（特别是英语）提供，要么以多语言模型的形式提供，这会降低单个语言的覆盖率。本文介绍了罗马尼亚语BERT，这是第一个在大型文本语料库上预训练的纯罗马尼亚语基于变压器的语言模型。我们讨论了语料库的组成和清理，模型的训练过程，以及在各种罗马尼亚数据集上对模型的广泛评估。我们不仅开放了模型本身的源代码，而且还开放了一个存储库，其中包含有关如何获取语料库、如何微调并在生产中使用该模型（带有实际示例）以及如何完全复制评估过程的信息。