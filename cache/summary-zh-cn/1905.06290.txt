Winograd Schema Challenge（WSC）数据集WSC273及其推理对应物WNLI是自然语言理解和常识推理的常用基准。在本文中，我们证明了当对类似的代词消歧问题数据集（表示为WSCR）进行微调时，WSC273上的三种语言模型的性能有了很大的提高。此外，我们还生成了一个类似WSC的大型无监督数据集。通过在引入的数据集和WSCR数据集上微调BERT语言模型，我们在WSC273和WNLI上实现了72.5%和74.7%的总体准确率，将以前的最先进解决方案分别提高了8.8%和9.6%。此外，我们的微调模型在Trichelair等人（2018）提出的WSC273的“复杂”子集上也始终更加稳健。