Transformer通过组合各种高级模型（如BERT和GPT），在NLP领域取得了巨大成功。然而，Transformer及其现有变体在捕获令牌距离方面可能不是最优的，因为这些方法使用的位置或距离嵌入通常无法保持真实距离的精确信息，这可能不利于建模上下文的顺序和关系。在本文中，我们提出了DA Transformer，这是一种能够利用真实距离的距离感知转换器。我们建议结合令牌之间的实际距离来重新缩放原始自我注意权重，该权重由注意查询和密钥之间的相关性计算。具体地说，在不同的自我注意头部中，每对标记之间的相对距离由不同的可学习参数加权，这些参数控制这些头部对长期或短期信息的不同偏好。由于原始加权真实距离可能不是调整自我注意权重的最佳方法，因此我们提出了一个可学习的sigmoid函数，将其映射到具有适当范围的重新缩放系数中。我们首先通过ReLU函数剪裁原始的自我注意权重，以保持非负性并引入稀疏性，然后将其与重新缩放的系数相乘，从而将真实距离信息编码到自我注意中。在五个基准数据集上的大量实验表明，DA Transformer可以有效地提高许多任务的性能，并且优于vanilla Transformer及其几个变体。