我们研究了在大规模多语言语料库（multilingual BERT）上训练的现成深层双向句子表示是否能够开发无监督的通用依赖解析器。这种方法只利用多种语言中的单语语料库，并且不需要任何翻译数据，因此适用于低资源语言。在我们的实验中，我们在使用单一系统的情况下，在共享任务的六种真正低资源语言的所有方面都优于最佳CoNLL 2018语言特定系统。然而，我们还发现：（i）当改变训练语言时，解析精度仍然存在显著差异；（ii）在某些目标语言中，零炮传输在所有测试条件下都失败，这引起了对整个方法“普遍性”的关注。