Transformer对于文本建模非常重要。然而，由于输入文本长度的二次复杂性，它在处理长文档时有困难。为了解决这个问题，我们提出了一种分层交互转换器（Hi-Transformer），用于高效的长文档建模。Hi Transformer以分层方式对文档建模，即首先学习句子表示，然后学习文档表示。它可以有效地降低复杂度，同时在每个句子的建模中捕获全局文档上下文。更具体地说，我们首先使用一个句子转换器来学习每个句子的表示。然后，我们使用文档转换器从这些句子表示中建模全局文档上下文。接下来，我们使用另一个句子转换器来增强使用全局文档上下文的句子建模。最后，我们使用分层池方法来实现文档嵌入。在三个基准数据集上的大量实验验证了Hi Transformer在长文档建模中的效率和有效性。