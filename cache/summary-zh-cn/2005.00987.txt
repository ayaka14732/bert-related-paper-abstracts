本文研究如何有效地将预先训练好的蒙面语言模型（MLM），如BERT，合并到用于语法错误纠正（GEC）的编码器-解码器（EncDec）模型中。这个问题的答案并不像人们想象的那么简单，因为以前将传销合并到EncDec模型中的常用方法在应用于GEC时有潜在的缺点。例如，GEC模型的输入分布可能与用于训练前传销的语料库的分布有很大不同（错误、笨拙等）；然而，在以前的方法中没有解决这个问题。我们的实验表明，我们提出的方法，即首先使用给定的GEC语料库对MLM进行微调，然后使用微调后的MLM输出作为GEC模型中的附加特征，最大限度地提高了MLM的效益。性能最佳的车型在BEA-2019和CoNLL-2014基准上实现了最先进的性能。我们的代码可在以下网站公开获取：https://github.com/kanekomasahiro/bert-gec.