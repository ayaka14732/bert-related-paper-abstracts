预训练语言模型（PrLMs）由于其从自我监督的预训练中学习通用语言表示的强大能力而表现出优异的性能。然而，即使在强大的PrLMs的帮助下，有效地从对话文本中获取任务相关知识仍然是一个挑战，因为对话文本通过说话人感知话语之间的相关性而丰富。在这项工作中，我们提出蜘蛛，结构预先训练的对话读者，捕捉对话独有的功能。为了模拟类似对话的特征，除了最初的LM目标外，我们还提出了两个训练目标：1）话语顺序恢复，它预测对话语境中置换话语的顺序；2） 句子主干正则化，对模型进行正则化，以提高总结的主谓宾三元组的事实正确性。在广泛使用的对话基准上的实验结果验证了新引入的自我监督任务的有效性。