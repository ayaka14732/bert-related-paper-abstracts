训练前的文本表示最近被证明可以显著提高许多自然语言处理任务的最新水平。预培训的中心目标是学习对后续任务有用的文本表示。然而，现有的方法是通过最小化代理目标来优化的，例如语言建模的负对数可能性。在这项工作中，我们引入了一种学习算法，它直接优化了模型学习文本表示的能力，从而有效地学习下游任务。我们通过一系列元训练步骤证明了多任务预训练和模型不可知元学习之间存在内在联系。BERT中采用的标准多任务学习目标是我们的学习算法的一个特例，其中元训练的深度为零。我们在两种环境下研究了该问题：无监督预训练和具有不同预训练对象的监督预训练，以验证我们方法的通用性。实验结果表明，我们的算法对各种下游任务都有改进，并能更好地学习初始化。