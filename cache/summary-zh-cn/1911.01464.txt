我们研究了多语言掩蔽语言建模的问题，即对来自多种语言的连接文本的单个模型的训练，并详细研究了影响这些模型对跨语言迁移如此有效的几个因素。我们发现，与之前的假设相反，即使单语语料库中没有共享词汇，而且文本来自非常不同的领域，迁移也是可能的。唯一的要求是在多语言编码器的顶层有一些共享参数。为了更好地理解这一结果，我们还表明，来自不同语言的独立训练模型的表示可以非常有效地在事后对齐，这有力地表明，与非上下文单词嵌入一样，在学习的嵌入空间中存在普遍的潜在对称性。对于多语言蒙面语言建模，这些对称性似乎会在联合训练过程中自动发现并对齐。