想象你在超市里。你篮子里有两个香蕉，想买四个苹果。你们一共有多少水果？这个看似简单的问题对于数据驱动的语言模型来说可能是一个挑战，即使是大规模培训。然而，我们期望这种通用语言模型除了具有典型的语言能力外，还具有一些数学能力。为了实现这一目标，我们调查了一个常用的语言模型，伯特，是否具有这样的数学能力，如果是，到什么程度。为此，我们在一个流行的数学问题数据集AQuA-RAT上对BERT进行了微调，并进行了一些测试，以更好地理解所学的表示法。由于我们教授用自然语言训练的模型做形式数学，我们假设这种模型将受益于解释数学结果如何推导的半形式步骤的训练。为了更好地适应这种训练，我们还提出了学习数学规则的新借口任务。我们称之为（邻居）推理顺序预测（ROP或NROP）。有了这个新模型，我们取得了比数据驱动基线更好的结果，甚至可以与更定制的模型媲美。我们还展示了如何减少这种模型中的位置偏差。