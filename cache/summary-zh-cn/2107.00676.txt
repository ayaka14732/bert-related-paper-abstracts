多语言语言模型（MLLMs），如mBERT、XLM、XLM-R、\textit{等）已经成为一种可行的选择，可以将预训练的力量引入到大量语言中。鉴于他们在零射击转移学习方面取得的成功，在以下方面出现了大量工作：（i）构建涵盖大量语言的更大的MLLMs；（ii）创建涵盖更广泛任务和语言的详尽基准，以评估MLLMs（iii）分析单语MLLMs的性能，零射击跨语言和双语任务（iv）理解MLLMs学习的通用语言模式（如果有）和（v）增强MLLMs（通常）有限的能力，以提高其在可见或甚至不可见语言上的表现。在这项调查中，我们回顾了现有文献，涵盖了与MLLMs相关的上述广泛研究领域。根据我们的调查，我们提出了一些未来研究的方向。