许多实际应用需要预测长序列时间序列，例如电力消耗规划。长序列时间序列预测（LSTF）对模型的预测能力提出了更高的要求，即能够有效地捕捉输出和输入之间精确的长期依赖耦合。最近的研究表明，变压器具有提高预测能力的潜力。然而，Transformer存在一些严重问题，使其无法直接应用于LSTF，包括二次时间复杂度、高内存使用率以及编码器-解码器体系结构的固有限制。为了解决这些问题，我们为LSTF设计了一个高效的基于转换器的模型，名为Informer，它具有三个显著的特点：（i）一个$ProbSparse$自我注意机制，它在时间复杂度和内存使用方面达到$O（L\log L）$，并且在序列依赖性对齐方面具有相当的性能。（ii）自注意提取通过将级联层输入减半来突出控制注意，并有效地处理超长输入序列。（iii）生成式解码器虽然概念上简单，但在一次正向操作而不是一步一步地预测长时间序列，这大大提高了长序列预测的推理速度。在四个大规模数据集上的大量实验表明，Informer的性能明显优于现有的方法，并为LSTF问题提供了一种新的解决方案。