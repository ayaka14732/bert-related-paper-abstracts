神经机器翻译模型往往偏向于在训练过程中看到的有限翻译参考文献。为了修正这种形式的过度拟合，在本文中，我们基于最近提出的BERTScore评估指标，提出了一种新的训练目标来微调模型。BERTScore是一个基于上下文嵌入的评分函数，它克服了基于n-gram的度量标准（例如同义词、释义）的典型限制，允许将与参考不同但在上下文嵌入空间中相近的翻译视为基本正确。为了能够使用BERTScore作为训练目标，我们提出了三种生成软预测的方法，允许网络保持端到端完全可微。在四对不同语言对上进行的实验表明，当微调强基线时，BLEU分数提高了0.58 pp（3.28%），BERTScore分数提高了0.76 pp（0.98%）。