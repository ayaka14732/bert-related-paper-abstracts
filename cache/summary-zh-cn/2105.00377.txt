像BERT这样的大规模预训练模型在各种自然语言处理（NLP）任务中取得了巨大成功，但如何使它们适应与数学相关的任务仍然是一个挑战。现有的预训练模型忽略了公式及其上下文之间的结构特征和语义对应关系。为了解决这些问题，我们提出了一种新的预训练模型，即\textbf{MathBERT}，它与数学公式及其相应的上下文联合训练。此外，为了进一步捕捉公式的语义层次结构特征，设计了一个新的预训练任务来预测从公式的语义结构表示算子树（OPT）中提取的蒙面公式子结构。我们在三个下游任务上进行了各种实验，以评估MathBERT的性能，包括数学信息检索、公式主题分类和公式标题生成。实验结果表明，MathBERT在所有这三个任务上都显著优于现有方法。此外，我们定性地证明了该预训练模型有效地捕获了公式的语义层次结构信息。据我们所知，MathBERT是第一个经过预训练的数学公式理解模型。