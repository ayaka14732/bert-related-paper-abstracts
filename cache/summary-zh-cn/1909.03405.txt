句子对的语义推理能力对于许多自然语言理解任务至关重要，例如自然语言推理和机器阅读理解。最近这些任务的一个显著改进来自于BERT。据报道，BERT中的下一句预测（NSP）学习两个句子之间的上下文关系，对于句子对输入的下游问题具有重要意义。尽管NSP有效，但我们认为NSP仍然缺乏区分隐含关联和浅层关联的基本信号。为了弥补这一缺陷，我们建议将NSP任务扩展为三类分类任务，其中包括前一句预测（PSP）类别。PSP的参与促使模型关注信息语义来确定句子顺序，从而提高语义理解能力。这种简单的修改产生了对香草的显著改善。为了进一步纳入文档级信息，NSP和PSP的范围被扩展到更广的范围，即NSP和PSP还包括紧密但不连续的句子，其噪声通过标签平滑技术得到缓解。定性和定量实验结果都证明了该方法的有效性。我们的方法不断提高NLI和MRC基准的性能，包括具有挑战性的HANS数据集，这表明文档级任务仍然有希望用于预培训。