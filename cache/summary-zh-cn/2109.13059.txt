在NLP中，大量任务涉及两个序列之间的成对比较（例如句子相似性和释义识别）。主要有两种公式用于句子对任务：双编码器和交叉编码器。双编码器产生固定维的句子表示，并且计算效率高，但是，它们通常不如交叉编码器。交叉编码器可以利用它们的注意力头来利用句子间的交互来获得更好的性能，但它们需要任务微调，并且计算成本更高。在本文中，我们提出了一个称为Trans-Encoder的完全无监督的句子表示模型，该模型将两种学习范式结合到一个迭代联合框架中，以同时学习增强的双编码器和交叉编码器。具体地说，在预训练语言模型（PLM）的基础上，我们首先将其转换为无监督的bi编码器，然后在bi编码器和交叉编码器任务公式之间交替。在每次交替中，一个任务公式将产生伪标签，作为另一个任务公式的学习信号。然后，我们提出了一种扩展，在多个并行PLM上执行这种自蒸馏方法，并使用它们的伪标签的平均值进行相互蒸馏。据我们所知，Trans-Encoder创建了第一个完全无监督的交叉编码器，也是一个用于句子相似性的最先进的无监督双编码器。Trans-encoder的双编码器和交叉编码器公式在句子相似性基准上都比最近提出的最先进的无监督句子编码器（如Mirror BERT和SimCSE）高出5%。