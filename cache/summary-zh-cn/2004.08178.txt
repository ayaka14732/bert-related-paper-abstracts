自我注意机制在各种顺序学习任务中取得了惊人的最新进展（SOTA），通过关注不同位置的所有全球环境，站在多头点积注意上。通过一条伪信息高速公路，我们引入了一个门控组件自依单元（SDU），它结合了LSTM样式的门控单元，以补充单个表示的多维潜在空间中的内部语义重要性。辅助的基于内容的SDU门允许通过跳过连接的调制潜在嵌入的信息流，从而通过梯度下降算法获得明显的收敛速度。我们可以揭示门机制在基于上下文的变压器模块中的作用，假设SDU门，特别是浅层SDU门，可以在优化过程中更快地向次优点移动。