句子嵌入是许多自然语言处理（NLP）系统的重要组成部分。与单词嵌入一样，句子嵌入通常在大型文本语料库上学习，然后转移到各种下游任务，如聚类和检索。与单词嵌入不同，学习句子嵌入的最高性能解决方案需要标记数据，从而将其用途限制在标记数据丰富的语言和领域。在本文中，我们提出了DeCLUTR：无监督文本表征的深层对比学习。受深度度量学习（DML）最新进展的启发，我们精心设计了一个用于学习通用句子嵌入的自监督目标，该目标不需要标记的训练数据。当用于扩展基于转换器的语言模型的预训练时，我们的方法缩小了通用句子编码器的无监督和有监督预训练之间的性能差距。重要的是，我们的实验表明，学习嵌入的质量与可训练参数的数量和未标记训练数据的数量成正比。我们的代码和预训练模型是公开的，可以很容易地适应新的领域或用于嵌入看不见的文本。